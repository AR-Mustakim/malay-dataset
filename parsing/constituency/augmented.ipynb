{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/huseinzol05/Malay-Dataset/master/parsing/constituency/indon.txt\n",
    "# !wget https://raw.githubusercontent.com/huseinzol05/Malay-Dataset/master/parsing/constituency/texts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya\n",
    "\n",
    "# electra = malaya.transformer.load(model = 'electra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/malaya/wordvector.py:114: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/malaya/wordvector.py:125: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_news, embedded_news = malaya.wordvector.load_news()\n",
    "word_vector_news = malaya.wordvector.load(embedded_news, vocab_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 1030)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('texts.txt') as fopen:\n",
    "    texts = fopen.read().split('\\n')\n",
    "\n",
    "with open('indon.txt') as fopen:\n",
    "    parsing = fopen.read().split('\\n')\n",
    "    \n",
    "len(texts), len(parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_parsing, test_parsing = train_test_split(texts, parsing, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_of(text):\n",
    "    return (\n",
    "        str.upper\n",
    "        if text.isupper()\n",
    "        else str.lower\n",
    "        if text.islower()\n",
    "        else str.title\n",
    "        if text.istitle()\n",
    "        else str\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove(string):\n",
    "    string = string.encode('ascii', errors='ignore').strip().decode()\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    return string\n",
    "    \n",
    "def augment(string, n = 7):\n",
    "    results = malaya.augmentation.synonym(string)\n",
    "#     try:\n",
    "#         results.extend(malaya.augmentation.transformer(string, electra))\n",
    "#     except:\n",
    "#         pass\n",
    "    try:\n",
    "        results.extend(malaya.augmentation.wordvector(\n",
    "            string, word_vector_news, soft = True\n",
    "        ))\n",
    "    except:\n",
    "        pass\n",
    "    results = list(set(results))\n",
    "    results = [remove(s) for s in results if s.lower() != string.lower()]\n",
    "    words = string.split()\n",
    "    results = [s.split() for s in results if len([w for w in s.split() if len(w) > 1]) == len(words)]\n",
    "    for i in range(len(results)):\n",
    "        for no, w in enumerate(words):\n",
    "            results[i][no] = case_of(w)(results[i][no])\n",
    "        results[i] = ' '.join(results[i])\n",
    "    return results[:n]\n",
    "\n",
    "def replace(string, actual_string, aug_string):\n",
    "    actual_string = actual_string.split()\n",
    "    aug_string = aug_string.split()\n",
    "    for no, word in enumerate(actual_string):\n",
    "        string = string.replace(word, aug_string[no])\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [01:08<00:00,  1.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results, out = [], []\n",
    "for i in tqdm(range(len(test_texts))):\n",
    "    try:\n",
    "        rows = augment(test_texts[i])\n",
    "        for row in rows:\n",
    "            results.append(replace(test_parsing[i], test_texts[i], row))\n",
    "            out.append(row)\n",
    "    except:\n",
    "        pass\n",
    "    results.append(test_parsing[i])\n",
    "    out.append(test_texts[i])\n",
    "    \n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test-augmented.txt', 'w') as fopen:\n",
    "    fopen.write('\\n'.join(results))\n",
    "    \n",
    "with open('test-augmented-texts.txt', 'w') as fopen:\n",
    "    fopen.write('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 927/927 [08:35<00:00,  1.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3989"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results, out = [], []\n",
    "for i in tqdm(range(len(train_texts))):\n",
    "    try:\n",
    "        rows = augment(train_texts[i])\n",
    "        for row in rows:\n",
    "            results.append(replace(train_parsing[i], train_texts[i], row))\n",
    "            out.append(row)\n",
    "    except:\n",
    "        pass\n",
    "    results.append(train_parsing[i])\n",
    "    out.append(train_texts[i])\n",
    "    \n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-augmented.txt', 'w') as fopen:\n",
    "    fopen.write('\\n'.join(results))\n",
    "    \n",
    "with open('train-augmented-texts.txt', 'w') as fopen:\n",
    "    fopen.write('\\n'.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
