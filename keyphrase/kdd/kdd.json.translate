[[{"string": "Applying data mining in investigating money laundering crimes In this paper , we study the problem of applying data mining to facilitate the investigation of money laundering crimes MLCs . We have identified a new paradigm of problems -- that of automatic community generation based on uni-party data , the data in which there is no direct or explicit link information available . Consequently , we have proposed a new methodology for Link Discovery based on Correlation Analysis LDCA . We have used MLC group model generation as an exemplary application of this problem paradigm , and have focused on this application to develop a specific method of automatic MLC group model generation based on timeline analysis using the LDCA methodology , called CORAL . A prototype of CORAL method has been implemented , and preliminary testing and evaluations based on a real MLC case data are reported . The contributions of this work are : 1 identification of the uni-party data community generation problem paradigm , 2 proposal of a new methodology LDCA to solve for problems in this paradigm , 3 formulation of the MLC group model generation problem as an example of this paradigm , 4 application of the LDCA methodology in developing a specific solution CORAL to the MLC group model generation problem , and 5 development , evaluation , and testing of the CORAL prototype in a real MLC case data .", "keywords": ["mlc group models", "timeline analysis", "community generation", "coral", "link discovery based on correlation analysis", "money laundering crimes", "uni-party data", "bi-party data", "histogram", "clustering"], "combined": "Applying data mining in investigating money laundering crimes In this paper , we study the problem of applying data mining to facilitate the investigation of money laundering crimes MLCs . We have identified a new paradigm of problems -- that of automatic community generation based on uni-party data , the data in which there is no direct or explicit link information available . Consequently , we have proposed a new methodology for Link Discovery based on Correlation Analysis LDCA . We have used MLC group model generation as an exemplary application of this problem paradigm , and have focused on this application to develop a specific method of automatic MLC group model generation based on timeline analysis using the LDCA methodology , called CORAL . A prototype of CORAL method has been implemented , and preliminary testing and evaluations based on a real MLC case data are reported . The contributions of this work are : 1 identification of the uni-party data community generation problem paradigm , 2 proposal of a new methodology LDCA to solve for problems in this paradigm , 3 formulation of the MLC group model generation problem as an example of this paradigm , 4 application of the LDCA methodology in developing a specific solution CORAL to the MLC group model generation problem , and 5 development , evaluation , and testing of the CORAL prototype in a real MLC case data . [[EENNDD]] mlc group models; timeline analysis; community generation; coral; link discovery based on correlation analysis; money laundering crimes; uni-party data; bi-party data; histogram; clustering"}, "Menerapkan perlombongan data dalam menyiasat jenayah pengubahan wang haram Dalam makalah ini, kami mengkaji masalah penerapan data mining untuk memudahkan penyiasatan MLC jenayah pengubahan wang haram. Kami telah mengenal pasti paradigma masalah baru - penjanaan komuniti automatik berdasarkan data uni-party, data di mana tidak ada maklumat pautan langsung atau eksplisit yang tersedia. Oleh itu, kami telah mencadangkan metodologi baru untuk Link Discovery berdasarkan Correlation Analysis LDCA. Kami telah menggunakan generasi model kumpulan MLC sebagai aplikasi teladan dari paradigma masalah ini, dan telah memfokuskan pada aplikasi ini untuk mengembangkan metode khusus generasi model kelompok MLC automatik berdasarkan analisis garis masa menggunakan metodologi LDCA, yang disebut CORAL. Prototaip kaedah CORAL telah dilaksanakan, dan ujian dan penilaian awal berdasarkan data kes MLC yang sebenarnya dilaporkan. Sumbangan karya ini adalah: 1 pengenalpastian paradigma masalah penjanaan komuniti data uni-party, 2 cadangan metodologi baru LDCA untuk menyelesaikan masalah dalam paradigma ini, 3 rumusan masalah penjanaan model kumpulan MLC sebagai contoh paradigma ini , 4 penerapan metodologi LDCA dalam mengembangkan solusi khusus CORAL untuk masalah generasi model kumpulan MLC, dan 5 pengembangan, penilaian, dan pengujian prototaip CORAL dalam data kes MLC yang sebenarnya. [[EENNDD]] model kumpulan mlc; analisis garis masa; generasi masyarakat; batu karang; penemuan pautan berdasarkan analisis korelasi; jenayah pengubahan wang haram; data uni-party; data dua pihak; histogram; pengelompokan"], [{"string": "An energy-efficient mobile recommender system The increasing availability of large-scale location traces creates unprecedent opportunities to change the paradigm for knowledge discovery in transportation systems . A particularly promising area is to extract energy-efficient transportation patterns green knowledge , which can be used as guidance for reducing inefficiencies in energy consumption of transportation sectors . However , extracting green knowledge from location traces is not a trivial task . Conventional data analysis tools are usually not customized for handling the massive quantity , complex , dynamic , and distributed nature of location traces . To that end , in this paper , we provide a focused study of extracting energy-efficient transportation patterns from location traces . Specifically , we have the initial focus on a sequence of mobile recommendations . As a case study , we develop a mobile recommender system which has the ability in recommending a sequence of pick-up points for taxi drivers or a sequence of potential parking positions . The goal of this mobile recommendation system is to maximize the probability of business success . Along this line , we provide a Potential Travel Distance PTD function for evaluating each candidate sequence . This PTD function possesses a monotone property which can be used to effectively prune the search space . Based on this PTD function , we develop two algorithms , LCP and SkyRoute , for finding the recommended routes . Finally , experimental results show that the proposed system can provide effective mobile sequential recommendation and the knowledge extracted from location traces can be used for coaching drivers and leading to the efficient use of energy .", "keywords": ["mobile recommender system", "trajectory data analysis"], "combined": "An energy-efficient mobile recommender system The increasing availability of large-scale location traces creates unprecedent opportunities to change the paradigm for knowledge discovery in transportation systems . A particularly promising area is to extract energy-efficient transportation patterns green knowledge , which can be used as guidance for reducing inefficiencies in energy consumption of transportation sectors . However , extracting green knowledge from location traces is not a trivial task . Conventional data analysis tools are usually not customized for handling the massive quantity , complex , dynamic , and distributed nature of location traces . To that end , in this paper , we provide a focused study of extracting energy-efficient transportation patterns from location traces . Specifically , we have the initial focus on a sequence of mobile recommendations . As a case study , we develop a mobile recommender system which has the ability in recommending a sequence of pick-up points for taxi drivers or a sequence of potential parking positions . The goal of this mobile recommendation system is to maximize the probability of business success . Along this line , we provide a Potential Travel Distance PTD function for evaluating each candidate sequence . This PTD function possesses a monotone property which can be used to effectively prune the search space . Based on this PTD function , we develop two algorithms , LCP and SkyRoute , for finding the recommended routes . Finally , experimental results show that the proposed system can provide effective mobile sequential recommendation and the knowledge extracted from location traces can be used for coaching drivers and leading to the efficient use of energy . [[EENNDD]] mobile recommender system; trajectory data analysis"}, "Sistem pengesyorkan mudah alih yang cekap tenaga Peningkatan ketersediaan jejak lokasi berskala besar mewujudkan peluang yang belum pernah terjadi sebelumnya untuk mengubah paradigma untuk penemuan pengetahuan dalam sistem pengangkutan. Kawasan yang sangat menjanjikan adalah dengan mengekstrak pengetahuan hijau corak pengangkutan yang cekap tenaga, yang dapat digunakan sebagai panduan untuk mengurangkan ketidakcekapan dalam penggunaan tenaga sektor pengangkutan. Walau bagaimanapun, mengambil pengetahuan hijau dari jejak lokasi bukanlah tugas yang remeh. Alat analisis data konvensional biasanya tidak disesuaikan untuk menangani kuantiti besar, kompleks, dinamik, dan sifat jejak lokasi yang diedarkan. Untuk tujuan itu, dalam makalah ini, kami memberikan kajian fokus untuk mengekstrak corak pengangkutan yang cekap tenaga dari jejak lokasi. Secara khusus, kami mempunyai fokus awal pada urutan cadangan mudah alih. Sebagai kajian kes, kami mengembangkan sistem pengesyorkan bergerak yang memiliki kemampuan dalam mengesyorkan urutan tempat pengambilan untuk pemandu teksi atau urutan posisi tempat letak kereta yang berpotensi. Matlamat sistem cadangan mudah alih ini adalah untuk memaksimumkan kemungkinan kejayaan perniagaan. Sejalan dengan ini, kami menyediakan fungsi Potensi Jarak Perjalanan PTD untuk menilai setiap urutan calon. Fungsi PTD ini memiliki sifat monoton yang dapat digunakan untuk memangkas ruang carian dengan berkesan. Berdasarkan fungsi PTD ini, kami mengembangkan dua algoritma, LCP dan SkyRoute, untuk mencari rute yang disyorkan. Akhirnya, hasil eksperimen menunjukkan bahawa sistem yang dicadangkan dapat memberikan cadangan berurutan mudah alih yang berkesan dan pengetahuan yang diambil dari jejak lokasi dapat digunakan untuk melatih pemandu dan membawa kepada penggunaan tenaga yang cekap. [[EENNDD]] sistem cadangan mudah alih; analisis data lintasan"], [{"string": "Towards exploratory test instance specific algorithms for high dimensional classification In an interactive classification application , a user may find it more valuable to develop a diagnostic decision support method which can reveal significant classification behavior of exemplar records . Such an approach has the additional advantage of being able to optimize the decision process for the individual record in order to design more effective classification methods . In this paper , we propose the Subspace Decision Path method which provides the user with the ability to interactively explore a small number of nodes of a hierarchical decision process so that the most significant classification characteristics for a given test instance are revealed . In addition , the SD-Path method can provide enormous interpretability by constructing views of the data in which the different classes are clearly separated out . Even in cases where the classification behavior of the test instance is ambiguous , the SD-Path method provides a diagnostic understanding of the characteristics which result in this ambiguity . Therefore , this method combines the abilities of the human and the computer in creating an effective diagnostic tool for instance-centered high dimensional classification .", "keywords": ["visual data mining", "database applications", "classification"], "combined": "Towards exploratory test instance specific algorithms for high dimensional classification In an interactive classification application , a user may find it more valuable to develop a diagnostic decision support method which can reveal significant classification behavior of exemplar records . Such an approach has the additional advantage of being able to optimize the decision process for the individual record in order to design more effective classification methods . In this paper , we propose the Subspace Decision Path method which provides the user with the ability to interactively explore a small number of nodes of a hierarchical decision process so that the most significant classification characteristics for a given test instance are revealed . In addition , the SD-Path method can provide enormous interpretability by constructing views of the data in which the different classes are clearly separated out . Even in cases where the classification behavior of the test instance is ambiguous , the SD-Path method provides a diagnostic understanding of the characteristics which result in this ambiguity . Therefore , this method combines the abilities of the human and the computer in creating an effective diagnostic tool for instance-centered high dimensional classification . [[EENNDD]] visual data mining; database applications; classification"}, "Ke arah algoritma khusus contoh ujian eksplorasi untuk klasifikasi dimensi tinggi Dalam aplikasi klasifikasi interaktif, pengguna mungkin lebih berguna untuk mengembangkan kaedah sokongan keputusan diagnostik yang dapat mendedahkan tingkah laku klasifikasi yang signifikan dari catatan contoh. Pendekatan sedemikian mempunyai kelebihan tambahan kerana dapat mengoptimumkan proses keputusan untuk catatan individu untuk merancang kaedah klasifikasi yang lebih berkesan. Dalam makalah ini, kami mencadangkan kaedah Subspace Decision Path yang memberi pengguna kemampuan untuk menjelajah secara interaktif sebilangan kecil node proses keputusan hierarki sehingga ciri klasifikasi yang paling signifikan untuk contoh ujian tertentu dinyatakan. Di samping itu, kaedah SD-Path dapat memberikan penafsiran yang sangat besar dengan membina pandangan data di mana kelas yang berbeza dipisahkan dengan jelas. Walaupun dalam keadaan di mana tingkah laku klasifikasi contoh ujian tidak jelas, kaedah SD-Path memberikan pemahaman diagnostik mengenai ciri-ciri yang mengakibatkan kesamaran ini. Oleh itu, kaedah ini menggabungkan kemampuan manusia dan komputer dalam membuat alat diagnostik yang berkesan untuk klasifikasi dimensi tinggi yang berpusat pada contohnya. [[EENNDD]] perlombongan data visual; aplikasi pangkalan data; pengelasan"], [{"string": "Entity categorization over large document collections Extracting entities such as people , movies from documents and identifying the categories such as painter , writer they belong to enable structured querying and data analysis over unstructured document collections . In this paper , we focus on the problem of categorizing extracted entities . Most prior approaches developed for this task only analyzed the local document context within which entities occur . In this paper , we significantly improve the accuracy of entity categorization by i considering an entity 's context across multiple documents containing it , and ii exploiting existing large lists of related entities e.g. , lists of actors , directors , books . These approaches introduce computational challenges because a the context of entities has to be aggregated across several documents and b the lists of related entities may be very large . We develop techniques to address these challenges . We present a thorough experimental study on real data sets that demonstrates the increase in accuracy and the scalability of our approaches .", "keywords": ["information extraction"], "combined": "Entity categorization over large document collections Extracting entities such as people , movies from documents and identifying the categories such as painter , writer they belong to enable structured querying and data analysis over unstructured document collections . In this paper , we focus on the problem of categorizing extracted entities . Most prior approaches developed for this task only analyzed the local document context within which entities occur . In this paper , we significantly improve the accuracy of entity categorization by i considering an entity 's context across multiple documents containing it , and ii exploiting existing large lists of related entities e.g. , lists of actors , directors , books . These approaches introduce computational challenges because a the context of entities has to be aggregated across several documents and b the lists of related entities may be very large . We develop techniques to address these challenges . We present a thorough experimental study on real data sets that demonstrates the increase in accuracy and the scalability of our approaches . [[EENNDD]] information extraction"}, "Pengkategorian entiti berbanding koleksi dokumen besar Mengambil entiti seperti orang, filem dari dokumen dan mengenal pasti kategori seperti pelukis, penulis yang mereka miliki untuk membolehkan pertanyaan tersusun dan analisis data berbanding koleksi dokumen yang tidak tersusun. Dalam makalah ini, kami memfokuskan pada masalah mengkategorikan entiti yang diekstrak. Sebilangan besar pendekatan terdahulu yang dikembangkan untuk tugas ini hanya menganalisis konteks dokumen tempatan di mana entiti berlaku. Dalam makalah ini, kami meningkatkan ketepatan pengkategorian entiti secara signifikan dengan mempertimbangkan konteks entiti di pelbagai dokumen yang mengandunginya, dan ii mengeksploitasi senarai besar entiti berkaitan yang ada, mis. , senarai pelakon, pengarah, buku. Pendekatan ini memperkenalkan cabaran komputasi kerana konteks entiti harus digabungkan di beberapa dokumen dan b senarai entiti yang berkaitan mungkin sangat besar. Kami mengembangkan teknik untuk menangani cabaran ini. Kami menyajikan kajian eksperimental menyeluruh mengenai set data sebenar yang menunjukkan peningkatan ketepatan dan skalabilitas pendekatan kami. [[EENNDD]] pengekstrakan maklumat"], [{"string": "Trading representability for scalability : adaptive multi-hyperplane machine for nonlinear classification Support Vector Machines SVMs are among the most popular and successful classification algorithms . Kernel SVMs often reach state-of-the-art accuracies , but suffer from the curse of kernelization due to linear model growth with data size on noisy data . Linear SVMs have the ability to efficiently learn from truly large data , but they are applicable to a limited number of domains due to low representational power . To fill the representability and scalability gap between linear and nonlinear SVMs , we propose the Adaptive Multi-hyperplane Machine AMM algorithm that accomplishes fast training and prediction and has capability to solve nonlinear classification problems . AMM model consists of a set of hyperplanes weights , each assigned to one of the multiple classes , and predicts based on the associated class of the weight that provides the largest prediction . The number of weights is automatically determined through an iterative algorithm based on the stochastic gradient descent algorithm which is guaranteed to converge to a local optimum . Since the generalization bound decreases with the number of weights , a weight pruning mechanism is proposed and analyzed . The experiments on several large data sets show that AMM is nearly as fast during training and prediction as the state-of-the-art linear SVM solver and that it can be orders of magnitude faster than kernel SVM . In accuracy , AMM is somewhere between linear and kernel SVMs . For example , on an OCR task with 8 million highly dimensional training examples , AMM trained in 300 seconds on a single-core processor had 0.54 % error rate , which was significantly lower than 2.03 % error rate of a linear SVM trained in the same time and comparable to 0.43 % error rate of a kernel SVM trained in 2 days on 512 processors . The results indicate that AMM could be an attractive option when solving large-scale classification problems . The software is available at www.dabi.temple.edu\\/~vucetic\\/AMM.html .", "keywords": ["large-scale learning", "nonlinear classification", "support vector machines", "stochastic gradient descent"], "combined": "Trading representability for scalability : adaptive multi-hyperplane machine for nonlinear classification Support Vector Machines SVMs are among the most popular and successful classification algorithms . Kernel SVMs often reach state-of-the-art accuracies , but suffer from the curse of kernelization due to linear model growth with data size on noisy data . Linear SVMs have the ability to efficiently learn from truly large data , but they are applicable to a limited number of domains due to low representational power . To fill the representability and scalability gap between linear and nonlinear SVMs , we propose the Adaptive Multi-hyperplane Machine AMM algorithm that accomplishes fast training and prediction and has capability to solve nonlinear classification problems . AMM model consists of a set of hyperplanes weights , each assigned to one of the multiple classes , and predicts based on the associated class of the weight that provides the largest prediction . The number of weights is automatically determined through an iterative algorithm based on the stochastic gradient descent algorithm which is guaranteed to converge to a local optimum . Since the generalization bound decreases with the number of weights , a weight pruning mechanism is proposed and analyzed . The experiments on several large data sets show that AMM is nearly as fast during training and prediction as the state-of-the-art linear SVM solver and that it can be orders of magnitude faster than kernel SVM . In accuracy , AMM is somewhere between linear and kernel SVMs . For example , on an OCR task with 8 million highly dimensional training examples , AMM trained in 300 seconds on a single-core processor had 0.54 % error rate , which was significantly lower than 2.03 % error rate of a linear SVM trained in the same time and comparable to 0.43 % error rate of a kernel SVM trained in 2 days on 512 processors . The results indicate that AMM could be an attractive option when solving large-scale classification problems . The software is available at www.dabi.temple.edu\\/~vucetic\\/AMM.html . [[EENNDD]] large-scale learning; nonlinear classification; support vector machines; stochastic gradient descent"}, "Perwakilan perdagangan untuk skalabiliti: mesin multi-hiperplan adaptif untuk klasifikasi tidak linear Mesin Vektor Sokongan SVM adalah antara algoritma klasifikasi yang paling popular dan berjaya. Kernel SVM sering mencapai ketepatan canggih, tetapi mengalami kutukan kernelisasi kerana pertumbuhan model linier dengan ukuran data pada data yang bising. Linear SVM memiliki kemampuan untuk belajar dengan efisien dari data yang benar-benar besar, tetapi berlaku untuk sejumlah domain karena daya representasi yang rendah. Untuk mengisi jurang keterwakilan dan skalabiliti antara SVM linear dan tidak linier, kami mencadangkan algoritma AMM Multi-hyperplane Machine Adaptive yang menyelesaikan latihan dan ramalan pantas dan mempunyai keupayaan untuk menyelesaikan masalah klasifikasi nonlinear. Model AMM terdiri daripada sekumpulan berat kapal terbang, masing-masing ditugaskan ke salah satu daripada beberapa kelas, dan meramalkan berdasarkan kelas berat yang berkaitan yang memberikan ramalan terbesar. Bilangan bobot secara automatik ditentukan melalui algoritma berulang berdasarkan algoritma penurunan kecerunan stokastik yang dijamin akan berkumpul ke optimum tempatan. Oleh kerana had generalisasi menurun dengan jumlah bobot, mekanisme pemangkasan berat badan dicadangkan dan dianalisis. Eksperimen pada beberapa set data besar menunjukkan bahawa AMM hampir sepantas latihan dan ramalan seperti penyelesai SVM linier canggih dan bahawa ia boleh menjadi pesanan magnitud lebih cepat daripada SVM kernel. Secara tepat, AMM berada di antara SVM linear dan kernel. Sebagai contoh, pada tugas OCR dengan 8 juta contoh latihan yang sangat dimensi, AMM yang dilatih dalam 300 saat pada pemproses teras tunggal mempunyai kadar ralat 0.54%, yang jauh lebih rendah daripada kadar ralat 2.03% dari SVM linear yang dilatih pada masa yang sama dan setanding dengan kadar kesalahan 0.43% kernel SVM yang dilatih dalam 2 hari pada 512 pemproses. Hasilnya menunjukkan bahawa AMM dapat menjadi pilihan yang menarik ketika menyelesaikan masalah pengelasan skala besar. Perisian ini boleh didapati di www.dabi.temple.edu \\ / ~ vucetic \\ /AMM.html. [[EENNDD]] pembelajaran berskala besar; klasifikasi tidak linear; mesin vektor sokongan; keturunan kecerunan stokastik"], [{"string": "Efficient and effective explanation of change in hierarchical summaries Dimension attributes in data warehouses are typically hierarchical e.g. , geographic locations in sales data , URLs in Web traffic logs . OLAP tools are used to summarize the measure attributes e.g. , total sales along a dimension hierarchy , and to characterize changes e.g. , trends and anomalies in a hierarchical summary over time . When thenumber of changes identified is large e.g. , total sales in many stores differed from their expected values , a parsimonious explanation of the most significant changes is desirable . In this paper , we propose a natural model of parsimonious explanation , as a composition of node weights along the root-to-leaf paths in a dimension hierarchy , which permits changes to be aggregated with maximal generalization along the dimension hierarchy . We formalize this model of explaining changes in hierarchical summaries and investigate the problem of identifying optimally parsimonious explanations on arbitrary rooted one dimensional tree hierarchies . We show that such explanations can be computed efficiently in time essentially proportional to the number of leaves and the depth of the hierarchy . Further , our method can produce parsimonious explanations from the output of any statistical model that provides predictions and confidence intervals , making it widely applicable . Our experiments use real data sets to demonstrate the utility and robustness of our proposed model for explaining significant changes , as well as its superior parsimony compared to alternatives .", "keywords": ["olap", "parsimonious explanations", "change", "hierarchical summary", "statistical model"], "combined": "Efficient and effective explanation of change in hierarchical summaries Dimension attributes in data warehouses are typically hierarchical e.g. , geographic locations in sales data , URLs in Web traffic logs . OLAP tools are used to summarize the measure attributes e.g. , total sales along a dimension hierarchy , and to characterize changes e.g. , trends and anomalies in a hierarchical summary over time . When thenumber of changes identified is large e.g. , total sales in many stores differed from their expected values , a parsimonious explanation of the most significant changes is desirable . In this paper , we propose a natural model of parsimonious explanation , as a composition of node weights along the root-to-leaf paths in a dimension hierarchy , which permits changes to be aggregated with maximal generalization along the dimension hierarchy . We formalize this model of explaining changes in hierarchical summaries and investigate the problem of identifying optimally parsimonious explanations on arbitrary rooted one dimensional tree hierarchies . We show that such explanations can be computed efficiently in time essentially proportional to the number of leaves and the depth of the hierarchy . Further , our method can produce parsimonious explanations from the output of any statistical model that provides predictions and confidence intervals , making it widely applicable . Our experiments use real data sets to demonstrate the utility and robustness of our proposed model for explaining significant changes , as well as its superior parsimony compared to alternatives . [[EENNDD]] olap; parsimonious explanations; change; hierarchical summary; statistical model"}, "Penjelasan yang cekap dan berkesan mengenai perubahan dalam ringkasan hierarki Atribut dimensi di gudang data biasanya bersifat hierarki mis. , lokasi geografi dalam data penjualan, URL dalam log lalu lintas Web. Alat OLAP digunakan untuk meringkaskan atribut ukuran mis. , jumlah penjualan sepanjang hierarki dimensi, dan untuk mencirikan perubahan mis. , trend dan anomali dalam ringkasan hierarki dari masa ke masa. Apabila jumlah perubahan yang dikenal pasti besar, mis. , jumlah penjualan di banyak kedai berbeza dengan nilai yang diharapkan, penjelasan yang pelik mengenai perubahan yang paling ketara adalah wajar. Dalam makalah ini, kami mencadangkan model semula jadi penjelasan parsimonis, sebagai komposisi bobot simpul di sepanjang jalur akar ke daun dalam hierarki dimensi, yang memungkinkan perubahan digabungkan dengan generalisasi maksimum sepanjang hierarki dimensi. Kami memformalkan model ini untuk menerangkan perubahan dalam ringkasan hierarki dan menyiasat masalah mengenal pasti penjelasan parsimonis secara optimum mengenai hierarki pohon satu dimensi yang berakar secara sewenang-wenang. Kami menunjukkan bahawa penjelasan seperti itu dapat dihitung dengan cekap dalam masa yang sebanding dengan jumlah daun dan kedalaman hierarki. Selanjutnya, kaedah kami dapat menghasilkan penjelasan yang tidak masuk akal dari keluaran model statistik apa pun yang memberikan ramalan dan selang keyakinan, menjadikannya dapat diterapkan secara meluas. Eksperimen kami menggunakan set data sebenar untuk menunjukkan kegunaan dan ketahanan model cadangan kami untuk menerangkan perubahan yang ketara, dan juga keperitannya yang unggul berbanding dengan alternatif. [[EENNDD]] olap; penjelasan parsimonious; ubah; ringkasan hierarki; model statistik"], [{"string": "Extracting decision trees from trained neural networks Neural Networks are successful in acquiring hidden knowledge in datasets . Their biggest weakness is that the knowledge they acquire is represented in a form not understandable to humans . Researchers tried to address this problem by extracting rules from trained Neural Networks . Most of the proposed rule extraction methods required specialized type of Neural Networks ; some required binary inputs and some were computationally expensive . Craven proposed extracting MofN type Decision Trees from Neural Networks . We believe MofN type Decision Trees are only good for MofN type problems and trees created for regular high dimensional real world problems may be very complex . In this paper , we introduced a new method for extracting regular C4 .5 like Decision Trees from trained Neural Networks . We showed that the new method DecText is effective in extracting high fidelity trees from trained networks . We also introduced a new discretization technique to make DecText be able to handle continuous features and a new pruning technique for finding simplest tree with the highest fidelity .", "keywords": ["self-modifying machines"], "combined": "Extracting decision trees from trained neural networks Neural Networks are successful in acquiring hidden knowledge in datasets . Their biggest weakness is that the knowledge they acquire is represented in a form not understandable to humans . Researchers tried to address this problem by extracting rules from trained Neural Networks . Most of the proposed rule extraction methods required specialized type of Neural Networks ; some required binary inputs and some were computationally expensive . Craven proposed extracting MofN type Decision Trees from Neural Networks . We believe MofN type Decision Trees are only good for MofN type problems and trees created for regular high dimensional real world problems may be very complex . In this paper , we introduced a new method for extracting regular C4 .5 like Decision Trees from trained Neural Networks . We showed that the new method DecText is effective in extracting high fidelity trees from trained networks . We also introduced a new discretization technique to make DecText be able to handle continuous features and a new pruning technique for finding simplest tree with the highest fidelity . [[EENNDD]] self-modifying machines"}, "Pengambilan keputusan dari rangkaian saraf terlatih Neural Networks berjaya memperoleh pengetahuan tersembunyi dalam set data. Kelemahan terbesar mereka adalah bahawa pengetahuan yang mereka peroleh diwakili dalam bentuk yang tidak dapat difahami oleh manusia. Para penyelidik cuba mengatasi masalah ini dengan mengambil peraturan dari Neural Networks yang terlatih. Sebilangan besar kaedah pengekstrakan peraturan yang dicadangkan memerlukan jenis Neural Networks yang khusus; beberapa input binari diperlukan dan sebahagiannya mahal. Craven mencadangkan penggalian Pokok Keputusan jenis MofN dari Neural Networks. Kami percaya Pokok Keputusan jenis MofN hanya baik untuk masalah jenis MofN dan pokok yang dicipta untuk masalah dunia nyata dimensi tinggi biasa mungkin sangat rumit. Dalam makalah ini, kami memperkenalkan kaedah baru untuk mengekstrak C4 .5 biasa seperti Pohon Keputusan dari Jaringan Neural terlatih. Kami menunjukkan bahawa kaedah baru DecText berkesan dalam mengekstrak pohon kesetiaan tinggi dari rangkaian terlatih. Kami juga memperkenalkan teknik diskretisasi baru untuk membuat DecText dapat menangani ciri berterusan dan teknik pemangkasan baru untuk mencari pokok termudah dengan kesetiaan tertinggi. [[EENNDD]] mesin ubahsuai sendiri"], [{"string": "Creating social networks to improve peer-to-peer networking We use knowledge discovery techniques to guide the creation of efficient overlay networks for peer-to-peer file sharing . An overlay network specifies the logical connections among peers in a network and is distinct from the physical connections of the network . It determines the order in which peers will be queried when a user is searching for a specific file . To better understand the role of the network overlay structure in the performance of peer-to-peer file sharing protocols , we compare several methods for creating overlay networks . We analyze the networks using data from a campus network for peer-to-peer file sharing that recorded anonymized data on 6,528 users sharing 291,925 music files over an 81-day period . We propose a novel protocol for overlay creation based on a model of user preference identified by latent-variable clustering with hierarchical Dirichlet processes HDPs . Our simulations and empirical studies show that the clusters of songs created by HDPs effectively model user behavior and can be used to create desirable network overlays that outperform alternative approaches .", "keywords": ["learning", "social networks", "hierarchical dirichlet processes", "overlay networks", "distributed hash tables", "peer-to-peer networks"], "combined": "Creating social networks to improve peer-to-peer networking We use knowledge discovery techniques to guide the creation of efficient overlay networks for peer-to-peer file sharing . An overlay network specifies the logical connections among peers in a network and is distinct from the physical connections of the network . It determines the order in which peers will be queried when a user is searching for a specific file . To better understand the role of the network overlay structure in the performance of peer-to-peer file sharing protocols , we compare several methods for creating overlay networks . We analyze the networks using data from a campus network for peer-to-peer file sharing that recorded anonymized data on 6,528 users sharing 291,925 music files over an 81-day period . We propose a novel protocol for overlay creation based on a model of user preference identified by latent-variable clustering with hierarchical Dirichlet processes HDPs . Our simulations and empirical studies show that the clusters of songs created by HDPs effectively model user behavior and can be used to create desirable network overlays that outperform alternative approaches . [[EENNDD]] learning; social networks; hierarchical dirichlet processes; overlay networks; distributed hash tables; peer-to-peer networks"}, "Membuat rangkaian sosial untuk meningkatkan rangkaian peer-to-peer Kami menggunakan teknik penemuan pengetahuan untuk memandu penciptaan rangkaian overlay yang cekap untuk perkongsian fail peer-to-peer. Rangkaian overlay menentukan hubungan logik antara rakan sebaya dalam rangkaian dan berbeza dengan sambungan fizikal rangkaian. Ini menentukan urutan di mana rakan sebaya akan ditanyakan ketika pengguna mencari file tertentu. Untuk lebih memahami peranan struktur overlay rangkaian dalam prestasi protokol perkongsian fail peer-to-peer, kami membandingkan beberapa kaedah untuk membuat rangkaian overlay. Kami menganalisis rangkaian menggunakan data dari rangkaian kampus untuk perkongsian fail peer-to-peer yang mencatat data tanpa nama pada 6,528 pengguna yang berkongsi 291,925 fail muzik dalam jangka masa 81 hari. Kami mencadangkan protokol baru untuk pembuatan overlay berdasarkan model pilihan pengguna yang dikenalpasti oleh pengelompokan laten-variabel dengan proses Dirichlet hierarki HDP. Simulasi dan kajian empirikal kami menunjukkan bahawa kumpulan lagu yang dibuat oleh HDP secara berkesan memodelkan tingkah laku pengguna dan dapat digunakan untuk membuat lapisan rangkaian yang diinginkan yang mengatasi pendekatan alternatif. [[EENNDD]] pembelajaran; rangkaian sosial; proses dirichlet hierarki; rangkaian overlay; jadual hash yang diedarkan; rangkaian peer-to-peer"], [{"string": "A generalized Co-HITS algorithm and its application to bipartite graphs Recently many data types arising from data mining and Web search applications can be modeled as bipartite graphs . Examples include queries and URLs in query logs , and authors and papers in scientific literature . However , one of the issues is that previous algorithms only consider the content and link information from one side of the bipartite graph . There is a lack of constraints to make sure the final relevance of the score propagation on the graph , as there are many noisy edges within the bipartite graph . In this paper , we propose a novel and general Co-HITS algorithm to incorporate the bipartite graph with the content information from both sides as well as the constraints of relevance . Moreover , we investigate the algorithm based on two frameworks , including the iterative and the regularization frameworks , and illustrate the generalized Co-HITS algorithm from different views . For the iterative framework , it contains HITS and personalized PageRank as special cases . In the regularization framework , we successfully build a connection with HITS , and develop a new cost function to consider the direct relationship between two entity sets , which leads to a significant improvement over the baseline method . To illustrate our methodology , we apply the Co-HITS algorithm , with many different settings , to the application of query suggestion by mining the AOL query log data . Experimental results demonstrate that CoRegu-0 .5 i.e. , a model of the regularization framework achieves the best performance with consistent and promising improvements .", "keywords": ["regularization", "bipartite graphs", "co-hits", "score propagation", "mutual reinforcement"], "combined": "A generalized Co-HITS algorithm and its application to bipartite graphs Recently many data types arising from data mining and Web search applications can be modeled as bipartite graphs . Examples include queries and URLs in query logs , and authors and papers in scientific literature . However , one of the issues is that previous algorithms only consider the content and link information from one side of the bipartite graph . There is a lack of constraints to make sure the final relevance of the score propagation on the graph , as there are many noisy edges within the bipartite graph . In this paper , we propose a novel and general Co-HITS algorithm to incorporate the bipartite graph with the content information from both sides as well as the constraints of relevance . Moreover , we investigate the algorithm based on two frameworks , including the iterative and the regularization frameworks , and illustrate the generalized Co-HITS algorithm from different views . For the iterative framework , it contains HITS and personalized PageRank as special cases . In the regularization framework , we successfully build a connection with HITS , and develop a new cost function to consider the direct relationship between two entity sets , which leads to a significant improvement over the baseline method . To illustrate our methodology , we apply the Co-HITS algorithm , with many different settings , to the application of query suggestion by mining the AOL query log data . Experimental results demonstrate that CoRegu-0 .5 i.e. , a model of the regularization framework achieves the best performance with consistent and promising improvements . [[EENNDD]] regularization; bipartite graphs; co-hits; score propagation; mutual reinforcement"}, "Algoritma Co-HITS yang umum dan aplikasinya pada grafik bipartit Baru-baru ini banyak jenis data yang timbul dari perlombongan data dan aplikasi carian Web dapat dimodelkan sebagai grafik bipartit. Contohnya termasuk pertanyaan dan URL dalam log pertanyaan, dan pengarang dan makalah dalam literatur ilmiah. Walau bagaimanapun, salah satu masalahnya ialah algoritma sebelumnya hanya mempertimbangkan kandungan dan maklumat pautan dari satu sisi graf bipartit. Terdapat kekurangan batasan untuk memastikan relevansi akhir penyebaran skor pada grafik, kerana terdapat banyak sisi bising dalam grafik bipartit. Dalam makalah ini, kami mengusulkan sebuah novel dan algoritma Co-HITS umum untuk menggabungkan graf bipartit dengan maklumat kandungan dari kedua-dua belah pihak serta kekangan yang relevan. Lebih-lebih lagi, kami menyiasat algoritma berdasarkan dua kerangka kerja, termasuk kerangka berulang dan regularisasi, dan menggambarkan algoritma Co-HITS umum dari pandangan yang berbeza. Untuk rangka berulang, ia mengandungi HITS dan PageRank yang diperibadikan sebagai kes khas. Dalam kerangka regularisasi, kami berjaya membangun hubungan dengan HITS, dan mengembangkan fungsi biaya baru untuk mempertimbangkan hubungan langsung antara dua set entitas, yang menyebabkan peningkatan yang signifikan terhadap kaedah dasar. Untuk menggambarkan metodologi kami, kami menerapkan algoritma Co-HITS, dengan banyak tetapan yang berbeza, untuk penerapan cadangan pertanyaan dengan melombong data log pertanyaan AOL. Hasil eksperimen menunjukkan bahawa CoRegu-0 .5, contohnya, model kerangka regulasi mencapai prestasi terbaik dengan peningkatan yang konsisten dan menjanjikan. [[EENNDD]] regularisasi; graf bipartit; aksi bersama; penyebaran skor; pengukuhan bersama"], [{"string": "Cross-sell : a fast promotion-tunable customer-item recommendation method based on conditionally independent probabilities", "keywords": ["imputation", "collaborative filtering", "recommendation", "cross-sell", "electronic commerce"], "combined": "Cross-sell : a fast promotion-tunable customer-item recommendation method based on conditionally independent probabilities [[EENNDD]] imputation; collaborative filtering; recommendation; cross-sell; electronic commerce"}, "Jual beli silang: kaedah cadangan item pelanggan yang dapat disesuaikan dengan promosi berdasarkan kebarangkalian bersyarat [[EENNDD]]; penapisan kolaboratif; cadangan; penjualan silang; perdagangan elektronik"], [{"string": "Adversarial classification Essentially all data mining algorithms assume that the data-generating process is independent of the data miner 's activities . However , in many domains , including spam detection , intrusion detection , fraud detection , surveillance and counter-terrorism , this is far from the case : the data is actively manipulated by an adversary seeking to make the classifier produce false negatives . In these domains , the performance of a classifier can degrade rapidly after it is deployed , as the adversary learns to defeat it . Currently the only solution to this is repeated , manual , ad hoc reconstruction of the classifier . In this paper we develop a formal framework and algorithms for this problem . We view classification as a game between the classifier and the adversary , and produce a classifier that is optimal given the adversary 's optimal strategy . Experiments in a spam detection domain show that this approach can greatly outperform a classifier learned in the standard way , and within the parameters of the problem automatically adapt the classifier to the adversary 's evolving manipulations .", "keywords": ["cost-sensitive learning", "naive bayes", "game theory", "spam detection", "integer linear programming"], "combined": "Adversarial classification Essentially all data mining algorithms assume that the data-generating process is independent of the data miner 's activities . However , in many domains , including spam detection , intrusion detection , fraud detection , surveillance and counter-terrorism , this is far from the case : the data is actively manipulated by an adversary seeking to make the classifier produce false negatives . In these domains , the performance of a classifier can degrade rapidly after it is deployed , as the adversary learns to defeat it . Currently the only solution to this is repeated , manual , ad hoc reconstruction of the classifier . In this paper we develop a formal framework and algorithms for this problem . We view classification as a game between the classifier and the adversary , and produce a classifier that is optimal given the adversary 's optimal strategy . Experiments in a spam detection domain show that this approach can greatly outperform a classifier learned in the standard way , and within the parameters of the problem automatically adapt the classifier to the adversary 's evolving manipulations . [[EENNDD]] cost-sensitive learning; naive bayes; game theory; spam detection; integer linear programming"}, "Klasifikasi adversari Pada dasarnya semua algoritma perlombongan data menganggap bahawa proses menghasilkan data tidak bergantung kepada aktiviti pelombong data. Namun, dalam banyak domain, termasuk pengesanan spam, pengesanan pencerobohan, pengesanan penipuan, pengawasan dan pengganas, ini jauh dari masalahnya: data dimanipulasi secara aktif oleh musuh yang berusaha membuat pengklasifikasi menghasilkan negatif palsu. Dalam domain ini, prestasi pengklasifikasi dapat merosot dengan cepat setelah diterapkan, ketika musuh belajar untuk mengalahkannya. Pada masa ini satu-satunya penyelesaian untuk ini adalah pengulangan semula, manual, ad hoc pengkelasan. Dalam makalah ini kami mengembangkan kerangka dan algoritma formal untuk masalah ini. Kami melihat klasifikasi sebagai permainan antara pengklasifikasi dan lawan, dan menghasilkan klasifikasi yang optimum memandangkan strategi optimum musuh. Eksperimen dalam domain pengesanan spam menunjukkan bahawa pendekatan ini dapat mengatasi pengklasifikasi yang dipelajari dengan cara biasa, dan dalam parameter masalah secara automatik menyesuaikan pengklasifikasi dengan manipulasi musuh yang sedang berkembang. [[EENNDD]] pembelajaran sensitif kos; bayes naif; teori permainan; pengesanan spam; pengaturcaraan linear integer"], [{"string": "Density-based clustering of uncertain data In many different application areas , e.g. sensor databases , location based services or face recognition systems , distances between odjects have to be computed based on vague and uncertain data . Commonly , the distances between these uncertain object descriptions are expressed by one numerical distance value . Based on such single-valued distance functions standard data mining algorithms can work without any changes . In this paper , we propose to express the similarity between two fuzzy objects by distance probability functions . These fuzzy distance functions assign a probability value to each possible distance value . By integrating these fuzzy distance functions directly into data mining algorithms , the full information provided by these functions is exploited . In order to demonstrate the benefits of this general approach , we enhance the density-based clustering algorithm DBSCAN so that it can work directly on these fuzzy distance functions . In a detailed experimental evaluation based on artificial and real-world data sets , we show the characteristics and benefits of our new approach .", "keywords": ["probabilistic algorithms", "fuzzy distance functions", "uncertain data", "density-based clustering"], "combined": "Density-based clustering of uncertain data In many different application areas , e.g. sensor databases , location based services or face recognition systems , distances between odjects have to be computed based on vague and uncertain data . Commonly , the distances between these uncertain object descriptions are expressed by one numerical distance value . Based on such single-valued distance functions standard data mining algorithms can work without any changes . In this paper , we propose to express the similarity between two fuzzy objects by distance probability functions . These fuzzy distance functions assign a probability value to each possible distance value . By integrating these fuzzy distance functions directly into data mining algorithms , the full information provided by these functions is exploited . In order to demonstrate the benefits of this general approach , we enhance the density-based clustering algorithm DBSCAN so that it can work directly on these fuzzy distance functions . In a detailed experimental evaluation based on artificial and real-world data sets , we show the characteristics and benefits of our new approach . [[EENNDD]] probabilistic algorithms; fuzzy distance functions; uncertain data; density-based clustering"}, "Penggabungan data ketumpatan berdasarkan kepadatan Di banyak bidang aplikasi yang berbeza, mis. pangkalan data sensor, perkhidmatan berasaskan lokasi atau sistem pengenalan wajah, jarak antara objek mesti dihitung berdasarkan data yang tidak jelas dan tidak pasti. Lazimnya, jarak antara keterangan objek tidak pasti ini dinyatakan dengan satu nilai jarak berangka. Berdasarkan fungsi jarak satu nilai seperti itu, algoritma perlombongan data standard dapat berfungsi tanpa sebarang perubahan. Dalam makalah ini, kami mencadangkan untuk menyatakan persamaan antara dua objek kabur dengan fungsi kebarangkalian jarak. Fungsi jarak kabur ini memberikan nilai kebarangkalian untuk setiap nilai jarak yang mungkin. Dengan mengintegrasikan fungsi jarak kabur ini secara langsung ke dalam algoritma perlombongan data, maklumat lengkap yang diberikan oleh fungsi ini dieksploitasi. Untuk menunjukkan faedah pendekatan umum ini, kami meningkatkan algoritma pengelompokan berdasarkan kepadatan DBSCAN sehingga dapat berfungsi secara langsung pada fungsi jarak kabur ini. Dalam penilaian eksperimental terperinci berdasarkan set data buatan dan dunia nyata, kami menunjukkan ciri dan faedah pendekatan baru kami. [[EENNDD]] algoritma probabilistik; fungsi jarak kabur; data yang tidak pasti; pengelompokan berdasarkan ketumpatan"], [{"string": "Social action tracking via noise tolerant time-varying factor graphs It is well known that users ' behaviors actions in a social network are influenced by various factors such as personal interests , social influence , and global trends . However , few publications systematically study how social actions evolve in a dynamic social network and to what extent different factors affect the user actions . In this paper , we propose a Noise Tolerant Time-varying Factor Graph Model NTT-FGM for modeling and predicting social actions . NTT-FGM simultaneously models social network structure , user attributes and user action history for better prediction of the users ' future actions . More specifically , a user 's action at time t is generated by her latent state at t , which is influenced by her attributes , her own latent state at time t-1 and her neighbors ' states at time t and t-1 . Based on this intuition , we formalize the social action tracking problem using the NTT-FGM model ; then present an efficient algorithm to learn the model , by combining the ideas from both continuous linear system and Markov random field . Finally , we present a case study of our model on predicting future social actions . We validate the model on three different types of real-world data sets . Qualitatively , our model can uncover some interesting patterns of the social dynamics . Quantitatively , experimental results show that the proposed method outperforms several baseline methods for action prediction .", "keywords": ["social action tracking", "time-varying factor graphs", "social influence analysis"], "combined": "Social action tracking via noise tolerant time-varying factor graphs It is well known that users ' behaviors actions in a social network are influenced by various factors such as personal interests , social influence , and global trends . However , few publications systematically study how social actions evolve in a dynamic social network and to what extent different factors affect the user actions . In this paper , we propose a Noise Tolerant Time-varying Factor Graph Model NTT-FGM for modeling and predicting social actions . NTT-FGM simultaneously models social network structure , user attributes and user action history for better prediction of the users ' future actions . More specifically , a user 's action at time t is generated by her latent state at t , which is influenced by her attributes , her own latent state at time t-1 and her neighbors ' states at time t and t-1 . Based on this intuition , we formalize the social action tracking problem using the NTT-FGM model ; then present an efficient algorithm to learn the model , by combining the ideas from both continuous linear system and Markov random field . Finally , we present a case study of our model on predicting future social actions . We validate the model on three different types of real-world data sets . Qualitatively , our model can uncover some interesting patterns of the social dynamics . Quantitatively , experimental results show that the proposed method outperforms several baseline methods for action prediction . [[EENNDD]] social action tracking; time-varying factor graphs; social influence analysis"}, "Penjejakan tindakan sosial melalui grafik faktor perbezaan masa yang bertolak ansur dengan kebisingan Telah diketahui bahawa tindakan tingkah laku pengguna dalam rangkaian sosial dipengaruhi oleh pelbagai faktor seperti minat peribadi, pengaruh sosial, dan trend global. Walau bagaimanapun, sebilangan penerbitan secara sistematik mengkaji bagaimana tindakan sosial berkembang dalam rangkaian sosial yang dinamik dan sejauh mana faktor yang berbeza mempengaruhi tindakan pengguna. Dalam makalah ini, kami mencadangkan Model Graf Faktor Bervariasi Masa Toleransi Kebisingan NTT-FGM untuk memodelkan dan meramalkan tindakan sosial. NTT-FGM secara serentak memodelkan struktur rangkaian sosial, atribut pengguna dan sejarah tindakan pengguna untuk ramalan yang lebih baik mengenai tindakan masa depan pengguna. Lebih khusus lagi, tindakan pengguna pada waktu t dihasilkan oleh keadaan pendamnya di t, yang dipengaruhi oleh sifatnya, keadaan pendamnya sendiri pada waktu t-1 dan keadaan jirannya pada waktu t dan t-1. Berdasarkan intuisi ini, kami memformalkan masalah pengesanan tindakan sosial menggunakan model NTT-FGM; kemudian kemukakan algoritma yang cekap untuk mempelajari model, dengan menggabungkan idea dari kedua-dua sistem linear berterusan dan bidang rawak Markov. Akhirnya, kami membentangkan kajian kes model kami mengenai ramalan tindakan sosial masa depan. Kami mengesahkan model pada tiga jenis kumpulan data dunia nyata yang berbeza. Secara kualitatif, model kami dapat mengungkap beberapa corak dinamika sosial yang menarik. Secara kuantitatif, hasil eksperimen menunjukkan bahawa kaedah yang dicadangkan mengatasi beberapa kaedah asas untuk ramalan tindakan. [[EENNDD]] penjejakan tindakan sosial; graf faktor yang berbeza-beza masa; analisis pengaruh sosial"], [{"string": "Probabilistic frequent itemset mining in uncertain databases Probabilistic frequent itemset mining in uncertain transaction databases semantically and computationally differs from traditional techniques applied to standard `` certain '' transaction databases . The consideration of existential uncertainty of item sets , indicating the probability that an item set occurs in a transaction , makes traditional techniques inapplicable . In this paper , we introduce new probabilistic formulations of frequent itemsets based on possible world semantics . In this probabilistic context , an itemset X is called frequent if the probability that X occurs in at least minSup transactions is above a given threshold \u03c4 . To the best of our knowledge , this is the first approach addressing this problem under possible worlds semantics . In consideration of the probabilistic formulations , we present a framework which is able to solve the Probabilistic Frequent Itemset Mining PFIM problem efficiently . An extensive experimental evaluation investigates the impact of our proposed techniques and shows that our approach is orders of magnitude faster than straight-forward approaches .", "keywords": ["frequent itemset mining", "probabilistic data", "uncertain databases", "probabilistic frequent itemsets"], "combined": "Probabilistic frequent itemset mining in uncertain databases Probabilistic frequent itemset mining in uncertain transaction databases semantically and computationally differs from traditional techniques applied to standard `` certain '' transaction databases . The consideration of existential uncertainty of item sets , indicating the probability that an item set occurs in a transaction , makes traditional techniques inapplicable . In this paper , we introduce new probabilistic formulations of frequent itemsets based on possible world semantics . In this probabilistic context , an itemset X is called frequent if the probability that X occurs in at least minSup transactions is above a given threshold \u03c4 . To the best of our knowledge , this is the first approach addressing this problem under possible worlds semantics . In consideration of the probabilistic formulations , we present a framework which is able to solve the Probabilistic Frequent Itemset Mining PFIM problem efficiently . An extensive experimental evaluation investigates the impact of our proposed techniques and shows that our approach is orders of magnitude faster than straight-forward approaches . [[EENNDD]] frequent itemset mining; probabilistic data; uncertain databases; probabilistic frequent itemsets"}, "Perlombongan itemet berkemungkinan probabilistik dalam pangkalan data tidak pasti Perlombongan itemet berkala probabilistik dalam pangkalan data transaksi tidak pasti berbeza secara semantik dan komputasi dari teknik tradisional yang diterapkan pada pangkalan data transaksi \"tertentu\" standard. Pertimbangan ketidaktentuan eksistensi set item, menunjukkan kebarangkalian bahawa set item berlaku dalam transaksi, menjadikan teknik tradisional tidak dapat digunakan. Dalam makalah ini, kami memperkenalkan formulasi probabilistik baru dari item item yang kerap berdasarkan semantik dunia yang mungkin. Dalam konteks probabilistik ini, itemet X disebut kerap jika kebarangkalian X berlaku dalam sekurang-kurangnya transaksi minSup berada di atas ambang batas \u03c4. Sepengetahuan kami, ini adalah pendekatan pertama untuk mengatasi masalah ini di bawah semantik dunia yang mungkin. Dengan mempertimbangkan formulasi probabilistik, kami menyajikan kerangka kerja yang dapat menyelesaikan masalah PFIM Mining Itemset Frequent Frequent Frequent. Penilaian eksperimental yang luas menyelidiki kesan teknik yang dicadangkan kami dan menunjukkan bahawa pendekatan kami adalah urutan besarnya lebih cepat daripada pendekatan lurus ke hadapan. [[EENNDD]] perlombongan itemet kerap; data kebarangkalian; pangkalan data yang tidak menentu; itemset kebarangkalian"], [{"string": "Regularized discriminant analysis for high dimensional , low sample size data Linear and Quadratic Discriminant Analysis have been used widely in many areas of data mining , machine learning , and bioinformatics . Friedman proposed a compromise between Linear and Quadratic Discriminant Analysis , called Regularized Discriminant Analysis RDA , which has been shown to be more flexible in dealing with various class distributions . RDA applies the regularization techniques by employing two regularization parameters , which are chosen to jointly maximize the classification performance . The optimal pair of parameters is commonly estimated via cross-validation from a set of candidate pairs . It is computationally prohibitive for high dimensional data , especially when the candidate set is large , which limits the applications of RDA to low dimensional data . In this paper , a novel algorithm for RDA is presented for high dimensional data . It can estimate the optimal regularization parameters from a large set of parameter candidates efficiently . Experiments on a variety of datasets confirm the claimed theoretical estimate of the efficiency , and also show that , for a properly chosen pair of regularization parameters , RDA performs favorably in classification , in comparison with other existing classification methods .", "keywords": ["regularization", "dimensionality reduction", "cross-validation", "quadratic discriminant analysis"], "combined": "Regularized discriminant analysis for high dimensional , low sample size data Linear and Quadratic Discriminant Analysis have been used widely in many areas of data mining , machine learning , and bioinformatics . Friedman proposed a compromise between Linear and Quadratic Discriminant Analysis , called Regularized Discriminant Analysis RDA , which has been shown to be more flexible in dealing with various class distributions . RDA applies the regularization techniques by employing two regularization parameters , which are chosen to jointly maximize the classification performance . The optimal pair of parameters is commonly estimated via cross-validation from a set of candidate pairs . It is computationally prohibitive for high dimensional data , especially when the candidate set is large , which limits the applications of RDA to low dimensional data . In this paper , a novel algorithm for RDA is presented for high dimensional data . It can estimate the optimal regularization parameters from a large set of parameter candidates efficiently . Experiments on a variety of datasets confirm the claimed theoretical estimate of the efficiency , and also show that , for a properly chosen pair of regularization parameters , RDA performs favorably in classification , in comparison with other existing classification methods . [[EENNDD]] regularization; dimensionality reduction; cross-validation; quadratic discriminant analysis"}, "Analisis diskriminan teratur untuk dimensi tinggi, data ukuran sampel rendah Analisis Diskriminan Linier dan Kuadratik telah digunakan secara meluas dalam banyak bidang perlombongan data, pembelajaran mesin, dan bioinformatik. Friedman mencadangkan kompromi antara Analisis Diskriminan Linier dan Kuadratik, yang disebut RDA Analisis Diskriminan Analisis, yang telah terbukti lebih fleksibel dalam menangani pelbagai pembahagian kelas. RDA menerapkan teknik regularisasi dengan menggunakan dua parameter regularisasi, yang dipilih untuk bersama-sama memaksimumkan prestasi klasifikasi. Pasangan parameter optimum biasanya diperkirakan melalui pengesahan silang dari sekumpulan pasangan calon. Hal ini sangat melarang untuk data dimensi tinggi, terutama ketika set calon besar, yang membatasi aplikasi RDA ke data dimensi rendah. Dalam makalah ini, algoritma novel untuk RDA dikemukakan untuk data dimensi tinggi. Ia dapat mengira parameter regularisasi optimum dari sekumpulan besar calon parameter dengan cekap. Eksperimen pada pelbagai kumpulan data mengesahkan anggaran teoritis kecekapan yang dituntut, dan juga menunjukkan bahawa, untuk sepasang parameter regularisasi yang dipilih dengan tepat, RDA menunjukkan prestasi yang baik dalam klasifikasi, dibandingkan dengan kaedah klasifikasi lain yang ada. [[EENNDD]] regularisasi; pengurangan dimensi; pengesahan bersilang; analisis diskriminasi kuadratik"], [{"string": "On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms", "keywords": ["learning"], "combined": "On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms [[EENNDD]] learning"}, "Pengesanan outlier tanpa pengawasan dalam talian menggunakan campuran terhingga dengan pembelajaran algoritma pembelajaran [[EENNDD]]"], [{"string": "Generalized additive neural networks", "keywords": ["additive models", "predictive modeling", "financial", "partial residuals"], "combined": "Generalized additive neural networks [[EENNDD]] additive models; predictive modeling; financial; partial residuals"}, "Rangkaian neural aditif umum [[EENNDD]] model aditif; pemodelan ramalan; kewangan; baki separa"], [{"string": "OLAP on search logs : an infrastructure supporting data-driven applications in search engines Search logs , which contain rich and up-to-date information about users ' needs and preferences , have become a critical data source for search engines . Recently , more and more data-driven applications are being developed in search engines based on search logs , such as query suggestion , keyword bidding , and dissatisfactory query analysis . In this paper , by observing that many data-driven applications in search engines highly rely on online mining of search logs , we develop an OLAP system on search logs which serves as an infrastructure supporting various data-driven applications . An empirical study using real data of over two billion query sessions demonstrates the usefulness and feasibility of our design .", "keywords": ["olap", "query session", "search log", "suffix tree"], "combined": "OLAP on search logs : an infrastructure supporting data-driven applications in search engines Search logs , which contain rich and up-to-date information about users ' needs and preferences , have become a critical data source for search engines . Recently , more and more data-driven applications are being developed in search engines based on search logs , such as query suggestion , keyword bidding , and dissatisfactory query analysis . In this paper , by observing that many data-driven applications in search engines highly rely on online mining of search logs , we develop an OLAP system on search logs which serves as an infrastructure supporting various data-driven applications . An empirical study using real data of over two billion query sessions demonstrates the usefulness and feasibility of our design . [[EENNDD]] olap; query session; search log; suffix tree"}, "OLAP pada log carian: infrastruktur yang menyokong aplikasi yang didorong oleh data di enjin carian Log carian, yang mengandungi maklumat yang kaya dan terkini mengenai keperluan dan pilihan pengguna, telah menjadi sumber data penting untuk enjin carian. Baru-baru ini, semakin banyak aplikasi berdasarkan data sedang dikembangkan di enjin carian berdasarkan log carian, seperti cadangan pertanyaan, penawaran kata kunci, dan analisis pertanyaan yang tidak memuaskan. Dalam makalah ini, dengan memperhatikan bahawa banyak aplikasi yang didorong oleh data dalam mesin pencari sangat bergantung pada perlombongan log carian dalam talian, kami mengembangkan sistem OLAP pada log carian yang berfungsi sebagai infrastruktur yang menyokong pelbagai aplikasi yang didorong oleh data. Kajian empirik menggunakan data sebenar lebih dari dua bilion sesi pertanyaan menunjukkan kegunaan dan kelayakan reka bentuk kami. [[EENNDD]] olap; sesi pertanyaan; log carian; pokok akhiran"], [{"string": "Alpha seeding for support vector machines", "keywords": ["implementation", "training speed-ups", "support vector machines", "classification"], "combined": "Alpha seeding for support vector machines [[EENNDD]] implementation; training speed-ups; support vector machines; classification"}, "Penyemaian alpha untuk pelaksanaan mesin vektor sokongan [[EENNDD]]; latihan laju; mesin vektor sokongan; pengelasan"], [{"string": "Querying multiple sets of discovered rules Rule mining is an important data mining task that has been applied to numerous real-world applications . Often a rule mining system generates a large number of rules and only a small subset of them is really useful in applications . Although there exist some systems allowing the user to query the discovered rules , they are less suitable for complex ad hoc querying of multiple data mining rulebases to retrieve interesting rules . In this paper , we propose a new powerful rule query language Rule-QL for querying multiple rulebases that is modeled after SQL and has rigorous theoretical foundations of a rule-based calculus . In particular , we first propose a rule-based calculus RC based on the first-order logic , and then present the language Rule-QL that is at least as expressive as the safe fragment of RC . We also propose a number of efficient query evaluation techniques for Rule-QL and test them experimentally on some representative queries to demonstrate the feasibility of Rule-QL .", "keywords": ["data mining queries", "rulebases", "query evaluation", "association rules"], "combined": "Querying multiple sets of discovered rules Rule mining is an important data mining task that has been applied to numerous real-world applications . Often a rule mining system generates a large number of rules and only a small subset of them is really useful in applications . Although there exist some systems allowing the user to query the discovered rules , they are less suitable for complex ad hoc querying of multiple data mining rulebases to retrieve interesting rules . In this paper , we propose a new powerful rule query language Rule-QL for querying multiple rulebases that is modeled after SQL and has rigorous theoretical foundations of a rule-based calculus . In particular , we first propose a rule-based calculus RC based on the first-order logic , and then present the language Rule-QL that is at least as expressive as the safe fragment of RC . We also propose a number of efficient query evaluation techniques for Rule-QL and test them experimentally on some representative queries to demonstrate the feasibility of Rule-QL . [[EENNDD]] data mining queries; rulebases; query evaluation; association rules"}, "Menanyakan beberapa set peraturan yang dijumpai Perlombongan peraturan adalah tugas perlombongan data penting yang telah diterapkan ke banyak aplikasi dunia nyata. Selalunya sistem perlombongan peraturan menghasilkan sebilangan besar peraturan dan hanya sebahagian kecil dari mereka yang benar-benar berguna dalam aplikasi. Walaupun ada beberapa sistem yang memungkinkan pengguna untuk menanyakan peraturan yang dijumpai, mereka kurang sesuai untuk pertanyaan ad hoc yang kompleks dari beberapa basis peraturan perlombongan data untuk mendapatkan peraturan yang menarik. Dalam makalah ini, kami mengusulkan bahasa query aturan baru yang kuat Rule-QL untuk menanyakan beberapa basis peraturan yang dimodelkan setelah SQL dan memiliki asas teoritis yang ketat dari kalkulus berasaskan peraturan. Khususnya, kami pertama kali mencadangkan kalkulus RC berdasarkan peraturan berdasarkan logik pesanan pertama, dan kemudian membentangkan bahasa Rule-QL yang paling tidak ekspresif seperti fragmen RC yang selamat. Kami juga mencadangkan sebilangan teknik penilaian pertanyaan yang efisien untuk Rule-QL dan mengujinya secara eksperimental pada beberapa pertanyaan perwakilan untuk menunjukkan kemungkinan Rule-QL. [[EENNDD]] pertanyaan perlombongan data; asas peraturan; penilaian pertanyaan; peraturan persatuan"], [{"string": "Rule interestingness analysis using OLAP operations The problem of interestingness of discovered rules has been investigated by many researchers . The issue is that data mining algorithms often generate too many rules , which make it very hard for the user to find the interesting ones . Over the years many techniques have been proposed . However , few have made it to real-life applications . Since August 2004 , we have been working on a major application for Motorola . The objective is to find causes of cellular phone call failures from a large amount of usage log data . Class association rules have been shown to be suitable for this type of diagnostic data mining application . We were also able to put several existing interestingness methods to the test , which revealed some major shortcomings . One of the main problems is that most existing methods treat rules individually . However , we discovered that users seldom regard a single rule to be interesting by itself . A rule is only interesting in the context of some other rules . Furthermore , in many cases , each individual rule may not be interesting , but a group of them together can represent an important piece of knowledge . This led us to discover a deficiency of the current rule mining paradigm . Using non-zero minimum support and non-zero minimum confidence eliminates a large amount of context information , which makes rule analysis difficult . This paper proposes a novel approach to deal with all of these issues , which casts rule analysis as OLAP operations and general impression mining . This approach enables the user to explore the knowledge space to find useful knowledge easily and systematically . It also provides a natural framework for visualization . As an evidence of its effectiveness , our system , called Opportunity Map , based on these ideas has been deployed , and it is in daily use in Motorola for finding actionable knowledge from its engineering and other types of data sets .", "keywords": ["class association rules", "olap", "interestingness analysis", "miscellaneous", "diagnostic data mining", "general impressions"], "combined": "Rule interestingness analysis using OLAP operations The problem of interestingness of discovered rules has been investigated by many researchers . The issue is that data mining algorithms often generate too many rules , which make it very hard for the user to find the interesting ones . Over the years many techniques have been proposed . However , few have made it to real-life applications . Since August 2004 , we have been working on a major application for Motorola . The objective is to find causes of cellular phone call failures from a large amount of usage log data . Class association rules have been shown to be suitable for this type of diagnostic data mining application . We were also able to put several existing interestingness methods to the test , which revealed some major shortcomings . One of the main problems is that most existing methods treat rules individually . However , we discovered that users seldom regard a single rule to be interesting by itself . A rule is only interesting in the context of some other rules . Furthermore , in many cases , each individual rule may not be interesting , but a group of them together can represent an important piece of knowledge . This led us to discover a deficiency of the current rule mining paradigm . Using non-zero minimum support and non-zero minimum confidence eliminates a large amount of context information , which makes rule analysis difficult . This paper proposes a novel approach to deal with all of these issues , which casts rule analysis as OLAP operations and general impression mining . This approach enables the user to explore the knowledge space to find useful knowledge easily and systematically . It also provides a natural framework for visualization . As an evidence of its effectiveness , our system , called Opportunity Map , based on these ideas has been deployed , and it is in daily use in Motorola for finding actionable knowledge from its engineering and other types of data sets . [[EENNDD]] class association rules; olap; interestingness analysis; miscellaneous; diagnostic data mining; general impressions"}, "Analisis ketertarikan peraturan menggunakan operasi OLAP Masalah keseronokan peraturan yang dijumpai telah disiasat oleh banyak penyelidik. Masalahnya ialah algoritma perlombongan data sering menghasilkan terlalu banyak peraturan, yang menyukarkan pengguna untuk mencari yang menarik. Selama bertahun-tahun banyak teknik telah dicadangkan. Walau bagaimanapun, hanya sedikit yang berjaya dalam aplikasi kehidupan sebenar. Sejak Ogos 2004, kami telah mengusahakan aplikasi utama untuk Motorola. Objektifnya adalah untuk mencari penyebab kegagalan panggilan telefon bimbit dari sejumlah besar data log penggunaan. Peraturan pergaulan kelas terbukti sesuai untuk aplikasi perlombongan data diagnostik ini. Kami juga dapat menguji beberapa kaedah menarik yang ada, yang menunjukkan beberapa kekurangan utama. Salah satu masalah utama adalah bahawa kebanyakan kaedah yang ada memperlakukan peraturan secara individu. Namun, kami mendapati bahawa pengguna jarang menganggap satu peraturan itu menarik dengan sendirinya. Peraturan hanya menarik dalam konteks beberapa peraturan lain. Selanjutnya, dalam banyak kes, setiap peraturan individu mungkin tidak menarik, tetapi sekumpulan mereka bersama-sama dapat mewakili pengetahuan yang penting. Ini menyebabkan kami menemui kekurangan paradigma perlombongan peraturan semasa. Menggunakan sokongan minimum bukan sifar dan keyakinan minimum bukan sifar menghilangkan sebilangan besar maklumat konteks, yang menyukarkan analisis peraturan. Makalah ini mencadangkan pendekatan baru untuk menangani semua masalah ini, yang menggunakan analisis peraturan sebagai operasi OLAP dan perlombongan kesan umum. Pendekatan ini membolehkan pengguna meneroka ruang pengetahuan untuk mencari pengetahuan berguna dengan mudah dan sistematik. Ini juga menyediakan kerangka semula jadi untuk visualisasi. Sebagai bukti keberkesanannya, sistem kami, yang disebut Peta Peluang, berdasarkan idea-idea ini telah digunakan, dan digunakan dalam Motorola setiap hari untuk mencari pengetahuan yang dapat ditindaklanjuti dari kejuruteraannya dan jenis set data yang lain. [[EENNDD]] peraturan persatuan kelas; olap; analisis minat; pelbagai; perlombongan data diagnostik; kesan umum"], [{"string": "Efficiently learning the accuracy of labeling sources for selective sampling Many scalable data mining tasks rely on active learning to provide the most useful accurately labeled instances . However , what if there are multiple labeling sources ` oracles ' or ` experts ' with different but unknown reliabilities ? With the recent advent of inexpensive and scalable online annotation tools , such as Amazon 's Mechanical Turk , the labeling process has become more vulnerable to noise - and without prior knowledge of the accuracy of each individual labeler . This paper addresses exactly such a challenge : how to jointly learn the accuracy of labeling sources and obtain the most informative labels for the active learning task at hand minimizing total labeling effort . More specifically , we present IEThresh Interval Estimate Threshold as a strategy to intelligently select the expert s with the highest estimated labeling accuracy . IEThresh estimates a confidence interval for the reliability of each expert and filters out the one s whose estimated upper-bound confidence interval is below a threshold - which jointly optimizes expected accuracy mean and need to better estimate the expert 's accuracy variance . Our framework is flexible enough to work with a wide range of different noise levels and outperforms baselines such as asking all available experts and random expert selection . In particular , IEThresh achieves a given level of accuracy with less than half the queries issued by all-experts labeling and less than a third the queries required by random expert selection on datasets such as the UCI mushroom one . The results show that our method naturally balances exploration and exploitation as it gains knowledge of which experts to rely upon , and selects them with increasing frequency .", "keywords": ["labeler selection", "estimation", "active learning", "noisy labelers"], "combined": "Efficiently learning the accuracy of labeling sources for selective sampling Many scalable data mining tasks rely on active learning to provide the most useful accurately labeled instances . However , what if there are multiple labeling sources ` oracles ' or ` experts ' with different but unknown reliabilities ? With the recent advent of inexpensive and scalable online annotation tools , such as Amazon 's Mechanical Turk , the labeling process has become more vulnerable to noise - and without prior knowledge of the accuracy of each individual labeler . This paper addresses exactly such a challenge : how to jointly learn the accuracy of labeling sources and obtain the most informative labels for the active learning task at hand minimizing total labeling effort . More specifically , we present IEThresh Interval Estimate Threshold as a strategy to intelligently select the expert s with the highest estimated labeling accuracy . IEThresh estimates a confidence interval for the reliability of each expert and filters out the one s whose estimated upper-bound confidence interval is below a threshold - which jointly optimizes expected accuracy mean and need to better estimate the expert 's accuracy variance . Our framework is flexible enough to work with a wide range of different noise levels and outperforms baselines such as asking all available experts and random expert selection . In particular , IEThresh achieves a given level of accuracy with less than half the queries issued by all-experts labeling and less than a third the queries required by random expert selection on datasets such as the UCI mushroom one . The results show that our method naturally balances exploration and exploitation as it gains knowledge of which experts to rely upon , and selects them with increasing frequency . [[EENNDD]] labeler selection; estimation; active learning; noisy labelers"}, "Mempelajari ketepatan sumber pelabelan dengan berkesan untuk pensampelan selektif Banyak tugas perlombongan data berskala bergantung pada pembelajaran aktif untuk memberikan contoh berlabel tepat yang paling berguna. Namun, bagaimana jika terdapat banyak sumber pelabelan \"oracle\" atau \"pakar\" dengan kebolehpercayaan yang berbeza tetapi tidak diketahui? Dengan munculnya alat anotasi dalam talian yang murah dan terukur, seperti Amazon Mechanical Turk, proses pelabelan menjadi lebih rentan terhadap kebisingan - dan tanpa pengetahuan awal mengenai ketepatan setiap pelabelan individu. Makalah ini menangani cabaran seperti itu: bagaimana untuk bersama-sama mempelajari ketepatan sumber pelabelan dan mendapatkan label yang paling bermaklumat untuk tugas pembelajaran aktif yang dapat meminimumkan jumlah usaha pelabelan. Lebih khusus lagi, kami menyajikan IEThresh Interval Estimate Threshold sebagai strategi untuk memilih pakar dengan bijak dengan anggaran ketepatan label tertinggi. IEThresh menganggarkan selang keyakinan untuk kebolehpercayaan setiap pakar dan menyaring satu yang anggaran selang keyakinan atasnya berada di bawah ambang - yang bersama-sama mengoptimumkan jangkaan ketepatan yang diharapkan dan perlu menganggarkan varians ketepatan pakar dengan lebih baik. Rangka kerja kami cukup fleksibel untuk bekerja dengan pelbagai tahap kebisingan yang berbeza dan mengatasi garis dasar seperti meminta semua pakar yang ada dan pemilihan pakar secara rawak. Khususnya, IEThresh mencapai tahap ketepatan yang diberikan dengan kurang daripada separuh pertanyaan yang dikeluarkan oleh pelabelan semua pakar dan kurang dari satu pertiga pertanyaan yang diperlukan oleh pemilihan pakar secara rawak pada set data seperti UCI jamur satu. Hasilnya menunjukkan bahawa kaedah kami secara semula jadi menyeimbangkan eksplorasi dan eksploitasi kerana memperoleh pengetahuan tentang mana pakar bergantung, dan memilih mereka dengan frekuensi yang semakin meningkat. [[EENNDD]] pemilihan pelabel; anggaran; pembelajaran aktif; pelabel yang bising"], [{"string": "Mining , indexing , and querying historical spatiotemporal data In many applications that track and analyze spatiotemporal data , movements obey periodic patterns ; the objects follow the same routes approximately over regular time intervals . For example , people wake up at the same time and follow more or less the same route to their work everyday . The discovery of hidden periodic patterns in spatiotemporal data , apart from unveiling important information to the data analyst , can facilitate data management substantially . Based on this observation , we propose a framework that analyzes , manages , and queries object movements that follow such patterns . We define the spatiotemporal periodic pattern mining problem and propose an effective and fast mining algorithm for retrieving maximal periodic patterns . We also devise a novel , specialized index structure that can benefit from the discovered patterns to support more efficient execution of spatiotemporal queries . We evaluate our methods experimentally using datasets with object trajectories that exhibit periodicity .", "keywords": ["trajectories", "indexing", "spatiotemporal data", "pattern mining"], "combined": "Mining , indexing , and querying historical spatiotemporal data In many applications that track and analyze spatiotemporal data , movements obey periodic patterns ; the objects follow the same routes approximately over regular time intervals . For example , people wake up at the same time and follow more or less the same route to their work everyday . The discovery of hidden periodic patterns in spatiotemporal data , apart from unveiling important information to the data analyst , can facilitate data management substantially . Based on this observation , we propose a framework that analyzes , manages , and queries object movements that follow such patterns . We define the spatiotemporal periodic pattern mining problem and propose an effective and fast mining algorithm for retrieving maximal periodic patterns . We also devise a novel , specialized index structure that can benefit from the discovered patterns to support more efficient execution of spatiotemporal queries . We evaluate our methods experimentally using datasets with object trajectories that exhibit periodicity . [[EENNDD]] trajectories; indexing; spatiotemporal data; pattern mining"}, "Perlombongan, pengindeksan, dan pertanyaan data spatiotemporal sejarah Dalam banyak aplikasi yang mengesan dan menganalisis data spatiotemporal, pergerakan mematuhi corak berkala; objek mengikuti laluan yang sama lebih kurang dalam selang waktu biasa Contohnya, orang bangun pada waktu yang sama dan mengikuti lebih kurang jalan yang sama ke tempat kerja mereka setiap hari. Penemuan corak berkala tersembunyi dalam data spatiotemporal, selain daripada menyingkap maklumat penting kepada penganalisis data, dapat memudahkan pengurusan data dengan ketara. Berdasarkan pemerhatian ini, kami mencadangkan kerangka kerja yang menganalisis, mengurus, dan menanyakan pergerakan objek yang mengikuti corak tersebut. Kami menentukan masalah perlombongan corak berkala spatiotemporal dan mencadangkan algoritma perlombongan yang berkesan dan pantas untuk mendapatkan corak berkala maksimum. Kami juga merancang novel, struktur indeks khusus yang dapat memanfaatkan corak yang dijumpai untuk menyokong pelaksanaan pertanyaan spatiotemporal yang lebih efisien. Kami menilai kaedah kami secara eksperimen menggunakan set data dengan lintasan objek yang menunjukkan berkala. [[EENNDD]] lintasan; pengindeksan; data spatiotemporal; perlombongan corak"], [{"string": "Direct mining of discriminative and essential frequent patterns via model-based search tree Frequent patterns provide solutions to datasets that do not have well-structured feature vectors . However , frequent pattern mining is non-trivial since the number of unique patterns is exponential but many are non-discriminative and correlated . Currently , frequent pattern mining is performed in two sequential steps : enumerating a set of frequent patterns , followed by feature selection . Although many methods have been proposed in the past few years on how to perform each separate step efficiently , there is still limited success in eventually finding highly compact and discriminative patterns . The culprit is due to the inherent nature of this widely adopted two-step approach . This paper discusses these problems and proposes a new and different method . It builds a decision tree that partitions the data onto different nodes . Then at each node , it directly discovers a discriminative pattern to further divide its examples into purer subsets . Since the number of examples towards leaf level is relatively small , the new approach is able to examine patterns with extremely low global support that could not be enumerated on the whole dataset by the two-step method . The discovered feature vectors are more accurate on some of the most difficult graph as well as frequent itemset problems than most recently proposed algorithms but the total size is typically 50 % or more smaller . Importantly , the minimum support of some discriminative patterns can be extremely low e.g. 0.03 % . In order to enumerate these low support patterns , state-of-the-art frequent pattern algorithm either can not finish due to huge memory consumption or have to enumerate 101 to 103 times more patterns before they can even be found . Software and datasets are available by contacting the author .", "keywords": ["itemsets", "scalability", "graph mining", "frequent pattern"], "combined": "Direct mining of discriminative and essential frequent patterns via model-based search tree Frequent patterns provide solutions to datasets that do not have well-structured feature vectors . However , frequent pattern mining is non-trivial since the number of unique patterns is exponential but many are non-discriminative and correlated . Currently , frequent pattern mining is performed in two sequential steps : enumerating a set of frequent patterns , followed by feature selection . Although many methods have been proposed in the past few years on how to perform each separate step efficiently , there is still limited success in eventually finding highly compact and discriminative patterns . The culprit is due to the inherent nature of this widely adopted two-step approach . This paper discusses these problems and proposes a new and different method . It builds a decision tree that partitions the data onto different nodes . Then at each node , it directly discovers a discriminative pattern to further divide its examples into purer subsets . Since the number of examples towards leaf level is relatively small , the new approach is able to examine patterns with extremely low global support that could not be enumerated on the whole dataset by the two-step method . The discovered feature vectors are more accurate on some of the most difficult graph as well as frequent itemset problems than most recently proposed algorithms but the total size is typically 50 % or more smaller . Importantly , the minimum support of some discriminative patterns can be extremely low e.g. 0.03 % . In order to enumerate these low support patterns , state-of-the-art frequent pattern algorithm either can not finish due to huge memory consumption or have to enumerate 101 to 103 times more patterns before they can even be found . Software and datasets are available by contacting the author . [[EENNDD]] itemsets; scalability; graph mining; frequent pattern"}, "Perlombongan langsung pola diskriminasi dan kerap penting melalui pokok carian berasaskan model Corak yang kerap memberikan penyelesaian kepada set data yang tidak mempunyai vektor ciri berstruktur dengan baik. Walau bagaimanapun, perlombongan corak yang kerap tidak sepele kerana bilangan corak unik adalah eksponen tetapi banyak yang tidak diskriminatif dan berkorelasi. Pada masa ini, perlombongan corak kerap dilakukan dalam dua langkah yang berurutan: menghitung sekumpulan corak yang kerap, diikuti dengan pemilihan fitur. Walaupun banyak kaedah telah diusulkan dalam beberapa tahun terakhir tentang bagaimana melakukan setiap langkah terpisah dengan cekap, masih ada keberhasilan yang terbatas dalam akhirnya menemukan pola yang sangat padat dan diskriminatif. Pelakunya adalah kerana sifat pendekatan dua langkah yang banyak digunakan ini. Makalah ini membincangkan masalah ini dan mencadangkan kaedah baru dan berbeza. Ia membina keputusan yang memisahkan data ke nod yang berlainan. Kemudian pada setiap simpul, ia secara langsung menemui corak diskriminatif untuk membahagikan contohnya menjadi subset yang lebih murni. Oleh kerana bilangan contoh ke arah daun agak kecil, pendekatan baru dapat memeriksa corak dengan sokongan global yang sangat rendah yang tidak dapat dihitung pada keseluruhan dataset dengan kaedah dua langkah. Vektor ciri yang dijumpai lebih tepat pada beberapa graf yang paling sukar serta masalah itemet yang kerap daripada algoritma yang dicadangkan baru-baru ini tetapi ukuran keseluruhan biasanya 50% atau lebih kecil. Yang penting, sokongan minimum beberapa corak diskriminasi boleh menjadi sangat rendah, mis. 0.03%. Untuk menghitung corak sokongan rendah ini, algoritma corak kerap canggih tidak dapat diselesaikan kerana penggunaan memori yang besar atau harus menghitung 101 hingga 103 kali lebih banyak corak sebelum ia dapat dijumpai. Perisian dan set data tersedia dengan menghubungi pengarang. [[EENNDD]] set item; skalabiliti; perlombongan grafik; corak yang kerap"], [{"string": "Group formation in large social networks : membership , growth , and evolution The processes by which communities come together , attract new members , and develop over time is a central research issue in the social sciences - political movements , professional organizations , and religious denominations all provide fundamental examples of such communities . In the digital domain , on-line groups are becoming increasingly prominent due to the growth of community and social networking sites such as MySpace and LiveJournal . However , the challenge of collecting and analyzing large-scale time-resolved data on social groups and communities has left most basic questions about the evolution of such groups largely unresolved : what are the structural features that influence whether individuals will join communities , which communities will grow rapidly , and how do the overlaps among pairs of communities change over time . Here we address these questions using two large sources of data : friendship links and community membership on LiveJournal , and co-authorship and conference publications in DBLP . Both of these datasets provide explicit user-defined communities , where conferences serve as proxies for communities in DBLP . We study how the evolution of these communities relates to properties such as the structure of the underlying social networks . We find that the propensity of individuals to join communities , and of communities to grow rapidly , depends in subtle ways on the underlying network structure . For example , the tendency of an individual to join a community is influenced not just by the number of friends he or she has within the community , but also crucially by how those friends are connected to one another . We use decision-tree techniques to identify the most significant structural determinants of these properties . We also develop a novel methodology for measuring movement of individuals between communities , and show how such movements are closely aligned with changes in the topics of interest within the communities .", "keywords": ["social networks", "diffusion of innovations", "on-line communities"], "combined": "Group formation in large social networks : membership , growth , and evolution The processes by which communities come together , attract new members , and develop over time is a central research issue in the social sciences - political movements , professional organizations , and religious denominations all provide fundamental examples of such communities . In the digital domain , on-line groups are becoming increasingly prominent due to the growth of community and social networking sites such as MySpace and LiveJournal . However , the challenge of collecting and analyzing large-scale time-resolved data on social groups and communities has left most basic questions about the evolution of such groups largely unresolved : what are the structural features that influence whether individuals will join communities , which communities will grow rapidly , and how do the overlaps among pairs of communities change over time . Here we address these questions using two large sources of data : friendship links and community membership on LiveJournal , and co-authorship and conference publications in DBLP . Both of these datasets provide explicit user-defined communities , where conferences serve as proxies for communities in DBLP . We study how the evolution of these communities relates to properties such as the structure of the underlying social networks . We find that the propensity of individuals to join communities , and of communities to grow rapidly , depends in subtle ways on the underlying network structure . For example , the tendency of an individual to join a community is influenced not just by the number of friends he or she has within the community , but also crucially by how those friends are connected to one another . We use decision-tree techniques to identify the most significant structural determinants of these properties . We also develop a novel methodology for measuring movement of individuals between communities , and show how such movements are closely aligned with changes in the topics of interest within the communities . [[EENNDD]] social networks; diffusion of innovations; on-line communities"}, "Pembentukan kumpulan dalam rangkaian sosial yang besar: keanggotaan, pertumbuhan, dan evolusi Proses di mana komuniti bersatu, menarik anggota baru, dan berkembang dari masa ke masa adalah isu penyelidikan utama dalam sains sosial - gerakan politik, organisasi profesional, dan denominasi agama semuanya menyediakan contoh asas komuniti tersebut. Dalam domain digital, kumpulan on-line menjadi semakin terkenal kerana pertumbuhan laman web komuniti dan sosial seperti MySpace dan LiveJournal. Walau bagaimanapun, cabaran mengumpul dan menganalisis data berskala besar pada kumpulan sosial dan komuniti telah menyebabkan kebanyakan persoalan asas mengenai evolusi kumpulan tersebut tidak dapat diselesaikan: apakah ciri struktur yang mempengaruhi sama ada individu akan bergabung dengan komuniti, komuniti mana yang akan berkembang pesat, dan bagaimana pertindihan antara pasangan masyarakat berubah dari masa ke masa. Di sini kita membahas persoalan ini menggunakan dua sumber data yang besar: tautan persahabatan dan keahlian komuniti di LiveJournal, dan penerbitan bersama dan penerbitan persidangan di DBLP. Kedua-dua set data ini menyediakan komuniti yang ditentukan pengguna secara eksplisit, di mana persidangan berfungsi sebagai proksi untuk komuniti di DBLP. Kami mengkaji bagaimana evolusi komuniti ini berkaitan dengan sifat seperti struktur rangkaian sosial yang mendasari. Kami mendapati bahawa kecenderungan individu untuk bergabung dengan komuniti, dan masyarakat untuk berkembang pesat, bergantung pada cara halus pada struktur rangkaian yang mendasari. Sebagai contoh, kecenderungan seseorang untuk bergabung dalam komuniti dipengaruhi bukan hanya oleh jumlah rakan yang dia ada dalam komuniti, tetapi juga sangat penting oleh bagaimana rakan-rakan itu saling berhubungan satu sama lain. Kami menggunakan teknik keputusan-keputusan untuk mengenal pasti penentu struktur yang paling ketara dari sifat ini. Kami juga mengembangkan metodologi baru untuk mengukur pergerakan individu di antara komuniti, dan menunjukkan bagaimana pergerakan tersebut berkait rapat dengan perubahan topik yang menarik dalam komuniti. [[EENNDD]] rangkaian sosial; penyebaran inovasi; komuniti dalam talian"], [{"string": "Random projection in dimensionality reduction : applications to image and text data Random projections have recently emerged as a powerful method for dimensionality reduction . Theoretical results indicate that the method preserves distances quite nicely ; however , empirical results are sparse . We present experimental results on using random projection as a dimensionality reduction tool in a number of cases , where the high dimensionality of the data would otherwise lead to burden-some computations . Our application areas are the processing of both noisy and noiseless images , and information retrieval in text documents . We show that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis : the similarity of data vectors is preserved well under random projection . However , using random projections is computationally significantly less expensive than using , e.g. , principal component analysis . We also show experimentally that using a sparse random matrix gives additional computational savings in random projection .", "keywords": ["random projection", "high-dimensional data", "image data", "dimensionality reduction", "text document data"], "combined": "Random projection in dimensionality reduction : applications to image and text data Random projections have recently emerged as a powerful method for dimensionality reduction . Theoretical results indicate that the method preserves distances quite nicely ; however , empirical results are sparse . We present experimental results on using random projection as a dimensionality reduction tool in a number of cases , where the high dimensionality of the data would otherwise lead to burden-some computations . Our application areas are the processing of both noisy and noiseless images , and information retrieval in text documents . We show that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis : the similarity of data vectors is preserved well under random projection . However , using random projections is computationally significantly less expensive than using , e.g. , principal component analysis . We also show experimentally that using a sparse random matrix gives additional computational savings in random projection . [[EENNDD]] random projection; high-dimensional data; image data; dimensionality reduction; text document data"}, "Unjuran secara rawak dalam pengurangan dimensi: aplikasi untuk data gambar dan teks Unjuran secara rawak baru-baru ini muncul sebagai kaedah yang kuat untuk pengurangan dimensi. Hasil teori menunjukkan bahawa kaedah ini mengekalkan jarak dengan baik; namun, keputusan empirikal jarang. Kami menyajikan hasil eksperimen menggunakan unjuran rawak sebagai alat pengurangan dimensi dalam beberapa kes, di mana dimensi tinggi data sebaliknya akan menyebabkan pengiraan yang membebankan. Kawasan aplikasi kami adalah pemprosesan gambar yang bising dan tidak bersuara, dan pengambilan maklumat dalam dokumen teks. Kami menunjukkan bahawa memproyeksikan data ke ruang bawah dimensi rawak menghasilkan hasil yang setanding dengan kaedah pengurangan dimensi konvensional seperti analisis komponen utama: kesamaan vektor data dipelihara dengan baik di bawah unjuran rawak. Walau bagaimanapun, menggunakan unjuran secara rawak jauh lebih murah daripada menggunakan, mis. , analisis komponen utama. Kami juga menunjukkan secara eksperimen bahawa menggunakan matriks rawak jarang memberikan penjimatan komputasi tambahan dalam unjuran rawak. [[EENNDD]] unjuran rawak; data dimensi tinggi; data gambar; pengurangan dimensi; data dokumen teks"], [{"string": "Feature selection in unsupervised learning via evolutionary search", "keywords": ["evolutionary search", "feature selection"], "combined": "Feature selection in unsupervised learning via evolutionary search [[EENNDD]] evolutionary search; feature selection"}, "Pemilihan ciri dalam pembelajaran tanpa pengawasan melalui carian evolusi [[EENNDD]] carian evolusi; pemilihan ciri"], [{"string": "Mining optimal decision trees from itemset lattices We present DL8 , an exact algorithm for finding a decision tree that optimizes a ranking function under size , depth , accuracy and leaf constraints . Because the discovery of optimal trees has high theoretical complexity , until now few efforts have been made to compute such trees for real-world datasets . An exact algorithm is of both scientific and practical interest . From a scientific point of view , it can be used as a gold standard to evaluate the performance of heuristic constraint-based decision tree learners and to gain new insight in traditional decision tree learners . From the application point of view , it can be used to discover trees that can not be found by heuristic decision tree learners . The key idea behind our algorithm is that there is a relation between constraints on decision trees and constraints on itemsets . We show that optimal decision trees can be extracted from lattices of itemsets in linear time . We give several strategies to efficiently build these lattices . Experiments show that under the same constraints , DL8 obtains better results than C4 .5 , which confirms that exhaustive search does not always imply overfitting . The results also show that DL8 is a useful and interesting tool to learn decision trees under constraints .", "keywords": ["constraint-based mining", "frequent itemsets", "decision trees", "formal concepts"], "combined": "Mining optimal decision trees from itemset lattices We present DL8 , an exact algorithm for finding a decision tree that optimizes a ranking function under size , depth , accuracy and leaf constraints . Because the discovery of optimal trees has high theoretical complexity , until now few efforts have been made to compute such trees for real-world datasets . An exact algorithm is of both scientific and practical interest . From a scientific point of view , it can be used as a gold standard to evaluate the performance of heuristic constraint-based decision tree learners and to gain new insight in traditional decision tree learners . From the application point of view , it can be used to discover trees that can not be found by heuristic decision tree learners . The key idea behind our algorithm is that there is a relation between constraints on decision trees and constraints on itemsets . We show that optimal decision trees can be extracted from lattices of itemsets in linear time . We give several strategies to efficiently build these lattices . Experiments show that under the same constraints , DL8 obtains better results than C4 .5 , which confirms that exhaustive search does not always imply overfitting . The results also show that DL8 is a useful and interesting tool to learn decision trees under constraints . [[EENNDD]] constraint-based mining; frequent itemsets; decision trees; formal concepts"}, "Menambang pohon keputusan yang optimum dari kisi itemet Kami menyajikan DL8, algoritma tepat untuk mencari pokok keputusan yang mengoptimumkan fungsi peringkat di bawah ukuran, kedalaman, ketepatan dan kekangan daun. Oleh kerana penemuan pokok yang optimum mempunyai kerumitan teori yang tinggi, hingga kini sedikit usaha dilakukan untuk mengira pokok-pokok tersebut untuk kumpulan data dunia nyata. Algoritma yang tepat mempunyai kepentingan saintifik dan praktikal. Dari sudut pandang ilmiah, ia dapat digunakan sebagai piawai emas untuk menilai prestasi pelajar pohon keputusan berdasarkan kekangan heuristik dan untuk mendapatkan wawasan baru dalam pelajar pohon keputusan tradisional. Dari sudut aplikasi, ia dapat digunakan untuk mencari pokok yang tidak dapat dijumpai oleh pelajar pohon keputusan heuristik. Idea utama di sebalik algoritma kami adalah bahawa terdapat hubungan antara kekangan pada pokok keputusan dan kekangan pada item item. Kami menunjukkan bahawa pohon keputusan yang optimum dapat diekstrak dari kisi set barang dalam masa linear. Kami memberikan beberapa strategi untuk membina kisi ini dengan cekap. Eksperimen menunjukkan bahawa di bawah kekangan yang sama, DL8 memperoleh hasil yang lebih baik daripada C4 .5, yang mengesahkan bahawa pencarian yang menyeluruh tidak selalu menunjukkan kelebihan. Hasilnya juga menunjukkan bahawa DL8 adalah alat yang berguna dan menarik untuk mempelajari pokok keputusan di bawah kekangan. [[EENNDD]] perlombongan berasaskan kekangan; set barang yang kerap; pokok keputusan; konsep formal"], [{"string": "The cost of privacy : destruction of data-mining utility in anonymized data publishing Re-identification is a major privacy threat to public datasets containing individual records . Many privacy protection algorithms rely on generalization and suppression of `` quasi-identifier '' attributes such as ZIP code and birthdate . Their objective is usually syntactic sanitization : for example , k-anonymity requires that each `` quasi-identifier '' tuple appear in at least k records , while l-diversity requires that the distribution of sensitive attributes for each quasi-identifier have high entropy . The utility of sanitized data is also measured syntactically , by the number of generalization steps applied or the number of records with the same quasi-identifier . In this paper , we ask whether generalization and suppression of quasi-identifiers offer any benefits over trivial sanitization which simply separates quasi-identifiers from sensitive attributes . Previous work showed that k-anonymous databases can be useful for data mining , but k-anonymization does not guarantee any privacy . By contrast , we measure the tradeoff between privacy how much can the adversary learn from the sanitized records ? and utility , measured as accuracy of data-mining algorithms executed on the same sanitized records . For our experimental evaluation , we use the same datasets from the UCI machine learning repository as were used in previous research on generalization and suppression . Our results demonstrate that even modest privacy gains require almost complete destruction of the data-mining utility . In most cases , trivial sanitization provides equivalent utility and better privacy than k-anonymity , l-diversity , and similar methods based on generalization and suppression .", "keywords": ["anonymity", "privacy", "utility"], "combined": "The cost of privacy : destruction of data-mining utility in anonymized data publishing Re-identification is a major privacy threat to public datasets containing individual records . Many privacy protection algorithms rely on generalization and suppression of `` quasi-identifier '' attributes such as ZIP code and birthdate . Their objective is usually syntactic sanitization : for example , k-anonymity requires that each `` quasi-identifier '' tuple appear in at least k records , while l-diversity requires that the distribution of sensitive attributes for each quasi-identifier have high entropy . The utility of sanitized data is also measured syntactically , by the number of generalization steps applied or the number of records with the same quasi-identifier . In this paper , we ask whether generalization and suppression of quasi-identifiers offer any benefits over trivial sanitization which simply separates quasi-identifiers from sensitive attributes . Previous work showed that k-anonymous databases can be useful for data mining , but k-anonymization does not guarantee any privacy . By contrast , we measure the tradeoff between privacy how much can the adversary learn from the sanitized records ? and utility , measured as accuracy of data-mining algorithms executed on the same sanitized records . For our experimental evaluation , we use the same datasets from the UCI machine learning repository as were used in previous research on generalization and suppression . Our results demonstrate that even modest privacy gains require almost complete destruction of the data-mining utility . In most cases , trivial sanitization provides equivalent utility and better privacy than k-anonymity , l-diversity , and similar methods based on generalization and suppression . [[EENNDD]] anonymity; privacy; utility"}, "Kos privasi: pemusnahan utiliti perlombongan data dalam penerbitan data tanpa nama. Pengenalan semula adalah ancaman privasi utama kepada kumpulan data awam yang mengandungi rekod individu. Banyak algoritma perlindungan privasi bergantung pada generalisasi dan penekanan atribut \"kuasi-identifier\" seperti kod ZIP dan tarikh lahir. Objektif mereka biasanya adalah pembersihan sintaksis: sebagai contoh, k-anonymity mensyaratkan setiap tuple \"kuasi-pengenal\" muncul dalam sekurang-kurangnya catatan k, sementara kepelbagaian l memerlukan penyebaran atribut sensitif untuk setiap pengenal kuasi mempunyai entropi tinggi . Utiliti data yang dibersihkan juga diukur secara sintaksis, dengan jumlah langkah generalisasi yang diterapkan atau jumlah rekod dengan pengenal kuasi yang sama. Dalam makalah ini, kami bertanya apakah generalisasi dan penekanan pengenal kuasi menawarkan apa-apa faedah berbanding pembersihan sepele yang hanya memisahkan pengenal kuasi dari atribut sensitif. Karya sebelumnya menunjukkan bahawa pangkalan data k-anonim dapat berguna untuk perlombongan data, tetapi k-anonimisasi tidak menjamin privasi apa pun. Sebaliknya, kami mengukur pertukaran antara privasi berapa banyak yang dapat dipelajari oleh musuh dari catatan yang dibersihkan? dan utiliti, diukur sebagai ketepatan algoritma perlombongan data yang dilaksanakan pada rekod yang sama. Untuk penilaian eksperimental kami, kami menggunakan set data yang sama dari repositori pembelajaran mesin UCI seperti yang digunakan dalam penyelidikan sebelumnya mengenai generalisasi dan penindasan. Hasil kajian kami menunjukkan bahawa keuntungan privasi yang sederhana memerlukan penghancuran hampir keseluruhan utiliti perlombongan data. Dalam kebanyakan kes, pembersihan sepele menyediakan utiliti yang setara dan privasi yang lebih baik daripada k-anonimiti, kepelbagaian l, dan kaedah serupa berdasarkan generalisasi dan penindasan. [[EENNDD]] tanpa nama; privasi; utiliti"], [{"string": "Anonymity-preserving data collection Protection of privacy has become an important problem in data mining . In particular , individuals have become increasingly unwilling to share their data , frequently resulting in individuals either refusing to share their data or providing incorrect data . In turn , such problems in data collection can affect the success of data mining , which relies on sufficient amounts of accurate data in order to produce meaningful results . Random perturbation and randomized response techniques can provide some level of privacy in data collection , but they have an associated cost in accuracy . Cryptographic privacy-preserving data mining methods provide good privacy and accuracy properties . However , in order to be efficient , those solutions must be tailored to specific mining tasks , thereby losing generality . In this paper , we propose efficient cryptographic techniques for online data collection in which data from a large number of respondents is collected anonymously , without the help of a trusted third party . That is , our solution allows the miner to collect the original data from each respondent , but in such a way that the miner can not link a respondent 's data to the respondent . An advantage of such a solution is that , because it does not change the actual data , its success does not depend on the underlying data mining problem . We provide proofs of the correctness and privacy of our solution , as well as experimental data that demonstrates its efficiency . We also extend our solution to tolerate certain kinds of malicious behavior of the participants .", "keywords": ["anonymity", "data mining", "data encryption", "data collection"], "combined": "Anonymity-preserving data collection Protection of privacy has become an important problem in data mining . In particular , individuals have become increasingly unwilling to share their data , frequently resulting in individuals either refusing to share their data or providing incorrect data . In turn , such problems in data collection can affect the success of data mining , which relies on sufficient amounts of accurate data in order to produce meaningful results . Random perturbation and randomized response techniques can provide some level of privacy in data collection , but they have an associated cost in accuracy . Cryptographic privacy-preserving data mining methods provide good privacy and accuracy properties . However , in order to be efficient , those solutions must be tailored to specific mining tasks , thereby losing generality . In this paper , we propose efficient cryptographic techniques for online data collection in which data from a large number of respondents is collected anonymously , without the help of a trusted third party . That is , our solution allows the miner to collect the original data from each respondent , but in such a way that the miner can not link a respondent 's data to the respondent . An advantage of such a solution is that , because it does not change the actual data , its success does not depend on the underlying data mining problem . We provide proofs of the correctness and privacy of our solution , as well as experimental data that demonstrates its efficiency . We also extend our solution to tolerate certain kinds of malicious behavior of the participants . [[EENNDD]] anonymity; data mining; data encryption; data collection"}, "Pengumpulan data pelindung nama tanpa nama Perlindungan privasi telah menjadi masalah penting dalam perlombongan data. Khususnya, individu semakin tidak mahu berkongsi data mereka, yang sering mengakibatkan individu enggan berkongsi data mereka atau memberikan data yang salah. Sebaliknya, masalah dalam pengumpulan data dapat mempengaruhi kejayaan perlombongan data, yang bergantung pada jumlah data yang tepat untuk menghasilkan hasil yang bermakna. Gangguan rawak dan teknik tindak balas rawak dapat memberikan beberapa tahap privasi dalam pengumpulan data, tetapi mereka mempunyai biaya yang tepat dalam ketepatan. Kaedah perlombongan data pemeliharaan privasi kriptografi memberikan sifat privasi dan ketepatan yang baik. Walau bagaimanapun, untuk menjadi cekap, penyelesaian tersebut mesti disesuaikan dengan tugas perlombongan tertentu, sehingga kehilangan keseluruhan. Dalam makalah ini, kami mencadangkan teknik kriptografi yang efisien untuk pengumpulan data dalam talian di mana data dari sebilangan besar responden dikumpulkan tanpa nama, tanpa bantuan pihak ketiga yang dipercayai. Maksudnya, penyelesaian kami membolehkan pelombong mengumpulkan data asal dari setiap responden, tetapi sedemikian rupa sehingga pelombong tidak dapat menghubungkan data responden dengan responden. Kelebihan penyelesaian seperti itu ialah, kerana tidak mengubah data sebenar, kejayaannya tidak bergantung pada masalah perlombongan data yang mendasari. Kami memberikan bukti kebenaran dan privasi penyelesaian kami, serta data eksperimen yang menunjukkan kecekapannya. Kami juga memberikan jalan keluar kami untuk bertoleransi dengan tingkah laku jahat peserta. [[EENNDD]] tanpa nama; perlombongan data; penyulitan data; pengumpulan data"], [{"string": "Efficient decision tree construction on streaming data Decision tree construction is a well studied problem in data mining . Recently , there has been much interest in mining streaming data . Domingos and Hulten have presented a one-pass algorithm for decision tree construction . Their work uses Hoeffding inequality to achieve a probabilistic bound on the accuracy of the tree constructed . In this paper , we revisit this problem . We make the following two contributions : 1 We present a numerical interval pruning NIP approach for efficiently processing numerical attributes . Our results show an average of 39 % reduction in execution times . 2 We exploit the properties of the gain function entropy and gini to reduce the sample size required for obtaining a given bound on the accuracy . Our experimental results show a 37 % reduction in the number of data instances required .", "keywords": ["streaming data", "decision tree", "learning", "database applications", "sampling"], "combined": "Efficient decision tree construction on streaming data Decision tree construction is a well studied problem in data mining . Recently , there has been much interest in mining streaming data . Domingos and Hulten have presented a one-pass algorithm for decision tree construction . Their work uses Hoeffding inequality to achieve a probabilistic bound on the accuracy of the tree constructed . In this paper , we revisit this problem . We make the following two contributions : 1 We present a numerical interval pruning NIP approach for efficiently processing numerical attributes . Our results show an average of 39 % reduction in execution times . 2 We exploit the properties of the gain function entropy and gini to reduce the sample size required for obtaining a given bound on the accuracy . Our experimental results show a 37 % reduction in the number of data instances required . [[EENNDD]] streaming data; decision tree; learning; database applications; sampling"}, "Pembinaan pohon keputusan yang cekap untuk streaming data Pembinaan pokok keputusan adalah masalah yang dikaji dengan baik dalam perlombongan data. Baru-baru ini, terdapat banyak minat untuk melombong data streaming. Domingos dan Hulten telah mengemukakan algoritma satu pas untuk pembinaan pokok keputusan. Karya mereka menggunakan ketaksamaan Hoeffding untuk mencapai kebarangkalian terikat pada ketepatan pokok yang dibina. Dalam makalah ini, kami mengkaji semula masalah ini. Kami memberikan dua sumbangan berikut: 1 Kami menyajikan pendekatan NIP pemangkasan selang berangka untuk memproses atribut numerik dengan cekap. Hasil kami menunjukkan pengurangan 39% pada masa pelaksanaan secara purata. 2 Kami mengeksploitasi sifat-sifat fungsi perolehan entropi dan gini untuk mengurangkan ukuran sampel yang diperlukan untuk mendapatkan terikat tertentu pada ketepatan. Hasil eksperimen kami menunjukkan pengurangan 37% dalam jumlah kejadian data yang diperlukan. [[EENNDD]] streaming data; pokok keputusan; belajar; aplikasi pangkalan data; persampelan"], [{"string": "Efficient data reduction with EASE A variety of mining and analysis problems -- ranging from association-rule discovery to contingency table analysis to materialization of certain approximate datacubes -- involve the extraction of knowledge from a set of categorical count data . Such data can be viewed as a collection of `` transactions , '' where a transaction is a fixed-length vector of counts . Classical algorithms for solving count-data problems require one or more computationally intensive passes over the entire database and can be prohibitively slow . One effective method for dealing with this ever-worsening scalability problem is to run the algorithms on a small sample of the data . We present a new data-reduction algorithm , called EASE , for producing such a sample . Like the FAST algorithm introduced by Chen et al. , EASE is especially designed for count data applications . Both EASE and FAST take a relatively large initial random sample and then deterministically produce a subsample whose `` distance '' -- appropriately defined -- from the complete database is minimal . Unlike FAST , which obtains the final subsample by quasi-greedy descent , EASE uses epsilon-approximation methods to obtain the final subsample by a process of repeated halving . Experiments both in the context of association rule mining and classical \u03c72 contingency-table analysis show that EASE outperforms both FAST and simple random sampling , sometimes dramatically .", "keywords": ["association rules", "data streams", "olap", "frequency estimation", "database applications", "sampling", "count dataset"], "combined": "Efficient data reduction with EASE A variety of mining and analysis problems -- ranging from association-rule discovery to contingency table analysis to materialization of certain approximate datacubes -- involve the extraction of knowledge from a set of categorical count data . Such data can be viewed as a collection of `` transactions , '' where a transaction is a fixed-length vector of counts . Classical algorithms for solving count-data problems require one or more computationally intensive passes over the entire database and can be prohibitively slow . One effective method for dealing with this ever-worsening scalability problem is to run the algorithms on a small sample of the data . We present a new data-reduction algorithm , called EASE , for producing such a sample . Like the FAST algorithm introduced by Chen et al. , EASE is especially designed for count data applications . Both EASE and FAST take a relatively large initial random sample and then deterministically produce a subsample whose `` distance '' -- appropriately defined -- from the complete database is minimal . Unlike FAST , which obtains the final subsample by quasi-greedy descent , EASE uses epsilon-approximation methods to obtain the final subsample by a process of repeated halving . Experiments both in the context of association rule mining and classical \u03c72 contingency-table analysis show that EASE outperforms both FAST and simple random sampling , sometimes dramatically . [[EENNDD]] association rules; data streams; olap; frequency estimation; database applications; sampling; count dataset"}, "Pengurangan data yang cekap dengan MUDAH Berbagai masalah perlombongan dan analisis - mulai dari penemuan peraturan persatuan hingga analisis jadual kontingensi hingga terwujudnya datacubes anggaran tertentu - melibatkan pengekstrakan pengetahuan dari sekumpulan data kiraan kategori. Data tersebut dapat dilihat sebagai kumpulan \"transaksi,\" di mana transaksi adalah vektor jumlah tetap. Algoritma klasik untuk menyelesaikan masalah data kiraan memerlukan satu atau lebih hantaran intensif komputasi ke seluruh pangkalan data dan boleh menjadi lambat. Salah satu kaedah yang berkesan untuk menangani masalah skalabiliti yang semakin buruk ini adalah dengan menjalankan algoritma pada sampel kecil data. Kami menyajikan algoritma pengurangan data baru, yang disebut EASE, untuk menghasilkan sampel sedemikian. Seperti algoritma FAST yang diperkenalkan oleh Chen et al. , EASE direka khas untuk aplikasi data kiraan. Kedua-dua EASE dan FAST mengambil sampel rawak awal yang agak besar dan kemudian secara pasti menghasilkan subsampel yang \"jarak\" - yang ditentukan dengan tepat - dari pangkalan data lengkap adalah minimum. Tidak seperti FAST, yang memperoleh subsampel akhir dengan keturunan yang hampir rakus, EASE menggunakan kaedah penghampiran epsilon untuk mendapatkan subsampel akhir dengan proses pembelahan berulang. Eksperimen baik dalam konteks perlombongan peraturan persatuan dan analisis jadual kontingensi \u03c72 klasik menunjukkan bahawa EASE mengungguli persampelan rawak CEPAT dan sederhana, kadang-kadang secara dramatik. [[EENNDD]] peraturan persatuan; aliran data; olap; anggaran frekuensi; aplikasi pangkalan data; persampelan; hitung set data"], [{"string": "Large-scale behavioral targeting Behavioral targeting BT leverages historical user behavior to select the ads most relevant to users to display . The state-of-the-art of BT derives a linear Poisson regression model from fine-grained user behavioral data and predicts click-through rate CTR from user history . We designed and implemented a highly scalable and efficient solution to BT using Hadoop MapReduce framework . With our parallel algorithm and the resulting system , we can build above 450 BT-category models from the entire Yahoo 's user base within one day , the scale that one can not even imagine with prior systems . Moreover , our approach has yielded 20 % CTR lift over the existing production system by leveraging the well-grounded probabilistic model fitted from a much larger training dataset . Specifically , our major contributions include : 1 A MapReduce statistical learning algorithm and implementation that achieve optimal data parallelism , task parallelism , and load balance in spite of the typically skewed distribution of domain data . 2 An in-place feature vector generation algorithm with linear time complexity O n regardless of the granularity of sliding target window . 3 An in-memory caching scheme that significantly reduces the number of disk IOs to make large-scale learning practical . 4 Highly efficient data structures and sparse representations of models and data to enable fast model updates . We believe that our work makes significant contributions to solving large-scale machine learning problems of industrial relevance in general . Finally , we report comprehensive experimental results , using industrial proprietary codebase and datasets .", "keywords": ["design methodology", "behavioral targeting", "grid computing", "large-scale"], "combined": "Large-scale behavioral targeting Behavioral targeting BT leverages historical user behavior to select the ads most relevant to users to display . The state-of-the-art of BT derives a linear Poisson regression model from fine-grained user behavioral data and predicts click-through rate CTR from user history . We designed and implemented a highly scalable and efficient solution to BT using Hadoop MapReduce framework . With our parallel algorithm and the resulting system , we can build above 450 BT-category models from the entire Yahoo 's user base within one day , the scale that one can not even imagine with prior systems . Moreover , our approach has yielded 20 % CTR lift over the existing production system by leveraging the well-grounded probabilistic model fitted from a much larger training dataset . Specifically , our major contributions include : 1 A MapReduce statistical learning algorithm and implementation that achieve optimal data parallelism , task parallelism , and load balance in spite of the typically skewed distribution of domain data . 2 An in-place feature vector generation algorithm with linear time complexity O n regardless of the granularity of sliding target window . 3 An in-memory caching scheme that significantly reduces the number of disk IOs to make large-scale learning practical . 4 Highly efficient data structures and sparse representations of models and data to enable fast model updates . We believe that our work makes significant contributions to solving large-scale machine learning problems of industrial relevance in general . Finally , we report comprehensive experimental results , using industrial proprietary codebase and datasets . [[EENNDD]] design methodology; behavioral targeting; grid computing; large-scale"}, "Penargetan tingkah laku berskala besar Penyasaran tingkah laku BT memanfaatkan tingkah laku pengguna sejarah untuk memilih iklan yang paling relevan untuk dipaparkan oleh pengguna. Canggih BT memperoleh model regresi Poisson linear dari data tingkah laku pengguna yang halus dan meramalkan CTR kadar klik-tayang dari sejarah pengguna. Kami merancang dan melaksanakan penyelesaian yang sangat skalabel dan efisien untuk BT menggunakan kerangka Hadoop MapReduce. Dengan algoritma selari dan sistem yang dihasilkan, kita dapat membina di atas 450 model kategori BT dari seluruh pangkalan pengguna Yahoo dalam satu hari, skala yang tidak dapat dibayangkan oleh sistem sebelumnya. Lebih-lebih lagi, pendekatan kami telah menghasilkan peningkatan CTR 20% ke atas sistem pengeluaran yang ada dengan memanfaatkan model probabilistik beralasan yang dipasang dari kumpulan data latihan yang jauh lebih besar. Secara khusus, sumbangan utama kami meliputi: 1 Algoritma pembelajaran statistik MapReduce dan pelaksanaan yang mencapai keseimbangan data yang optimum, paralelisme tugas, dan keseimbangan beban terlepas dari sebaran data domain yang biasanya miring. 2 Algoritma penghasilan vektor ciri di tempat dengan kerumitan masa linear O n tanpa mengira butiran tetingkap sasaran gelongsor. 3 Skema cache dalam memori yang secara signifikan mengurangkan bilangan IO cakera untuk menjadikan pembelajaran berskala besar praktikal. 4 Struktur data yang sangat cekap dan perwakilan model dan data yang jarang untuk membolehkan kemas kini model cepat. Kami percaya bahawa kerja kami memberikan sumbangan yang besar untuk menyelesaikan masalah pembelajaran mesin berskala besar yang relevan dengan industri pada umumnya. Akhirnya, kami melaporkan hasil eksperimen yang komprehensif, menggunakan pangkalan data dan set data proprietari industri. [[EENNDD]] metodologi reka bentuk; penyasaran tingkah laku; pengkomputeran grid; berskala besar"], [{"string": "Adversarial learning Many classification tasks , such as spam filtering , intrusion detection , and terrorism detection , are complicated by an adversary who wishes to avoid detection . Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier 2 . In this paper , we introduce the adversarial classifier reverse engineering ACRE learning problem , the task of learning sufficient information about a classifier to construct adversarial attacks . We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering .", "keywords": ["linear classifiers", "adversarial classification", "spam", "miscellaneous"], "combined": "Adversarial learning Many classification tasks , such as spam filtering , intrusion detection , and terrorism detection , are complicated by an adversary who wishes to avoid detection . Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier 2 . In this paper , we introduce the adversarial classifier reverse engineering ACRE learning problem , the task of learning sufficient information about a classifier to construct adversarial attacks . We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering . [[EENNDD]] linear classifiers; adversarial classification; spam; miscellaneous"}, "Pembelajaran Adversarial Banyak tugas klasifikasi, seperti penyaringan spam, pengesanan pencerobohan, dan pengesanan keganasan, rumit oleh musuh yang ingin mengelakkan pengesanan. Kerja sebelumnya mengenai klasifikasi lawan telah membuat anggapan yang tidak realistik bahawa penyerang mempunyai pengetahuan yang sempurna mengenai pengkelasan 2. Dalam makalah ini, kami memperkenalkan masalah pembelajaran ACRE teknik pengkelasan terbalik lawan, tugas mempelajari maklumat yang mencukupi mengenai pengklasifikasi untuk membina serangan lawan. Kami menyajikan algoritma yang cekap untuk pengkelasan linear kejuruteraan terbalik dengan ciri berterusan atau Boolean dan menunjukkan keberkesanannya menggunakan data sebenar dari domain penapisan spam. [[EENNDD]] pengkelasan linear; klasifikasi lawan; spam; pelbagai"], [{"string": "Fast vertical mining using diffsets A number of vertical mining algorithms have been proposed recently for association mining , which have shown to be very effective and usually outperform horizontal approaches . The main advantage of the vertical format is support for fast frequency counting via intersection operations on transaction ids tids and automatic pruning of irrelevant data . The main problem with these approaches is when intermediate results of vertical tid lists become too large for memory , thus affecting the algorithm scalability . In this paper we present a novel vertical data representation called Diffset , that only keeps track of differences in the tids of a candidate pattern from its generating frequent patterns . We show that diffsets drastically cut down the size of memory required to store intermediate results . We show how diffsets , when incorporated into previous vertical mining methods , increase the performance significantly .", "keywords": ["frequent itemsets", "diffsets", "database applications", "association rule mining"], "combined": "Fast vertical mining using diffsets A number of vertical mining algorithms have been proposed recently for association mining , which have shown to be very effective and usually outperform horizontal approaches . The main advantage of the vertical format is support for fast frequency counting via intersection operations on transaction ids tids and automatic pruning of irrelevant data . The main problem with these approaches is when intermediate results of vertical tid lists become too large for memory , thus affecting the algorithm scalability . In this paper we present a novel vertical data representation called Diffset , that only keeps track of differences in the tids of a candidate pattern from its generating frequent patterns . We show that diffsets drastically cut down the size of memory required to store intermediate results . We show how diffsets , when incorporated into previous vertical mining methods , increase the performance significantly . [[EENNDD]] frequent itemsets; diffsets; database applications; association rule mining"}, "Perlombongan menegak pantas menggunakan diffsets Sebilangan algoritma perlombongan menegak telah dicadangkan baru-baru ini untuk perlombongan persatuan, yang telah terbukti sangat berkesan dan biasanya mengatasi pendekatan mendatar. Kelebihan utama format menegak ialah sokongan untuk pengiraan frekuensi pantas melalui operasi persimpangan pada tawaran id transaksi dan pemangkasan data yang tidak berkaitan secara automatik. Masalah utama dengan pendekatan ini adalah ketika hasil perantaraan senarai pasang menegak menjadi terlalu besar untuk memori, sehingga mempengaruhi skalabilitas algoritma. Dalam makalah ini kami menyajikan representasi data menegak novel yang disebut Diffset, yang hanya mengesan perbezaan gelombang pola calon dari corak yang sering dihasilkan. Kami menunjukkan bahawa perbezaan secara drastik mengurangkan ukuran memori yang diperlukan untuk menyimpan hasil pertengahan. Kami menunjukkan bagaimana perbezaan, apabila dimasukkan ke dalam kaedah perlombongan menegak sebelumnya, meningkatkan prestasi dengan ketara. [[EENNDD]] kumpulan item yang kerap; perbezaan; aplikasi pangkalan data; perlombongan peraturan persatuan"], [{"string": "Learning nonstationary models of normal network traffic for detecting novel attacks Traditional intrusion detection systems IDS detect attacks by comparing current behavior to signatures of known attacks . One main drawback is the inability of detecting new attacks which do not have known signatures . In this paper we propose a learning algorithm that constructs models of normal behavior from attack-free network traffic . Behavior that deviates from the learned normal model signals possible novel attacks . Our IDS is unique in two respects . First , it is nonstationary , modeling probabilities based on the time since the last event rather than on average rate . This prevents alarm floods . Second , the IDS learns protocol vocabularies at the data link through application layers in order to detect unknown attacks that attempt to exploit implementation errors in poorly tested features of the target software . On the 1999 DARPA IDS evaluation data set 9 , we detect 70 of 180 attacks with 100 false alarms , about evenly divided between user behavioral anomalies IP addresses and ports , as modeled by most other systems and protocol anomalies . Because our methods are unconventional there is a significant non-overlap of our IDS with the original DARPA participants , which implies that they could be combined to increase coverage .", "keywords": ["security and protection", "unauthorized access"], "combined": "Learning nonstationary models of normal network traffic for detecting novel attacks Traditional intrusion detection systems IDS detect attacks by comparing current behavior to signatures of known attacks . One main drawback is the inability of detecting new attacks which do not have known signatures . In this paper we propose a learning algorithm that constructs models of normal behavior from attack-free network traffic . Behavior that deviates from the learned normal model signals possible novel attacks . Our IDS is unique in two respects . First , it is nonstationary , modeling probabilities based on the time since the last event rather than on average rate . This prevents alarm floods . Second , the IDS learns protocol vocabularies at the data link through application layers in order to detect unknown attacks that attempt to exploit implementation errors in poorly tested features of the target software . On the 1999 DARPA IDS evaluation data set 9 , we detect 70 of 180 attacks with 100 false alarms , about evenly divided between user behavioral anomalies IP addresses and ports , as modeled by most other systems and protocol anomalies . Because our methods are unconventional there is a significant non-overlap of our IDS with the original DARPA participants , which implies that they could be combined to increase coverage . [[EENNDD]] security and protection; unauthorized access"}, "Mempelajari model lalu lintas rangkaian normal yang tidak stabil untuk mengesan serangan baru Sistem pengesanan pencerobohan tradisional IDS mengesan serangan dengan membandingkan tingkah laku semasa dengan tandatangan serangan yang diketahui. Satu kelemahan utama adalah ketidakmampuan mengesan serangan baru yang tidak mempunyai tanda tangan yang diketahui. Dalam makalah ini kami mencadangkan algoritma pembelajaran yang membina model tingkah laku normal dari lalu lintas rangkaian bebas serangan. Tingkah laku yang menyimpang dari model normal yang dipelajari memberi isyarat kemungkinan serangan novel. IDS kami unik dalam dua aspek. Pertama, kebarangkalian pemodelan tidak stasioner berdasarkan masa sejak peristiwa terakhir dan bukannya pada kadar purata. Ini mengelakkan berlakunya banjir penggera. Kedua, IDS mempelajari perbendaharaan kata protokol di pautan data melalui lapisan aplikasi untuk mengesan serangan yang tidak diketahui yang berusaha mengeksploitasi kesalahan pelaksanaan pada fitur perisian sasaran yang kurang diuji. Pada kumpulan data penilaian DARPA IDS 1999, kami mengesan 70 dari 180 serangan dengan 100 penggera palsu, yang terbahagi secara merata antara alamat IP dan pelabuhan anomali tingkah laku pengguna, seperti yang dimodelkan oleh kebanyakan anomali sistem dan protokol lain. Kerana kaedah kami tidak konvensional, terdapat banyak tumpang tindih IDS kami dengan peserta DARPA yang asal, yang menunjukkan bahawa mereka dapat digabungkan untuk meningkatkan liputan. [[EENNDD]] keselamatan dan perlindungan; akses tidak dibenarkan"], [{"string": "CloseGraph : mining closed frequent graph patterns Recent research on pattern discovery has progressed form mining frequent itemsets and sequences to mining structured patterns including trees , lattices , and graphs . As a general data structure , graph can model complicated relations among data with wide applications in bioinformatics , Web exploration , and etc. . However , mining large graph patterns in challenging due to the presence of an exponential number of frequent subgraphs . Instead of mining all the subgraphs , we propose to mine closed frequent graph patterns . A graph g is closed in a database if there exists no proper supergraph of g that has the same support as g. A closed graph pattern mining algorithm , CloseGraph , is developed by exploring several interesting pruning methods . Our performance study shows that CloseGraph not only dramatically reduces unnecessary subgraphs to be generated but also substantially increases the efficiency of mining , especially in the presence of large graph patterns .", "keywords": ["frequent graph", "closed pattern", "database applications", "graph representation", "canonical label"], "combined": "CloseGraph : mining closed frequent graph patterns Recent research on pattern discovery has progressed form mining frequent itemsets and sequences to mining structured patterns including trees , lattices , and graphs . As a general data structure , graph can model complicated relations among data with wide applications in bioinformatics , Web exploration , and etc. . However , mining large graph patterns in challenging due to the presence of an exponential number of frequent subgraphs . Instead of mining all the subgraphs , we propose to mine closed frequent graph patterns . A graph g is closed in a database if there exists no proper supergraph of g that has the same support as g. A closed graph pattern mining algorithm , CloseGraph , is developed by exploring several interesting pruning methods . Our performance study shows that CloseGraph not only dramatically reduces unnecessary subgraphs to be generated but also substantially increases the efficiency of mining , especially in the presence of large graph patterns . [[EENNDD]] frequent graph; closed pattern; database applications; graph representation; canonical label"}, "CloseGraph: perlombongan corak grafik yang kerap ditutup Penelitian terkini mengenai penemuan corak telah berkembang dari perlombongan item dan urutan yang kerap untuk melombong corak berstruktur termasuk pokok, kisi, dan grafik. Sebagai struktur data umum, grafik dapat memodelkan hubungan yang rumit antara data dengan aplikasi yang luas dalam bioinformatik, penerokaan Web, dan lain-lain. Walau bagaimanapun, melombong corak grafik yang besar dalam keadaan mencabar kerana terdapatnya jumlah subgraf yang kerap. Daripada melombong semua subgraf, kami mencadangkan untuk melengkapkan pola grafik yang kerap ditutup. Graf g ditutup dalam pangkalan data jika tidak ada supergraf g yang betul yang mempunyai sokongan yang sama dengan g. Algoritma perlombongan corak grafik tertutup, CloseGraph, dikembangkan dengan meneroka beberapa kaedah pemangkasan yang menarik. Kajian prestasi kami menunjukkan bahawa CloseGraph bukan sahaja secara dramatik mengurangkan subgraf yang tidak diperlukan untuk dihasilkan tetapi juga meningkatkan kecekapan perlombongan, terutamanya dengan adanya corak grafik yang besar. [[EENNDD]] graf yang kerap; corak tertutup; aplikasi pangkalan data; perwakilan grafik; label kanonik"], [{"string": "Algorithms for time series knowledge mining Temporal patterns composed of symbolic intervals are commonly formulated with Allen 's interval relations originating in temporal reasoning . This representation has severe disadvantages for knowledge discovery . The Time Series Knowledge Representation TSKR is a new hierarchical language for interval patterns expressing the temporal concepts of coincidence and partial order . We present effective and efficient mining algorithms for such patterns based on itemset techniques . A novel form of search space pruning effectively reduces the size of the mining result to ease interpretation and speed up the algorithms . On a real data set a concise set of TSKR patterns can explain the underlying temporal phenomena , whereas the patterns found with Allen 's relations are far more numerous yet only explain fragments of the data .", "keywords": ["knowledge discovery", "interval patterns", "pattern recognition", "time series"], "combined": "Algorithms for time series knowledge mining Temporal patterns composed of symbolic intervals are commonly formulated with Allen 's interval relations originating in temporal reasoning . This representation has severe disadvantages for knowledge discovery . The Time Series Knowledge Representation TSKR is a new hierarchical language for interval patterns expressing the temporal concepts of coincidence and partial order . We present effective and efficient mining algorithms for such patterns based on itemset techniques . A novel form of search space pruning effectively reduces the size of the mining result to ease interpretation and speed up the algorithms . On a real data set a concise set of TSKR patterns can explain the underlying temporal phenomena , whereas the patterns found with Allen 's relations are far more numerous yet only explain fragments of the data . [[EENNDD]] knowledge discovery; interval patterns; pattern recognition; time series"}, "Algoritma untuk penambangan pengetahuan siri masa Corak temporal yang terdiri daripada selang simbolik biasanya dirumuskan dengan hubungan selang Allen yang berasal dari penaakulan temporal. Perwakilan ini mempunyai kelemahan yang teruk untuk penemuan pengetahuan. Perwakilan Pengetahuan Siri Masa TSKR adalah bahasa hierarki baru untuk corak selang yang menyatakan konsep temporal kebetulan dan tertib separa. Kami mengemukakan algoritma perlombongan yang berkesan dan cekap untuk corak tersebut berdasarkan teknik itemet. Pemangkasan ruang carian yang baru secara berkesan mengurangkan ukuran hasil perlombongan untuk memudahkan penafsiran dan mempercepat algoritma. Pada set data nyata, satu set ringkas pola TSKR dapat menjelaskan fenomena temporal yang mendasari, sedangkan pola yang dijumpai dengan hubungan Allen jauh lebih banyak tetapi hanya menjelaskan fragmen data. [[EENNDD]] penemuan pengetahuan; corak selang; pengecaman corak; siri masa"], [{"string": "IntelliClean : a knowledge-based intelligent data cleaner", "keywords": ["learning"], "combined": "IntelliClean : a knowledge-based intelligent data cleaner [[EENNDD]] learning"}, "IntelliClean: pembelajaran pembersih data pintar berasaskan pengetahuan [[EENNDD]]"], [{"string": "Model-based overlapping clustering While the vast majority of clustering algorithms are partitional , many real world datasets have inherently overlapping clusters . Several approaches to finding overlapping clusters have come from work on analysis of biological datasets . In this paper , we interpret an overlapping clustering model proposed by Segal et al. 23 as a generalization of Gaussian mixture models , and we extend it to an overlapping clustering model based on mixtures of any regular exponential family distribution and the corresponding Bregman divergence . We provide the necessary algorithm modifications for this extension , and present results on synthetic data as well as subsets of 20-Newsgroups and EachMovie datasets .", "keywords": ["overlapping clustering", "bregman divergences", "learning", "graphical model", "exponential model", "high-dimensional clustering"], "combined": "Model-based overlapping clustering While the vast majority of clustering algorithms are partitional , many real world datasets have inherently overlapping clusters . Several approaches to finding overlapping clusters have come from work on analysis of biological datasets . In this paper , we interpret an overlapping clustering model proposed by Segal et al. 23 as a generalization of Gaussian mixture models , and we extend it to an overlapping clustering model based on mixtures of any regular exponential family distribution and the corresponding Bregman divergence . We provide the necessary algorithm modifications for this extension , and present results on synthetic data as well as subsets of 20-Newsgroups and EachMovie datasets . [[EENNDD]] overlapping clustering; bregman divergences; learning; graphical model; exponential model; high-dimensional clustering"}, "Penggabungan tumpang tindih berdasarkan model Walaupun sebahagian besar algoritma pengelompokan bersifat separa, banyak kumpulan data dunia nyata sememangnya mempunyai kelompok yang tumpang tindih. Beberapa pendekatan untuk mencari kelompok yang tumpang tindih berasal dari kerja analisis kumpulan data biologi. Dalam makalah ini, kami menafsirkan model pengelompokan bertindih yang dicadangkan oleh Segal et al. 23 sebagai generalisasi model campuran Gaussian, dan kami memperluasnya ke model pengelompokan bertindih berdasarkan campuran mana-mana taburan keluarga eksponensial biasa dan perbezaan Bregman yang sesuai. Kami memberikan pengubahsuaian algoritma yang diperlukan untuk peluasan ini, dan membentangkan hasil pada data sintetik serta subkumpulan 20 kumpulan Berita dan kumpulan data EveryMovie. [[EENNDD]] pengelompokan bertindih; perbezaan bregman; belajar; model grafik; model eksponensial; pengelompokan dimensi tinggi"], [{"string": "Mining relational data through correlation-based multiple view validation Commercial relational databases currently store vast amounts of real-world data . The data within these relational repositories are represented by multiple relations , which are inter-connected by means of foreign key joins . The mining of such interrelated data poses a major challenge to the data mining community . Unfortunately , traditional data mining algorithms usually only explore one relation , the so-called target relation , thus excluding crucial knowledge embedded in the related so-called background relations . In this paper , we propose a novel approach for classifying relational such domains . This strategy employs multiple views to capture crucial information not only from the target relation , but also from related relations . This information is integrated into the relational mining process . The framework presented here , firstly , explore the relational domain to partition its features space into multiple subsets . Subsequently , these subsets are used to construct multiple uncorrelated views , based on a novel correlation-based view validation method , against the target concept . Finally , the knowledge possessed by multiple views are incorporated into a meta-learning mechanism to augment one another . Based on this framework , a wide range of conventional data mining methods can be applied to mine relational databases . Our experiments on benchmark real-world data sets show that the proposed method achieves promising results both in terms of overall accuracy obtained and run time , when compared with two other relational data mining approaches .", "keywords": ["multi-view learning", "relational database", "multi-relational data mining", "classification"], "combined": "Mining relational data through correlation-based multiple view validation Commercial relational databases currently store vast amounts of real-world data . The data within these relational repositories are represented by multiple relations , which are inter-connected by means of foreign key joins . The mining of such interrelated data poses a major challenge to the data mining community . Unfortunately , traditional data mining algorithms usually only explore one relation , the so-called target relation , thus excluding crucial knowledge embedded in the related so-called background relations . In this paper , we propose a novel approach for classifying relational such domains . This strategy employs multiple views to capture crucial information not only from the target relation , but also from related relations . This information is integrated into the relational mining process . The framework presented here , firstly , explore the relational domain to partition its features space into multiple subsets . Subsequently , these subsets are used to construct multiple uncorrelated views , based on a novel correlation-based view validation method , against the target concept . Finally , the knowledge possessed by multiple views are incorporated into a meta-learning mechanism to augment one another . Based on this framework , a wide range of conventional data mining methods can be applied to mine relational databases . Our experiments on benchmark real-world data sets show that the proposed method achieves promising results both in terms of overall accuracy obtained and run time , when compared with two other relational data mining approaches . [[EENNDD]] multi-view learning; relational database; multi-relational data mining; classification"}, "Melombong data hubungan melalui pengesahan pelbagai pandangan berdasarkan korelasi Pangkalan data hubungan komersial kini menyimpan sejumlah besar data dunia nyata. Data dalam repositori relasional ini diwakili oleh pelbagai hubungan, yang saling berkaitan melalui gabungan kunci asing. Perlombongan data yang saling berkaitan tersebut menimbulkan cabaran besar bagi komuniti perlombongan data. Malangnya, algoritma perlombongan data tradisional biasanya hanya meneroka satu hubungan, yang disebut hubungan sasaran, sehingga tidak termasuk pengetahuan penting yang tertanam dalam hubungan latar belakang yang disebut. Dalam makalah ini, kami mencadangkan pendekatan baru untuk mengklasifikasikan domain hubungan seperti itu. Strategi ini menggunakan pelbagai pandangan untuk menangkap maklumat penting bukan hanya dari hubungan sasaran, tetapi juga dari hubungan yang berkaitan. Maklumat ini disatukan ke dalam proses perlombongan hubungan. Rangka kerja yang dibentangkan di sini, pertama, meneroka domain hubungan untuk membahagi ruang ciri menjadi beberapa subset. Selepas itu, subset ini digunakan untuk membina beberapa pandangan yang tidak berkorelasi, berdasarkan kaedah validasi pandangan berdasarkan korelasi baru, terhadap konsep sasaran. Akhirnya, pengetahuan yang dimiliki oleh pelbagai pandangan dimasukkan ke dalam mekanisme meta-pembelajaran untuk saling meningkatkan. Berdasarkan kerangka ini, pelbagai kaedah perlombongan data konvensional dapat diterapkan pada pangkalan data hubungan tambang. Eksperimen kami pada set data dunia nyata penanda aras menunjukkan bahawa kaedah yang dicadangkan mencapai hasil yang menjanjikan baik dari segi ketepatan keseluruhan yang diperoleh dan masa berjalan, jika dibandingkan dengan dua pendekatan perlombongan data hubungan lainnya. [[EENNDD]] pembelajaran pelbagai pandangan; pangkalan data hubungan; perlombongan data pelbagai hubungan; pengelasan"], [{"string": "Active learning using adaptive resampling", "keywords": ["classification", "active learning", "learning", "adaptive resampling", "machine learning"], "combined": "Active learning using adaptive resampling [[EENNDD]] classification; active learning; learning; adaptive resampling; machine learning"}, "Pembelajaran aktif menggunakan pengkelasan semula adaptif [[EENNDD]]; pembelajaran aktif; belajar; penyesuaian semula adaptif; pembelajaran mesin"], [{"string": "iLink : search and routing in social networks The growth of Web 2.0 and fundamental theoretical breakthroughs have led to an avalanche of interest in social networks . This paper focuses on the problem of modeling how social networks accomplish tasks through peer production style collaboration . We propose a general interaction model for the underlying social networks and then a specific model iLink for social search and message routing . A key contribution here is the development of a general learning framework for making such online peer production systems work at scale . The iLink model has been used to develop a system for FAQ generation in a social network FAQtory , and experience with its application in the context of a full-scale learning-driven workflow application CALO is reported . We also discuss methods of adapting iLink technology for use in military knowledge sharing portals and other message routing systems . Finally , the paper shows the connection of iLink to SQM , a theoretical model for social search that is a generalization of Markov Decision Processes and the popular Pagerank model .", "keywords": ["message routing", "peer production", "smart rss", "social faq generation", "social search", "learning", "expert identification"], "combined": "iLink : search and routing in social networks The growth of Web 2.0 and fundamental theoretical breakthroughs have led to an avalanche of interest in social networks . This paper focuses on the problem of modeling how social networks accomplish tasks through peer production style collaboration . We propose a general interaction model for the underlying social networks and then a specific model iLink for social search and message routing . A key contribution here is the development of a general learning framework for making such online peer production systems work at scale . The iLink model has been used to develop a system for FAQ generation in a social network FAQtory , and experience with its application in the context of a full-scale learning-driven workflow application CALO is reported . We also discuss methods of adapting iLink technology for use in military knowledge sharing portals and other message routing systems . Finally , the paper shows the connection of iLink to SQM , a theoretical model for social search that is a generalization of Markov Decision Processes and the popular Pagerank model . [[EENNDD]] message routing; peer production; smart rss; social faq generation; social search; learning; expert identification"}, "iLink: pencarian dan perutean dalam rangkaian sosial Pertumbuhan Web 2.0 dan penemuan teoritis asas telah menyebabkan longsornya minat dalam rangkaian sosial. Makalah ini memfokuskan pada masalah pemodelan bagaimana jaringan sosial menyelesaikan tugas melalui kolaborasi gaya produksi rakan sebaya. Kami mencadangkan model interaksi umum untuk rangkaian sosial yang mendasari dan kemudian model khusus iLink untuk carian sosial dan penghalaan mesej. Sumbangan utama di sini adalah pengembangan kerangka pembelajaran umum untuk menjadikan sistem pengeluaran rakan sebaya dalam talian berfungsi pada skala besar. Model iLink telah digunakan untuk mengembangkan sistem untuk pembuatan FAQ dalam FAQtory rangkaian sosial, dan pengalaman dengan penerapannya dalam konteks aplikasi aliran kerja didorong pembelajaran penuh CALO dilaporkan. Kami juga membincangkan kaedah penyesuaian teknologi iLink untuk digunakan dalam portal perkongsian pengetahuan ketenteraan dan sistem penghalaan mesej yang lain. Akhirnya, makalah ini menunjukkan hubungan iLink ke SQM, model teori untuk carian sosial yang merupakan generalisasi Proses Keputusan Markov dan model Pagerank yang popular. [[EENNDD]] penghalaan mesej; pengeluaran rakan sebaya; rss pintar; generasi faq sosial; carian sosial; belajar; pengenalan ahli"], [{"string": "Proximal support vector machine classifiers Instead of a standard support vector machine SVM that classifies points by assigning them to one of two disjoint half-spaces , points are classified by assigning them to the closest of two parallel planes in input or feature space that are pushed apart as far as possible . This formulation , which can also be interpreted as regularized least squares and considered in the much more general context of regularized networks 8 , 9 , leads to an extremely fast and simple algorithm for generating a linear or nonlinear classifier that merely requires the solution of a single system of linear equations . In contrast , standard SVMs solve a quadratic or a linear program that require considerably longer computational time . Computational results on publicly available datasets indicate that the proposed proximal SVM classifier has comparable test set correctness to that of standard SVM classifiers , but with considerably faster computational time that can be an order of magnitude faster . The linear proximal SVM can easily handle large datasets as indicated by the classification of a 2 million point 10-attribute set in 20.8 seconds . All computational results are based on 6 lines of MATLAB code .", "keywords": ["data classification", "linear equations", "support vector machines"], "combined": "Proximal support vector machine classifiers Instead of a standard support vector machine SVM that classifies points by assigning them to one of two disjoint half-spaces , points are classified by assigning them to the closest of two parallel planes in input or feature space that are pushed apart as far as possible . This formulation , which can also be interpreted as regularized least squares and considered in the much more general context of regularized networks 8 , 9 , leads to an extremely fast and simple algorithm for generating a linear or nonlinear classifier that merely requires the solution of a single system of linear equations . In contrast , standard SVMs solve a quadratic or a linear program that require considerably longer computational time . Computational results on publicly available datasets indicate that the proposed proximal SVM classifier has comparable test set correctness to that of standard SVM classifiers , but with considerably faster computational time that can be an order of magnitude faster . The linear proximal SVM can easily handle large datasets as indicated by the classification of a 2 million point 10-attribute set in 20.8 seconds . All computational results are based on 6 lines of MATLAB code . [[EENNDD]] data classification; linear equations; support vector machines"}, "Pengelaskan mesin vektor sokongan proksimal Daripada mesin vektor sokongan standard SVM yang mengklasifikasikan titik dengan memberikannya kepada salah satu daripada dua ruang separuh terasing, titik dikelaskan dengan menetapkannya pada jarak yang terdekat dari dua satah selari dalam ruang input atau ciri yang ditolak sejauh mungkin . Rumusan ini, yang juga dapat ditafsirkan sebagai kuadrat terkecil dan dipertimbangkan dalam konteks yang lebih umum dari rangkaian teratur 8, 9, membawa kepada algoritma yang sangat cepat dan sederhana untuk menghasilkan pengklasifikasi linier atau tidak linier yang hanya memerlukan penyelesaian satu sistem persamaan linear. Sebaliknya, SVM standard menyelesaikan program kuadratik atau linear yang memerlukan masa pengiraan yang jauh lebih lama. Hasil komputasi pada kumpulan data yang tersedia untuk umum menunjukkan bahawa pengkelasan SVM proksimal yang dicadangkan mempunyai ketepatan set ujian yang setara dengan pengelaskan SVM standard, tetapi dengan waktu pengiraan yang jauh lebih cepat yang dapat menjadi urutan magnitud lebih cepat. SVM proksimal linear dapat dengan mudah menangani set data besar seperti yang ditunjukkan oleh klasifikasi atribut 10 juta titik 10 juta dalam 20.8 saat. Semua hasil pengiraan berdasarkan 6 baris kod MATLAB. [[EENNDD]] pengkelasan data; persamaan linear; mesin vektor sokongan"], [{"string": "Eliminating noisy information in Web pages for data mining A commercial Web page typically contains many information blocks . Apart from the main content blocks , it usually has such blocks as navigation panels , copyright and privacy notices , and advertisements for business purposes and for easy user access . We call these blocks that are not the main content blocks of the page the noisy blocks . We show that the information contained in these noisy blocks can seriously harm Web data mining . Eliminating these noises is thus of great importance . In this paper , we propose a noise elimination technique based on the following observation : In a given Web site , noisy blocks usually share some common contents and presentation styles , while the main content blocks of the pages are often diverse in their actual contents and\\/or presentation styles . Based on this observation , we propose a tree structure , called Style Tree , to capture the common presentation styles and the actual contents of the pages in a given Web site . By sampling the pages of the site , a Style Tree can be built for the site , which we call the Site Style Tree SST . We then introduce an information based measure to determine which parts of the SST represent noises and which parts represent the main contents of the site . The SST is employed to detect and eliminate noises in any Web page of the site by mapping this page to the SST . The proposed technique is evaluated with two data mining tasks , Web page clustering and classification . Experimental results show that our noise elimination technique is able to improve the mining results significantly .", "keywords": ["noise detection", "web mining", "noise elimination"], "combined": "Eliminating noisy information in Web pages for data mining A commercial Web page typically contains many information blocks . Apart from the main content blocks , it usually has such blocks as navigation panels , copyright and privacy notices , and advertisements for business purposes and for easy user access . We call these blocks that are not the main content blocks of the page the noisy blocks . We show that the information contained in these noisy blocks can seriously harm Web data mining . Eliminating these noises is thus of great importance . In this paper , we propose a noise elimination technique based on the following observation : In a given Web site , noisy blocks usually share some common contents and presentation styles , while the main content blocks of the pages are often diverse in their actual contents and\\/or presentation styles . Based on this observation , we propose a tree structure , called Style Tree , to capture the common presentation styles and the actual contents of the pages in a given Web site . By sampling the pages of the site , a Style Tree can be built for the site , which we call the Site Style Tree SST . We then introduce an information based measure to determine which parts of the SST represent noises and which parts represent the main contents of the site . The SST is employed to detect and eliminate noises in any Web page of the site by mapping this page to the SST . The proposed technique is evaluated with two data mining tasks , Web page clustering and classification . Experimental results show that our noise elimination technique is able to improve the mining results significantly . [[EENNDD]] noise detection; web mining; noise elimination"}, "Menghilangkan maklumat yang bising di laman web untuk perlombongan data Halaman Web komersial biasanya mengandungi banyak blok maklumat. Terlepas dari blok isi utama, biasanya memiliki blok seperti panel navigasi, pemberitahuan hak cipta dan privasi, dan iklan untuk tujuan perniagaan dan untuk akses pengguna yang mudah. Kami memanggil blok ini bukan blok kandungan utama halaman sebagai blok bising. Kami menunjukkan bahawa maklumat yang terdapat dalam blok bising ini boleh membahayakan perlombongan data Web. Oleh itu, penghapusan suara ini sangat penting. Dalam makalah ini, kami mencadangkan teknik penghapusan kebisingan berdasarkan pemerhatian berikut: Di laman web tertentu, blok bising biasanya berkongsi beberapa kandungan dan gaya persembahan yang biasa, sementara blok isi utama halaman sering beragam dalam kandungan sebenarnya dan \\ / atau gaya persembahan. Berdasarkan pemerhatian ini, kami mengusulkan struktur pohon, yang disebut Gaya Pohon, untuk menangkap gaya penyampaian umum dan isi sebenar halaman di laman web tertentu. Dengan mengambil contoh halaman dari laman web, Style Tree dapat dibangun untuk laman web ini, yang kita namakan sebagai Site Style Tree SST. Kami kemudian memperkenalkan langkah berdasarkan maklumat untuk menentukan bahagian mana dari SST mewakili suara dan bahagian mana yang mewakili isi utama laman web ini. SST digunakan untuk mengesan dan menghilangkan suara di mana-mana laman web laman web ini dengan memetakan halaman ini ke SST. Teknik yang dicadangkan dinilai dengan dua tugas perlombongan data, pengelompokan halaman dan klasifikasi. Hasil eksperimen menunjukkan bahawa teknik penghapusan kebisingan kami dapat meningkatkan hasil perlombongan dengan ketara. [[EENNDD]] pengesanan bunyi; perlombongan web; penghapusan bunyi"], [{"string": "Combinational collaborative filtering for personalized community recommendation Rapid growth in the amount of data available on social networking sites has made information retrieval increasingly challenging for users . In this paper , we propose a collaborative filtering method , Combinational Collaborative Filtering CCF , to perform personalized community recommendations by considering multiple types of co-occurrences in social data at the same time . This filtering method fuses semantic and user information , then applies a hybrid training strategy that combines Gibbs sampling and Expectation-Maximization algorithm . To handle the large-scale dataset , parallel computing is used to speed up the model training . Through an empirical study on the Orkut dataset , we show CCF to be both effective and scalable .", "keywords": ["collaborative filtering", "probabilistic models", "miscellaneous", "personalized recommendation"], "combined": "Combinational collaborative filtering for personalized community recommendation Rapid growth in the amount of data available on social networking sites has made information retrieval increasingly challenging for users . In this paper , we propose a collaborative filtering method , Combinational Collaborative Filtering CCF , to perform personalized community recommendations by considering multiple types of co-occurrences in social data at the same time . This filtering method fuses semantic and user information , then applies a hybrid training strategy that combines Gibbs sampling and Expectation-Maximization algorithm . To handle the large-scale dataset , parallel computing is used to speed up the model training . Through an empirical study on the Orkut dataset , we show CCF to be both effective and scalable . [[EENNDD]] collaborative filtering; probabilistic models; miscellaneous; personalized recommendation"}, "Penapisan kolaboratif gabungan untuk cadangan komuniti yang diperibadikan Pertumbuhan pesat dalam jumlah data yang terdapat di laman rangkaian sosial menjadikan pencarian maklumat semakin mencabar bagi pengguna. Dalam makalah ini, kami mencadangkan kaedah penyaringan kolaboratif, Combinational Collaborative Filtering CCF, untuk melakukan cadangan komuniti yang diperibadikan dengan mempertimbangkan beberapa jenis kejadian bersama dalam data sosial pada masa yang sama. Kaedah penyaringan ini menggabungkan maklumat semantik dan pengguna, kemudian menerapkan strategi latihan hibrid yang menggabungkan pensampelan Gibbs dan algoritma Ekspektasi-Pemaksimalan. Untuk menangani set data berskala besar, pengkomputeran selari digunakan untuk mempercepat latihan model. Melalui kajian empirikal pada kumpulan data Orkut, kami menunjukkan CCF berkesan dan boleh diskalakan. [[EENNDD]] penapisan kolaboratif; model kebarangkalian; pelbagai; cadangan yang diperibadikan"], [{"string": "Scalable look-ahead linear regression trees Most decision tree algorithms base their splitting decisions on a piecewise constant model . Often these splitting algorithms are extrapolated to trees with non-constant models at the leaf nodes . The motivation behind Look-ahead Linear Regression Trees LLRT is that out of all the methods proposed to date , there has been no scalable approach to exhaustively evaluate all possible models in the leaf nodes in order to obtain an optimal split . Using several optimizations , LLRT is able to generate and evaluate thousands of linear regression models per second . This allows for a near-exhaustive evaluation of all possible splits in a node , based on the quality of fit of linear regression models in the resulting branches . We decompose the calculation of the Residual Sum of Squares in such a way that a large part of it is pre-computed . The resulting method is highly scalable . We observe it to obtain high predictive accuracy for problems with strong mutual dependencies between attributes . We report on experiments with two simulated and seven real data sets .", "keywords": ["predictive model", "linear regression tree", "regression", "scalable algorithms", "models", "model tree"], "combined": "Scalable look-ahead linear regression trees Most decision tree algorithms base their splitting decisions on a piecewise constant model . Often these splitting algorithms are extrapolated to trees with non-constant models at the leaf nodes . The motivation behind Look-ahead Linear Regression Trees LLRT is that out of all the methods proposed to date , there has been no scalable approach to exhaustively evaluate all possible models in the leaf nodes in order to obtain an optimal split . Using several optimizations , LLRT is able to generate and evaluate thousands of linear regression models per second . This allows for a near-exhaustive evaluation of all possible splits in a node , based on the quality of fit of linear regression models in the resulting branches . We decompose the calculation of the Residual Sum of Squares in such a way that a large part of it is pre-computed . The resulting method is highly scalable . We observe it to obtain high predictive accuracy for problems with strong mutual dependencies between attributes . We report on experiments with two simulated and seven real data sets . [[EENNDD]] predictive model; linear regression tree; regression; scalable algorithms; models; model tree"}, "Pohon regresi linier yang dapat dilihat berskala Sebilangan besar algoritma pohon keputusan mendasarkan keputusan pemisahan mereka pada model pemalar sepotong. Selalunya algoritma pemisahan ini diekstrapolasi ke pokok dengan model tidak tetap di simpul daun. Motivasi di sebalik LLRT Pohon Regresi Linear ke depan adalah bahawa dari semua kaedah yang dicadangkan hingga kini, belum ada pendekatan yang dapat ditingkatkan untuk menilai secara menyeluruh semua model yang mungkin di simpul daun untuk mendapatkan perpecahan yang optimum. Dengan menggunakan beberapa pengoptimuman, LLRT dapat menghasilkan dan menilai ribuan model regresi linear sesaat. Ini memungkinkan penilaian hampir lengkap dari semua kemungkinan perpecahan dalam nod, berdasarkan kualiti muat model regresi linier pada cabang yang dihasilkan. Kami menguraikan pengiraan Jumlah Residual Kuadrat sedemikian rupa sehingga sebahagian besarnya dihitung sebelumnya. Kaedah yang dihasilkan sangat berskala. Kami memerhatikannya untuk mendapatkan ketepatan ramalan yang tinggi untuk masalah dengan saling bergantung yang kuat antara atribut. Kami melaporkan eksperimen dengan dua simulasi dan tujuh set data sebenar. [[EENNDD]] model ramalan; pokok regresi linear; regresi; algoritma berskala; model; pokok model"], [{"string": "Efficient closed pattern mining in the presence of tough block constraints Various constrained frequent pattern mining problem formulations and associated algorithms have been developed that enable the user to specify various itemset-based constraints that better capture the underlying application requirements and characteristics . In this paper we introduce a new class of block constraints that determine the significance of an itemset pattern by considering the dense block that is formed by the pattern 's items and its associated set of transactions . Block constraints provide a natural framework by which a number of important problems can be specified and make it possible to solve numerous problems on binary and real-valued datasets . However , developing computationally efficient algorithms to find these block constraints poses a number of challenges as unlike the different itemset-based constraints studied earlier , these block constraints are tough as they are neither anti-monotone , monotone , nor convertible . To overcome this problem , we introduce a new class of pruning methods that significantly reduce the overall search space and present a computationally efficient and scalable algorithm called CBMiner to find the closed itemsets that satisfy the block constraints .", "keywords": ["tough constraint", "closed pattern", "block constraint"], "combined": "Efficient closed pattern mining in the presence of tough block constraints Various constrained frequent pattern mining problem formulations and associated algorithms have been developed that enable the user to specify various itemset-based constraints that better capture the underlying application requirements and characteristics . In this paper we introduce a new class of block constraints that determine the significance of an itemset pattern by considering the dense block that is formed by the pattern 's items and its associated set of transactions . Block constraints provide a natural framework by which a number of important problems can be specified and make it possible to solve numerous problems on binary and real-valued datasets . However , developing computationally efficient algorithms to find these block constraints poses a number of challenges as unlike the different itemset-based constraints studied earlier , these block constraints are tough as they are neither anti-monotone , monotone , nor convertible . To overcome this problem , we introduce a new class of pruning methods that significantly reduce the overall search space and present a computationally efficient and scalable algorithm called CBMiner to find the closed itemsets that satisfy the block constraints . [[EENNDD]] tough constraint; closed pattern; block constraint"}, "Perlombongan corak tertutup yang cekap dengan adanya kekangan blok yang sukar Pelbagai rumusan masalah perlombongan corak yang kerap dan algoritma yang berkaitan telah dikembangkan yang membolehkan pengguna menentukan pelbagai kekangan berdasarkan itemet yang lebih baik menangkap keperluan dan ciri aplikasi yang mendasari. Dalam makalah ini kami memperkenalkan kelas batasan blok baru yang menentukan kepentingan corak itemet dengan mempertimbangkan blok padat yang dibentuk oleh item corak dan set transaksi yang berkaitan. Kekangan blok menyediakan kerangka semula jadi di mana sejumlah masalah penting dapat ditentukan dan memungkinkan untuk menyelesaikan banyak masalah pada set data binari dan bernilai sebenar. Walau bagaimanapun, mengembangkan algoritma yang cekap secara komputasi untuk menemui kekangan blok ini menimbulkan sejumlah cabaran kerana tidak seperti kekangan berdasarkan itemet yang berbeza yang dikaji sebelumnya, batasan blok ini sukar kerana ia bukan anti-monoton, monoton, atau boleh ditukar. Untuk mengatasi masalah ini, kami memperkenalkan kelas kaedah pemangkasan baru yang secara signifikan mengurangkan keseluruhan ruang carian dan menghadirkan algoritma yang cekap dan boleh diskalakan yang disebut CBMiner untuk mencari set barang tertutup yang memenuhi batasan blok. [[EENNDD]] kekangan yang sukar; corak tertutup; kekangan blok"], [{"string": "Sparsification of influence networks We present Spine , an efficient algorithm for finding the `` backbone '' of an influence network . Given a social graph and a log of past propagations , we build an instance of the independent-cascade model that describes the propagations . We aim at reducing the complexity of that model , while preserving most of its accuracy in describing the data . We show that the problem is inapproximable and we present an optimal , dynamic-programming algorithm , whose search space , albeit exponential , is typically much smaller than that of the brute force , exhaustive-search approach . Seeking a practical , scalable approach to sparsification , we devise Spine , a greedy , efficient algorithm with practically little compromise in quality . We claim that sparsification is a fundamental data-reduction operation with many applications , ranging from visualization to exploratory and descriptive data analysis . As a proof of concept , we use Spine on real-world datasets , revealing the backbone of their influence-propagation networks . Moreover , we apply Spine as a pre-processing step for the influence-maximization problem , showing that computations on sparsified models give up little accuracy , but yield significant improvements in terms of scalability .", "keywords": ["social networks", "influence", "propagation"], "combined": "Sparsification of influence networks We present Spine , an efficient algorithm for finding the `` backbone '' of an influence network . Given a social graph and a log of past propagations , we build an instance of the independent-cascade model that describes the propagations . We aim at reducing the complexity of that model , while preserving most of its accuracy in describing the data . We show that the problem is inapproximable and we present an optimal , dynamic-programming algorithm , whose search space , albeit exponential , is typically much smaller than that of the brute force , exhaustive-search approach . Seeking a practical , scalable approach to sparsification , we devise Spine , a greedy , efficient algorithm with practically little compromise in quality . We claim that sparsification is a fundamental data-reduction operation with many applications , ranging from visualization to exploratory and descriptive data analysis . As a proof of concept , we use Spine on real-world datasets , revealing the backbone of their influence-propagation networks . Moreover , we apply Spine as a pre-processing step for the influence-maximization problem , showing that computations on sparsified models give up little accuracy , but yield significant improvements in terms of scalability . [[EENNDD]] social networks; influence; propagation"}, "Sparifikasi rangkaian pengaruh Kami menyajikan Spine, algoritma yang cekap untuk mencari \"tulang belakang\" rangkaian pengaruh. Memandangkan grafik sosial dan log penyebaran masa lalu, kami membina contoh model kaskade bebas yang menggambarkan penyebaran. Kami bertujuan mengurangkan kerumitan model itu, sambil mengekalkan sebahagian besar ketepatannya dalam menerangkan data. Kami menunjukkan bahawa masalahnya tidak dapat didekati dan kami menghadirkan algoritma pengaturcaraan dinamis yang optimum, yang ruang pencariannya, walaupun eksponensial, biasanya jauh lebih kecil daripada pendekatan brute force, pencarian menyeluruh. Dengan mencari pendekatan praktikal dan berskala untuk sparsifikasi, kami merancang Spine, algoritma yang tamak dan cekap dengan kualiti kompromi yang sedikit. Kami mendakwa bahawa sparsifikasi adalah operasi pengurangan data yang mendasar dengan banyak aplikasi, mulai dari visualisasi hingga analisis data eksploratif dan deskriptif. Sebagai bukti konsep, kami menggunakan Spine pada kumpulan data dunia nyata, yang memperlihatkan tulang belakang rangkaian penyebaran pengaruh mereka. Lebih-lebih lagi, kami menerapkan Spine sebagai langkah pra-pemrosesan untuk masalah pengaruh-memaksimalkan, menunjukkan bahawa pengiraan pada model-model yang dipisahkan memberikan sedikit ketepatan, tetapi menghasilkan peningkatan yang signifikan dari segi skalabilitas. [[EENNDD]] rangkaian sosial; pengaruh; penyebaran"], [{"string": "Using graph-based metrics with empirical risk minimization to speed up active learning on networked data Active and semi-supervised learning are important techniques when labeled data are scarce . Recently a method was suggested for combining active learning with a semi-supervised learning algorithm that uses Gaussian fields and harmonic functions . This classifier is relational in nature : it relies on having the data presented as a partially labeled graph also known as a within-network learning problem . This work showed yet again that empirical risk minimization ERM was the best method to find the next instance to label and provided an efficient way to compute ERM with the semi-supervised classifier . The computational problem with ERM is that it relies on computing the risk for all possible instances . If we could limit the candidates that should be investigated , then we can speed up active learning considerably . In the case where the data is graphical in nature , we can leverage the graph structure to rapidly identify instances that are likely to be good candidates for labeling . This paper describes a novel hybrid approach of using of community finding and social network analytic centrality measures to identify good candidates for labeling and then using ERM to find the best instance in this candidate set . We show on real-world data that we can limit the ERM computations to a fraction of instances with comparable performance .", "keywords": ["closeness centrality", "empirical risk minimization", "active learning", "community finding", "statistical relational learning", "betweenness centrality", "semi-supervised learning", "within-network learning", "social network analysis", "clustering"], "combined": "Using graph-based metrics with empirical risk minimization to speed up active learning on networked data Active and semi-supervised learning are important techniques when labeled data are scarce . Recently a method was suggested for combining active learning with a semi-supervised learning algorithm that uses Gaussian fields and harmonic functions . This classifier is relational in nature : it relies on having the data presented as a partially labeled graph also known as a within-network learning problem . This work showed yet again that empirical risk minimization ERM was the best method to find the next instance to label and provided an efficient way to compute ERM with the semi-supervised classifier . The computational problem with ERM is that it relies on computing the risk for all possible instances . If we could limit the candidates that should be investigated , then we can speed up active learning considerably . In the case where the data is graphical in nature , we can leverage the graph structure to rapidly identify instances that are likely to be good candidates for labeling . This paper describes a novel hybrid approach of using of community finding and social network analytic centrality measures to identify good candidates for labeling and then using ERM to find the best instance in this candidate set . We show on real-world data that we can limit the ERM computations to a fraction of instances with comparable performance . [[EENNDD]] closeness centrality; empirical risk minimization; active learning; community finding; statistical relational learning; betweenness centrality; semi-supervised learning; within-network learning; social network analysis; clustering"}, "Menggunakan metrik berasaskan grafik dengan pengurangan risiko empirikal untuk mempercepat pembelajaran aktif pada data rangkaian Pembelajaran aktif dan separa penyeliaan adalah teknik penting apabila data berlabel jarang. Baru-baru ini satu kaedah dicadangkan untuk menggabungkan pembelajaran aktif dengan algoritma pembelajaran separa penyeliaan yang menggunakan bidang Gaussian dan fungsi harmonik. Pengelasan ini bersifat relasional: bergantung pada data yang disajikan sebagai grafik berlabel sebahagian juga dikenali sebagai masalah pembelajaran dalam rangkaian. Karya ini menunjukkan sekali lagi bahawa ERM pengurangan risiko empirikal adalah kaedah terbaik untuk mencari contoh seterusnya untuk memberi label dan menyediakan kaedah yang berkesan untuk mengira ERM dengan pengkelasan separa yang diawasi. Masalah komputasi dengan ERM adalah bahawa ia bergantung pada pengiraan risiko untuk semua kemungkinan kejadian. Sekiranya kita dapat mengehadkan calon yang harus diselidiki, maka kita dapat mempercepat pembelajaran aktif. Sekiranya data bersifat grafis, kita dapat memanfaatkan struktur grafik untuk mengenal pasti kejadian yang cepat menjadi calon pelabelan. Makalah ini menerangkan pendekatan hibrid baru menggunakan kaedah pencarian masyarakat dan analitik sentraliti rangkaian sosial untuk mengenal pasti calon pelabelan yang baik dan kemudian menggunakan ERM untuk mencari contoh terbaik dalam kumpulan calon ini. Kami menunjukkan pada data dunia nyata bahawa kami dapat mengehadkan pengiraan ERM kepada sebahagian kecil keadaan dengan prestasi yang setanding. [[EENNDD]] pusat kedekatan; pengurangan risiko empirikal; pembelajaran aktif; penemuan masyarakat; pembelajaran perhubungan statistik; sentraliti antara; pembelajaran separa penyeliaan; pembelajaran dalam rangkaian; analisis rangkaian sosial; pengelompokan"], [{"string": "Finding similar files in large document repositories Hewlett-Packard has many millions of technical support documents in a variety of collections . As part of content management , such collections are periodically merged and groomed . In the process , it becomes important to identify and weed out support documents that are largely duplicates of newer versions . Doing so improves the quality of the collection , eliminates chaff from search results , and improves customer satisfaction . The technical challenge is that through workflow and human processes , the knowledge of which documents are related is often lost . We required a method that could identify similar documents based on their content alone , without relying on metadata , which may be corrupt or missing . We present an approach for finding similar files that scales up to large document repositories . It is based on chunking the byte stream to find unique signatures that may be shared in multiple files . An analysis of the file-chunk graph yields clusters of related files . An optional bipartite graph partitioning algorithm can be applied to greatly increase scalability .", "keywords": ["content analysis and indexing", "similarity", "document management", "near duplicate detection", "scalability", "content management"], "combined": "Finding similar files in large document repositories Hewlett-Packard has many millions of technical support documents in a variety of collections . As part of content management , such collections are periodically merged and groomed . In the process , it becomes important to identify and weed out support documents that are largely duplicates of newer versions . Doing so improves the quality of the collection , eliminates chaff from search results , and improves customer satisfaction . The technical challenge is that through workflow and human processes , the knowledge of which documents are related is often lost . We required a method that could identify similar documents based on their content alone , without relying on metadata , which may be corrupt or missing . We present an approach for finding similar files that scales up to large document repositories . It is based on chunking the byte stream to find unique signatures that may be shared in multiple files . An analysis of the file-chunk graph yields clusters of related files . An optional bipartite graph partitioning algorithm can be applied to greatly increase scalability . [[EENNDD]] content analysis and indexing; similarity; document management; near duplicate detection; scalability; content management"}, "Mencari fail serupa di repositori dokumen besar Hewlett-Packard mempunyai berjuta-juta dokumen sokongan teknikal dalam pelbagai koleksi. Sebagai sebahagian daripada pengurusan kandungan, koleksi seperti itu digabungkan dan disusun secara berkala. Dalam prosesnya, menjadi penting untuk mengenal pasti dan menyingkirkan dokumen sokongan yang sebahagian besarnya merupakan pendua versi yang lebih baru. Melakukannya meningkatkan kualiti koleksi, menghilangkan sekam dari hasil carian, dan meningkatkan kepuasan pelanggan. Cabaran teknikal adalah bahawa melalui aliran kerja dan proses manusia, pengetahuan mengenai dokumen yang berkaitan sering hilang. Kami memerlukan kaedah yang dapat mengenal pasti dokumen serupa berdasarkan kandungannya sahaja, tanpa bergantung pada metadata, yang mungkin rosak atau hilang. Kami menyajikan pendekatan untuk mencari fail serupa yang menimbang hingga ke repositori dokumen besar. Ini berdasarkan pada pemotongan aliran bait untuk mencari tandatangan unik yang dapat dikongsi dalam beberapa fail. Analisis grafik sekeping fail menghasilkan kumpulan fail yang berkaitan. Algoritma pemisahan graf bipartit pilihan boleh digunakan untuk meningkatkan skalabiliti. [[EENNDD]] analisis kandungan dan pengindeksan; persamaan; pengurusan dokumen; berhampiran pengesanan pendua; skalabiliti; pengurusan kandungan"], [{"string": "Mining the network value of customers One of the major applications of data mining is in helping companies determine which potential customers to market to . If the expected profit from a customer is greater than the cost of marketing to her , the marketing action for that customer is executed . So far , work in this area has considered only the intrinsic value of the customer i. e , the expected profit from sales to her . We propose to model also the customer 's network value : the expected profit from sales to other customers she may influence to buy , the customers those may influence , and so on recursively . Instead of viewing a market as a set of independent entities , we view it as a social network and model it as a Markov random field . We show the advantages of this approach using a social network mined from a collaborative filtering database . Marketing that exploits the network value of customers -- also known as viral marketing -- can be extremely effective , but is still a black art . Our work can be viewed as a step towards providing a more solid foundation for it , taking advantage of the availability of large relevant databases .", "keywords": ["collaborative filtering", "viral marketing", "markov random fields", "direct marketing", "social networks", "dependency networks"], "combined": "Mining the network value of customers One of the major applications of data mining is in helping companies determine which potential customers to market to . If the expected profit from a customer is greater than the cost of marketing to her , the marketing action for that customer is executed . So far , work in this area has considered only the intrinsic value of the customer i. e , the expected profit from sales to her . We propose to model also the customer 's network value : the expected profit from sales to other customers she may influence to buy , the customers those may influence , and so on recursively . Instead of viewing a market as a set of independent entities , we view it as a social network and model it as a Markov random field . We show the advantages of this approach using a social network mined from a collaborative filtering database . Marketing that exploits the network value of customers -- also known as viral marketing -- can be extremely effective , but is still a black art . Our work can be viewed as a step towards providing a more solid foundation for it , taking advantage of the availability of large relevant databases . [[EENNDD]] collaborative filtering; viral marketing; markov random fields; direct marketing; social networks; dependency networks"}, "Menambang nilai rangkaian pelanggan Salah satu aplikasi utama perlombongan data adalah dengan membantu syarikat menentukan calon pelanggan untuk memasarkan. Sekiranya jangkaan keuntungan dari pelanggan lebih besar daripada biaya pemasaran untuknya, tindakan pemasaran untuk pelanggan tersebut dilaksanakan. Setakat ini, pekerjaan di kawasan ini hanya mempertimbangkan nilai intrinsik pelanggan i. e, jangkaan keuntungan dari penjualan kepadanya. Kami mengusulkan untuk memodelkan juga nilai jaringan pelanggan: keuntungan yang diharapkan dari penjualan kepada pelanggan lain yang mungkin dia mempengaruhi untuk dibeli, pelanggan yang dapat mempengaruhi, dan seterusnya secara berulang. Daripada melihat pasaran sebagai kumpulan entiti bebas, kami melihatnya sebagai rangkaian sosial dan memodelkannya sebagai medan rawak Markov. Kami menunjukkan kelebihan pendekatan ini menggunakan rangkaian sosial yang ditambang dari pangkalan data penapisan kolaboratif. Pemasaran yang mengeksploitasi nilai rangkaian pelanggan - juga dikenal sebagai pemasaran viral - boleh menjadi sangat berkesan, tetapi masih merupakan seni hitam. Kerja kami dapat dilihat sebagai langkah untuk menyediakan asas yang lebih kukuh untuk itu, memanfaatkan ketersediaan pangkalan data yang relevan. [[EENNDD]] penapisan kolaboratif; pemasaran viral; medan rawak markov; pemasaran langsung; rangkaian sosial; rangkaian pergantungan"], [{"string": "Transforming data to satisfy privacy constraints Data on individuals and entities are being collected widely . These data can contain information that explicitly identifies the individual e.g. , social security number . Data can also contain other kinds of personal information e.g. , date of birth , zip code , gender that are potentially identifying when linked with other available data sets . Data are often shared for business or legal reasons . This paper addresses the important issue of preserving the anonymity of the individuals or entities during the data dissemination process . We explore preserving the anonymity by the use of generalizations and suppressions on the potentially identifying portions of the data . We extend earlier works in this area along various dimensions . First , satisfying privacy constraints is considered in conjunction with the usage for the data being disseminated . This allows us to optimize the process of preserving privacy for the specified usage . In particular , we investigate the privacy transformation in the context of data mining applications like building classification and regression models . Second , our work improves on previous approaches by allowing more flexible generalizations for the data . Lastly , this is combined with a more thorough exploration of the solution space using the genetic algorithm framework . These extensions allow us to transform the data so that they are more useful for their intended purpose while satisfying the privacy constraints .", "keywords": ["generalization", "predictive modeling", "data transformation", "suppression"], "combined": "Transforming data to satisfy privacy constraints Data on individuals and entities are being collected widely . These data can contain information that explicitly identifies the individual e.g. , social security number . Data can also contain other kinds of personal information e.g. , date of birth , zip code , gender that are potentially identifying when linked with other available data sets . Data are often shared for business or legal reasons . This paper addresses the important issue of preserving the anonymity of the individuals or entities during the data dissemination process . We explore preserving the anonymity by the use of generalizations and suppressions on the potentially identifying portions of the data . We extend earlier works in this area along various dimensions . First , satisfying privacy constraints is considered in conjunction with the usage for the data being disseminated . This allows us to optimize the process of preserving privacy for the specified usage . In particular , we investigate the privacy transformation in the context of data mining applications like building classification and regression models . Second , our work improves on previous approaches by allowing more flexible generalizations for the data . Lastly , this is combined with a more thorough exploration of the solution space using the genetic algorithm framework . These extensions allow us to transform the data so that they are more useful for their intended purpose while satisfying the privacy constraints . [[EENNDD]] generalization; predictive modeling; data transformation; suppression"}, "Mengubah data untuk memenuhi batasan privasi Data mengenai individu dan entiti dikumpulkan secara meluas. Data ini boleh mengandungi maklumat yang secara jelas mengenali individu tersebut mis. , nombor keselamatan sosial . Data juga boleh mengandungi jenis maklumat peribadi yang lain, mis. , tarikh lahir, poskod, jantina yang berpotensi mengenal pasti apabila dihubungkan dengan set data lain yang tersedia. Data sering dikongsi untuk tujuan perniagaan atau undang-undang. Makalah ini membahas isu penting menjaga kerahsiaan individu atau entiti semasa proses penyebaran data. Kami meneroka untuk mengekalkan anonim dengan penggunaan generalisasi dan penindasan pada bahagian data yang berpotensi mengenal pasti. Kami memperluaskan karya terdahulu di kawasan ini dengan pelbagai dimensi. Pertama, kekangan privasi yang memuaskan dipertimbangkan bersamaan dengan penggunaan data yang disebarkan. Ini membolehkan kita mengoptimumkan proses menjaga privasi untuk penggunaan yang ditentukan. Khususnya, kami menyelidiki transformasi privasi dalam konteks aplikasi perlombongan data seperti klasifikasi bangunan dan model regresi. Kedua, kerja kami meningkatkan pendekatan sebelumnya dengan membenarkan generalisasi yang lebih fleksibel untuk data. Terakhir, ini digabungkan dengan penerokaan ruang penyelesaian yang lebih teliti menggunakan kerangka algoritma genetik. Sambungan ini membolehkan kami mengubah data agar lebih berguna untuk tujuan yang dimaksudkan sambil memenuhi batasan privasi. [[EENNDD]] generalisasi; pemodelan ramalan; transformasi data; penindasan"], [{"string": "Semi-supervised learning with data calibration for long-term time series forecasting Many time series prediction methods have focused on single step or short term prediction problems due to the inherent difficulty in controlling the propagation of errors from one prediction step to the next step . Yet , there is a broad range of applications such as climate impact assessments and urban growth planning that require long term forecasting capabilities for strategic decision making . Training an accurate model that produces reliable long term predictions would require an extensive amount of historical data , which are either unavailable or expensive to acquire . For some of these domains , there are alternative ways to generate potential scenarios for the future using computer-driven simulation models , such as global climate and traffic demand models . However , the data generated by these models are currently utilized in a supervised learning setting , where a predictive model trained on past observations is used to estimate the future values . In this paper , we present a semi-supervised learning framework for long-term time series forecasting based on Hidden Markov Model Regression . A covariance alignment method is also developed to deal with the issue of inconsistencies between historical and model simulation data . We evaluated our approach on data sets from a variety of domains , including climate modeling . Our experimental results demonstrate the efficacy of the approach compared to other supervised learning methods for long-term time series forecasting .", "keywords": ["time series prediction", "miscellaneous", "semi-supervised learning"], "combined": "Semi-supervised learning with data calibration for long-term time series forecasting Many time series prediction methods have focused on single step or short term prediction problems due to the inherent difficulty in controlling the propagation of errors from one prediction step to the next step . Yet , there is a broad range of applications such as climate impact assessments and urban growth planning that require long term forecasting capabilities for strategic decision making . Training an accurate model that produces reliable long term predictions would require an extensive amount of historical data , which are either unavailable or expensive to acquire . For some of these domains , there are alternative ways to generate potential scenarios for the future using computer-driven simulation models , such as global climate and traffic demand models . However , the data generated by these models are currently utilized in a supervised learning setting , where a predictive model trained on past observations is used to estimate the future values . In this paper , we present a semi-supervised learning framework for long-term time series forecasting based on Hidden Markov Model Regression . A covariance alignment method is also developed to deal with the issue of inconsistencies between historical and model simulation data . We evaluated our approach on data sets from a variety of domains , including climate modeling . Our experimental results demonstrate the efficacy of the approach compared to other supervised learning methods for long-term time series forecasting . [[EENNDD]] time series prediction; miscellaneous; semi-supervised learning"}, "Pembelajaran separa penyeliaan dengan penentukuran data untuk peramalan siri masa jangka panjang. Banyak kaedah ramalan siri masa telah menumpukan pada masalah ramalan langkah tunggal atau jangka pendek kerana kesukaran yang wujud dalam mengawal penyebaran kesalahan dari satu langkah ramalan ke langkah seterusnya. Namun, terdapat banyak aplikasi seperti penilaian dampak iklim dan perencanaan pertumbuhan bandar yang memerlukan kemampuan meramalkan jangka panjang untuk membuat keputusan strategis. Melatih model tepat yang menghasilkan ramalan jangka panjang yang boleh dipercayai memerlukan sejumlah besar data sejarah, yang tidak tersedia atau mahal untuk diperoleh. Untuk beberapa domain ini, ada cara alternatif untuk menghasilkan senario berpotensi untuk masa depan menggunakan model simulasi yang didorong oleh komputer, seperti model permintaan iklim dan lalu lintas global. Namun, data yang dihasilkan oleh model-model ini saat ini digunakan dalam pengaturan pembelajaran yang diawasi, di mana model ramalan yang dilatih pada pengamatan masa lalu digunakan untuk memperkirakan nilai-nilai masa depan. Dalam makalah ini, kami memaparkan kerangka pembelajaran separa pengawasan untuk ramalan siri masa jangka panjang berdasarkan Hidden Markov Model Regression. Kaedah penjajaran kovarians juga dikembangkan untuk menangani masalah ketidakkonsistenan antara data simulasi sejarah dan model. Kami menilai pendekatan kami pada set data dari pelbagai domain, termasuk pemodelan iklim. Hasil eksperimen kami menunjukkan keberkesanan pendekatan berbanding kaedah pembelajaran lain yang diselia untuk ramalan siri masa jangka panjang. [[EENNDD]] ramalan siri masa; pelbagai; pembelajaran separa penyeliaan"], [{"string": "Towards scalable support vector machines using squashing", "keywords": ["scalability", "database applications", "boosting", "squashing", "support vector machines"], "combined": "Towards scalable support vector machines using squashing [[EENNDD]] scalability; database applications; boosting; squashing; support vector machines"}, "Ke arah mesin vektor sokongan berskala dengan menggunakan skaling [[EENNDD]]; aplikasi pangkalan data; meningkatkan; mencangkung; mesin vektor sokongan"], [{"string": "Feature selection methods for text classification We consider feature selection for text classification both theoretically and empirically . Our main result is an unsupervised feature selection strategy for which we give worst-case theoretical guarantees on the generalization power of the resultant classification function f with respect to the classification function f obtained when keeping all the features . To the best of our knowledge , this is the first feature selection method with such guarantees . In addition , the analysis leads to insights as to when and why this feature selection strategy will perform well in practice . We then use the TechTC-100 , 20-Newsgroups , and Reuters-RCV2 data sets to evaluate empirically the performance of this and two simpler but related feature selection strategies against two commonly-used strategies . Our empirical evaluation shows that the strategy with provable performance guarantees performs well in comparison with other commonly-used feature selection strategies . In addition , it performs better on certain datasets under very aggressive feature selection .", "keywords": ["text classification", "miscellaneous", "random sampling", "regularized least squares classification", "feature selection"], "combined": "Feature selection methods for text classification We consider feature selection for text classification both theoretically and empirically . Our main result is an unsupervised feature selection strategy for which we give worst-case theoretical guarantees on the generalization power of the resultant classification function f with respect to the classification function f obtained when keeping all the features . To the best of our knowledge , this is the first feature selection method with such guarantees . In addition , the analysis leads to insights as to when and why this feature selection strategy will perform well in practice . We then use the TechTC-100 , 20-Newsgroups , and Reuters-RCV2 data sets to evaluate empirically the performance of this and two simpler but related feature selection strategies against two commonly-used strategies . Our empirical evaluation shows that the strategy with provable performance guarantees performs well in comparison with other commonly-used feature selection strategies . In addition , it performs better on certain datasets under very aggressive feature selection . [[EENNDD]] text classification; miscellaneous; random sampling; regularized least squares classification; feature selection"}, "Kaedah pemilihan ciri untuk klasifikasi teks Kami mempertimbangkan pemilihan ciri untuk klasifikasi teks secara teori dan empirik. Hasil utama kami adalah strategi pemilihan ciri tanpa pengawasan yang kami berikan jaminan teoritis terburuk mengenai kekuatan generalisasi fungsi klasifikasi yang dihasilkan f berkenaan dengan fungsi klasifikasi yang diperoleh ketika menyimpan semua fitur. Sepengetahuan kami, ini adalah kaedah pemilihan ciri pertama dengan jaminan tersebut. Di samping itu, analisis membawa kepada pandangan tentang kapan dan mengapa strategi pemilihan ciri ini akan berfungsi dengan baik dalam praktik. Kami kemudian menggunakan kumpulan data TechTC-100, 20-Newsgroup, dan Reuters-RCV2 untuk menilai prestasi secara empirik ini dan dua strategi pemilihan ciri yang lebih mudah tetapi berkaitan dengan dua strategi yang biasa digunakan. Penilaian empirikal kami menunjukkan bahawa strategi dengan jaminan prestasi yang terbukti dapat berfungsi dengan baik dibandingkan dengan strategi pemilihan ciri lain yang biasa digunakan. Selain itu, ia berfungsi lebih baik pada set data tertentu di bawah pemilihan ciri yang sangat agresif. [[EENNDD]] pengelasan teks; pelbagai; persampelan rawak; klasifikasi kuasa dua terkecil; pemilihan ciri"], [{"string": "A classification-based methodology for planning audit strategies in fraud detection", "keywords": ["decision trees", "classification", "fraud detection", "logic-based database languages", "integration of querying and mining", "knowledge discovery in databases"], "combined": "A classification-based methodology for planning audit strategies in fraud detection [[EENNDD]] decision trees; classification; fraud detection; logic-based database languages; integration of querying and mining; knowledge discovery in databases"}, "Metodologi berasaskan klasifikasi untuk merancang strategi audit dalam penentuan keputusan penipuan [[EENNDD]]; pengelasan; pengesanan penipuan; bahasa pangkalan data berasaskan logik; penggabungan pertanyaan dan perlombongan; penemuan pengetahuan dalam pangkalan data"], [{"string": "Scalable discovery of hidden emails from large folders The popularity of email has triggered researchers to look for ways to help users better organize the enormous amount of information stored in their email folders . One challenge that has not been studied extensively in text mining is the identification and reconstruction of hidden emails . A hidden email is an original email that has been quoted in at least one email in a folder , but does not present itself in the same folder . It may have been un intentionally deleted or may never have been received . The discovery and reconstruction of hidden emails is critical for many applications including email classification , summarization and forensics . This paper proposes a framework for reconstructing hidden emails using the embedded quotations found in messages further down the thread hierarchy . We evaluate the robustness and scalability of our framework by using the Enron public email corpus . Our experiments show that hidden emails exist widely in that corpus and also that our optimization techniques are effective in processing large email folders .", "keywords": ["forensics", "hidden email", "text mining"], "combined": "Scalable discovery of hidden emails from large folders The popularity of email has triggered researchers to look for ways to help users better organize the enormous amount of information stored in their email folders . One challenge that has not been studied extensively in text mining is the identification and reconstruction of hidden emails . A hidden email is an original email that has been quoted in at least one email in a folder , but does not present itself in the same folder . It may have been un intentionally deleted or may never have been received . The discovery and reconstruction of hidden emails is critical for many applications including email classification , summarization and forensics . This paper proposes a framework for reconstructing hidden emails using the embedded quotations found in messages further down the thread hierarchy . We evaluate the robustness and scalability of our framework by using the Enron public email corpus . Our experiments show that hidden emails exist widely in that corpus and also that our optimization techniques are effective in processing large email folders . [[EENNDD]] forensics; hidden email; text mining"}, "Penemuan e-mel tersembunyi dari folder besar Skala populariti e-mel telah mendorong para penyelidik untuk mencari cara untuk membantu pengguna menyusun sejumlah besar maklumat yang tersimpan dalam folder e-mel mereka. Satu cabaran yang belum banyak dikaji dalam perlombongan teks adalah mengenal pasti dan membina semula e-mel tersembunyi. E-mel tersembunyi adalah e-mel asli yang telah disebutkan dalam sekurang-kurangnya satu e-mel dalam folder, tetapi tidak terdapat dalam folder yang sama. Ia mungkin telah dihapus secara tidak sengaja atau mungkin tidak pernah diterima. Penemuan dan penyusunan semula e-mel tersembunyi sangat penting untuk banyak aplikasi termasuk klasifikasi e-mel, ringkasan dan forensik. Makalah ini mencadangkan kerangka kerja untuk membina semula e-mel tersembunyi menggunakan petikan tertanam yang terdapat dalam mesej yang lebih jauh di bawah hierarki utas. Kami menilai ketahanan dan skalabilitas kerangka kerja kami dengan menggunakan korporat e-mel awam Enron. Eksperimen kami menunjukkan bahawa e-mel tersembunyi wujud secara meluas di korpus itu dan juga bahawa teknik pengoptimuman kami berkesan dalam memproses folder e-mel yang besar. [[EENNDD]] forensik; e-mel tersembunyi; perlombongan teks"], [{"string": "On compressing social networks Motivated by structural properties of the Web graph that support efficient data structures for in memory adjacency queries , we study the extent to which a large network can be compressed . Boldi and Vigna WWW 2004 , showed that Web graphs can be compressed down to three bits of storage per edge ; we study the compressibility of social networks where again adjacency queries are a fundamental primitive . To this end , we propose simple combinatorial formulations that encapsulate efficient compressibility of graphs . We show that some of the problems are NP-hard yet admit effective heuristics , some of which can exploit properties of social networks such as link reciprocity . Our extensive experiments show that social networks and the Web graph exhibit vastly different compressibility characteristics .", "keywords": ["social networks", "compression", "linear arrangement", "reciprocity"], "combined": "On compressing social networks Motivated by structural properties of the Web graph that support efficient data structures for in memory adjacency queries , we study the extent to which a large network can be compressed . Boldi and Vigna WWW 2004 , showed that Web graphs can be compressed down to three bits of storage per edge ; we study the compressibility of social networks where again adjacency queries are a fundamental primitive . To this end , we propose simple combinatorial formulations that encapsulate efficient compressibility of graphs . We show that some of the problems are NP-hard yet admit effective heuristics , some of which can exploit properties of social networks such as link reciprocity . Our extensive experiments show that social networks and the Web graph exhibit vastly different compressibility characteristics . [[EENNDD]] social networks; compression; linear arrangement; reciprocity"}, "Pada memampatkan rangkaian sosial yang Dimotivasi oleh sifat struktur grafik Web yang menyokong struktur data yang cekap dalam pertanyaan penambah ingatan, kami mengkaji sejauh mana rangkaian besar dapat dimampatkan. Boldi dan Vigna WWW 2004, menunjukkan bahawa grafik Web boleh dimampatkan hingga tiga bit simpanan per tepi; kami mengkaji kebolehmampatan rangkaian sosial di mana sekali lagi pertanyaan pendekatan adalah primitif asas. Untuk tujuan ini, kami mencadangkan formulasi kombinasi sederhana yang merangkumi kebolehmampatan grafik yang cekap. Kami menunjukkan bahawa beberapa masalah NP-keras tetapi mengakui heuristik yang berkesan, beberapa di antaranya dapat memanfaatkan sifat rangkaian sosial seperti timbal balik pautan. Eksperimen kami yang luas menunjukkan bahawa rangkaian sosial dan grafik Web menunjukkan ciri-ciri kebolehmampatan yang sangat berbeza. [[EENNDD]] rangkaian sosial; pemampatan; susunan linear; timbal balik"], [{"string": "An objective evaluation criterion for clustering We propose and test an objective criterion for evaluation of clustering performance : How well does a clustering algorithm run on unlabeled data aid a classification algorithm ? The accuracy is quantified using the PAC-MDL bound 3 in a semisupervised setting . Clustering algorithms which naturally separate the data according to hidden labels with a small number of clusters perform well . A simple extension of the argument leads to an objective model selection method . Experimental results on text analysis datasets demonstrate that this approach empirically results in very competitive bounds on test set performance on natural datasets .", "keywords": ["mdl", "pac bounds", "evaluation", "clustering"], "combined": "An objective evaluation criterion for clustering We propose and test an objective criterion for evaluation of clustering performance : How well does a clustering algorithm run on unlabeled data aid a classification algorithm ? The accuracy is quantified using the PAC-MDL bound 3 in a semisupervised setting . Clustering algorithms which naturally separate the data according to hidden labels with a small number of clusters perform well . A simple extension of the argument leads to an objective model selection method . Experimental results on text analysis datasets demonstrate that this approach empirically results in very competitive bounds on test set performance on natural datasets . [[EENNDD]] mdl; pac bounds; evaluation; clustering"}, "Kriteria penilaian objektif untuk pengelompokan Kami mencadangkan dan menguji kriteria objektif untuk penilaian prestasi pengelompokan: Sejauh manakah algoritma pengelompokan dijalankan pada data tanpa label membantu algoritma klasifikasi? Ketepatan diukur menggunakan PAC-MDL terikat 3 dalam pengaturan semisuperawasan. Algoritma pengelompokan yang secara semula jadi memisahkan data mengikut label tersembunyi dengan sebilangan kecil kelompok berfungsi dengan baik. Sambungan hujah yang sederhana membawa kepada kaedah pemilihan model objektif. Hasil eksperimen pada set data analisis teks menunjukkan bahawa pendekatan ini secara empirik menghasilkan batasan yang sangat kompetitif terhadap prestasi set ujian pada set data semula jadi. [[EENNDD]] mdl; had pac; penilaian; pengelompokan"], [{"string": "Probabilistic topic models with biased propagation on heterogeneous information networks With the development of Web applications , textual documents are not only getting richer , but also ubiquitously interconnected with users and other objects in various ways , which brings about text-rich heterogeneous information networks . Topic models have been proposed and shown to be useful for document analysis , and the interactions among multi-typed objects play a key role at disclosing the rich semantics of the network . However , most of topic models only consider the textual information while ignore the network structures or can merely integrate with homogeneous networks . None of them can handle heterogeneous information network well . In this paper , we propose a novel topic model with biased propagation TMBP algorithm to directly incorporate heterogeneous information network with topic modeling in a unified way . The underlying intuition is that multi-typed objects should be treated differently along with their inherent textual information and the rich semantics of the heterogeneous information network . A simple and unbiased topic propagation across such a heterogeneous network does not make much sense . Consequently , we investigate and develop two biased propagation frameworks , the biased random walk framework and the biased regularization framework , for the TMBP algorithm from different perspectives , which can discover latent topics and identify clusters of multi-typed objects simultaneously . We extensively evaluate the proposed approach and compare to the state-of-the-art techniques on several datasets . Experimental results demonstrate that the improvement in our proposed approach is consistent and promising .", "keywords": ["heterogeneous information network", "biased propagation", "topic modeling"], "combined": "Probabilistic topic models with biased propagation on heterogeneous information networks With the development of Web applications , textual documents are not only getting richer , but also ubiquitously interconnected with users and other objects in various ways , which brings about text-rich heterogeneous information networks . Topic models have been proposed and shown to be useful for document analysis , and the interactions among multi-typed objects play a key role at disclosing the rich semantics of the network . However , most of topic models only consider the textual information while ignore the network structures or can merely integrate with homogeneous networks . None of them can handle heterogeneous information network well . In this paper , we propose a novel topic model with biased propagation TMBP algorithm to directly incorporate heterogeneous information network with topic modeling in a unified way . The underlying intuition is that multi-typed objects should be treated differently along with their inherent textual information and the rich semantics of the heterogeneous information network . A simple and unbiased topic propagation across such a heterogeneous network does not make much sense . Consequently , we investigate and develop two biased propagation frameworks , the biased random walk framework and the biased regularization framework , for the TMBP algorithm from different perspectives , which can discover latent topics and identify clusters of multi-typed objects simultaneously . We extensively evaluate the proposed approach and compare to the state-of-the-art techniques on several datasets . Experimental results demonstrate that the improvement in our proposed approach is consistent and promising . [[EENNDD]] heterogeneous information network; biased propagation; topic modeling"}, "Model topik probabilistik dengan penyebaran berat sebelah pada rangkaian maklumat heterogen Dengan pengembangan aplikasi Web, dokumen teks tidak hanya semakin kaya, tetapi juga saling terhubung dengan pengguna dan objek lain dengan pelbagai cara, yang mewujudkan rangkaian maklumat heterogen kaya teks. Model-model topik telah diusulkan dan terbukti berguna untuk analisis dokumen, dan interaksi antara objek berbilang menaip memainkan peranan penting dalam mengungkap semantik rangkaian yang kaya. Walau bagaimanapun, kebanyakan model topik hanya mempertimbangkan maklumat teks sambil mengabaikan struktur rangkaian atau hanya dapat disatukan dengan rangkaian homogen. Tiada satu pun dari mereka yang dapat menangani rangkaian maklumat heterogen dengan baik. Dalam makalah ini, kami mencadangkan model topik baru dengan algoritma TMBP penyebaran bias untuk secara langsung menggabungkan rangkaian maklumat heterogen dengan pemodelan topik dengan cara yang bersatu. Intuisi yang mendasari adalah bahawa objek berbilang jenis harus diperlakukan secara berbeza bersama dengan maklumat teks yang melekat dan semantik yang kaya dengan rangkaian maklumat yang heterogen. Penyebaran topik yang sederhana dan tidak berat sebelah di rangkaian heterogen seperti itu tidak masuk akal. Oleh yang demikian, kami menyiasat dan mengembangkan dua kerangka penyebaran berat sebelah, kerangka berjalan rawak bias dan kerangka pengatur bias, untuk algoritma TMBP dari perspektif yang berbeza, yang dapat menemui topik pendam dan mengenal pasti kumpulan objek berbilang jenis secara serentak. Kami secara meluas menilai pendekatan yang dicadangkan dan membandingkan dengan teknik canggih pada beberapa set data. Hasil eksperimen menunjukkan bahawa peningkatan dalam pendekatan yang dicadangkan kami konsisten dan menjanjikan. [[EENNDD]] rangkaian maklumat heterogen; penyebaran berat sebelah; pemodelan topik"], [{"string": "A refinement approach to handling model misfit in text categorization Text categorization or classification is the automated assigning of text documents to pre-defined classes based on their contents . This problem has been studied in information retrieval , machine learning and data mining . So far , many effective techniques have been proposed . However , most techniques are based on some underlying models and\\/or assumptions . When the data fits the model well , the classification accuracy will be high . However , when the data does not fit the model well , the classification accuracy can be very low . In this paper , we propose a refinement approach to dealing with this problem of model misfit . We show that we do not need to change the classification technique itself or its underlying model to make it more flexible . Instead , we propose to use successive refinements of classification on the training data to correct the model misfit . We apply the proposed technique to improve the classification performance of two simple and efficient text classifiers , the Rocchio classifier and the na\u00efve Bayesian classifier . These techniques are suitable for very large text collections because they allow the data to reside on disk and need only one scan of the data to build a text classifier . Extensive experiments on two benchmark document corpora show that the proposed technique is able to improve text categorization accuracy of the two techniques dramatically . In particular , our refined model is able to improve the na\u00efve Bayesian or Rocchio classifier 's prediction performance by 45 % on average .", "keywords": ["na\u00efve bayesian classifier", "rocchio algorithm", "text categorization", "probabilistic algorithms"], "combined": "A refinement approach to handling model misfit in text categorization Text categorization or classification is the automated assigning of text documents to pre-defined classes based on their contents . This problem has been studied in information retrieval , machine learning and data mining . So far , many effective techniques have been proposed . However , most techniques are based on some underlying models and\\/or assumptions . When the data fits the model well , the classification accuracy will be high . However , when the data does not fit the model well , the classification accuracy can be very low . In this paper , we propose a refinement approach to dealing with this problem of model misfit . We show that we do not need to change the classification technique itself or its underlying model to make it more flexible . Instead , we propose to use successive refinements of classification on the training data to correct the model misfit . We apply the proposed technique to improve the classification performance of two simple and efficient text classifiers , the Rocchio classifier and the na\u00efve Bayesian classifier . These techniques are suitable for very large text collections because they allow the data to reside on disk and need only one scan of the data to build a text classifier . Extensive experiments on two benchmark document corpora show that the proposed technique is able to improve text categorization accuracy of the two techniques dramatically . In particular , our refined model is able to improve the na\u00efve Bayesian or Rocchio classifier 's prediction performance by 45 % on average . [[EENNDD]] na\u00efve bayesian classifier; rocchio algorithm; text categorization; probabilistic algorithms"}, "Pendekatan penyempurnaan untuk menangani model yang tidak sesuai dalam pengkategorian teks Pengkategorian teks atau klasifikasi adalah pemberian dokumen teks secara automatik ke kelas yang telah ditentukan berdasarkan kandungannya. Masalah ini telah dikaji dalam pencarian maklumat, pembelajaran mesin dan perlombongan data. Setakat ini, banyak teknik berkesan telah dicadangkan. Walau bagaimanapun, kebanyakan teknik didasarkan pada beberapa model yang mendasari dan \\ / atau andaian. Apabila data sesuai dengan model, ketepatan klasifikasi akan tinggi. Walau bagaimanapun, apabila data tidak sesuai dengan model, ketepatan klasifikasi dapat sangat rendah. Dalam makalah ini, kami mencadangkan pendekatan penyempurnaan untuk menangani masalah ketidaksesuaian model ini. Kami menunjukkan bahawa kami tidak perlu mengubah teknik klasifikasi itu sendiri atau model yang mendasarinya untuk menjadikannya lebih fleksibel. Sebaliknya, kami mencadangkan untuk menggunakan penyempurnaan klasifikasi berturut-turut pada data latihan untuk memperbaiki model yang tidak sesuai. Kami mengaplikasikan teknik yang dicadangkan untuk meningkatkan prestasi klasifikasi dua pengkelasan teks sederhana dan cekap, pengklasifikasi Rocchio dan pengklasifikasi Bayesian yang naif. Teknik-teknik ini sesuai untuk koleksi teks yang sangat besar kerana membolehkan data berada di dalam cakera dan hanya memerlukan satu imbasan data untuk membina pengkelasan teks. Eksperimen yang meluas pada dua korporat dokumen penanda aras menunjukkan bahawa teknik yang dicadangkan dapat meningkatkan ketepatan pengkategorian teks dari kedua-dua teknik tersebut secara mendadak. Khususnya, model yang disempurnakan kami dapat meningkatkan prestasi ramalan pengklasifikasi Bayesian atau Rocchio yang naif secara purata sebanyak 45%. [[EENNDD]] pengkelasan bayesian naif; algoritma rocchio; pengkategorian teks; algoritma probabilistik"], [{"string": "Applying data mining techniques to address disaster information management challenges on mobile devices The improvement of Crisis Management and Disaster Recovery techniques are national priorities in the wake of man-made and nature inflicted calamities of the last decade . Our prior work has demonstrated that the efficiency of sharing and managing information plays an important role in business recovery efforts after disaster event . With the proliferation of smart phones and wireless tablets , professionals who have an operational responsibility in disaster situations are relying on such devices to maintain communication . Further , with the rise of social media , technology savvy consumers are also using these devices extensively for situational updates . In this paper , we address several critical tasks which can facilitate information sharing and collaboration between both private and public sector participants for major disaster recovery planning and management . We design and implement an All-Hazard Disaster Situation Browser ADSB system that runs on Apple 's mobile operating system iOS and iPhone and iPad mobile devices . Our proposed techniques create a collaborative solution on a mobile platform using advanced data mining and information retrieval techniques for disaster preparedness and recovery that helps impacted communities better understand the current disaster situation and how the community is recovering . Specifically , hierarchical summarization techniques are used to generate brief reviews from a large collection of reports at different granularities ; probabilistic models are proposed to dynamically generate query forms based on user 's feedback ; and recommendation techniques are adapted to help users identify potential contacts for report sharing and community organization . Furthermore , the developed techniques are designed to be all-hazard capable so that they can be used in earthquake , terrorism , or other unanticipated disaster situations .", "keywords": ["hierarchical summarization", "user recommendation", "dynamic query form", "disaster information management"], "combined": "Applying data mining techniques to address disaster information management challenges on mobile devices The improvement of Crisis Management and Disaster Recovery techniques are national priorities in the wake of man-made and nature inflicted calamities of the last decade . Our prior work has demonstrated that the efficiency of sharing and managing information plays an important role in business recovery efforts after disaster event . With the proliferation of smart phones and wireless tablets , professionals who have an operational responsibility in disaster situations are relying on such devices to maintain communication . Further , with the rise of social media , technology savvy consumers are also using these devices extensively for situational updates . In this paper , we address several critical tasks which can facilitate information sharing and collaboration between both private and public sector participants for major disaster recovery planning and management . We design and implement an All-Hazard Disaster Situation Browser ADSB system that runs on Apple 's mobile operating system iOS and iPhone and iPad mobile devices . Our proposed techniques create a collaborative solution on a mobile platform using advanced data mining and information retrieval techniques for disaster preparedness and recovery that helps impacted communities better understand the current disaster situation and how the community is recovering . Specifically , hierarchical summarization techniques are used to generate brief reviews from a large collection of reports at different granularities ; probabilistic models are proposed to dynamically generate query forms based on user 's feedback ; and recommendation techniques are adapted to help users identify potential contacts for report sharing and community organization . Furthermore , the developed techniques are designed to be all-hazard capable so that they can be used in earthquake , terrorism , or other unanticipated disaster situations . [[EENNDD]] hierarchical summarization; user recommendation; dynamic query form; disaster information management"}, "Mengaplikasikan teknik perlombongan data untuk menangani cabaran pengurusan maklumat bencana pada peranti mudah alih Peningkatan teknik Pengurusan Krisis dan Pemulihan Bencana adalah keutamaan nasional setelah berlakunya bencana buatan manusia dan alam semenjak dekad yang lalu. Kerja terdahulu kami telah membuktikan bahawa kecekapan berkongsi dan mengurus maklumat memainkan peranan penting dalam usaha pemulihan perniagaan selepas peristiwa bencana. Dengan berkembangnya telefon pintar dan tablet tanpa wayar, profesional yang mempunyai tanggungjawab operasi dalam situasi bencana bergantung pada peranti tersebut untuk mengekalkan komunikasi. Lebih jauh lagi, dengan munculnya media sosial, pengguna yang menggunakan teknologi juga menggunakan peranti ini secara meluas untuk kemas kini keadaan. Dalam makalah ini, kami membahas beberapa tugas penting yang dapat mempermudah perkongsian maklumat dan kerjasama antara peserta sektor swasta dan awam untuk perancangan dan pengurusan pemulihan bencana utama. Kami merancang dan melaksanakan sistem ADSB Browser Situasi Bencana Seluruh Bahaya yang berjalan pada sistem operasi mudah alih Apple, peranti mudah alih iOS dan iPhone dan iPad. Teknik yang dicadangkan kami membuat penyelesaian kolaboratif pada platform mudah alih menggunakan teknik perlombongan data dan pengambilan maklumat yang maju untuk kesiapsiagaan dan pemulihan bencana yang membantu masyarakat yang terkena dampak memahami situasi bencana semasa dan bagaimana masyarakat sedang pulih. Secara khusus, teknik ringkasan hierarki digunakan untuk menghasilkan tinjauan ringkas dari koleksi laporan yang besar pada butiran yang berbeza; model probabilistik dicadangkan untuk menghasilkan borang pertanyaan secara dinamik berdasarkan maklum balas pengguna; dan teknik cadangan disesuaikan untuk membantu pengguna mengenal pasti potensi kenalan untuk perkongsian laporan dan organisasi masyarakat. Selanjutnya, teknik yang dibangunkan dirancang untuk semua bahaya sehingga dapat digunakan dalam gempa bumi, terorisme, atau situasi bencana lain yang tidak disangka-sangka. [[EENNDD]] ringkasan hierarki; cadangan pengguna; borang pertanyaan dinamik; pengurusan maklumat bencana"], [{"string": "Finding recent frequent itemsets adaptively over online data streams A data stream is a massive unbounded sequence of data elements continuously generated at a rapid rate . Consequently , the knowledge embedded in a data stream is more likely to be changed as time goes by . Identifying the recent change of a data stream , specially for an online data stream , can provide valuable information for the analysis of the data stream . In addition , monitoring the continuous variation of a data stream enables to find the gradual change of embedded knowledge . However , most of mining algorithms over a data stream do not differentiate the information of recently generated transactions from the obsolete information of old transactions which may be no longer useful or possibly invalid at present . This paper proposes a data mining method for finding recent frequent itemsets adaptively over an online data stream . The effect of old transactions on the mining result of the data steam is diminished by decaying the old occurrences of each itemset as time goes by . Furthermore , several optimization techniques are devised to minimize processing time as well as main memory usage . Finally , the proposed method is analyzed by a series of experiments .", "keywords": ["recent frequent itemsets", "delayed-insertion", "data stream", "database applications", "pruning of itemsets", "decay mechanism"], "combined": "Finding recent frequent itemsets adaptively over online data streams A data stream is a massive unbounded sequence of data elements continuously generated at a rapid rate . Consequently , the knowledge embedded in a data stream is more likely to be changed as time goes by . Identifying the recent change of a data stream , specially for an online data stream , can provide valuable information for the analysis of the data stream . In addition , monitoring the continuous variation of a data stream enables to find the gradual change of embedded knowledge . However , most of mining algorithms over a data stream do not differentiate the information of recently generated transactions from the obsolete information of old transactions which may be no longer useful or possibly invalid at present . This paper proposes a data mining method for finding recent frequent itemsets adaptively over an online data stream . The effect of old transactions on the mining result of the data steam is diminished by decaying the old occurrences of each itemset as time goes by . Furthermore , several optimization techniques are devised to minimize processing time as well as main memory usage . Finally , the proposed method is analyzed by a series of experiments . [[EENNDD]] recent frequent itemsets; delayed-insertion; data stream; database applications; pruning of itemsets; decay mechanism"}, "Mencari set item kerap baru secara adaptif melalui aliran data dalam talian Aliran data adalah urutan elemen data yang besar tanpa had yang terus dihasilkan pada kadar yang cepat. Akibatnya, pengetahuan yang tertanam dalam aliran data lebih cenderung diubah seiring berjalannya waktu. Mengenal pasti perubahan aliran data baru-baru ini, khas untuk aliran data dalam talian, dapat memberikan maklumat berharga untuk analisis aliran data. Di samping itu, memantau variasi berterusan aliran data memungkinkan untuk mencari perubahan pengetahuan tertanam secara beransur-ansur. Walau bagaimanapun, sebahagian besar algoritma perlombongan melalui aliran data tidak membezakan maklumat urus niaga yang baru dihasilkan dari maklumat usang transaksi lama yang mungkin tidak lagi berguna atau mungkin tidak sah pada masa ini. Makalah ini mencadangkan kaedah perlombongan data untuk mencari set item kerap baru secara adaptif melalui aliran data dalam talian. Kesan urus niaga lama ke atas hasil penambangan dari stim data dikurangkan dengan merosakkan kejadian lama setiap itemet seiring berjalannya waktu. Selanjutnya, beberapa teknik pengoptimuman dirancang untuk meminimumkan masa pemprosesan dan juga penggunaan memori utama. Akhirnya, kaedah yang dicadangkan dianalisis dengan satu siri eksperimen. [[EENNDD]] kumpulan item baru-baru ini; penangguhan tertunda; aliran data; aplikasi pangkalan data; pemangkasan set barang; mekanisme pereputan"], [{"string": "Inverted matrix : efficient discovery of frequent items in large datasets in the context of interactive mining Existing association rule mining algorithms suffer from many problems when mining massive transactional datasets . One major problem is the high memory dependency : either the gigantic data structure built is assumed to fit in main memory , or the recursive mining process is too voracious in memory resources . Another major impediment is the repetitive and interactive nature of any knowledge discovery process . To tune parameters , many runs of the same algorithms are necessary leading to the building of these huge data structures time and again . This paper proposes a new disk-based association rule mining algorithm called Inverted Matrix , which achieves its efficiency by applying three new ideas . First , transactional data is converted into a new database layout called Inverted Matrix that prevents multiple scanning of the database during the mining phase , in which finding frequent patterns could be achieved in less than a full scan with random access . Second , for each frequent item , a relatively small independent tree is built summarizing co-occurrences . Finally , a simple and non-recursive mining process reduces the memory requirements as minimum candidacy generation and counting is needed . Experimental studies reveal that our Inverted Matrix approach outperform FP-Tree especially in mining very large transactional databases with a very large number of unique items . Our random access disk-based approach is particularly advantageous in a repetitive and interactive setting .", "keywords": ["inverted matrix", "association rules", "cofi-tree", "database applications", "frequent patterns mining"], "combined": "Inverted matrix : efficient discovery of frequent items in large datasets in the context of interactive mining Existing association rule mining algorithms suffer from many problems when mining massive transactional datasets . One major problem is the high memory dependency : either the gigantic data structure built is assumed to fit in main memory , or the recursive mining process is too voracious in memory resources . Another major impediment is the repetitive and interactive nature of any knowledge discovery process . To tune parameters , many runs of the same algorithms are necessary leading to the building of these huge data structures time and again . This paper proposes a new disk-based association rule mining algorithm called Inverted Matrix , which achieves its efficiency by applying three new ideas . First , transactional data is converted into a new database layout called Inverted Matrix that prevents multiple scanning of the database during the mining phase , in which finding frequent patterns could be achieved in less than a full scan with random access . Second , for each frequent item , a relatively small independent tree is built summarizing co-occurrences . Finally , a simple and non-recursive mining process reduces the memory requirements as minimum candidacy generation and counting is needed . Experimental studies reveal that our Inverted Matrix approach outperform FP-Tree especially in mining very large transactional databases with a very large number of unique items . Our random access disk-based approach is particularly advantageous in a repetitive and interactive setting . [[EENNDD]] inverted matrix; association rules; cofi-tree; database applications; frequent patterns mining"}, "Matriks terbalik: penemuan cekap item kerap dalam set data besar dalam konteks perlombongan interaktif Algoritma perlombongan peraturan persatuan sedia ada mengalami banyak masalah ketika melombong set data transaksi besar-besaran. Salah satu masalah utama adalah ketergantungan memori yang tinggi: sama ada struktur data raksasa yang dibina dianggap sesuai dengan memori utama, atau proses penambangan rekursif terlalu rakus dalam sumber memori. Halangan utama yang lain adalah sifat berulang dan interaktif dari sebarang proses penemuan pengetahuan. Untuk menyesuaikan parameter, diperlukan banyak algoritma yang sama yang membawa kepada pembinaan struktur data besar ini berulang-ulang kali. Makalah ini mencadangkan algoritma perlombongan peraturan persatuan berasaskan cakera baru yang disebut Inverted Matrix, yang mencapai kecekapannya dengan menerapkan tiga idea baru. Pertama, data transaksional diubah menjadi susun atur pangkalan data baru yang disebut Inverted Matrix yang menghalang pengimbasan pangkalan data berganda semasa fasa perlombongan, di mana mencari corak yang kerap dapat dicapai dalam waktu kurang dari imbasan penuh dengan akses rawak. Kedua, untuk setiap item yang kerap, pokok bebas yang agak kecil dibina merangkum kejadian bersama. Akhirnya, proses perlombongan yang sederhana dan tidak berulang dapat mengurangkan keperluan memori kerana penjanaan dan penghitungan pencalonan minimum diperlukan. Kajian eksperimen menunjukkan bahawa pendekatan Matriks Terbalik kami mengatasi FP-Tree terutamanya dalam melombong pangkalan data transaksi yang sangat besar dengan sebilangan besar item unik. Pendekatan berasaskan cakera akses rawak kami sangat menguntungkan dalam suasana berulang dan interaktif. [[EENNDD]] matriks terbalik; peraturan persatuan; pokok cofi; aplikasi pangkalan data; perlombongan corak yang kerap"], [{"string": "Generating English summaries of time series data using the Gricean maxims We are developing technology for generating English textual summaries of time-series data , in three domains : weather forecasts , gas-turbine sensor readings , and hospital intensive care data . Our weather-forecast generator is currently operational and being used daily by a meteorological company . We generate summaries in three steps : a selecting the most important trends and patterns to communicate ; b mapping these patterns onto words and phrases ; and c generating actual texts based on these words and phrases . In this paper we focus on the first step , a , selecting the information to communicate , and describe how we perform this using modified versions of standard data analysis algorithms such as segmentation . The modifications arose out of empirical work with users and domain experts , and in fact can all be regarded as applications of the Gricean maxims of Quality , Quantity , Relevance , and Manner , which describe how a cooperative speaker should behave in order to help a hearer correctly interpret a text . The Gricean maxims are perhaps a key element of adapting data analysis algorithms for effective communication of information to human users , and should be considered by other researchers interested in communicating data to human users .", "keywords": ["gricean maxims", "summarization", "natural language processing", "time series data"], "combined": "Generating English summaries of time series data using the Gricean maxims We are developing technology for generating English textual summaries of time-series data , in three domains : weather forecasts , gas-turbine sensor readings , and hospital intensive care data . Our weather-forecast generator is currently operational and being used daily by a meteorological company . We generate summaries in three steps : a selecting the most important trends and patterns to communicate ; b mapping these patterns onto words and phrases ; and c generating actual texts based on these words and phrases . In this paper we focus on the first step , a , selecting the information to communicate , and describe how we perform this using modified versions of standard data analysis algorithms such as segmentation . The modifications arose out of empirical work with users and domain experts , and in fact can all be regarded as applications of the Gricean maxims of Quality , Quantity , Relevance , and Manner , which describe how a cooperative speaker should behave in order to help a hearer correctly interpret a text . The Gricean maxims are perhaps a key element of adapting data analysis algorithms for effective communication of information to human users , and should be considered by other researchers interested in communicating data to human users . [[EENNDD]] gricean maxims; summarization; natural language processing; time series data"}, "Menghasilkan ringkasan bahasa Inggeris data siri masa menggunakan pepatah Gricean. Kami sedang mengembangkan teknologi untuk menghasilkan ringkasan teks Inggeris data siri masa, dalam tiga domain: ramalan cuaca, bacaan sensor turbin gas, dan data perawatan intensif hospital. Penjana ramalan cuaca kami kini beroperasi dan digunakan setiap hari oleh syarikat meteorologi. Kami menghasilkan ringkasan dalam tiga langkah: memilih trend dan corak yang paling penting untuk berkomunikasi; b memetakan corak ini ke kata dan frasa; dan c menghasilkan teks sebenar berdasarkan perkataan dan frasa ini. Dalam makalah ini kami memfokuskan pada langkah pertama, a, memilih maklumat untuk berkomunikasi, dan menerangkan bagaimana kami melakukan ini menggunakan versi yang diubah suai dari algoritma analisis data standard seperti segmentasi. Pengubahsuaian itu berlaku hasil kerja empirik dengan pengguna dan pakar domain, dan sebenarnya semuanya dapat dianggap sebagai aplikasi dari kualitas Gricean, Kualitas, Kuantitas, Relevansi, dan Cara, yang menggambarkan bagaimana penutur koperasi harus bersikap untuk membantu pendengar mentafsir teks dengan betul. Pepatah Gricean mungkin merupakan elemen utama untuk menyesuaikan algoritma analisis data untuk komunikasi maklumat yang berkesan kepada pengguna manusia, dan harus dipertimbangkan oleh penyelidik lain yang berminat untuk menyampaikan data kepada pengguna manusia. [[EENNDD]] pepatah gricean; ringkasan; pemprosesan bahasa semula jadi; data siri masa"], [{"string": "Scalable influence maximization for prevalent viral marketing in large-scale social networks Influence maximization , defined by Kempe , Kleinberg , and Tardos 2003 , is the problem of finding a small set of seed nodes in a social network that maximizes the spread of influence under certain influence cascade models . The scalability of influence maximization is a key factor for enabling prevalent viral marketing in large-scale online social networks . Prior solutions , such as the greedy algorithm of Kempe et al. 2003 and its improvements are slow and not scalable , while other heuristic algorithms do not provide consistently good performance on influence spreads . In this paper , we design a new heuristic algorithm that is easily scalable to millions of nodes and edges in our experiments . Our algorithm has a simple tunable parameter for users to control the balance between the running time and the influence spread of the algorithm . Our results from extensive simulations on several real-world and synthetic networks demonstrate that our algorithm is currently the best scalable solution to the influence maximization problem : a our algorithm scales beyond million-sized graphs where the greedy algorithm becomes infeasible , and b in all size ranges , our algorithm performs consistently well in influence spread -- it is always among the best algorithms , and in most cases it significantly outperforms all other scalable heuristics to as much as 100 % -- 260 % increase in influence spread .", "keywords": ["influence maximization", "viral marketing", "social networks"], "combined": "Scalable influence maximization for prevalent viral marketing in large-scale social networks Influence maximization , defined by Kempe , Kleinberg , and Tardos 2003 , is the problem of finding a small set of seed nodes in a social network that maximizes the spread of influence under certain influence cascade models . The scalability of influence maximization is a key factor for enabling prevalent viral marketing in large-scale online social networks . Prior solutions , such as the greedy algorithm of Kempe et al. 2003 and its improvements are slow and not scalable , while other heuristic algorithms do not provide consistently good performance on influence spreads . In this paper , we design a new heuristic algorithm that is easily scalable to millions of nodes and edges in our experiments . Our algorithm has a simple tunable parameter for users to control the balance between the running time and the influence spread of the algorithm . Our results from extensive simulations on several real-world and synthetic networks demonstrate that our algorithm is currently the best scalable solution to the influence maximization problem : a our algorithm scales beyond million-sized graphs where the greedy algorithm becomes infeasible , and b in all size ranges , our algorithm performs consistently well in influence spread -- it is always among the best algorithms , and in most cases it significantly outperforms all other scalable heuristics to as much as 100 % -- 260 % increase in influence spread . [[EENNDD]] influence maximization; viral marketing; social networks"}, "Memaksimumkan pengaruh berskala untuk pemasaran virus yang lazim di rangkaian sosial berskala besar Pemaksimakan pengaruh, yang ditakrifkan oleh Kempe, Kleinberg, dan Tardos 2003, adalah masalah mencari sekumpulan kecil benih dalam rangkaian sosial yang memaksimumkan penyebaran pengaruh di bawah pengaruh tertentu model lata. Skalabilitas memaksimumkan pengaruh adalah faktor utama untuk memungkinkan pemasaran virus yang berlaku di rangkaian sosial dalam talian berskala besar. Penyelesaian sebelumnya, seperti algoritma tamak Kempe et al. 2003 dan penambahbaikannya perlahan dan tidak boleh diskalakan, sementara algoritma heuristik yang lain tidak memberikan prestasi yang baik secara konsisten pada spread pengaruh. Dalam makalah ini, kami merancang algoritma heuristik baru yang mudah ditingkatkan kepada berjuta-juta nod dan tepi dalam eksperimen kami. Algoritma kami mempunyai parameter yang dapat disesuaikan untuk pengguna untuk mengawal keseimbangan antara masa berjalan dan pengaruh penyebaran algoritma. Hasil kami dari simulasi yang meluas di beberapa rangkaian dunia nyata dan sintetik menunjukkan bahawa algoritma kami pada masa ini adalah penyelesaian terbaik untuk masalah pemaksimalan pengaruh: algoritma kami menimbang skala berukuran juta di mana algoritma tamak menjadi tidak dapat dilaksanakan, dan b dalam semua ukuran julat, algoritma kami menunjukkan prestasi yang baik secara konsisten dalam penyebaran pengaruh - ia selalu merupakan antara algoritma terbaik, dan dalam kebanyakan kes, ia mengatasi semua heuristik berskala lain sehingga 100% - 260% peningkatan pengaruh penyebaran. [[EENNDD]] mempengaruhi pemaksaan; pemasaran viral; rangkaian sosial"], [{"string": "Tensor-CUR decompositions for tensor-based data Motivated by numerous applications in which the data may be modeled by a variable subscripted by three or more indices , we develop a tensor-based extension of the matrix CUR decomposition . The tensor-CUR decomposition is most relevant as a data analysis tool when the data consist of one mode that is qualitatively different than the others . In this case , the tensor-CUR decomposition approximately expresses the original data tensor in terms of a basis consisting of underlying subtensors that are actual data elements and thus that have natural interpretation in terms ofthe processes generating the data . In order to demonstrate the general applicability of this tensor decomposition , we apply it to problems in two diverse domains of data analysis : hyperspectral medical image analysis and consumer recommendation system analysis . In the hyperspectral data application , the tensor-CUR decomposition is used to compress the data , and we show that classification quality is not substantially reduced even after substantial data compression . In the recommendation system application , the tensor-CUR decomposition is used to reconstruct missing entries in a user-product-product preference tensor , and we show that high quality recommendations can be made on the basis of a small number of basis users and a small number of product-product comparisons from a new user .", "keywords": ["tensor cur", "miscellaneous", "hyperspectral image analysis", "cur decomposition", "recommendation system analysis"], "combined": "Tensor-CUR decompositions for tensor-based data Motivated by numerous applications in which the data may be modeled by a variable subscripted by three or more indices , we develop a tensor-based extension of the matrix CUR decomposition . The tensor-CUR decomposition is most relevant as a data analysis tool when the data consist of one mode that is qualitatively different than the others . In this case , the tensor-CUR decomposition approximately expresses the original data tensor in terms of a basis consisting of underlying subtensors that are actual data elements and thus that have natural interpretation in terms ofthe processes generating the data . In order to demonstrate the general applicability of this tensor decomposition , we apply it to problems in two diverse domains of data analysis : hyperspectral medical image analysis and consumer recommendation system analysis . In the hyperspectral data application , the tensor-CUR decomposition is used to compress the data , and we show that classification quality is not substantially reduced even after substantial data compression . In the recommendation system application , the tensor-CUR decomposition is used to reconstruct missing entries in a user-product-product preference tensor , and we show that high quality recommendations can be made on the basis of a small number of basis users and a small number of product-product comparisons from a new user . [[EENNDD]] tensor cur; miscellaneous; hyperspectral image analysis; cur decomposition; recommendation system analysis"}, "Penguraian Tensor-CUR untuk data berdasarkan tensor Dimotivasi oleh banyak aplikasi di mana data dapat dimodelkan oleh pemboleh ubah yang dilanggan oleh tiga atau lebih indeks, kami mengembangkan perpanjangan berdasarkan tensor dekomposisi matriks CUR. Penguraian tensor-CUR paling relevan sebagai alat analisis data apabila data terdiri dari satu mod yang berbeza secara kualitatif daripada yang lain. Dalam kes ini, penguraian tensor-CUR menyatakan tensor data asal dari segi asas yang terdiri daripada subtensor yang mendasari yang merupakan elemen data sebenar dan dengan itu mempunyai tafsiran semula jadi dari segi proses menghasilkan data. Untuk menunjukkan kegunaan umum penguraian tensor ini, kami menerapkannya pada masalah dalam dua domain analisis data yang beragam: analisis gambar perubatan hiperspectral dan analisis sistem cadangan pengguna. Dalam aplikasi data hiperspectral, penguraian tensor-CUR digunakan untuk memampatkan data, dan kami menunjukkan bahawa kualiti klasifikasi tidak berkurang secara substansial bahkan setelah pemampatan data yang besar. Dalam aplikasi sistem cadangan, penguraian tensor-CUR digunakan untuk membina semula entri yang hilang dalam tensor pilihan produk-produk-pengguna, dan kami menunjukkan bahawa cadangan berkualiti tinggi dapat dibuat berdasarkan sejumlah kecil pengguna dasar dan sedikit bilangan perbandingan produk-produk dari pengguna baru. [[EENNDD]] tensor cur; pelbagai; analisis gambar hiperspectral; penguraian cur; analisis sistem cadangan"], [{"string": "Sampling-based sequential subgroup mining Subgroup discovery is a learning task that aims at finding interesting rules from classified examples . The search is guided by a utility function , trading off the coverage of rules against their statistical unusualness . One shortcoming of existing approaches is that they do not incorporate prior knowledge . To this end a novel generic sampling strategy is proposed . It allows to turn pattern mining into an iterative process . In each iteration the focus of subgroup discovery lies on those patterns that are unexpected with respect to prior knowledge and previously discovered patterns . The result of this technique is a small diverse set of understandable rules that characterise a specified property of interest . As another contribution this article derives a simple connection between subgroup discovery and classifier induction . For a popular utility function this connection allows to apply any standard rule induction algorithm to the task of subgroup discovery after a step of stratified resampling . The proposed techniques are empirically compared to state of the art subgroup discovery algorithms .", "keywords": ["prior knowledge", "subgroup discovery", "sampling"], "combined": "Sampling-based sequential subgroup mining Subgroup discovery is a learning task that aims at finding interesting rules from classified examples . The search is guided by a utility function , trading off the coverage of rules against their statistical unusualness . One shortcoming of existing approaches is that they do not incorporate prior knowledge . To this end a novel generic sampling strategy is proposed . It allows to turn pattern mining into an iterative process . In each iteration the focus of subgroup discovery lies on those patterns that are unexpected with respect to prior knowledge and previously discovered patterns . The result of this technique is a small diverse set of understandable rules that characterise a specified property of interest . As another contribution this article derives a simple connection between subgroup discovery and classifier induction . For a popular utility function this connection allows to apply any standard rule induction algorithm to the task of subgroup discovery after a step of stratified resampling . The proposed techniques are empirically compared to state of the art subgroup discovery algorithms . [[EENNDD]] prior knowledge; subgroup discovery; sampling"}, "Perlombongan subkumpulan berurutan berdasarkan penemuan Subkumpulan penemuan adalah tugas pembelajaran yang bertujuan untuk mencari peraturan menarik dari contoh terperingkat. Pencarian dipandu oleh fungsi utiliti, memperdagangkan liputan peraturan terhadap kebiasaan statistik mereka. Salah satu kelemahan pendekatan yang ada ialah mereka tidak memasukkan pengetahuan sebelumnya. Untuk tujuan ini dicadangkan strategi pengambilan sampel generik. Ia membolehkan mengubah perlombongan corak menjadi proses berulang. Dalam setiap iterasi, fokus penemuan subkumpulan terletak pada corak yang tidak dijangka berkaitan dengan pengetahuan sebelumnya dan corak yang dijumpai sebelumnya. Hasil dari teknik ini adalah sekumpulan peraturan yang dapat difahami yang berbeza yang mencirikan harta benda tertentu. Sebagai sumbangan lain artikel ini memperoleh hubungan mudah antara penemuan subkumpulan dan induksi pengkelasan. Untuk fungsi utiliti yang popular, sambungan ini memungkinkan untuk menerapkan algoritma induksi peraturan standard untuk tugas penemuan subkelompok setelah langkah pengambilan sampel berstrata. Teknik yang dicadangkan secara empirik dibandingkan dengan algoritma penemuan subkumpulan canggih. [[EENNDD]] pengetahuan sebelumnya; penemuan subkumpulan; persampelan"], [{"string": "Latent aspect rating analysis without aspect keyword supervision Mining detailed opinions buried in the vast amount of review text data is an important , yet quite challenging task with widespread applications in multiple domains . Latent Aspect Rating Analysis LARA refers to the task of inferring both opinion ratings on topical aspects e.g. , location , service of a hotel and the relative weights reviewers have placed on each aspect based on review content and the associated overall ratings . A major limitation of previous work on LARA is the assumption of pre-specified aspects by keywords . However , the aspect information is not always available , and it may be difficult to pre-define appropriate aspects without a good knowledge about what aspects are actually commented on in the reviews . In this paper , we propose a unified generative model for LARA , which does not need pre-specified aspect keywords and simultaneously mines 1 latent topical aspects , 2 ratings on each identified aspect , and 3 weights placed on different aspects by a reviewer . Experiment results on two different review data sets demonstrate that the proposed model can effectively perform the Latent Aspect Rating Analysis task without the supervision of aspect keywords . Because of its generality , the proposed model can be applied to explore all kinds of opinionated text data containing overall sentiment judgments and support a wide range of interesting application tasks , such as aspect-based opinion summarization , personalized entity ranking and recommendation , and reviewer behavior analysis .", "keywords": ["review mining", "latent rating analysis", "information search and retrieval", "aspect identification"], "combined": "Latent aspect rating analysis without aspect keyword supervision Mining detailed opinions buried in the vast amount of review text data is an important , yet quite challenging task with widespread applications in multiple domains . Latent Aspect Rating Analysis LARA refers to the task of inferring both opinion ratings on topical aspects e.g. , location , service of a hotel and the relative weights reviewers have placed on each aspect based on review content and the associated overall ratings . A major limitation of previous work on LARA is the assumption of pre-specified aspects by keywords . However , the aspect information is not always available , and it may be difficult to pre-define appropriate aspects without a good knowledge about what aspects are actually commented on in the reviews . In this paper , we propose a unified generative model for LARA , which does not need pre-specified aspect keywords and simultaneously mines 1 latent topical aspects , 2 ratings on each identified aspect , and 3 weights placed on different aspects by a reviewer . Experiment results on two different review data sets demonstrate that the proposed model can effectively perform the Latent Aspect Rating Analysis task without the supervision of aspect keywords . Because of its generality , the proposed model can be applied to explore all kinds of opinionated text data containing overall sentiment judgments and support a wide range of interesting application tasks , such as aspect-based opinion summarization , personalized entity ranking and recommendation , and reviewer behavior analysis . [[EENNDD]] review mining; latent rating analysis; information search and retrieval; aspect identification"}, "Analisis penilaian aspek laten tanpa pengawasan kata kunci aspek Menambang pendapat terperinci yang terkumpul dalam sejumlah besar data teks ulasan adalah tugas penting, namun cukup mencabar dengan aplikasi yang meluas dalam pelbagai domain. Latent Aspect Rating Analysis LARA merujuk kepada tugas menyimpulkan kedua-dua penilaian pendapat mengenai aspek topikal mis. , lokasi, perkhidmatan hotel dan penimbang berat relatif telah meletakkan setiap aspek berdasarkan kandungan ulasan dan penilaian keseluruhan yang berkaitan. Batasan utama kerja sebelumnya pada LARA adalah andaian aspek yang ditentukan sebelumnya dengan kata kunci. Walau bagaimanapun, maklumat aspek tidak selalu tersedia, dan mungkin sukar untuk menentukan aspek yang sesuai tanpa pengetahuan yang baik mengenai aspek apa yang sebenarnya dikomentari dalam ulasan. Dalam makalah ini, kami mencadangkan model generatif terpadu untuk LARA, yang tidak memerlukan kata kunci aspek yang ditentukan sebelumnya dan secara serentak melombong 1 aspek topikal pendam, 2 penilaian pada setiap aspek yang dikenal pasti, dan 3 bobot yang diletakkan pada aspek yang berbeza oleh pengulas. Hasil eksperimen pada dua kumpulan data tinjauan yang berbeza menunjukkan bahawa model yang dicadangkan dapat melaksanakan tugas Analisis Penilaian Laten Aspek dengan berkesan tanpa pengawasan kata kunci aspek. Kerana sifatnya yang umum, model yang diusulkan dapat diterapkan untuk menjelajahi semua jenis data teks yang disusun yang berisi penilaian sentimen keseluruhan dan mendukung berbagai tugas aplikasi yang menarik, seperti ringkasan pendapat berdasarkan aspek, peringkat dan cadangan entiti yang diperibadikan, dan perilaku pengulas analisis. [[EENNDD]] mengkaji perlombongan; analisis penilaian pendam; carian dan pengambilan maklumat; pengenalpastian aspek"], [{"string": "Overlapping experiment infrastructure : more , better , faster experimentation At Google , experimentation is practically a mantra ; we evaluate almost every change that potentially affects what our users experience . Such changes include not only obvious user-visible changes such as modifications to a user interface , but also more subtle changes such as different machine learning algorithms that might affect ranking or content selection . Our insatiable appetite for experimentation has led us to tackle the problems of how to run more experiments , how to run experiments that produce better decisions , and how to run them faster . In this paper , we describe Google 's overlapping experiment infrastructure that is a key component to solving these problems . In addition , because an experiment infrastructure alone is insufficient , we also discuss the associated tools and educational processes required to use it effectively . We conclude by describing trends that show the success of this overall experimental environment . While the paper specifically describes the experiment system and experimental processes we have in place at Google , we believe they can be generalized and applied by any entity interested in using experimentation to improve search engines and other web applications .", "keywords": ["website testing", "a/b testing", "multivariable testing", "controlled experiments"], "combined": "Overlapping experiment infrastructure : more , better , faster experimentation At Google , experimentation is practically a mantra ; we evaluate almost every change that potentially affects what our users experience . Such changes include not only obvious user-visible changes such as modifications to a user interface , but also more subtle changes such as different machine learning algorithms that might affect ranking or content selection . Our insatiable appetite for experimentation has led us to tackle the problems of how to run more experiments , how to run experiments that produce better decisions , and how to run them faster . In this paper , we describe Google 's overlapping experiment infrastructure that is a key component to solving these problems . In addition , because an experiment infrastructure alone is insufficient , we also discuss the associated tools and educational processes required to use it effectively . We conclude by describing trends that show the success of this overall experimental environment . While the paper specifically describes the experiment system and experimental processes we have in place at Google , we believe they can be generalized and applied by any entity interested in using experimentation to improve search engines and other web applications . [[EENNDD]] website testing; a/b testing; multivariable testing; controlled experiments"}, "Infrastruktur percubaan bertindih: eksperimen lebih banyak, lebih baik, lebih pantas Di Google, percubaan secara praktiknya adalah mantra; kami menilai hampir setiap perubahan yang berpotensi mempengaruhi apa yang dialami pengguna kami. Perubahan tersebut merangkumi bukan sahaja perubahan yang dapat dilihat oleh pengguna seperti perubahan pada antara muka pengguna, tetapi juga perubahan yang lebih halus seperti algoritma pembelajaran mesin yang berbeza yang mungkin mempengaruhi peringkat atau pemilihan kandungan. Selera makan yang tidak puas untuk eksperimen telah mendorong kita untuk mengatasi masalah bagaimana menjalankan lebih banyak eksperimen, bagaimana menjalankan eksperimen yang menghasilkan keputusan yang lebih baik, dan bagaimana menjalankannya dengan lebih cepat. Dalam makalah ini, kami menerangkan infrastruktur eksperimen Google yang tumpang tindih yang merupakan komponen utama untuk menyelesaikan masalah ini. Sebagai tambahan, kerana infrastruktur eksperimen sahaja tidak mencukupi, kami juga membincangkan alat dan proses pendidikan yang diperlukan untuk menggunakannya dengan berkesan. Kami membuat kesimpulan dengan menerangkan trend yang menunjukkan kejayaan persekitaran eksperimen keseluruhan ini. Walaupun makalah ini secara khusus menerangkan sistem eksperimen dan proses eksperimen yang kami lakukan di Google, kami percaya proses tersebut dapat digeneralisasikan dan diterapkan oleh mana-mana entiti yang berminat menggunakan eksperimen untuk meningkatkan mesin pencari dan aplikasi web lain. [[EENNDD]] ujian laman web; ujian a / b; ujian berbilang pembolehubah; eksperimen terkawal"], [{"string": "Topics over time : a non-Markov continuous-time model of topical trends This paper presents an LDA-style topic model that captures not only the low-dimensional structure of data , but also how the structure changes over time . Unlike other recent work that relies on Markov assumptions or discretization of time , here each topic is associated with a continuous distribution over timestamps , and for each generated document , the mixture distribution over topics is influenced by both word co-occurrences and the document 's timestamp . Thus , the meaning of a particular topic can be relied upon as constant , but the topics ' occurrence and correlations change significantly over time . We present results on nine months of personal email , 17 years of NIPS research papers and over 200 years of presidential state-of-the-union addresses , showing improved topics , better timestamp prediction , and interpretable trends .", "keywords": ["graphical models", "learning", "topic modeling", "temporal analysis"], "combined": "Topics over time : a non-Markov continuous-time model of topical trends This paper presents an LDA-style topic model that captures not only the low-dimensional structure of data , but also how the structure changes over time . Unlike other recent work that relies on Markov assumptions or discretization of time , here each topic is associated with a continuous distribution over timestamps , and for each generated document , the mixture distribution over topics is influenced by both word co-occurrences and the document 's timestamp . Thus , the meaning of a particular topic can be relied upon as constant , but the topics ' occurrence and correlations change significantly over time . We present results on nine months of personal email , 17 years of NIPS research papers and over 200 years of presidential state-of-the-union addresses , showing improved topics , better timestamp prediction , and interpretable trends . [[EENNDD]] graphical models; learning; topic modeling; temporal analysis"}, "Topik dari masa ke masa: model tren topikal masa berterusan bukan Markov Makalah ini membentangkan model topik gaya LDA yang menangkap bukan sahaja struktur data dimensi rendah, tetapi juga bagaimana struktur berubah dari masa ke masa. Tidak seperti karya baru-baru ini yang bergantung pada andaian Markov atau diskretisasi masa, di sini setiap topik dikaitkan dengan pengedaran berterusan mengenai cap waktu, dan untuk setiap dokumen yang dihasilkan, pengagihan campuran atas topik dipengaruhi oleh kedua-dua kata bersama dan dokumen tersebut. cap waktu. Oleh itu, makna topik tertentu dapat diandalkan sebagai tetap, tetapi kejadian dan korelasi topik berubah dengan ketara dari masa ke masa. Kami membentangkan hasil pada sembilan bulan e-mel peribadi, 17 tahun kertas penyelidikan NIPS dan lebih dari 200 tahun alamat negara presiden, yang menunjukkan topik yang lebih baik, ramalan cap waktu yang lebih baik, dan trend yang dapat ditafsirkan. [[EENNDD]] model grafik; belajar; pemodelan topik; analisis temporal"], [{"string": "Visually mining and monitoring massive time series Moments before the launch of every space vehicle , engineering discipline specialists must make a critical go\\/no-go decision . The cost of a false positive , allowing a launch in spite of a fault , or a false negative , stopping a potentially successful launch , can be measured in the tens of millions of dollars , not including the cost in morale and other more intangible detriments . The Aerospace Corporation is responsible for providing engineering assessments critical to the go\\/no-go decision for every Department of Defense space vehicle . These assessments are made by constantly monitoring streaming telemetry data in the hours before launch . We will introduce VizTree , a novel time-series visualization tool to aid the Aerospace analysts who must make these engineering assessments . VizTree was developed at the University of California , Riverside and is unique in that the same tool is used for mining archival data and monitoring incoming live telemetry . The use of a single tool for both aspects of the task allows a natural and intuitive transfer of mined knowledge to the monitoring task . Our visualization approach works by transforming the time series into a symbolic representation , and encoding the data in a modified suffix tree in which the frequency and other properties of patterns are mapped onto colors and other visual properties . We demonstrate the utility of our system by comparing it with state-of-the-art batch algorithms on several real and synthetic datasets .", "keywords": ["pattern discovery", "anomaly detection", "time series", "motif discovery", "visualization"], "combined": "Visually mining and monitoring massive time series Moments before the launch of every space vehicle , engineering discipline specialists must make a critical go\\/no-go decision . The cost of a false positive , allowing a launch in spite of a fault , or a false negative , stopping a potentially successful launch , can be measured in the tens of millions of dollars , not including the cost in morale and other more intangible detriments . The Aerospace Corporation is responsible for providing engineering assessments critical to the go\\/no-go decision for every Department of Defense space vehicle . These assessments are made by constantly monitoring streaming telemetry data in the hours before launch . We will introduce VizTree , a novel time-series visualization tool to aid the Aerospace analysts who must make these engineering assessments . VizTree was developed at the University of California , Riverside and is unique in that the same tool is used for mining archival data and monitoring incoming live telemetry . The use of a single tool for both aspects of the task allows a natural and intuitive transfer of mined knowledge to the monitoring task . Our visualization approach works by transforming the time series into a symbolic representation , and encoding the data in a modified suffix tree in which the frequency and other properties of patterns are mapped onto colors and other visual properties . We demonstrate the utility of our system by comparing it with state-of-the-art batch algorithms on several real and synthetic datasets . [[EENNDD]] pattern discovery; anomaly detection; time series; motif discovery; visualization"}, "Melombong secara visual dan memantau siri masa yang besar Beberapa saat sebelum pelancaran setiap kenderaan angkasa, pakar disiplin kejuruteraan mesti membuat keputusan go / / go-go yang kritikal. Kos positif positif, membenarkan pelancaran walaupun ada kesalahan, atau negatif palsu, menghentikan pelancaran yang berpotensi berjaya, dapat diukur dalam puluhan juta dolar, tidak termasuk biaya moral dan kerugian lain yang lebih tidak ketara. Aerospace Corporation bertanggungjawab untuk memberikan penilaian kejuruteraan yang penting bagi keputusan go \\ / no-go untuk setiap kenderaan ruang angkasa Jabatan Pertahanan. Penilaian ini dibuat dengan sentiasa memantau streaming data telemetri beberapa jam sebelum dilancarkan. Kami akan memperkenalkan VizTree, alat visualisasi siri masa baru untuk membantu penganalisis Aeroangkasa yang mesti membuat penilaian kejuruteraan ini. VizTree dikembangkan di University of California, Riverside dan unik kerana alat yang sama digunakan untuk melombong data arkib dan memantau telemetri langsung yang masuk. Penggunaan satu alat untuk kedua-dua aspek tugas memungkinkan pemindahan pengetahuan yang dilombong secara semula jadi dan intuitif ke tugas pemantauan. Pendekatan visualisasi kami berfungsi dengan mengubah siri masa menjadi representasi simbolik, dan mengekodkan data dalam pohon akhiran yang diubah di mana frekuensi dan sifat corak lain dipetakan ke warna dan sifat visual lain. Kami menunjukkan kegunaan sistem kami dengan membandingkannya dengan algoritma kumpulan canggih pada beberapa set data sebenar dan sintetik. [[EENNDD]] penemuan corak; pengesanan anomali; siri masa; penemuan motif; visualisasi"], [{"string": "A framework for analysis of dynamic social networks Finding patterns of social interaction within a population has wide-ranging applications including : disease modeling , cultural and information transmission , and behavioral ecology . Social interactions are often modeled with networks . A key characteristic of social interactions is their continual change . However , most past analyses of social networks are essentially static in that all information about the time that social interactions take place is discarded . In this paper , we propose a new mathematical and computational framework that enables analysis of dynamic social networks and that explicitly makes use of information about when social interactions occur .", "keywords": ["disease spread", "dynamic social networks", "model development"], "combined": "A framework for analysis of dynamic social networks Finding patterns of social interaction within a population has wide-ranging applications including : disease modeling , cultural and information transmission , and behavioral ecology . Social interactions are often modeled with networks . A key characteristic of social interactions is their continual change . However , most past analyses of social networks are essentially static in that all information about the time that social interactions take place is discarded . In this paper , we propose a new mathematical and computational framework that enables analysis of dynamic social networks and that explicitly makes use of information about when social interactions occur . [[EENNDD]] disease spread; dynamic social networks; model development"}, "Kerangka kerja untuk analisis rangkaian sosial yang dinamis Mencari corak interaksi sosial dalam populasi mempunyai aplikasi yang luas termasuk: pemodelan penyakit, penyebaran budaya dan maklumat, dan ekologi tingkah laku. Interaksi sosial sering dimodelkan dengan rangkaian. Ciri utama interaksi sosial adalah perubahan berterusan mereka. Walau bagaimanapun, kebanyakan analisis masa lalu mengenai rangkaian sosial pada dasarnya bersifat statik kerana semua maklumat mengenai masa interaksi sosial berlaku dibuang. Dalam makalah ini, kami mengusulkan kerangka kerja matematik dan komputasi baru yang memungkinkan analisis rangkaian sosial yang dinamik dan yang secara eksplisit menggunakan maklumat mengenai kapan interaksi sosial berlaku. [[EENNDD]] penyebaran penyakit; rangkaian sosial yang dinamik; pembangunan model"], [{"string": "Mining quantitative correlated patterns using an information-theoretic approach Existing research on mining quantitative databases mainly focuses on mining associations . However , mining associations is too expensive to be practical in many cases . In this paper , we study mining correlations from quantitative databases and show that it is a more effective approach than mining associations . We propose a new notion of Quantitative Correlated Patterns QCPs , which is founded on two formal concepts , mutual information and all-confidence . We first devise a normalization on mutual information and apply it to QCP mining to capture the dependency between the attributes . We further adopt all-confidence as a quality measure to control , at a finer granularity , the dependency between the attributes with specific quantitative intervals . We also propose a supervised method to combine the consecutive intervals of the quantitative attributes based on mutual information , such that the interval combining is guided by the dependency between the attributes . We develop an algorithm , QCoMine , to efficiently mine QCPs by utilizing normalized mutual information and all-confidence to perform a two-level pruning . Our experiments verify the efficiency of QCoMine and the quality of the QCPs .", "keywords": ["mutual information", "correlated patterns", "quantitative databases", "information-theoretic approach"], "combined": "Mining quantitative correlated patterns using an information-theoretic approach Existing research on mining quantitative databases mainly focuses on mining associations . However , mining associations is too expensive to be practical in many cases . In this paper , we study mining correlations from quantitative databases and show that it is a more effective approach than mining associations . We propose a new notion of Quantitative Correlated Patterns QCPs , which is founded on two formal concepts , mutual information and all-confidence . We first devise a normalization on mutual information and apply it to QCP mining to capture the dependency between the attributes . We further adopt all-confidence as a quality measure to control , at a finer granularity , the dependency between the attributes with specific quantitative intervals . We also propose a supervised method to combine the consecutive intervals of the quantitative attributes based on mutual information , such that the interval combining is guided by the dependency between the attributes . We develop an algorithm , QCoMine , to efficiently mine QCPs by utilizing normalized mutual information and all-confidence to perform a two-level pruning . Our experiments verify the efficiency of QCoMine and the quality of the QCPs . [[EENNDD]] mutual information; correlated patterns; quantitative databases; information-theoretic approach"}, "Melombong corak korelasi kuantitatif menggunakan pendekatan teori-maklumat Penyelidikan yang ada pada pangkalan data kuantitatif perlombongan terutamanya menumpukan pada persatuan perlombongan. Walau bagaimanapun, persatuan perlombongan terlalu mahal untuk praktikal dalam banyak kes. Dalam makalah ini, kami mengkaji korelasi perlombongan dari pangkalan data kuantitatif dan menunjukkan bahawa ia adalah pendekatan yang lebih berkesan daripada persatuan perlombongan. Kami mencadangkan pengertian baru mengenai QCP Pola Berkorelasi Kuantitatif, yang didasarkan pada dua konsep formal, maklumat bersama dan keyakinan penuh. Kami pertama kali membuat normalisasi pada maklumat bersama dan menerapkannya pada QCP mining untuk menangkap pergantungan antara atribut. Kami selanjutnya menerapkan keyakinan penuh sebagai ukuran kualiti untuk mengendalikan, pada butiran yang lebih halus, kebergantungan antara atribut dengan selang kuantitatif tertentu. Kami juga mencadangkan kaedah yang diawasi untuk menggabungkan selang berturut-turut atribut kuantitatif berdasarkan maklumat bersama, sehingga penggabungan selang dipandu oleh ketergantungan antara atribut. Kami mengembangkan algoritma, QCoMine, untuk menambang QCP dengan cekap dengan menggunakan maklumat bersama dan keyakinan yang normal untuk melakukan pemangkasan dua peringkat. Eksperimen kami mengesahkan kecekapan QCoMine dan kualiti QCP. [[EENNDD]] maklumat bersama; corak berkorelasi; pangkalan data kuantitatif; pendekatan maklumat-teori"], [{"string": "A visual-analytic toolkit for dynamic interaction graphs In this article we describe a visual-analytic tool for the interrogation of evolving interaction network data such as those found in social , bibliometric , WWW and biological applications . The tool we have developed incorporates common visualization paradigms such as zooming , coarsening and filtering while naturally integrating information extracted by a previously described event-driven framework for characterizing the evolution of such networks . The visual front-end provides features that are specifically useful in the analysis of interaction networks , capturing the dynamic nature of both individual entities as well as interactions among them . The tool provides the user with the option of selecting multiple views , designed to capture different aspects of the evolving graph from the perspective of a node , a community or a subset of nodes of interest . Standard visual templates and cues are used to highlight critical changes that have occurred during the evolution of the network . A key challenge we address in this work is that of scalability - handling large graphs both in terms of the efficiency of the back-end , and in terms of the efficiency of the visual layout and rendering . Two case studies based on bibliometric and Wikipedia data are presented to demonstrate the utility of the toolkit for visual knowledge discovery .", "keywords": ["visual analytics", "dynamic interaction networks", "graph visualization"], "combined": "A visual-analytic toolkit for dynamic interaction graphs In this article we describe a visual-analytic tool for the interrogation of evolving interaction network data such as those found in social , bibliometric , WWW and biological applications . The tool we have developed incorporates common visualization paradigms such as zooming , coarsening and filtering while naturally integrating information extracted by a previously described event-driven framework for characterizing the evolution of such networks . The visual front-end provides features that are specifically useful in the analysis of interaction networks , capturing the dynamic nature of both individual entities as well as interactions among them . The tool provides the user with the option of selecting multiple views , designed to capture different aspects of the evolving graph from the perspective of a node , a community or a subset of nodes of interest . Standard visual templates and cues are used to highlight critical changes that have occurred during the evolution of the network . A key challenge we address in this work is that of scalability - handling large graphs both in terms of the efficiency of the back-end , and in terms of the efficiency of the visual layout and rendering . Two case studies based on bibliometric and Wikipedia data are presented to demonstrate the utility of the toolkit for visual knowledge discovery . [[EENNDD]] visual analytics; dynamic interaction networks; graph visualization"}, "Kit alat visual-analitik untuk grafik interaksi dinamik Dalam artikel ini kami menerangkan alat visual-analitik untuk interogasi data rangkaian interaksi yang berkembang seperti yang terdapat dalam aplikasi sosial, bibliometrik, WWW dan biologi. Alat yang kami kembangkan menggabungkan paradigma visualisasi umum seperti zoom, kasar dan penapisan sambil menyatukan maklumat secara semula jadi yang diekstraksi oleh kerangka kerja peristiwa yang dijelaskan sebelumnya untuk mencirikan evolusi rangkaian tersebut. Bahagian depan visual menyediakan ciri-ciri yang berguna secara khusus dalam analisis rangkaian interaksi, menangkap sifat dinamik kedua-dua entiti individu dan juga interaksi di antara mereka. Alat ini memberi pengguna pilihan untuk memilih beberapa paparan, yang dirancang untuk menangkap aspek yang berbeza dari grafik yang berkembang dari perspektif nod, komuniti atau sekumpulan nod yang menarik. Templat dan isyarat visual standard digunakan untuk menyoroti perubahan kritikal yang telah berlaku semasa evolusi rangkaian. Cabaran utama yang kami hadapi dalam karya ini adalah skalabiliti - menangani grafik besar baik dari segi kecekapan back-end, dan dari segi kecekapan susun atur visual dan rendering. Dua kajian kes berdasarkan data bibliometrik dan Wikipedia dikemukakan untuk menunjukkan kegunaan kit alat untuk penemuan pengetahuan visual. [[EENNDD]] analisis visual; rangkaian interaksi dinamik; visualisasi grafik"], [{"string": "Model compression Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers . Unfortunately , the space required to store this many classifiers , and the time required to execute them at run-time , prohibits their use in applications where test sets are large e.g. Google , where storage space is at a premium e.g. PDAs , and where computational power is limited e.g. hea-ring aids . We present a method for `` compressing '' large , complex ensembles into smaller , faster models , usually without significant loss in performance .", "keywords": ["model compression", "supervised learning"], "combined": "Model compression Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers . Unfortunately , the space required to store this many classifiers , and the time required to execute them at run-time , prohibits their use in applications where test sets are large e.g. Google , where storage space is at a premium e.g. PDAs , and where computational power is limited e.g. hea-ring aids . We present a method for `` compressing '' large , complex ensembles into smaller , faster models , usually without significant loss in performance . [[EENNDD]] model compression; supervised learning"}, "Pemampatan model Selalunya model pembelajaran yang diselia dengan prestasi terbaik adalah kumpulan beratus-ratus atau ribuan pengkelasan peringkat asas. Malangnya, ruang yang diperlukan untuk menyimpan banyak pengklasifikasi ini, dan masa yang diperlukan untuk melaksanakannya pada waktu berjalan, melarang penggunaannya dalam aplikasi yang set ujiannya besar, mis. Google, di mana ruang simpanan adalah premium, mis. PDA, dan di mana daya komputasi terhad mis. alat pendengaran . Kami menyajikan kaedah untuk \"memampatkan\" ansambel besar dan kompleks ke dalam model yang lebih kecil dan lebih cepat, biasanya tanpa penurunan prestasi. [[EENNDD]] pemampatan model; pembelajaran yang diselia"], [{"string": "Improving spatial locality of programs via data mining In most computer systems , page fault rate is currently minimized by generic page replacement algorithms which try to model the temporal locality inherent in programs . In this paper , we propose two algorithms , one greedy and the other stochastic , designed for program specific code restructuring as a means of increasing spatial locality within a program . Both algorithms effectively decrease average working set size and hence the page fault rate . Our methods are more effective than traditional approaches due to use of domain information . We illustrate the efficacy of our algorithms on actual data mining algorithms .", "keywords": ["code restructuring", "page clustering", "database applications", "program locality"], "combined": "Improving spatial locality of programs via data mining In most computer systems , page fault rate is currently minimized by generic page replacement algorithms which try to model the temporal locality inherent in programs . In this paper , we propose two algorithms , one greedy and the other stochastic , designed for program specific code restructuring as a means of increasing spatial locality within a program . Both algorithms effectively decrease average working set size and hence the page fault rate . Our methods are more effective than traditional approaches due to use of domain information . We illustrate the efficacy of our algorithms on actual data mining algorithms . [[EENNDD]] code restructuring; page clustering; database applications; program locality"}, "Memperbaiki lokaliti spasial program melalui perlombongan data Di kebanyakan sistem komputer, kadar kesalahan halaman kini diminimumkan oleh algoritma penggantian halaman generik yang cuba memodelkan lokaliti temporal yang wujud dalam program. Dalam makalah ini, kami mencadangkan dua algoritma, satu rakus dan satu lagi stokastik, yang dirancang untuk penyusunan semula kod khusus program sebagai cara untuk meningkatkan lokaliti ruang dalam suatu program. Kedua-dua algoritma secara efektif menurunkan purata ukuran set kerja dan dengan itu kadar kesalahan halaman. Kaedah kami lebih berkesan daripada pendekatan tradisional kerana penggunaan maklumat domain. Kami menggambarkan keberkesanan algoritma kami terhadap algoritma perlombongan data sebenar. [[EENNDD]] penyusunan semula kod; pengelompokan halaman; aplikasi pangkalan data; lokaliti program"], [{"string": "When do data mining results violate privacy ? Privacy-preserving data mining has concentrated on obtaining valid results when the input data is private . An extreme example is Secure Multiparty Computation-based methods , where only the results are revealed . However , this still leaves a potential privacy breach : Do the results themselves violate privacy ? This paper explores this issue , developing a framework under which this question can be addressed . Metrics are proposed , along with analysis that those metrics are consistent in the face of apparent problems .", "keywords": ["privacy", "inference"], "combined": "When do data mining results violate privacy ? Privacy-preserving data mining has concentrated on obtaining valid results when the input data is private . An extreme example is Secure Multiparty Computation-based methods , where only the results are revealed . However , this still leaves a potential privacy breach : Do the results themselves violate privacy ? This paper explores this issue , developing a framework under which this question can be addressed . Metrics are proposed , along with analysis that those metrics are consistent in the face of apparent problems . [[EENNDD]] privacy; inference"}, "Bilakah hasil perlombongan data melanggar privasi? Perlombongan data yang memelihara privasi telah tertumpu pada memperoleh hasil yang sah apabila data input bersifat peribadi. Contoh yang melampau adalah kaedah berasaskan Pengiraan Multiparty Aman, di mana hanya hasilnya yang dinyatakan. Walau bagaimanapun, ini masih meninggalkan kemungkinan pelanggaran privasi: Adakah hasilnya sendiri melanggar privasi? Makalah ini meneroka masalah ini, mengembangkan kerangka kerja di mana persoalan ini dapat ditangani. Metrik dicadangkan, bersama dengan analisis bahawa metrik tersebut konsisten dalam menghadapi masalah yang jelas. [[EENNDD]] privasi; kesimpulan"], [{"string": "Query result clustering for object-level search Query result clustering has recently attracted a lot of attention to provide users with a succinct overview of relevant results . However , little work has been done on organizing the query results for object-level search . Object-level search result clustering is challenging because we need to support diverse similarity notions over object-specific features such as the price and weight of a product of heterogeneous domains . To address this challenge , we propose a hybrid subspace clustering algorithm called Hydra . Algorithm Hydra captures the user perception of diverse similarity notions from millions of Web pages and disambiguates different senses using feature-based subspace locality measures . Our proposed solution , by combining wisdom of crowds and wisdom of data , achieves robustness and efficiency over existing approaches . We extensively evaluate our proposed framework and demonstrate how to enrich user experiences in object-level search using a real-world product search scenarios .", "keywords": ["object-level search", "subspace clustering", "performance evaluation"], "combined": "Query result clustering for object-level search Query result clustering has recently attracted a lot of attention to provide users with a succinct overview of relevant results . However , little work has been done on organizing the query results for object-level search . Object-level search result clustering is challenging because we need to support diverse similarity notions over object-specific features such as the price and weight of a product of heterogeneous domains . To address this challenge , we propose a hybrid subspace clustering algorithm called Hydra . Algorithm Hydra captures the user perception of diverse similarity notions from millions of Web pages and disambiguates different senses using feature-based subspace locality measures . Our proposed solution , by combining wisdom of crowds and wisdom of data , achieves robustness and efficiency over existing approaches . We extensively evaluate our proposed framework and demonstrate how to enrich user experiences in object-level search using a real-world product search scenarios . [[EENNDD]] object-level search; subspace clustering; performance evaluation"}, "Pengelompokan hasil pertanyaan untuk pencarian tingkat objek Pengelompokan hasil pertanyaan baru-baru ini menarik banyak perhatian untuk memberikan gambaran ringkas ringkas mengenai hasil yang relevan kepada pengguna. Walau bagaimanapun, sedikit usaha dilakukan untuk mengatur hasil pertanyaan untuk carian tahap objek. Pengelompokan hasil carian tahap objek sangat mencabar kerana kita perlu menyokong pengertian kesamaan yang berbeza-beza terhadap ciri khusus objek seperti harga dan berat produk domain yang heterogen. Untuk mengatasi cabaran ini, kami mencadangkan algoritma pengelompokan ruang bawah tanah hibrid yang disebut Hydra. Algoritma Hydra menangkap persepsi pengguna terhadap konsep persamaan yang beragam dari berjuta-juta halaman Web dan menghilangkan deria yang berbeza menggunakan langkah-langkah lokaliti ruang bawah tanah berdasarkan ciri. Penyelesaian yang dicadangkan kami, dengan menggabungkan kebijaksanaan orang banyak dan kebijaksanaan data, mencapai kekuatan dan kecekapan berbanding pendekatan yang ada. Kami menilai secara meluas kerangka cadangan kami dan menunjukkan bagaimana memperkayakan pengalaman pengguna dalam carian tahap objek menggunakan senario carian produk dunia nyata. [[EENNDD]] carian tahap objek; pengelompokan ruang bawah tanah; penilaian prestasi"], [{"string": "Friendship and mobility : user movement in location-based social networks Even though human movement and mobility patterns have a high degree of freedom and variation , they also exhibit structural patterns due to geographic and social constraints . Using cell phone location data , as well as data from two online location-based social networks , we aim to understand what basic laws govern human motion and dynamics . We find that humans experience a combination of periodic movement that is geographically limited and seemingly random jumps correlated with their social networks . Short-ranged travel is periodic both spatially and temporally and not effected by the social network structure , while long-distance travel is more influenced by social network ties . We show that social relationships can explain about 10 % to 30 % of all human movement , while periodic behavior explains 50 % to 70 % . Based on our findings , we develop a model of human mobility that combines periodic short range movements with travel due to the social network structure . We show that our model reliably predicts the locations and dynamics of future human movement and gives an order of magnitude better performance than present models of human mobility .", "keywords": ["social networks", "human mobility", "communication networks"], "combined": "Friendship and mobility : user movement in location-based social networks Even though human movement and mobility patterns have a high degree of freedom and variation , they also exhibit structural patterns due to geographic and social constraints . Using cell phone location data , as well as data from two online location-based social networks , we aim to understand what basic laws govern human motion and dynamics . We find that humans experience a combination of periodic movement that is geographically limited and seemingly random jumps correlated with their social networks . Short-ranged travel is periodic both spatially and temporally and not effected by the social network structure , while long-distance travel is more influenced by social network ties . We show that social relationships can explain about 10 % to 30 % of all human movement , while periodic behavior explains 50 % to 70 % . Based on our findings , we develop a model of human mobility that combines periodic short range movements with travel due to the social network structure . We show that our model reliably predicts the locations and dynamics of future human movement and gives an order of magnitude better performance than present models of human mobility . [[EENNDD]] social networks; human mobility; communication networks"}, "Persahabatan dan mobiliti: pergerakan pengguna dalam jaringan sosial berdasarkan lokasi Walaupun corak pergerakan dan pergerakan manusia mempunyai tahap kebebasan dan variasi yang tinggi, mereka juga menunjukkan corak struktur kerana kekangan geografi dan sosial. Dengan menggunakan data lokasi telefon bimbit, serta data dari dua rangkaian sosial berasaskan lokasi dalam talian, kami bertujuan untuk memahami undang-undang asas apa yang mengatur pergerakan dan dinamika manusia. Kami mendapati bahawa manusia mengalami kombinasi pergerakan berkala yang terhad secara geografi dan lonjakan yang kelihatan rawak yang berkaitan dengan rangkaian sosial mereka. Perjalanan jarak jauh berkala secara spasial dan temporal dan tidak dipengaruhi oleh struktur rangkaian sosial, sementara perjalanan jarak jauh lebih banyak dipengaruhi oleh hubungan jaringan sosial. Kami menunjukkan bahawa hubungan sosial dapat menjelaskan sekitar 10% hingga 30% dari semua pergerakan manusia, sementara tingkah laku berkala menjelaskan 50% hingga 70%. Berdasarkan penemuan kami, kami mengembangkan model mobiliti manusia yang menggabungkan pergerakan jarak pendek berkala dengan perjalanan kerana struktur rangkaian sosial. Kami menunjukkan bahawa model kami dengan tepat meramalkan lokasi dan dinamika pergerakan manusia masa depan dan memberikan pesanan prestasi yang lebih baik daripada model mobiliti manusia sekarang. [[EENNDD]] rangkaian sosial; mobiliti manusia; rangkaian komunikasi"], [{"string": "Predictive client-side profiles for personalized advertising Personalization is ubiquitous in modern online applications as it provides significant improvements in user experience by adapting it to inferred user preferences . However , there are increasing concerns related to issues of privacy and control of the user data that is aggregated by online systems to power personalized experiences . These concerns are particularly significant for user profile aggregation in online advertising . This paper describes a practical , learning-driven client-side personalization approach for keyword advertising platforms , an emerging application previously not addressed in literature . Our approach relies on storing user-specific information entirely within the user 's control in a browser cookie or browser local storage , thus allowing the user to view , edit or purge it at any time e.g. , via a dedicated webpage . We develop a principled , utility-based formulation for the problem of iteratively updating user profiles stored client-side , which relies on calibrated prediction of future user activity . While optimal profile construction is NP-hard for pay-per-click advertising with bid increments , it can be efficiently solved via a greedy approximation algorithm guaranteed to provide a near-optimal solution due to the fact that keyword profile utility is submodular : it exhibits the property of diminishing returns with increasing profile size . We empirically evaluate client-side keyword profiles for keyword advertising on a large-scale dataset from a major search engine . Experiments demonstrate that predictive client-side personalization allows ad platforms to retain almost all of the revenue gains from personalization even if they give users the freedom to opt out of behavior tracking backed by server-side storage . Additionally , we show that advertisers can potentially increase their return on investment significantly by utilizing bid increments for keyword profiles in their ad campaigns .", "keywords": ["client-side personalization", "online advertising"], "combined": "Predictive client-side profiles for personalized advertising Personalization is ubiquitous in modern online applications as it provides significant improvements in user experience by adapting it to inferred user preferences . However , there are increasing concerns related to issues of privacy and control of the user data that is aggregated by online systems to power personalized experiences . These concerns are particularly significant for user profile aggregation in online advertising . This paper describes a practical , learning-driven client-side personalization approach for keyword advertising platforms , an emerging application previously not addressed in literature . Our approach relies on storing user-specific information entirely within the user 's control in a browser cookie or browser local storage , thus allowing the user to view , edit or purge it at any time e.g. , via a dedicated webpage . We develop a principled , utility-based formulation for the problem of iteratively updating user profiles stored client-side , which relies on calibrated prediction of future user activity . While optimal profile construction is NP-hard for pay-per-click advertising with bid increments , it can be efficiently solved via a greedy approximation algorithm guaranteed to provide a near-optimal solution due to the fact that keyword profile utility is submodular : it exhibits the property of diminishing returns with increasing profile size . We empirically evaluate client-side keyword profiles for keyword advertising on a large-scale dataset from a major search engine . Experiments demonstrate that predictive client-side personalization allows ad platforms to retain almost all of the revenue gains from personalization even if they give users the freedom to opt out of behavior tracking backed by server-side storage . Additionally , we show that advertisers can potentially increase their return on investment significantly by utilizing bid increments for keyword profiles in their ad campaigns . [[EENNDD]] client-side personalization; online advertising"}, "Profil sisi pelanggan untuk iklan yang diperibadikan Pemperibadian ada di mana-mana dalam aplikasi dalam talian moden kerana memberikan peningkatan yang signifikan dalam pengalaman pengguna dengan menyesuaikannya dengan pilihan pengguna yang disimpulkan. Walau bagaimanapun, terdapat peningkatan keprihatinan yang berkaitan dengan masalah privasi dan kawalan data pengguna yang digabungkan oleh sistem dalam talian untuk memperkuat pengalaman yang dipersonalisasi. Keprihatinan ini sangat penting untuk penggabungan profil pengguna dalam iklan dalam talian. Makalah ini menerangkan pendekatan pemperibadian sisi klien yang praktikal dan didorong oleh pembelajaran untuk platform pengiklanan kata kunci, sebuah aplikasi yang muncul yang sebelumnya tidak ditangani dalam literatur. Pendekatan kami bergantung pada menyimpan maklumat khusus pengguna sepenuhnya dalam kawalan pengguna dalam kuki penyemak imbas atau penyimpanan tempatan penyemak imbas, sehingga membolehkan pengguna melihat, menyunting atau membersihkannya pada bila-bila masa, mis. , melalui laman web khusus. Kami mengembangkan rumusan berprinsip, berasaskan utiliti untuk masalah mengemas kini profil pengguna yang tersimpan secara berulang-ulang di sisi pelanggan, yang bergantung pada ramalan yang dikalibrasi mengenai aktiviti pengguna masa depan. Walaupun pembinaan profil yang optimum sukar dilakukan untuk iklan bayar per klik dengan kenaikan bida, ia dapat diselesaikan dengan cekap melalui algoritma pendekatan tamak yang dijamin dapat memberikan penyelesaian yang hampir optimum kerana fakta bahawa utiliti profil kata kunci adalah submodular: ia menunjukkan harta pulangan yang semakin berkurang dengan peningkatan ukuran profil. Kami secara empirik menilai profil kata kunci pelanggan untuk iklan kata kunci pada set data berskala besar dari enjin carian utama. Eksperimen menunjukkan bahawa pemperibadian sisi pelanggan yang meramalkan membolehkan platform iklan mengekalkan hampir semua keuntungan hasil daripada pemperibadian walaupun mereka memberikan kebebasan kepada pengguna untuk memilih untuk tidak mengikuti penjejakan tingkah laku yang disokong oleh penyimpanan di sisi pelayan. Selain itu, kami menunjukkan bahawa pengiklan berpotensi dapat meningkatkan pulangan pelaburan mereka secara signifikan dengan menggunakan kenaikan tawaran untuk profil kata kunci dalam kempen iklan mereka. [[EENNDD]] pemperibadian sisi pelanggan; iklan dalam talian"], [{"string": "Mining social networks for personalized email prioritization Email is one of the most prevalent communication tools today , and solving the email overload problem is pressingly urgent . A good way to alleviate email overload is to automatically prioritize received messages according to the priorities of each user . However , research on statistical learning methods for fully personalized email prioritization PEP has been sparse due to privacy issues , since people are reluctant to share personal messages and importance judgments with the research community . It is therefore important to develop and evaluate PEP methods under the assumption that only limited training examples can be available , and that the system can only have the personal email data of each user during the training and testing of the model for that user . This paper presents the first study to the best of our knowledge under such an assumption . Specifically , we focus on analysis of personal social networks to capture user groups and to obtain rich features that represent the social roles from the viewpoint of a particular user . We also developed a novel semi-supervised transductive learning algorithm that propagates importance labels from training examples to test examples through message and user nodes in a personal email network . These methods together enable us to obtain an enriched vector representation of each new email message , which consists of both standard features of an email message such as words in the title or body , sender and receiver IDs , etc. and the induced social features from the sender and receivers of the message . Using the enriched vector representation as the input in SVM classifiers to predict the importance level for each test message , we obtained significant performance improvement over the baseline system without induced social features in our experiments on a multi-user data collection . We obtained significant performance improvement over the baseline system without induced social features in our experiments on a multi-user data collection : the relative error reduction in MAE was 31 % in micro-averaging , and 14 % in macro-averaging .", "keywords": ["text mining", "applications", "miscellaneous", "email prioritization", "social network", "clustering"], "combined": "Mining social networks for personalized email prioritization Email is one of the most prevalent communication tools today , and solving the email overload problem is pressingly urgent . A good way to alleviate email overload is to automatically prioritize received messages according to the priorities of each user . However , research on statistical learning methods for fully personalized email prioritization PEP has been sparse due to privacy issues , since people are reluctant to share personal messages and importance judgments with the research community . It is therefore important to develop and evaluate PEP methods under the assumption that only limited training examples can be available , and that the system can only have the personal email data of each user during the training and testing of the model for that user . This paper presents the first study to the best of our knowledge under such an assumption . Specifically , we focus on analysis of personal social networks to capture user groups and to obtain rich features that represent the social roles from the viewpoint of a particular user . We also developed a novel semi-supervised transductive learning algorithm that propagates importance labels from training examples to test examples through message and user nodes in a personal email network . These methods together enable us to obtain an enriched vector representation of each new email message , which consists of both standard features of an email message such as words in the title or body , sender and receiver IDs , etc. and the induced social features from the sender and receivers of the message . Using the enriched vector representation as the input in SVM classifiers to predict the importance level for each test message , we obtained significant performance improvement over the baseline system without induced social features in our experiments on a multi-user data collection . We obtained significant performance improvement over the baseline system without induced social features in our experiments on a multi-user data collection : the relative error reduction in MAE was 31 % in micro-averaging , and 14 % in macro-averaging . [[EENNDD]] text mining; applications; miscellaneous; email prioritization; social network; clustering"}, "Melombong rangkaian sosial untuk memprioritaskan e-mel yang diperibadikan E-mel adalah salah satu alat komunikasi yang paling kerap berlaku sekarang, dan menyelesaikan masalah kelebihan e-mel sangat mendesak. Cara yang baik untuk mengurangkan beban e-mel adalah dengan mengutamakan mesej yang diterima secara automatik mengikut keutamaan setiap pengguna. Walau bagaimanapun, penyelidikan mengenai kaedah pembelajaran statistik untuk PEP keutamaan e-mel yang diperibadikan sepenuhnya jarang berlaku kerana masalah privasi, kerana orang enggan berkongsi mesej peribadi dan penilaian penting dengan komuniti penyelidik. Oleh itu, penting untuk mengembangkan dan menilai kaedah PEP dengan anggapan bahawa hanya contoh latihan terhad yang tersedia, dan sistem hanya dapat memiliki data e-mel peribadi setiap pengguna semasa latihan dan ujian model untuk pengguna tersebut. Makalah ini membentangkan kajian pertama sejauh pengetahuan kami berdasarkan andaian sedemikian. Secara khusus, kami memfokuskan pada analisis rangkaian sosial peribadi untuk menangkap kumpulan pengguna dan mendapatkan ciri kaya yang mewakili peranan sosial dari sudut pandang pengguna tertentu. Kami juga mengembangkan algoritma pembelajaran transduktif separa penyeliaan novel yang menyebarkan label kepentingan dari contoh latihan hingga contoh ujian melalui mesej dan node pengguna dalam rangkaian e-mel peribadi. Kaedah ini bersama-sama membolehkan kita memperoleh gambaran vektor yang diperkaya dari setiap mesej e-mel baru, yang terdiri daripada kedua-dua ciri standard mesej e-mel seperti kata-kata dalam tajuk atau badan, ID pengirim dan penerima, dan lain-lain dan ciri sosial yang disebabkan oleh penghantar dan penerima mesej. Dengan menggunakan perwakilan vektor yang diperkaya sebagai input dalam pengkelasan SVM untuk meramalkan tahap kepentingan bagi setiap mesej ujian, kami memperoleh peningkatan prestasi yang signifikan terhadap sistem asas tanpa ciri sosial yang disebabkan dalam eksperimen kami pada pengumpulan data berbilang pengguna. Kami memperoleh peningkatan prestasi yang ketara berbanding sistem asas tanpa ciri sosial yang diinduksi dalam eksperimen kami pada pengumpulan data berbilang pengguna: pengurangan ralat relatif dalam MAE adalah 31% pada mikro-rata-rata, dan 14% pada makro-rata-rata. [[EENNDD]] perlombongan teks; permohonan; pelbagai; keutamaan e-mel; rangkaian sosial; pengelompokan"], [{"string": "Adaptive query processing for time-series data", "keywords": ["time-series query processing"], "combined": "Adaptive query processing for time-series data [[EENNDD]] time-series query processing"}, "Pemprosesan pertanyaan adaptif untuk data siri masa [[EENNDD]] pemprosesan pertanyaan siri masa"], [{"string": "Tumor cell identification using features rules Advances in imaging techniques have led to large repositories of images . There is an increasing demand for automated systems that can analyze complex medical images and extract meaningful information for mining patterns . Here , we describe a real-life image mining application to the problem of tumor cell counting . The quantitative analysis of tumor cells is fundamental to characterizing the activity of tumor cells . Existing approaches are mostly manual , time-consuming and subjective . Efforts to automate the process of cell counting have largely focused on using image processing techniques only . Our studies indicate that image processing alone is unable to give accurate results . In this paper , we examine the use of extracted features rules to aid in the process of tumor cell counting . We propose a robust local adaptive thresholding and dynamic water immersion algorithms to segment regions of interesting from background . Meaningful features are then extracted from the segmented regions . A number of base classifiers are built to generate features rules to help identify the tumor cell . Two voting strategies are implemented to combine the base classifiers into a meta-classifier . Experiment results indicate that this process of using extracted features rules to help identify tumor cell leads to better accuracy than pure image processing techniques alone .", "keywords": ["meta classifier", "dynamic water immersion", "majority vote", "identification", "features rules", "local adaptive thresholding", "weighted vote"], "combined": "Tumor cell identification using features rules Advances in imaging techniques have led to large repositories of images . There is an increasing demand for automated systems that can analyze complex medical images and extract meaningful information for mining patterns . Here , we describe a real-life image mining application to the problem of tumor cell counting . The quantitative analysis of tumor cells is fundamental to characterizing the activity of tumor cells . Existing approaches are mostly manual , time-consuming and subjective . Efforts to automate the process of cell counting have largely focused on using image processing techniques only . Our studies indicate that image processing alone is unable to give accurate results . In this paper , we examine the use of extracted features rules to aid in the process of tumor cell counting . We propose a robust local adaptive thresholding and dynamic water immersion algorithms to segment regions of interesting from background . Meaningful features are then extracted from the segmented regions . A number of base classifiers are built to generate features rules to help identify the tumor cell . Two voting strategies are implemented to combine the base classifiers into a meta-classifier . Experiment results indicate that this process of using extracted features rules to help identify tumor cell leads to better accuracy than pure image processing techniques alone . [[EENNDD]] meta classifier; dynamic water immersion; majority vote; identification; features rules; local adaptive thresholding; weighted vote"}, "Pengenalpastian sel tumor menggunakan peraturan ciri Kemajuan dalam teknik pencitraan telah menyebabkan penyimpanan gambar yang besar. Terdapat permintaan yang semakin meningkat untuk sistem automatik yang dapat menganalisis gambar perubatan yang kompleks dan mengekstrak maklumat yang bermakna untuk corak perlombongan. Di sini, kami menerangkan aplikasi perlombongan gambar kehidupan sebenar untuk masalah pengiraan sel tumor. Analisis kuantitatif sel tumor adalah asas untuk mencirikan aktiviti sel tumor. Pendekatan yang ada kebanyakannya manual, memakan masa dan subjektif. Usaha untuk mengautomatikkan proses penghitungan sel sebagian besar tertumpu pada penggunaan teknik pemprosesan gambar sahaja. Kajian kami menunjukkan bahawa pemprosesan gambar sahaja tidak dapat memberikan hasil yang tepat. Dalam makalah ini, kami meneliti penggunaan aturan fitur yang diekstrak untuk membantu proses penghitungan sel tumor. Kami mencadangkan algoritma penyebaran ambang batas adaptif tempatan dan dinamik untuk membahagi kawasan yang menarik dari latar belakang. Ciri-ciri yang bermakna kemudian diekstrak dari kawasan yang tersegmentasi. Sejumlah pengkelasan asas dibina untuk menghasilkan peraturan ciri untuk membantu mengenal pasti sel tumor. Dua strategi pengundian dilaksanakan untuk menggabungkan pengklasifikasi dasar menjadi pengkelasan meta. Hasil eksperimen menunjukkan bahawa proses penggunaan kaedah ciri yang diekstrak ini untuk membantu mengenal pasti sel tumor membawa kepada ketepatan yang lebih baik daripada teknik pemprosesan gambar tulen sahaja. [[EENNDD]] pengeluar meta; rendaman air dinamik; suara majoriti; pengenalan diri; peraturan ciri; ambang penyesuaian tempatan; undi berwajaran"], [{"string": "Mining discrete patterns via binary matrix factorization Mining discrete patterns in binary data is important for subsampling , compression , and clustering . We consider rank-one binary matrix approximations that identify the dominant patterns of the data , while preserving its discrete property . A best approximation on such data has a minimum set of inconsistent entries , i.e. , mismatches between the given binary data and the approximate matrix . Due to the hardness of the problem , previous accounts of such problems employ heuristics and the resulting approximation may be far away from the optimal one . In this paper , we show that the rank-one binary matrix approximation can be reformulated as a 0-1 integer linear program ILP . However , the ILP formulation is computationally expensive even for small-size matrices . We propose a linear program LP relaxation , which is shown to achieve a guaranteed approximation error bound . We further extend the proposed formulations using the regularization technique , which is commonly employed to address overfitting . The LP formulation is restricted to medium-size matrices , due to the large number of variables involved for large matrices . Interestingly , we show that the proposed approximate formulation can be transformed into an instance of the minimum s-t cut problem , which can be solved efficiently by finding maximum flows . Our empirical study shows the efficiency of the proposed algorithm based on the maximum flow . Results also confirm the established theoretical bounds .", "keywords": ["regularization", "binary matrix factorization", "minimum cut", "integer linear program", "maximum flow", "rank-one"], "combined": "Mining discrete patterns via binary matrix factorization Mining discrete patterns in binary data is important for subsampling , compression , and clustering . We consider rank-one binary matrix approximations that identify the dominant patterns of the data , while preserving its discrete property . A best approximation on such data has a minimum set of inconsistent entries , i.e. , mismatches between the given binary data and the approximate matrix . Due to the hardness of the problem , previous accounts of such problems employ heuristics and the resulting approximation may be far away from the optimal one . In this paper , we show that the rank-one binary matrix approximation can be reformulated as a 0-1 integer linear program ILP . However , the ILP formulation is computationally expensive even for small-size matrices . We propose a linear program LP relaxation , which is shown to achieve a guaranteed approximation error bound . We further extend the proposed formulations using the regularization technique , which is commonly employed to address overfitting . The LP formulation is restricted to medium-size matrices , due to the large number of variables involved for large matrices . Interestingly , we show that the proposed approximate formulation can be transformed into an instance of the minimum s-t cut problem , which can be solved efficiently by finding maximum flows . Our empirical study shows the efficiency of the proposed algorithm based on the maximum flow . Results also confirm the established theoretical bounds . [[EENNDD]] regularization; binary matrix factorization; minimum cut; integer linear program; maximum flow; rank-one"}, "Melombong corak diskrit melalui pemodelan matriks binari Melombong corak diskrit dalam data binari adalah penting untuk subsampling, pemampatan, dan pengelompokan. Kami mempertimbangkan penghitungan matriks binari peringkat satu yang mengenal pasti corak data yang dominan, sambil mengekalkan sifat diskritnya. Pendekatan terbaik pada data tersebut mempunyai sekumpulan entri minimum yang tidak konsisten, iaitu, ketidakcocokan antara data binari yang diberikan dan matriks anggaran. Oleh kerana kesukaran masalah, akaun sebelumnya mengenai masalah tersebut menggunakan heuristik dan penghampiran yang dihasilkan mungkin jauh dari yang optimum. Dalam makalah ini, kami menunjukkan bahawa pendekatan matriks binari peringkat satu dapat dirumuskan semula sebagai ILP program linear integer 0-1. Walau bagaimanapun, rumusan ILP sangat mahal walaupun untuk matriks bersaiz kecil. Kami mencadangkan kelonggaran LP program linear, yang ditunjukkan untuk mencapai ralat penghampiran yang dijamin. Kami selanjutnya memperluas formulasi yang diusulkan menggunakan teknik regularisasi, yang biasanya digunakan untuk mengatasi overfitting. Rumusan LP dibatasi pada matriks ukuran sederhana, kerana sebilangan besar pemboleh ubah yang terlibat untuk matriks besar. Menariknya, kami menunjukkan bahawa rumusan perkiraan yang diusulkan dapat diubah menjadi contoh masalah pemotongan s-t minimum, yang dapat diselesaikan dengan efisien dengan mencari aliran maksimum. Kajian empirikal kami menunjukkan kecekapan algoritma yang dicadangkan berdasarkan aliran maksimum. Hasil juga mengesahkan batas teori yang telah ditetapkan. [[EENNDD]] regularisasi; pemfaktoran matriks binari; pemotongan minimum; program linear integer; aliran maksimum; peringkat satu"], [{"string": "The `` DGX '' distribution for mining massive , skewed data Skewed distributions appear very often in practice . Unfortunately , the traditional Zipf distribution often fails to model them well . In this paper , we propose a new probability distribution , the Discrete Gaussian Exponential DGX , to achieve excellent fits in a wide variety of settings ; our new distribution includes the Zipf distribution as a special case . We present a statistically sound method for estimating the DGX parameters based on maximum likelihood estimation MLE . We applied DGX to a wide variety of real world data sets , such as sales data from a large retailer chain , us-age data from AT&T , and Internet clickstream data ; in all cases , DGX fits these distributions very well , with almost a 99 % correlation coefficient in quantile-quantile plots . Our algorithm also scales very well because it requires only a single pass over the data . Finally , we illustrate the power of DGX as a new tool for data mining tasks , such as outlier detection .", "keywords": ["rank-frequency plot", "maximum likelihood estimation", "frequency-count plot", "dgx", "zipf's law", "lognormal distribution", "outlier detection"], "combined": "The `` DGX '' distribution for mining massive , skewed data Skewed distributions appear very often in practice . Unfortunately , the traditional Zipf distribution often fails to model them well . In this paper , we propose a new probability distribution , the Discrete Gaussian Exponential DGX , to achieve excellent fits in a wide variety of settings ; our new distribution includes the Zipf distribution as a special case . We present a statistically sound method for estimating the DGX parameters based on maximum likelihood estimation MLE . We applied DGX to a wide variety of real world data sets , such as sales data from a large retailer chain , us-age data from AT&T , and Internet clickstream data ; in all cases , DGX fits these distributions very well , with almost a 99 % correlation coefficient in quantile-quantile plots . Our algorithm also scales very well because it requires only a single pass over the data . Finally , we illustrate the power of DGX as a new tool for data mining tasks , such as outlier detection . [[EENNDD]] rank-frequency plot; maximum likelihood estimation; frequency-count plot; dgx; zipf's law; lognormal distribution; outlier detection"}, "Pengedaran \"DGX\" untuk melombong data besar, miring Pengedaran miring muncul sangat kerap dalam praktik. Malangnya, pengedaran Zipf tradisional sering gagal untuk memodelkannya dengan baik. Dalam makalah ini, kami mencadangkan sebaran kebarangkalian baru, Discrete Gaussian Exponential DGX, untuk mencapai tahap yang sangat baik dalam pelbagai tetapan; pengedaran baru kami merangkumi pengedaran Zipf sebagai kes khas. Kami menyajikan kaedah statistik yang tepat untuk menganggarkan parameter DGX berdasarkan anggaran kemungkinan maksimum MLE. Kami menggunakan DGX untuk pelbagai kumpulan data dunia nyata, seperti data penjualan dari rangkaian peruncit besar, data usia dari AT&T, dan data aliran klik Internet; dalam semua kes, DGX sesuai dengan pengagihan ini, dengan hampir 99% pekali korelasi dalam plot kuantil-kuantil. Algoritma kami juga berskala dengan baik kerana hanya memerlukan satu hantaran data. Akhirnya, kami menggambarkan kehebatan DGX sebagai alat baru untuk tugas perlombongan data, seperti pengesanan luar. [[EENNDD]] plot frekuensi peringkat; anggaran kemungkinan maksimum; plot kiraan kekerapan; dgx; undang-undang zipf; pengedaran log normal; pengesanan luar"], [{"string": "PEBL : positive example based learning for Web page classification using SVM Web page classification is one of the essential techniques for Web mining . Specifically , classifying Web pages of a user-interesting class is the first step of mining interesting information from the Web . However , constructing a classifier for an interesting class requires laborious pre-processing such as collecting positive and negative training examples . For instance , in order to construct a `` homepage '' classifier , one needs to collect a sample of homepages positive examples and a sample of non-homepages negative examples . In particular , collecting negative training examples requires arduous work and special caution to avoid biasing them . We introduce in this paper the Positive Example Based Learning PEBL framework for Web page classification which eliminates the need for manually collecting negative training examples in pre-processing . We present an algorithm called Mapping-Convergence M-C that achieves classification accuracy with positive and unlabeled data as high as that of traditional SVM with positive and negative data . Our experiments show that when the M-C algorithm uses the same amount of positive examples as that of traditional SVM , the M-C algorithm performs as well as traditional SVM .", "keywords": ["unlabeled data", "svm", "labeled data", "mapping-convergence algorithm"], "combined": "PEBL : positive example based learning for Web page classification using SVM Web page classification is one of the essential techniques for Web mining . Specifically , classifying Web pages of a user-interesting class is the first step of mining interesting information from the Web . However , constructing a classifier for an interesting class requires laborious pre-processing such as collecting positive and negative training examples . For instance , in order to construct a `` homepage '' classifier , one needs to collect a sample of homepages positive examples and a sample of non-homepages negative examples . In particular , collecting negative training examples requires arduous work and special caution to avoid biasing them . We introduce in this paper the Positive Example Based Learning PEBL framework for Web page classification which eliminates the need for manually collecting negative training examples in pre-processing . We present an algorithm called Mapping-Convergence M-C that achieves classification accuracy with positive and unlabeled data as high as that of traditional SVM with positive and negative data . Our experiments show that when the M-C algorithm uses the same amount of positive examples as that of traditional SVM , the M-C algorithm performs as well as traditional SVM . [[EENNDD]] unlabeled data; svm; labeled data; mapping-convergence algorithm"}, "PEBL: pembelajaran berdasarkan contoh positif untuk klasifikasi halaman Web menggunakan klasifikasi halaman Web SVM adalah salah satu teknik penting untuk perlombongan Web. Secara khusus, mengklasifikasikan halaman Web kelas menarik pengguna adalah langkah pertama untuk melombong maklumat menarik dari Web. Walau bagaimanapun, membina pengkelasan untuk kelas yang menarik memerlukan proses awal yang sukar seperti mengumpulkan contoh latihan positif dan negatif. Sebagai contoh, untuk membina pengkelasan \"beranda\", seseorang perlu mengumpulkan contoh contoh positif laman utama dan contoh contoh negatif bukan beranda. Khususnya, mengumpulkan contoh latihan negatif memerlukan kerja keras dan berhati-hati untuk mengelakkannya. Kami memperkenalkan dalam makalah ini kerangka PEBL Pembelajaran Berasaskan Contoh Positif untuk klasifikasi halaman Web yang menghilangkan keperluan untuk mengumpulkan contoh latihan negatif secara manual dalam pra-pemprosesan. Kami menyajikan algoritma yang disebut Mapping-Convergence M-C yang mencapai ketepatan klasifikasi dengan data positif dan tidak berlabel setinggi SVM tradisional dengan data positif dan negatif. Eksperimen kami menunjukkan bahawa apabila algoritma M-C menggunakan jumlah contoh positif yang sama dengan SVM tradisional, algoritma M-C juga sama seperti SVM tradisional. [[EENNDD]] data tidak berlabel; svm; data berlabel; algoritma pemetaan-penumpuan"], [{"string": "Learning to predict train wheel failures This paper describes a successful but challenging application of data mining in the railway industry . The objective is to optimize maintenance and operation of trains through prognostics of wheel failures . In addition to reducing maintenance costs , the proposed technology will help improve railway safety and augment throughput . Building on established techniques from data mining and machine learning , we present a methodology to learn models to predict train wheel failures from readily available operational and maintenance data . This methodology addresses various data mining tasks such as automatic labeling , feature extraction , model building , model fusion , and evaluation . After a detailed description of the methodology , we report results from large-scale experiments . These results clearly show the great potential of this innovative application of data mining in the railway industry .", "keywords": ["model fusion", "decision support", "model evaluation", "model building", "wheel failure prediction", "methodology", "machine learning"], "combined": "Learning to predict train wheel failures This paper describes a successful but challenging application of data mining in the railway industry . The objective is to optimize maintenance and operation of trains through prognostics of wheel failures . In addition to reducing maintenance costs , the proposed technology will help improve railway safety and augment throughput . Building on established techniques from data mining and machine learning , we present a methodology to learn models to predict train wheel failures from readily available operational and maintenance data . This methodology addresses various data mining tasks such as automatic labeling , feature extraction , model building , model fusion , and evaluation . After a detailed description of the methodology , we report results from large-scale experiments . These results clearly show the great potential of this innovative application of data mining in the railway industry . [[EENNDD]] model fusion; decision support; model evaluation; model building; wheel failure prediction; methodology; machine learning"}, "Belajar untuk meramalkan kegagalan roda kereta api Makalah ini menjelaskan penerapan perlombongan data yang berjaya tetapi mencabar dalam industri kereta api. Objektifnya adalah untuk mengoptimumkan penyelenggaraan dan operasi kereta api melalui prognostik kegagalan roda. Di samping mengurangkan kos penyelenggaraan, teknologi yang dicadangkan akan membantu meningkatkan keselamatan kereta api dan meningkatkan hasil. Berdasarkan teknik yang telah ditetapkan dari perlombongan data dan pembelajaran mesin, kami menyajikan metodologi untuk mempelajari model untuk meramalkan kegagalan roda kereta api dari data operasi dan penyelenggaraan yang tersedia. Metodologi ini menangani pelbagai tugas perlombongan data seperti pelabelan automatik, pengekstrakan fitur, pembuatan model, gabungan model, dan penilaian. Setelah penerangan terperinci mengenai metodologi, kami melaporkan hasil dari eksperimen berskala besar. Hasil ini jelas menunjukkan potensi besar dari aplikasi inovatif perlombongan data ini dalam industri kereta api. [[EENNDD]] gabungan model; sokongan keputusan; penilaian model; bangunan model; ramalan kegagalan roda; metodologi; pembelajaran mesin"], [{"string": "Handling concept drifts in incremental learning with support vector machines", "keywords": ["learning"], "combined": "Handling concept drifts in incremental learning with support vector machines [[EENNDD]] learning"}, "Mengendalikan konsep bergerak dalam pembelajaran tambahan dengan pembelajaran mesin vektor sokongan [[EENNDD]]"], [{"string": "On-board analysis of uncalibrated data for a spacecraft at mars Analyzing data on-board a spacecraft as it is collected enables several advanced spacecraft capabilities , such as prioritizing observations to make the best use of limited bandwidth and reacting to dynamic events as they happen . In this paper , we describe how we addressed the unique challenges associated with on-board mining of data as it is collected : uncalibrated data , noisy observations , and severe limitations on computational and memory resources . The goal of this effort , which falls into the emerging application area of spacecraft-based data mining , was to study three specific science phenomena on Mars . Following previous work that used a linear support vector machine SVM on-board the Earth Observing 1 EO-1 spacecraft , we developed three data mining techniques for use on-board the Mars Odyssey spacecraft . These methods range from simple thresholding to state-of-the-art reduced-set SVM technology . We tested these algorithms on archived data in a flight software testbed . We also describe a significant , serendipitous science discovery of this data mining effort : the confirmation of a water ice annulus around the north polar cap of Mars . We conclude with a discussion on lessons learned in developing algorithms for use on-board a spacecraft .", "keywords": ["lessons learned", "resource-constrained computing", "on-board data mining", "real-time data analysis"], "combined": "On-board analysis of uncalibrated data for a spacecraft at mars Analyzing data on-board a spacecraft as it is collected enables several advanced spacecraft capabilities , such as prioritizing observations to make the best use of limited bandwidth and reacting to dynamic events as they happen . In this paper , we describe how we addressed the unique challenges associated with on-board mining of data as it is collected : uncalibrated data , noisy observations , and severe limitations on computational and memory resources . The goal of this effort , which falls into the emerging application area of spacecraft-based data mining , was to study three specific science phenomena on Mars . Following previous work that used a linear support vector machine SVM on-board the Earth Observing 1 EO-1 spacecraft , we developed three data mining techniques for use on-board the Mars Odyssey spacecraft . These methods range from simple thresholding to state-of-the-art reduced-set SVM technology . We tested these algorithms on archived data in a flight software testbed . We also describe a significant , serendipitous science discovery of this data mining effort : the confirmation of a water ice annulus around the north polar cap of Mars . We conclude with a discussion on lessons learned in developing algorithms for use on-board a spacecraft . [[EENNDD]] lessons learned; resource-constrained computing; on-board data mining; real-time data analysis"}, "Analisis on-board data yang tidak dikalibrasi untuk kapal angkasa di mars Menganalisis data di dalam kapal angkasa ketika dikumpulkan memungkinkan beberapa keupayaan kapal angkasa maju, seperti memprioritaskan pemerhatian untuk menggunakan sebaik-baiknya lebar jalur terhad dan bertindak balas terhadap peristiwa dinamik ketika mereka berlaku. Dalam makalah ini, kami menerangkan bagaimana kami menangani tantangan unik yang berkaitan dengan perlombongan data semasa dikumpulkan: data yang tidak dikalibrasi, pemerhatian yang bising, dan batasan yang teruk pada sumber daya komputer dan memori. Matlamat usaha ini, yang termasuk dalam bidang aplikasi perlombongan data berasaskan kapal angkasa yang baru muncul, adalah untuk mengkaji tiga fenomena sains khusus di Marikh. Mengikuti karya sebelumnya yang menggunakan mesin vektor sokongan linear SVM di atas kapal angkasa Bumi Mengamati 1 EO-1, kami mengembangkan tiga teknik perlombongan data untuk digunakan di atas kapal angkasa Mars Odyssey. Kaedah ini berkisar dari ambang sederhana hingga teknologi SVM set-of-the-art canggih. Kami menguji algoritma ini pada data yang diarkibkan di ruang ujian perisian penerbangan. Kami juga menerangkan penemuan sains penting yang signifikan dari usaha perlombongan data ini: pengesahan anulus ais air di sekitar topi kutub utara Mars. Kami mengakhiri dengan perbincangan mengenai pelajaran yang dipelajari dalam mengembangkan algoritma untuk penggunaan kapal angkasa. [[EENNDD]] pelajaran yang dipelajari; pengkomputeran terhad sumber; perlombongan data on-board; analisis data masa nyata"], [{"string": "Probabilistic latent semantic visualization : topic model for visualizing documents We propose a visualization method based on a topic model for discrete data such as documents . Unlike conventional visualization methods based on pairwise distances such as multi-dimensional scaling , we consider a mapping from the visualization space into the space of documents as a generative process of documents . In the model , both documents and topics are assumed to have latent coordinates in a two - or three-dimensional Euclidean space , or visualization space . The topic proportions of a document are determined by the distances between the document and the topics in the visualization space , and each word is drawn from one of the topics according to its topic proportions . A visualization , i.e. latent coordinates of documents , can be obtained by fitting the model to a given set of documents using the EM algorithm , resulting in documents with similar topics being embedded close together . We demonstrate the effectiveness of the proposed model by visualizing document and movie data sets , and quantitatively compare it with conventional visualization methods .", "keywords": ["probabilistic latent semantic analysis", "visualization", "topic model"], "combined": "Probabilistic latent semantic visualization : topic model for visualizing documents We propose a visualization method based on a topic model for discrete data such as documents . Unlike conventional visualization methods based on pairwise distances such as multi-dimensional scaling , we consider a mapping from the visualization space into the space of documents as a generative process of documents . In the model , both documents and topics are assumed to have latent coordinates in a two - or three-dimensional Euclidean space , or visualization space . The topic proportions of a document are determined by the distances between the document and the topics in the visualization space , and each word is drawn from one of the topics according to its topic proportions . A visualization , i.e. latent coordinates of documents , can be obtained by fitting the model to a given set of documents using the EM algorithm , resulting in documents with similar topics being embedded close together . We demonstrate the effectiveness of the proposed model by visualizing document and movie data sets , and quantitatively compare it with conventional visualization methods . [[EENNDD]] probabilistic latent semantic analysis; visualization; topic model"}, "Visualisasi semantik probabilistik: model topik untuk memvisualisasikan dokumen Kami mencadangkan kaedah visualisasi berdasarkan model topik untuk data diskrit seperti dokumen. Tidak seperti kaedah visualisasi konvensional berdasarkan jarak berpasangan seperti penskalaan pelbagai dimensi, kami menganggap pemetaan dari ruang visualisasi ke ruang dokumen sebagai proses generatif dokumen. Dalam model tersebut, kedua-dua dokumen dan topik diasumsikan mempunyai koordinat laten dalam ruang Euclidean dua - atau tiga dimensi, atau ruang visualisasi. Perkadaran topik dokumen ditentukan oleh jarak antara dokumen dan topik di ruang visualisasi, dan setiap kata diambil dari salah satu topik mengikut perkadaran topiknya. Visualisasi, iaitu koordinat laten dokumen, dapat diperoleh dengan memasangkan model ke sekumpulan dokumen tertentu menggunakan algoritma EM, sehingga dokumen dengan topik serupa disatukan. Kami menunjukkan keberkesanan model yang dicadangkan dengan memvisualisasikan kumpulan data dokumen dan filem, dan membandingkannya secara kuantitatif dengan kaedah visualisasi konvensional. [[EENNDD]] analisis semantik laten probabilistik; visualisasi; model topik"], [{"string": "Disease progression modeling from historical clinical databases This paper considers the problem of modeling disease progression from historical clinical databases , with the ultimate objective of stratifying patients into groups with clearly distinguishable prognoses or suitability for different treatment strategies . To meet this objective , we describe a procedure that first fits clinical variables measured over time to a disease progression model . The resulting parameter estimates are then used as the basis for a stepwise clustering procedure to stratify patients into groups with distinct survival characteristics . As a practical illustration , we apply this procedure to survival prediction , using a liver transplant database from the National Institute of Diabetes and Digestive and Kidney Diseases NIDDK .", "keywords": ["censoring", "logistic model", "disease progression modeling", "niddk liver transplant database", "cluster analysis", "model development"], "combined": "Disease progression modeling from historical clinical databases This paper considers the problem of modeling disease progression from historical clinical databases , with the ultimate objective of stratifying patients into groups with clearly distinguishable prognoses or suitability for different treatment strategies . To meet this objective , we describe a procedure that first fits clinical variables measured over time to a disease progression model . The resulting parameter estimates are then used as the basis for a stepwise clustering procedure to stratify patients into groups with distinct survival characteristics . As a practical illustration , we apply this procedure to survival prediction , using a liver transplant database from the National Institute of Diabetes and Digestive and Kidney Diseases NIDDK . [[EENNDD]] censoring; logistic model; disease progression modeling; niddk liver transplant database; cluster analysis; model development"}, "Pemodelan perkembangan penyakit dari pangkalan data klinikal sejarah Makalah ini mempertimbangkan masalah pemodelan perkembangan penyakit dari pangkalan data klinikal sejarah, dengan objektif akhir untuk mengelompokkan pesakit ke dalam kumpulan dengan prognosis atau kesesuaian yang dapat dibezakan untuk strategi rawatan yang berbeza. Untuk memenuhi objektif ini, kami menerangkan prosedur yang pertama kali menyesuaikan pemboleh ubah klinikal yang diukur dari masa ke masa dengan model perkembangan penyakit. Anggaran parameter yang dihasilkan kemudian digunakan sebagai dasar untuk prosedur pengelompokan bertahap untuk mengelompokkan pesakit ke dalam kelompok dengan ciri-ciri kelangsungan hidup yang berbeza. Sebagai gambaran praktikal, kami menerapkan prosedur ini untuk ramalan kelangsungan hidup, menggunakan pangkalan data transplantasi hati dari Institut Nasional Diabetes dan Pencernaan dan Penyakit Ginjal NIDDK. [[EENNDD]] menapis; model logistik; pemodelan perkembangan penyakit; pangkalan data pemindahan hati niddk; analisis kluster; pembangunan model"], [{"string": "Mining sequential patterns from probabilistic databases We consider sequential pattern mining in situations where there is uncertainty about which source an event is associated with . We model this in the probabilistic database framework and consider the problem of enumerating all sequences whose expected support is sufficiently large . Unlike frequent itemset mining in probabilistic databases C. Aggarwal et al. . KDD ' 09 ; Chui et al. , PAKDD ' 07 ; Chui and Kao , PAKDD ' 08 , we use dynamic programming DP to compute the probability that a source supports a sequence , and show that this suffices to compute the expected support of a sequential pattern . Next , we embed this DP algorithm into candidate generate-and-test approaches , and explore the pattern lattice both in a breadth-first similar to GSP and a depth-first similar to SPAM manner . We propose optimizations for efficiently computing the frequent 1-sequences , for re-using previously-computed results through incremental support computation , and for elmiminating candidate sequences without computing their support via probabilistic pruning . Preliminary experiments show that our optimizations are effective in improving the CPU cost .", "keywords": ["probabilistic databases", "mining complex sequential data", "mining uncertain data", "novel models and algorithms"], "combined": "Mining sequential patterns from probabilistic databases We consider sequential pattern mining in situations where there is uncertainty about which source an event is associated with . We model this in the probabilistic database framework and consider the problem of enumerating all sequences whose expected support is sufficiently large . Unlike frequent itemset mining in probabilistic databases C. Aggarwal et al. . KDD ' 09 ; Chui et al. , PAKDD ' 07 ; Chui and Kao , PAKDD ' 08 , we use dynamic programming DP to compute the probability that a source supports a sequence , and show that this suffices to compute the expected support of a sequential pattern . Next , we embed this DP algorithm into candidate generate-and-test approaches , and explore the pattern lattice both in a breadth-first similar to GSP and a depth-first similar to SPAM manner . We propose optimizations for efficiently computing the frequent 1-sequences , for re-using previously-computed results through incremental support computation , and for elmiminating candidate sequences without computing their support via probabilistic pruning . Preliminary experiments show that our optimizations are effective in improving the CPU cost . [[EENNDD]] probabilistic databases; mining complex sequential data; mining uncertain data; novel models and algorithms"}, "Melombong corak urutan dari pangkalan data probabilistik Kami menganggap perlombongan pola berurutan dalam situasi di mana terdapat ketidakpastian mengenai sumber mana peristiwa dikaitkan. Kami memodelkannya dalam kerangka pangkalan data probabilistik dan mempertimbangkan masalah penghitungan semua urutan yang sokongannya diharapkan cukup besar. Tidak seperti perlombongan itemet yang kerap dalam pangkalan data probabilistik C. Aggarwal et al. . KDD '09; Chui et al. , PAKDD '07; Chui dan Kao, PAKDD '08, kami menggunakan DP pengaturcaraan dinamik untuk menghitung kebarangkalian sumber menyokong urutan, dan menunjukkan bahawa ini cukup untuk mengira sokongan yang diharapkan dari corak berurutan. Seterusnya, kami memasukkan algoritma DP ini ke dalam pendekatan menghasilkan-dan-ujian calon, dan meneroka kisi corak keduanya sama besarnya dengan GSP dan kedalaman-pertama serupa dengan cara SPAM. Kami mencadangkan pengoptimuman untuk menghitung urutan 1 yang kerap dengan cekap, untuk menggunakan semula hasil yang dihitung sebelumnya melalui pengiraan sokongan tambahan, dan untuk mengecam urutan calon tanpa menghitung sokongan mereka melalui pemangkasan probabilistik. Eksperimen awal menunjukkan bahawa pengoptimuman kami berkesan dalam meningkatkan kos CPU. [[EENNDD]] pangkalan data probabilistik; data urutan kompleks perlombongan; melombong data tidak menentu; model dan algoritma novel"], [{"string": "Clustering with relative constraints Recent studies have suggested using relative distance comparisons as constraints to represent domain knowledge . A natural extension to relative comparisons is the combination of two comparisons defined on the same set of three instances . Constraints in this form , termed Relative Constraints , provide a unified knowledge representation for both partitional and hierarchical clusterings . But many key properties of relative constraints remain unknown . In this paper , we answer the following important questions that enable the broader application of relative constraints in general clustering problems : '' Feasibility : Does there exist a clustering that satisfies a given set of relative constraints ? consistency of constraints `` Completeness : Given a set of consistent relative constraints , how can one derive a complete clustering without running into dead-ends ? '' Informativeness : How can one extract the most informative relative constraints from given knowledge sources ? We show that any hierarchical domain knowledge can be easily represented by relative constraints . We further present a hierarchical algorithm that finds a clustering satisfying all given constraints in polynomial time . Experiments showed that our algorithm achieves significantly higher accuracy than the existing metric learning approach based on relative comparisons .", "keywords": ["relative constraints", "hierarchical clustering", "constrained clustering"], "combined": "Clustering with relative constraints Recent studies have suggested using relative distance comparisons as constraints to represent domain knowledge . A natural extension to relative comparisons is the combination of two comparisons defined on the same set of three instances . Constraints in this form , termed Relative Constraints , provide a unified knowledge representation for both partitional and hierarchical clusterings . But many key properties of relative constraints remain unknown . In this paper , we answer the following important questions that enable the broader application of relative constraints in general clustering problems : '' Feasibility : Does there exist a clustering that satisfies a given set of relative constraints ? consistency of constraints `` Completeness : Given a set of consistent relative constraints , how can one derive a complete clustering without running into dead-ends ? '' Informativeness : How can one extract the most informative relative constraints from given knowledge sources ? We show that any hierarchical domain knowledge can be easily represented by relative constraints . We further present a hierarchical algorithm that finds a clustering satisfying all given constraints in polynomial time . Experiments showed that our algorithm achieves significantly higher accuracy than the existing metric learning approach based on relative comparisons . [[EENNDD]] relative constraints; hierarchical clustering; constrained clustering"}, "Pengelompokan dengan batasan relatif Kajian terbaru telah mencadangkan penggunaan perbandingan jarak relatif sebagai kekangan untuk mewakili pengetahuan domain. Peluasan semula jadi untuk perbandingan relatif adalah gabungan dua perbandingan yang ditentukan pada set tiga kejadian yang sama. Kekangan dalam bentuk ini, yang disebut Kekangan Relatif, memberikan perwakilan pengetahuan yang disatukan untuk kedua-dua gabungan dan hierarki. Tetapi banyak sifat utama kekangan relatif masih belum diketahui. Dalam makalah ini, kami menjawab pertanyaan-pertanyaan penting berikut yang memungkinkan penerapan batasan relatif yang lebih luas dalam masalah pengelompokan umum: '' Kelayakan: Apakah ada pengelompokan yang memenuhi sekumpulan batasan relatif yang diberikan? konsistensi kekangan `` Kelengkapan: Mengingat sekumpulan kekangan relatif yang konsisten, bagaimana seseorang dapat memperoleh pengelompokan lengkap tanpa mengalami jalan buntu? Maklumat: Bagaimana seseorang dapat mengekstrak kekangan relatif yang paling bermaklumat dari sumber pengetahuan yang diberikan? Kami menunjukkan bahawa sebarang pengetahuan domain hirarki dapat dilambangkan dengan mudah oleh batasan relatif. Kami seterusnya menyajikan algoritma hierarki yang menemui kumpulan yang memenuhi semua kekangan yang diberikan dalam masa polinomial. Eksperimen menunjukkan bahawa algoritma kami mencapai ketepatan yang jauh lebih tinggi daripada pendekatan pembelajaran metrik yang ada berdasarkan perbandingan relatif. [[EENNDD]] kekangan relatif; pengelompokan hierarki; pengelompokan terkawal"], [{"string": "Finding a team of experts in social networks Given a task T , a pool of individuals X with different skills , and a social network G that captures the compatibility among these individuals , we study the problem of finding X , a subset of X , to perform the task . We call this the TEAM FORMATION problem . We require that members of X ' not only meet the skill requirements of the task , but can also work effectively together as a team . We measure effectiveness using the communication cost incurred by the subgraph in G that only involves X ' . We study two variants of the problem for two different communication-cost functions , and show that both variants are NP-hard . We explore their connections with existing combinatorial problems and give novel algorithms for their solution . To the best of our knowledge , this is the first work to consider the TEAM FORMATION problem in the presence of a social network of individuals . Experiments on the DBLP dataset show that our framework works well in practice and gives useful and intuitive results .", "keywords": ["social networks", "team formation"], "combined": "Finding a team of experts in social networks Given a task T , a pool of individuals X with different skills , and a social network G that captures the compatibility among these individuals , we study the problem of finding X , a subset of X , to perform the task . We call this the TEAM FORMATION problem . We require that members of X ' not only meet the skill requirements of the task , but can also work effectively together as a team . We measure effectiveness using the communication cost incurred by the subgraph in G that only involves X ' . We study two variants of the problem for two different communication-cost functions , and show that both variants are NP-hard . We explore their connections with existing combinatorial problems and give novel algorithms for their solution . To the best of our knowledge , this is the first work to consider the TEAM FORMATION problem in the presence of a social network of individuals . Experiments on the DBLP dataset show that our framework works well in practice and gives useful and intuitive results . [[EENNDD]] social networks; team formation"}, "Mencari pasukan pakar dalam rangkaian sosial Mengingat tugas T, kumpulan individu X dengan kemahiran yang berbeza, dan rangkaian sosial G yang menangkap keserasian di antara individu-individu ini, kami mengkaji masalah mencari X, subkumpulan X, untuk melakukan tugas . Kami menyebutnya masalah TEORI PEMBENTUKAN. Kami menghendaki agar anggota X 'tidak hanya memenuhi keperluan kemahiran dalam tugas tersebut, tetapi juga dapat bekerja bersama secara efektif sebagai satu pasukan. Kami mengukur keberkesanan menggunakan kos komunikasi yang ditanggung oleh subgraf dalam G yang hanya melibatkan X '. Kami mengkaji dua varian masalah untuk dua fungsi kos komunikasi yang berbeza, dan menunjukkan bahawa kedua-dua varian itu sukar untuk NP. Kami meneroka hubungan mereka dengan masalah kombinasi yang ada dan memberikan algoritma baru untuk penyelesaiannya. Sepengetahuan kami, ini adalah karya pertama yang mempertimbangkan masalah FORMASI PASUKAN dengan adanya rangkaian sosial individu. Eksperimen pada set data DBLP menunjukkan bahawa kerangka kerja kami berfungsi dengan baik dalam praktik dan memberikan hasil yang berguna dan intuitif. [[EENNDD]] rangkaian sosial; pembentukan pasukan"], [{"string": "Succinct summarization of transactional databases : an overlapped hyperrectangle scheme Transactional data are ubiquitous . Several methods , including frequent itemsets mining and co-clustering , have been proposed to analyze transactional databases . In this work , we propose a new research problem to succinctly summarize transactional databases . Solving this problem requires linking the high level structure of the database to a potentially huge number of frequent itemsets . We formulate this problem as a set covering problem using overlapped hyperrectangles ; we then prove that this problem and its several variations are NP-hard . We develop an approximation algorithm HYPER which can achieve a ln k + 1 approximation ratio in polynomial time . We propose a pruning strategy that can significantly speed up the processing of our algorithm . Additionally , we propose an efficient algorithm to further summarize the set of hyperrectangles by allowing false positive conditions . A detailed study using both real and synthetic datasets shows the effectiveness and efficiency of our approaches in summarizing transactional databases .", "keywords": ["set cover", "hyperrectangle", "summarization", "transactional databases"], "combined": "Succinct summarization of transactional databases : an overlapped hyperrectangle scheme Transactional data are ubiquitous . Several methods , including frequent itemsets mining and co-clustering , have been proposed to analyze transactional databases . In this work , we propose a new research problem to succinctly summarize transactional databases . Solving this problem requires linking the high level structure of the database to a potentially huge number of frequent itemsets . We formulate this problem as a set covering problem using overlapped hyperrectangles ; we then prove that this problem and its several variations are NP-hard . We develop an approximation algorithm HYPER which can achieve a ln k + 1 approximation ratio in polynomial time . We propose a pruning strategy that can significantly speed up the processing of our algorithm . Additionally , we propose an efficient algorithm to further summarize the set of hyperrectangles by allowing false positive conditions . A detailed study using both real and synthetic datasets shows the effectiveness and efficiency of our approaches in summarizing transactional databases . [[EENNDD]] set cover; hyperrectangle; summarization; transactional databases"}, "Ringkasan ringkas pangkalan data transaksional: skema hiper segiempat bertindih Data transaksi ada di mana-mana. Beberapa kaedah, termasuk penambangan item dan penggabungan bersama, telah diusulkan untuk menganalisis pangkalan data transaksi. Dalam karya ini, kami mengusulkan masalah penyelidikan baru untuk meringkaskan pangkalan data transaksional. Menyelesaikan masalah ini memerlukan menghubungkan struktur pangkalan data tahap tinggi dengan sejumlah item yang berpotensi besar. Kami merumuskan masalah ini sebagai satu set yang merangkumi masalah menggunakan hyperrectangles yang bertindih; kami kemudian membuktikan bahawa masalah ini dan beberapa variasinya sukar dilakukan. Kami mengembangkan algoritma penghampiran HYPER yang dapat mencapai nisbah perkiraan ln k + 1 dalam masa polinomial. Kami mencadangkan strategi pemangkasan yang dapat mempercepat pemprosesan algoritma kami dengan ketara. Selain itu, kami mencadangkan algoritma yang cekap untuk meringkaskan kumpulan hiperjajaran dengan membenarkan keadaan positif yang salah. Kajian terperinci yang menggunakan kumpulan data sebenar dan sintetik menunjukkan keberkesanan dan kecekapan pendekatan kami dalam meringkaskan pangkalan data transaksional. [[EENNDD]] set penutup; hiper segi empat tepat; ringkasan; pangkalan data transaksi"], [{"string": "Generalized clustering , supervised learning , and data assignment Clustering algorithms have become increasingly important in handling and analyzing data . Considerable work has been done in devising effective but increasingly specific clustering algorithms . In contrast , we have developed a generalized framework that accommodates diverse clustering algorithms in a systematic way . This framework views clustering as a general process of iterative optimization that includes modules for supervised learning and instance assignment . The framework has also suggested several novel clustering methods . In this paper , we investigate experimentally the efficacy of these algorithms and test some hypotheses about the relation between such unsupervised techniques and the supervised methods embedded in them .", "keywords": ["supervised learning", "learning", "clustering", "iterative optimization"], "combined": "Generalized clustering , supervised learning , and data assignment Clustering algorithms have become increasingly important in handling and analyzing data . Considerable work has been done in devising effective but increasingly specific clustering algorithms . In contrast , we have developed a generalized framework that accommodates diverse clustering algorithms in a systematic way . This framework views clustering as a general process of iterative optimization that includes modules for supervised learning and instance assignment . The framework has also suggested several novel clustering methods . In this paper , we investigate experimentally the efficacy of these algorithms and test some hypotheses about the relation between such unsupervised techniques and the supervised methods embedded in them . [[EENNDD]] supervised learning; learning; clustering; iterative optimization"}, "Pengelompokan umum, pembelajaran diawasi, dan penugasan data Algoritma pengelompokan menjadi semakin penting dalam mengendalikan dan menganalisis data. Kerja yang besar telah dilakukan dalam merangka algoritma pengelompokan yang berkesan tetapi semakin spesifik. Sebaliknya, kami telah mengembangkan kerangka umum yang mengakomodasi pelbagai algoritma pengelompokan secara sistematik. Kerangka ini melihat pengelompokan sebagai proses umum pengoptimuman berulang yang merangkumi modul untuk pembelajaran yang diselia dan penugasan contoh. Kerangka ini juga telah mencadangkan beberapa kaedah pengelompokan novel. Dalam makalah ini, kami menyiasat secara eksperimental keberkesanan algoritma ini dan menguji beberapa hipotesis mengenai hubungan antara teknik yang tidak diawasi dan kaedah yang diselia yang tertanam di dalamnya. [[EENNDD]] pembelajaran yang diselia; belajar; pengelompokan; pengoptimuman berulang"], [{"string": "Universal multi-dimensional scaling In this paper , we propose a unified algorithmic framework for solving many known variants of MDS . Our algorithm is a simple iterative scheme with guaranteed convergence , and is modular ; by changing the internals of a single subroutine in the algorithm , we can switch cost functions and target spaces easily . In addition to the formal guarantees of convergence , our algorithms are accurate ; in most cases , they converge to better quality solutions than existing methods in comparable time . Moreover , they have a small memory footprint and scale effectively for large data sets . We expect that this framework will be useful for a number of MDS variants that have not yet been studied . Our framework extends to embedding high-dimensional points lying on a sphere to points on a lower dimensional sphere , preserving geodesic distances . As a complement to this result , we also extend the Johnson-Lindenstrauss Lemma to this spherical setting , by showing that projecting to a random O 1 \\/ \u00b52 log n - dimensional sphere causes only an eps-distortion in the geodesic distances .", "keywords": ["multi-dimensional scaling", "dimensionality reduction"], "combined": "Universal multi-dimensional scaling In this paper , we propose a unified algorithmic framework for solving many known variants of MDS . Our algorithm is a simple iterative scheme with guaranteed convergence , and is modular ; by changing the internals of a single subroutine in the algorithm , we can switch cost functions and target spaces easily . In addition to the formal guarantees of convergence , our algorithms are accurate ; in most cases , they converge to better quality solutions than existing methods in comparable time . Moreover , they have a small memory footprint and scale effectively for large data sets . We expect that this framework will be useful for a number of MDS variants that have not yet been studied . Our framework extends to embedding high-dimensional points lying on a sphere to points on a lower dimensional sphere , preserving geodesic distances . As a complement to this result , we also extend the Johnson-Lindenstrauss Lemma to this spherical setting , by showing that projecting to a random O 1 \\/ \u00b52 log n - dimensional sphere causes only an eps-distortion in the geodesic distances . [[EENNDD]] multi-dimensional scaling; dimensionality reduction"}, "Penskalaan pelbagai dimensi universal Dalam makalah ini, kami mencadangkan kerangka algoritma terpadu untuk menyelesaikan banyak varian MDS yang diketahui. Algoritma kami adalah skema lelaran sederhana dengan penumpuan terjamin, dan modular; dengan mengubah dalaman subrutin tunggal dalam algoritma, kita dapat menukar fungsi kos dan ruang sasaran dengan mudah. Sebagai tambahan kepada jaminan penumpuan formal, algoritma kami tepat; dalam kebanyakan kes, mereka bertemu dengan penyelesaian yang lebih berkualiti daripada kaedah yang ada dalam masa yang setanding. Lebih-lebih lagi, mereka mempunyai jejak memori dan skala yang berkesan untuk set data yang besar. Kami menjangkakan bahawa kerangka ini akan berguna untuk sebilangan varian MDS yang belum dipelajari. Kerangka kerja kami merangkumi memasukkan titik dimensi tinggi yang terletak di sfera ke titik pada sfera dimensi yang lebih rendah, menjaga jarak geodesi. Sebagai pelengkap kepada hasil ini, kami juga memperluas Johnson-Lindenstrauss Lemma ke tetapan sfera ini, dengan menunjukkan bahawa memproyeksikan ke bulatan log n - dimensi O 1 \\ / \u00b52 secara rawak menyebabkan hanya distorsi eps pada jarak geodesi. [[EENNDD]] penskalaan pelbagai dimensi; pengurangan dimensi"], [{"string": "Mining the space of graph properties Existing data mining algorithms on graphs look for nodes satisfying specific properties , such as specific notions of structural similarity or specific measures of link-based importance . While such analyses for predetermined properties can be effective in well-understood domains , sometimes identifying an appropriate property for analysis can be a challenge , and focusing on a single property may neglect other important aspects of the data . In this paper , we develop a foundation for mining the properties themselves . We present a theoretical framework defining the space of graph properties , a variety of mining queries enabled by the framework , techniques to handle the enormous size of the query space , and an experimental system called F-Miner that demonstrates the utility and feasibility of property mining .", "keywords": ["graph mining"], "combined": "Mining the space of graph properties Existing data mining algorithms on graphs look for nodes satisfying specific properties , such as specific notions of structural similarity or specific measures of link-based importance . While such analyses for predetermined properties can be effective in well-understood domains , sometimes identifying an appropriate property for analysis can be a challenge , and focusing on a single property may neglect other important aspects of the data . In this paper , we develop a foundation for mining the properties themselves . We present a theoretical framework defining the space of graph properties , a variety of mining queries enabled by the framework , techniques to handle the enormous size of the query space , and an experimental system called F-Miner that demonstrates the utility and feasibility of property mining . [[EENNDD]] graph mining"}, "Melombong ruang sifat grafik Algoritma perlombongan data yang ada pada grafik mencari nod yang memenuhi sifat tertentu, seperti tanggapan spesifik mengenai kesamaan struktur atau langkah-langkah khusus kepentingan berdasarkan pautan. Walaupun analisis seperti itu untuk sifat yang telah ditentukan dapat efektif dalam domain yang difahami dengan baik, kadang-kadang mengenal pasti harta yang sesuai untuk analisis boleh menjadi suatu cabaran, dan memfokuskan pada satu harta boleh mengabaikan aspek penting lain dari data. Dalam makalah ini, kami mengembangkan asas untuk melombong harta tanah itu sendiri. Kami menyajikan kerangka teori yang menentukan ruang sifat grafik, pelbagai pertanyaan perlombongan yang diaktifkan oleh kerangka kerja, teknik untuk menangani ukuran ruang pertanyaan yang sangat besar, dan sistem eksperimental yang disebut F-Miner yang menunjukkan kegunaan dan kelayakan perlombongan harta tanah . [[EENNDD]] perlombongan grafik"], [{"string": "Deriving marketing intelligence from online discussion Weblogs and message boards provide online forums for discussion that record the voice of the public . Woven into this mass of discussion is a wide range of opinion and commentary about consumer products . This presents an opportunity for companies to understand and respond to the consumer by analyzing this unsolicited feedback . Given the volume , format and content of the data , the appropriate approach to understand this data is to use large-scale web and text data mining technologies . This paper argues that applications for mining large volumes of textual data for marketing intelligence should provide two key elements : a suite of powerful mining and visualization technologies and an interactive analysis environment which allows for rapid generation and testing of hypotheses . This paper presents such a system that gathers and annotates online discussion relating to consumer products using a wide variety of state-of-the-art techniques , including crawling , wrapping , search , text classification and computational linguistics . Marketing intelligence is derived through an interactive analysis framework uniquely configured to leverage the connectivity and content of annotated online discussion .", "keywords": ["computational linguistics", "text mining", "information retrieval", "information search and retrieval", "content systems", "machine learning"], "combined": "Deriving marketing intelligence from online discussion Weblogs and message boards provide online forums for discussion that record the voice of the public . Woven into this mass of discussion is a wide range of opinion and commentary about consumer products . This presents an opportunity for companies to understand and respond to the consumer by analyzing this unsolicited feedback . Given the volume , format and content of the data , the appropriate approach to understand this data is to use large-scale web and text data mining technologies . This paper argues that applications for mining large volumes of textual data for marketing intelligence should provide two key elements : a suite of powerful mining and visualization technologies and an interactive analysis environment which allows for rapid generation and testing of hypotheses . This paper presents such a system that gathers and annotates online discussion relating to consumer products using a wide variety of state-of-the-art techniques , including crawling , wrapping , search , text classification and computational linguistics . Marketing intelligence is derived through an interactive analysis framework uniquely configured to leverage the connectivity and content of annotated online discussion . [[EENNDD]] computational linguistics; text mining; information retrieval; information search and retrieval; content systems; machine learning"}, "Mendapatkan kecerdasan pemasaran dari perbincangan dalam talian Weblog dan papan mesej menyediakan forum dalam talian untuk perbincangan yang merakam suara orang ramai. Terkait dengan perbincangan ini adalah pelbagai pendapat dan komen mengenai produk pengguna. Ini memberi peluang kepada syarikat untuk memahami dan memberi respons kepada pengguna dengan menganalisis maklum balas yang tidak diminta ini. Mengingat jumlah, format dan kandungan data, pendekatan yang tepat untuk memahami data ini adalah dengan menggunakan teknologi perlombongan data web dan teks berskala besar. Makalah ini berpendapat bahawa aplikasi untuk melombong sejumlah besar data teks untuk kecerdasan pemasaran harus menyediakan dua elemen utama: rangkaian teknologi perlombongan dan visualisasi yang kuat dan persekitaran analisis interaktif yang memungkinkan untuk menghasilkan dan menguji hipotesis dengan cepat. Makalah ini menyajikan sistem sedemikian yang mengumpulkan dan memberi penjelasan mengenai perbincangan dalam talian yang berkaitan dengan produk pengguna menggunakan pelbagai teknik canggih, termasuk merangkak, membungkus, mencari, klasifikasi teks dan linguistik komputasi. Kepintaran pemasaran dihasilkan melalui kerangka analisis interaktif yang dikonfigurasikan secara unik untuk memanfaatkan hubungan dan kandungan perbincangan dalam talian yang dijelaskan. [[EENNDD]] linguistik komputasi; perlombongan teks; pengambilan maklumat; carian dan pengambilan maklumat; sistem kandungan; pembelajaran mesin"], [{"string": "Integrating feature and instance selection for text classification Instance selection and feature selection are two orthogonal methods for reducing the amount and complexity of data . Feature selection aims at the reduction of redundant features in a dataset whereas instance selection aims at the reduction of the number of instances . So far , these two methods have mostly been considered in isolation . In this paper , we present a new algorithm , which we call FIS Feature and Instance Selection that targets both problems simultaneously in the context of text classificationOur experiments on the Reuters and 20-Newsgroups datasets show that FIS considerably reduces both the number of features and the number of instances . The accuracy of a range of classifiers including Na\u00efve Bayes , TAN and LB considerably improves when using the FIS preprocessed datasets , matching and exceeding that of Support Vector Machines , which is currently considered to be one of the best text classification methods . In all cases the results are much better compared to Mutual Information based feature selection . The training and classification speed of all classifiers is also greatly improved .", "keywords": ["design methodology", "applications"], "combined": "Integrating feature and instance selection for text classification Instance selection and feature selection are two orthogonal methods for reducing the amount and complexity of data . Feature selection aims at the reduction of redundant features in a dataset whereas instance selection aims at the reduction of the number of instances . So far , these two methods have mostly been considered in isolation . In this paper , we present a new algorithm , which we call FIS Feature and Instance Selection that targets both problems simultaneously in the context of text classificationOur experiments on the Reuters and 20-Newsgroups datasets show that FIS considerably reduces both the number of features and the number of instances . The accuracy of a range of classifiers including Na\u00efve Bayes , TAN and LB considerably improves when using the FIS preprocessed datasets , matching and exceeding that of Support Vector Machines , which is currently considered to be one of the best text classification methods . In all cases the results are much better compared to Mutual Information based feature selection . The training and classification speed of all classifiers is also greatly improved . [[EENNDD]] design methodology; applications"}, "Mengintegrasikan pemilihan ciri dan contoh untuk klasifikasi teks Pemilihan instance dan pemilihan ciri adalah dua kaedah ortogonal untuk mengurangkan jumlah dan kerumitan data. Pemilihan fitur bertujuan untuk pengurangan fitur berlebihan dalam set data sedangkan pemilihan contoh bertujuan untuk pengurangan jumlah kejadian. Setakat ini, kedua-dua kaedah ini telah banyak dipertimbangkan secara terpisah. Dalam makalah ini, kami menyajikan algoritma baru, yang kami panggil FIS Feature and Instance Selection yang mensasarkan kedua-dua masalah secara serentak dalam konteks klasifikasi teks Eksperimen kami di kumpulan data Reuters dan 20-Newsgroups menunjukkan bahawa FIS banyak mengurangkan bilangan ciri dan bilangan kejadian. Ketepatan pelbagai pengklasifikasi termasuk Na\u00efve Bayes, TAN dan LB jauh bertambah baik ketika menggunakan set data yang diproses FIS, sepadan dan melebihi yang terdapat pada Mesin Vektor Sokongan, yang kini dianggap sebagai salah satu kaedah klasifikasi teks terbaik. Dalam semua kes, hasilnya jauh lebih baik dibandingkan dengan pemilihan ciri berdasarkan Maklumat Saling. Kepantasan latihan dan klasifikasi semua pengklasifikasi juga bertambah baik. [[EENNDD]] metodologi reka bentuk; aplikasi"], [{"string": "Data mining with sparse grids using simplicial basis functions Recently we presented a new approach 18 to the classification problem arising in data mining . It is based on the regularization network approach but , in contrast to other methods which employ ansatz functions associated to data points , we use a grid in the usually high-dimensional feature space for the minimization process . To cope with the curse of dimensionality , we employ sparse grids 49 . Thus , only O hn-1nd-1 instead of O hn-d grid points and unknowns are involved . Here d denotes the dimension of the feature space and hn = 2-n gives the mesh size . We use the sparse grid combination technique 28 where the classification problem is discretized and solved on a sequence of conventional grids with uniform mesh sizes in each dimension . The sparse grid solution is then obtained by linear combination . In contrast to our former work , where d-linear functions were used , we now apply linear basis functions based on a simplicial discretization . This allows to handle more dimensions and the algorithm needs less operations per data point . We describe the sparse grid combination technique for the classification problem , give implementational details and discuss the complexity of the algorithm . It turns out that the method scales linearly with the number of given data points . Finally we report on the quality of the classifier built by our new method on data sets with up to 10 dimensions . It turns out that our new method achieves correctness rates which are competitive to that of the best existing methods .", "keywords": ["combination technique", "classification", "simplicial discretization", "sparse grids", "approximation"], "combined": "Data mining with sparse grids using simplicial basis functions Recently we presented a new approach 18 to the classification problem arising in data mining . It is based on the regularization network approach but , in contrast to other methods which employ ansatz functions associated to data points , we use a grid in the usually high-dimensional feature space for the minimization process . To cope with the curse of dimensionality , we employ sparse grids 49 . Thus , only O hn-1nd-1 instead of O hn-d grid points and unknowns are involved . Here d denotes the dimension of the feature space and hn = 2-n gives the mesh size . We use the sparse grid combination technique 28 where the classification problem is discretized and solved on a sequence of conventional grids with uniform mesh sizes in each dimension . The sparse grid solution is then obtained by linear combination . In contrast to our former work , where d-linear functions were used , we now apply linear basis functions based on a simplicial discretization . This allows to handle more dimensions and the algorithm needs less operations per data point . We describe the sparse grid combination technique for the classification problem , give implementational details and discuss the complexity of the algorithm . It turns out that the method scales linearly with the number of given data points . Finally we report on the quality of the classifier built by our new method on data sets with up to 10 dimensions . It turns out that our new method achieves correctness rates which are competitive to that of the best existing methods . [[EENNDD]] combination technique; classification; simplicial discretization; sparse grids; approximation"}, "Perlombongan data dengan grid jarang menggunakan fungsi asas sederhana Baru-baru ini kami mengemukakan pendekatan baru 18 untuk masalah klasifikasi yang timbul dalam perlombongan data. Ini didasarkan pada pendekatan jaringan regularisasi tetapi, berbeza dengan kaedah lain yang menggunakan fungsi ansatz yang terkait dengan titik data, kami menggunakan grid di ruang fitur dimensi tinggi biasanya untuk proses pengurangan. Untuk mengatasi sumpahan dimensi, kita menggunakan grid jarang 49. Oleh itu, hanya titik grid O hn-1nd-1 dan bukan grid O hn-d yang terlibat dan tidak diketahui yang terlibat. Di sini d menunjukkan dimensi ruang ciri dan hn = 2-n memberikan ukuran mesh. Kami menggunakan teknik kombinasi grid jarang yang mana masalah klasifikasi diskrit dan diselesaikan berdasarkan urutan grid konvensional dengan ukuran mesh seragam dalam setiap dimensi. Penyelesaian grid jarang diperoleh dengan kombinasi linear. Berbeza dengan karya terdahulu kami, di mana fungsi d-linear digunakan, kami sekarang menerapkan fungsi dasar linear berdasarkan diskritisasi sederhana. Ini memungkinkan untuk menangani lebih banyak dimensi dan algoritma memerlukan lebih sedikit operasi setiap titik data. Kami menerangkan teknik kombinasi grid jarang untuk masalah klasifikasi, memberikan perincian pelaksanaan dan membincangkan kerumitan algoritma. Ternyata kaedah itu skala secara linear dengan jumlah titik data yang diberikan. Akhirnya kami melaporkan kualiti pengkelasan yang dibina dengan kaedah baru kami pada set data dengan 10 dimensi. Ternyata kaedah baru kami mencapai kadar kebenaran yang bersaing dengan kaedah terbaik yang ada. [[EENNDD]] teknik gabungan; pengelasan; budi bicara mudah; grid jarang; penghampiran"], [{"string": "Privacy-preserving k-means clustering over vertically partitioned data Privacy and security concerns can prevent sharing of data , derailing data mining projects . Distributed knowledge discovery , if done correctly , can alleviate this problem . The key is to obtain valid results , while providing guarantees on the non disclosure of data . We present a method for k-means clustering when different sites contain different attributes for a common set of entities . Each site learns the cluster of each entity , but learns nothing about the attributes at other sites .", "keywords": ["privacy"], "combined": "Privacy-preserving k-means clustering over vertically partitioned data Privacy and security concerns can prevent sharing of data , derailing data mining projects . Distributed knowledge discovery , if done correctly , can alleviate this problem . The key is to obtain valid results , while providing guarantees on the non disclosure of data . We present a method for k-means clustering when different sites contain different attributes for a common set of entities . Each site learns the cluster of each entity , but learns nothing about the attributes at other sites . [[EENNDD]] privacy"}, "Pemeliharaan privasi k-bermaksud pengelompokan data berpartisi secara vertikal Kebimbangan privasi dan keselamatan dapat mencegah perkongsian data, menggagalkan projek perlombongan data. Penemuan pengetahuan yang diedarkan, jika dilakukan dengan betul, dapat meredakan masalah ini. Kuncinya adalah untuk mendapatkan hasil yang sah, sambil memberikan jaminan untuk tidak mendedahkan data. Kami menyajikan kaedah untuk k-means clustering apabila laman web yang berlainan mengandungi atribut yang berbeza untuk sekumpulan entiti yang sama. Setiap laman web mempelajari kumpulan setiap entiti, tetapi tidak mempelajari apa-apa mengenai atribut di laman web lain. [[EENNDD]] privasi"], [{"string": "Fast nearest-neighbor search in disk-resident graphs Link prediction , personalized graph search , fraud detection , and many such graph mining problems revolve around the computation of the most `` similar '' k nodes to a given query node . One widely used class of similarity measures is based on random walks on graphs , e.g. , personalized pagerank , hitting and commute times , and simrank . There are two fundamental problems associated with these measures . First , existing online algorithms typically examine the local neighborhood of the query node which can become significantly slower whenever high-degree nodes are encountered a common phenomenon in real-world graphs . We prove that turning high degree nodes into sinks results in only a small approximation error , while greatly improving running times . The second problem is that of computing similarities at query time when the graph is too large to be memory-resident . The obvious solution is to split the graph into clusters of nodes and store each cluster on a disk page ; ideally random walks will rarely cross cluster boundaries and cause page-faults . Our contributions here are twofold : a we present an efficient deterministic algorithm to find the k closest neighbors in terms of personalized pagerank of any query node in such a clustered graph , and b we develop a clustering algorithm RWDISK that uses only sequential sweeps over data files . Empirical results on several large publicly available graphs like DBLP , Citeseer and Live-Journal ~ 90 M edges demonstrate that turning high degree nodes into sinks not only improves running time of RWDISK by a factor of 3 but also boosts link prediction accuracy by a factor of 4 on average . We also show that RWDISK returns more desirable high conductance and small size clusters than the popular clustering algorithm METIS , while requiring much less memory . Finally our deterministic algorithm for computing nearest neighbors incurs far fewer page-faults factor of 5 than actually simulating random walks .", "keywords": ["external memory", "graphs", "random walks", "link prediction"], "combined": "Fast nearest-neighbor search in disk-resident graphs Link prediction , personalized graph search , fraud detection , and many such graph mining problems revolve around the computation of the most `` similar '' k nodes to a given query node . One widely used class of similarity measures is based on random walks on graphs , e.g. , personalized pagerank , hitting and commute times , and simrank . There are two fundamental problems associated with these measures . First , existing online algorithms typically examine the local neighborhood of the query node which can become significantly slower whenever high-degree nodes are encountered a common phenomenon in real-world graphs . We prove that turning high degree nodes into sinks results in only a small approximation error , while greatly improving running times . The second problem is that of computing similarities at query time when the graph is too large to be memory-resident . The obvious solution is to split the graph into clusters of nodes and store each cluster on a disk page ; ideally random walks will rarely cross cluster boundaries and cause page-faults . Our contributions here are twofold : a we present an efficient deterministic algorithm to find the k closest neighbors in terms of personalized pagerank of any query node in such a clustered graph , and b we develop a clustering algorithm RWDISK that uses only sequential sweeps over data files . Empirical results on several large publicly available graphs like DBLP , Citeseer and Live-Journal ~ 90 M edges demonstrate that turning high degree nodes into sinks not only improves running time of RWDISK by a factor of 3 but also boosts link prediction accuracy by a factor of 4 on average . We also show that RWDISK returns more desirable high conductance and small size clusters than the popular clustering algorithm METIS , while requiring much less memory . Finally our deterministic algorithm for computing nearest neighbors incurs far fewer page-faults factor of 5 than actually simulating random walks . [[EENNDD]] external memory; graphs; random walks; link prediction"}, "Pencarian tetangga terdekat yang cepat dalam grafik pemacu cakera Ramalan pautan, carian grafik yang diperibadikan, pengesanan penipuan, dan banyak masalah perlombongan grafik berkisar pada pengiraan node k yang paling \"serupa\" dengan nod pertanyaan yang diberikan. Satu kelas ukuran kesamaan yang banyak digunakan adalah berdasarkan jalan grafik secara rawak, mis. , pagerank diperibadikan, memukul dan berulang-alik, dan simrank. Terdapat dua masalah asas yang berkaitan dengan langkah-langkah ini. Pertama, algoritma dalam talian yang ada biasanya meneliti persekitaran tempatan node pertanyaan yang boleh menjadi jauh lebih perlahan setiap kali node darjah tinggi mengalami fenomena biasa dalam grafik dunia nyata. Kami membuktikan bahawa mengubah node darjah tinggi menjadi sink hanya menghasilkan ralat penghampiran kecil, sekaligus meningkatkan masa berjalan. Masalah kedua ialah kesamaan pengkomputeran pada waktu pertanyaan apabila grafik terlalu besar untuk dijadikan memori. Penyelesaian yang jelas adalah dengan memisahkan grafik menjadi kumpulan nod dan menyimpan setiap kluster pada halaman cakera; jalan rawak secara ideal jarang melintasi sempadan dan menyebabkan kerosakan halaman. Sumbangan kami di sini dua kali ganda: a kami mengemukakan algoritma deterministik yang cekap untuk mencari jiran terdekat k dari segi pagerank yang diperibadikan dari mana-mana node pertanyaan dalam grafik yang dikelompokkan, dan b kami mengembangkan algoritma kluster RWDISK yang hanya menggunakan sapuan berurutan ke atas fail data . Hasil empirik pada beberapa grafik besar yang tersedia untuk umum seperti tepi DBLP, Citeseer dan Live-Journal ~ 90 M menunjukkan bahawa mengubah nod darjah tinggi ke sink tidak hanya meningkatkan masa berjalan RWDISK dengan faktor 3 tetapi juga meningkatkan ketepatan ramalan pautan dengan faktor 4 secara purata. Kami juga menunjukkan bahawa RWDISK mengembalikan kluster konduktiviti tinggi dan saiz kecil yang lebih dikehendaki daripada algoritma pengelompokan METIS yang popular, sementara memerlukan memori yang jauh lebih sedikit. Akhirnya algoritma deterministik kami untuk mengira jiran terdekat mempunyai faktor kesalahan halaman 5 yang jauh lebih sedikit daripada sebenarnya meniru jalan rawak. [[EENNDD]] memori luaran; grafik; jalan rawak; ramalan pautan"], [{"string": "Automatic record linkage using seeded nearest neighbor and support vector machine classification The task of linking databases is an important step in an increasing number of data mining projects , because linked data can contain information that is not available otherwise , or that would require time-consuming and expensive collection of specific data . The aim of linking is to match and aggregate all records that refer to the same entity . One of the major challenges when linking large databases is the efficient and accurate classification of record pairs into matches and non-matches . While traditionally classification was based on manually-set thresholds or on statistical procedures , many of the more recently developed classification methods are based on supervised learning techniques . They therefore require training data , which is often not available in real world situations or has to be prepared manually , an expensive , cumbersome and time-consuming process . The author has previously presented a novel two-step approach to automatic record pair classification 6 , 7 . In the first step of this approach , training examples of high quality are automatically selected from the compared record pairs , and used in the second step to train a support vector machine SVM classifier . Initial experiments showed the feasibility of the approach , achieving results that outperformed k-means clustering . In this paper , two variations of this approach are presented . The first is based on a nearest-neighbour classifier , while the second improves a SVM classifier by iteratively adding more examples into the training sets . Experimental results show that this two-step approach can achieve better classification results than other unsupervised approaches .", "keywords": ["nearest neighbour", "entity resolution", "support vector machine", "data matching", "data linkage", "deduplication"], "combined": "Automatic record linkage using seeded nearest neighbor and support vector machine classification The task of linking databases is an important step in an increasing number of data mining projects , because linked data can contain information that is not available otherwise , or that would require time-consuming and expensive collection of specific data . The aim of linking is to match and aggregate all records that refer to the same entity . One of the major challenges when linking large databases is the efficient and accurate classification of record pairs into matches and non-matches . While traditionally classification was based on manually-set thresholds or on statistical procedures , many of the more recently developed classification methods are based on supervised learning techniques . They therefore require training data , which is often not available in real world situations or has to be prepared manually , an expensive , cumbersome and time-consuming process . The author has previously presented a novel two-step approach to automatic record pair classification 6 , 7 . In the first step of this approach , training examples of high quality are automatically selected from the compared record pairs , and used in the second step to train a support vector machine SVM classifier . Initial experiments showed the feasibility of the approach , achieving results that outperformed k-means clustering . In this paper , two variations of this approach are presented . The first is based on a nearest-neighbour classifier , while the second improves a SVM classifier by iteratively adding more examples into the training sets . Experimental results show that this two-step approach can achieve better classification results than other unsupervised approaches . [[EENNDD]] nearest neighbour; entity resolution; support vector machine; data matching; data linkage; deduplication"}, "Perhubungan rekod automatik menggunakan tetangga terdekat yang disusun dan menyokong klasifikasi mesin vektor Tugas menghubungkan pangkalan data adalah langkah penting dalam peningkatan jumlah projek perlombongan data, kerana data yang dipautkan dapat berisi maklumat yang tidak tersedia sebaliknya, atau yang memerlukan waktu dan pengumpulan data tertentu yang mahal. Tujuan penghubung adalah untuk memadankan dan mengumpulkan semua rekod yang merujuk kepada entiti yang sama. Salah satu cabaran utama ketika menghubungkan pangkalan data yang besar adalah pengelasan pasangan rekod yang cekap dan tepat menjadi padanan dan bukan padanan. Walaupun secara tradisional klasifikasi berdasarkan ambang yang ditetapkan secara manual atau berdasarkan prosedur statistik, banyak kaedah klasifikasi yang baru dikembangkan berdasarkan teknik pembelajaran yang diselia. Oleh itu, mereka memerlukan data latihan, yang sering tidak tersedia dalam situasi dunia nyata atau harus disiapkan secara manual, proses yang mahal, membebankan dan memakan masa. Penulis sebelum ini telah mengemukakan pendekatan dua langkah baru untuk klasifikasi pasangan rekod automatik 6, 7. Pada langkah pertama pendekatan ini, contoh latihan berkualiti tinggi secara automatik dipilih dari pasangan rekod yang dibandingkan, dan digunakan pada langkah kedua untuk melatih mesin pengelas SVM vektor sokongan. Eksperimen awal menunjukkan kelayakan pendekatan, mencapai hasil yang mengalahkan k-berarti kluster. Dalam makalah ini, dua variasi pendekatan ini dikemukakan. Yang pertama berdasarkan pengklasifikasi terdekat-jiran, sementara yang kedua meningkatkan pengklasifikasi SVM dengan secara berulang menambahkan lebih banyak contoh ke dalam set latihan. Hasil eksperimen menunjukkan bahawa pendekatan dua langkah ini dapat mencapai hasil klasifikasi yang lebih baik daripada pendekatan lain yang tidak diawasi. [[EENNDD]] jiran terdekat; ketetapan entiti; mesin vektor sokongan; pemadanan data; perkaitan data; dedikasi"], [{"string": "Primal sparse Max-margin Markov networks Max-margin Markov networks M3N have shown great promise in structured prediction and relational learning . Due to the KKT conditions , the M3N enjoys dual sparsity . However , the existing M3N formulation does not enjoy primal sparsity , which is a desirable property for selecting significant features and reducing the risk of over-fitting . In this paper , we present an l1-norm regularized max-margin Markov network l1-M3N , which enjoys dual and primal sparsity simultaneously . To learn an l1-M3N , we present three methods including projected sub-gradient , cutting-plane , and a novel EM-style algorithm , which is based on an equivalence between l1-M3N and an adaptive M3N . We perform extensive empirical studies on both synthetic and real data sets . Our experimental results show that : 1 l1-M3N can effectively select significant features ; 2 l1-M3N can perform as well as the pseudo-primal sparse Laplace M3N in prediction accuracy , while consistently outperforms other competing methods that enjoy either primal or dual sparsity ; and 3 the EM-algorithm is more robust than the other two in pre-diction accuracy and time efficiency .", "keywords": ["primal sparsity", "l_1-norm max-margin markov networks", "dual sparsity"], "combined": "Primal sparse Max-margin Markov networks Max-margin Markov networks M3N have shown great promise in structured prediction and relational learning . Due to the KKT conditions , the M3N enjoys dual sparsity . However , the existing M3N formulation does not enjoy primal sparsity , which is a desirable property for selecting significant features and reducing the risk of over-fitting . In this paper , we present an l1-norm regularized max-margin Markov network l1-M3N , which enjoys dual and primal sparsity simultaneously . To learn an l1-M3N , we present three methods including projected sub-gradient , cutting-plane , and a novel EM-style algorithm , which is based on an equivalence between l1-M3N and an adaptive M3N . We perform extensive empirical studies on both synthetic and real data sets . Our experimental results show that : 1 l1-M3N can effectively select significant features ; 2 l1-M3N can perform as well as the pseudo-primal sparse Laplace M3N in prediction accuracy , while consistently outperforms other competing methods that enjoy either primal or dual sparsity ; and 3 the EM-algorithm is more robust than the other two in pre-diction accuracy and time efficiency . [[EENNDD]] primal sparsity; l_1-norm max-margin markov networks; dual sparsity"}, "Rangkaian Markal marginal minimum Markov Rangkaian Markov margin-maksimum M3N telah menunjukkan janji besar dalam ramalan berstruktur dan pembelajaran hubungan. Oleh kerana keadaan KKT, M3N menikmati jarak yang rendah. Walau bagaimanapun, formulasi M3N yang ada tidak menikmati sparsity primal, yang merupakan sifat yang diinginkan untuk memilih ciri yang signifikan dan mengurangkan risiko over-fitting. Dalam makalah ini, kami menyajikan jaringan Markov margin -1 l1-M3N normal l1-norm yang teratur, yang menikmati jarak jarang dan primal secara serentak. Untuk mempelajari l1-M3N, kami menyajikan tiga kaedah termasuk diproyeksikan sub-kecerunan, bidang pemotongan, dan algoritma gaya EM baru, yang didasarkan pada kesetaraan antara l1-M3N dan M3N adaptif. Kami melakukan kajian empirikal yang luas pada kedua-dua set data sintetik dan sebenar. Hasil eksperimen kami menunjukkan bahawa: 1 l1-M3N dapat memilih ciri penting dengan berkesan; 2 l1-M3N dapat berfungsi sama seperti Laplace M3N pseudo-primal sparse dalam ketepatan ramalan, sementara secara konsisten mengungguli kaedah bersaing lain yang menikmati sparsity primal atau dual; dan 3 algoritma EM lebih mantap daripada dua yang lain dalam ketepatan pra-diksi dan kecekapan masa. [[EENNDD]] sparsity primal; rangkaian markov margin maksimum l_1-norma; dua sparsiti"], [{"string": "Out-of-core frequent pattern mining on a commodity PC In this work we focus on the problem of frequent itemset mining on large , out-of-core data sets . After presenting a characterization of existing out-of-core frequent itemset mining algorithms and their drawbacks , we introduce our efficient , highly scalable solution . Presented in the context of the FPGrowth algorithm , our technique involves several novel I\\/O-conscious optimizations , such as approximate hash-based sorting and blocking , and leverages recent architectural advancements in commodity computers , such as 64-bit processing . We evaluate the proposed optimizations on truly large data sets , up to 75GB , and show they yield greater than a 400-fold execution time improvement . Finally , we discuss the impact of this research in the context of other pattern mining challenges , such as sequence mining and graph mining .", "keywords": ["itemsets", "secondary memory", "out of core", "pattern mining"], "combined": "Out-of-core frequent pattern mining on a commodity PC In this work we focus on the problem of frequent itemset mining on large , out-of-core data sets . After presenting a characterization of existing out-of-core frequent itemset mining algorithms and their drawbacks , we introduce our efficient , highly scalable solution . Presented in the context of the FPGrowth algorithm , our technique involves several novel I\\/O-conscious optimizations , such as approximate hash-based sorting and blocking , and leverages recent architectural advancements in commodity computers , such as 64-bit processing . We evaluate the proposed optimizations on truly large data sets , up to 75GB , and show they yield greater than a 400-fold execution time improvement . Finally , we discuss the impact of this research in the context of other pattern mining challenges , such as sequence mining and graph mining . [[EENNDD]] itemsets; secondary memory; out of core; pattern mining"}, "Perlombongan corak kerap di luar teras pada PC komoditi Dalam kerja ini kita menumpukan pada masalah perlombongan itemet kerap pada set data yang besar di luar teras. Setelah membentangkan ciri algoritma perlombongan itemet out-of-core yang kerap ada dan kekurangannya, kami memperkenalkan penyelesaian kami yang cekap dan sangat berskala. Dikemukakan dalam konteks algoritma FPGrowth, teknik kami melibatkan beberapa pengoptimuman sedar I / / O, seperti perkiraan penyortiran dan penyekat berdasarkan hash, dan memanfaatkan kemajuan seni bina terkini dalam komputer komoditi, seperti pemprosesan 64-bit. Kami menilai pengoptimuman yang dicadangkan pada set data yang benar-benar besar, hingga 75 GB, dan menunjukkan hasilnya lebih baik daripada peningkatan masa pelaksanaan 400 kali ganda. Akhirnya, kami membincangkan kesan penyelidikan ini dalam konteks cabaran perlombongan corak lain, seperti perlombongan urutan dan perlombongan grafik. [[EENNDD]] set item; memori sekunder; di luar teras; perlombongan corak"], [{"string": "Selecting the right interestingness measure for association patterns Many techniques for association rule mining and feature selection require a suitable metric to capture the dependencies among variables in a data set . For example , metrics such as support , confidence , lift , correlation , and collective strength are often used to determine the interestingness of association patterns . However , many such measures provide conflicting information about the interestingness of a pattern , and the best metric to use for a given application domain is rarely known . In this paper , we present an overview of various measures proposed in the statistics , machine learning and data mining literature . We describe several key properties one should examine in order to select the right measure for a given application domain . A comparative study of these properties is made using twenty one of the existing measures . We show that each measure has different properties which make them useful for some application domains , but not for others . We also present two scenarios in which most of the existing measures agree with each other , namely , support-based pruning and table standardization . Finally , we present an algorithm to select a small set of tables such that an expert can select a desirable measure by looking at just this small set of tables .", "keywords": ["associations", "interestingness measure", "contingency tables"], "combined": "Selecting the right interestingness measure for association patterns Many techniques for association rule mining and feature selection require a suitable metric to capture the dependencies among variables in a data set . For example , metrics such as support , confidence , lift , correlation , and collective strength are often used to determine the interestingness of association patterns . However , many such measures provide conflicting information about the interestingness of a pattern , and the best metric to use for a given application domain is rarely known . In this paper , we present an overview of various measures proposed in the statistics , machine learning and data mining literature . We describe several key properties one should examine in order to select the right measure for a given application domain . A comparative study of these properties is made using twenty one of the existing measures . We show that each measure has different properties which make them useful for some application domains , but not for others . We also present two scenarios in which most of the existing measures agree with each other , namely , support-based pruning and table standardization . Finally , we present an algorithm to select a small set of tables such that an expert can select a desirable measure by looking at just this small set of tables . [[EENNDD]] associations; interestingness measure; contingency tables"}, "Memilih ukuran minat yang tepat untuk corak persatuan Banyak teknik untuk perlombongan peraturan persatuan dan pemilihan ciri memerlukan metrik yang sesuai untuk menangkap pergantungan antara pemboleh ubah dalam satu set data. Sebagai contoh, metrik seperti sokongan, keyakinan, peningkatan, korelasi, dan kekuatan kolektif sering digunakan untuk menentukan menariknya corak pergaulan. Walau bagaimanapun, banyak langkah seperti itu memberikan maklumat yang bertentangan mengenai keseronokan corak, dan metrik terbaik untuk digunakan untuk domain aplikasi tertentu jarang diketahui. Dalam makalah ini, kami menyajikan gambaran umum mengenai berbagai ukuran yang dicadangkan dalam statistik, pembelajaran mesin dan literatur data mining. Kami menerangkan beberapa sifat utama yang harus dikaji untuk memilih ukuran yang tepat untuk domain aplikasi tertentu. Kajian perbandingan sifat-sifat ini dibuat dengan menggunakan dua puluh satu ukuran yang ada. Kami menunjukkan bahawa setiap ukuran mempunyai sifat yang berbeza yang menjadikannya berguna untuk beberapa domain aplikasi, tetapi tidak untuk yang lain. Kami juga menyajikan dua senario di mana sebahagian besar langkah-langkah yang ada saling setuju antara satu sama lain, iaitu pemangkasan berasaskan sokongan dan standardisasi jadual. Akhirnya, kami menyajikan algoritma untuk memilih sekumpulan kecil jadual sehingga seorang pakar dapat memilih ukuran yang diinginkan dengan melihat hanya sekumpulan jadual kecil ini. [[EENNDD]] persatuan; ukuran minat; jadual kontingensi"], [{"string": "Clustering based large margin classification : a scalable approach using SOCP formulation This paper presents a novel Second Order Cone Programming SOCP formulation for large scale binary classification tasks . Assuming that the class conditional densities are mixture distributions , where each component of the mixture has a spherical covariance , the second order statistics of the components can be estimated efficiently using clustering algorithms like BIRCH . For each cluster , the second order moments are used to derive a second order cone constraint via a Chebyshev-Cantelli inequality . This constraint ensures that any data point in the cluster is classified correctly with a high probability . This leads to a large margin SOCP formulation whose size depends on the number of clusters rather than the number of training data points . Hence , the proposed formulation scales well for large datasets when compared to the state-of-the-art classifiers , Support Vector Machines SVMs . Experiments on real world and synthetic datasets show that the proposed algorithm outperforms SVM solvers in terms of training time and achieves similar accuracies .", "keywords": ["gaussian mixture models", "large margin classification", "birch", "scalability"], "combined": "Clustering based large margin classification : a scalable approach using SOCP formulation This paper presents a novel Second Order Cone Programming SOCP formulation for large scale binary classification tasks . Assuming that the class conditional densities are mixture distributions , where each component of the mixture has a spherical covariance , the second order statistics of the components can be estimated efficiently using clustering algorithms like BIRCH . For each cluster , the second order moments are used to derive a second order cone constraint via a Chebyshev-Cantelli inequality . This constraint ensures that any data point in the cluster is classified correctly with a high probability . This leads to a large margin SOCP formulation whose size depends on the number of clusters rather than the number of training data points . Hence , the proposed formulation scales well for large datasets when compared to the state-of-the-art classifiers , Support Vector Machines SVMs . Experiments on real world and synthetic datasets show that the proposed algorithm outperforms SVM solvers in terms of training time and achieves similar accuracies . [[EENNDD]] gaussian mixture models; large margin classification; birch; scalability"}, "Klasifikasi margin besar berdasarkan pengelompokan: pendekatan berskala menggunakan formulasi SOCP Makalah ini mengemukakan rumusan SOCP Pengaturcaraan Kedua Pesanan Kerucut Kedua untuk tugas klasifikasi binari skala besar. Dengan andaian bahawa ketumpatan bersyarat kelas adalah taburan campuran, di mana setiap komponen campuran mempunyai kovarian sfera, statistik urutan kedua komponen dapat dianggarkan dengan berkesan menggunakan algoritma kluster seperti BIRCH. Untuk setiap kluster, momen urutan kedua digunakan untuk memperoleh kekangan urutan kedua melalui ketaksamaan Chebyshev-Cantelli. Kekangan ini memastikan bahawa setiap titik data dalam kluster dikelaskan dengan betul dengan kebarangkalian yang tinggi. Ini membawa kepada formulasi SOCP margin besar yang ukurannya bergantung pada jumlah kelompok dan bukannya jumlah titik data latihan. Oleh itu, rumusan yang dicadangkan dinilai dengan baik untuk set data yang besar jika dibandingkan dengan pengklasifikasi canggih, SVM Mesin Vektor Sokongan. Eksperimen di dunia nyata dan kumpulan data sintetik menunjukkan bahawa algoritma yang dicadangkan mengatasi pemecah SVM dari segi masa latihan dan mencapai ketepatan yang serupa. [[EENNDD]] model campuran gaussian; klasifikasi margin besar; birch; skalabiliti"], [{"string": "A scalable two-stage approach for a class of dimensionality reduction techniques Dimensionality reduction plays an important role in many data mining applications involving high-dimensional data . Many existing dimensionality reduction techniques can be formulated as a generalized eigenvalue problem , which does not scale to large-size problems . Prior work transforms the generalized eigenvalue problem into an equivalent least squares formulation , which can then be solved efficiently . However , the equivalence relationship only holds under certain assumptions without regularization , which severely limits their applicability in practice . In this paper , an efficient two-stage approach is proposed to solve a class of dimensionality reduction techniques , including Canonical Correlation Analysis , Orthonormal Partial Least Squares , linear Discriminant Analysis , and Hypergraph Spectral Learning . The proposed two-stage approach scales linearly in terms of both the sample size and data dimensionality . The main contributions of this paper include 1 we rigorously establish the equivalence relationship between the proposed two-stage approach and the original formulation without any assumption ; and 2 we show that the equivalence relationship still holds in the regularization setting . We have conducted extensive experiments using both synthetic and real-world data sets . Our experimental results confirm the equivalence relationship established in this paper . Results also demonstrate the scalability of the proposed two-stage approach .", "keywords": ["generalized eigenvalue problem", "regularization", "dimensionality reduction", "scalability", "least squares"], "combined": "A scalable two-stage approach for a class of dimensionality reduction techniques Dimensionality reduction plays an important role in many data mining applications involving high-dimensional data . Many existing dimensionality reduction techniques can be formulated as a generalized eigenvalue problem , which does not scale to large-size problems . Prior work transforms the generalized eigenvalue problem into an equivalent least squares formulation , which can then be solved efficiently . However , the equivalence relationship only holds under certain assumptions without regularization , which severely limits their applicability in practice . In this paper , an efficient two-stage approach is proposed to solve a class of dimensionality reduction techniques , including Canonical Correlation Analysis , Orthonormal Partial Least Squares , linear Discriminant Analysis , and Hypergraph Spectral Learning . The proposed two-stage approach scales linearly in terms of both the sample size and data dimensionality . The main contributions of this paper include 1 we rigorously establish the equivalence relationship between the proposed two-stage approach and the original formulation without any assumption ; and 2 we show that the equivalence relationship still holds in the regularization setting . We have conducted extensive experiments using both synthetic and real-world data sets . Our experimental results confirm the equivalence relationship established in this paper . Results also demonstrate the scalability of the proposed two-stage approach . [[EENNDD]] generalized eigenvalue problem; regularization; dimensionality reduction; scalability; least squares"}, "Pendekatan dua peringkat yang boleh diskalakan untuk kelas teknik pengurangan dimensionaliti Pengurangan dimensi memainkan peranan penting dalam banyak aplikasi perlombongan data yang melibatkan data dimensi tinggi. Banyak teknik pengurangan dimensi yang ada dapat dirumuskan sebagai masalah nilai eigen yang umum, yang tidak berukuran dengan masalah ukuran besar. Kerja sebelumnya mengubah masalah nilai eigen umum menjadi rumus kuasa dua setara, yang kemudian dapat diselesaikan dengan cekap. Walau bagaimanapun, hubungan kesetaraan hanya berlaku di bawah andaian tertentu tanpa pengaturcaraan, yang sangat membatasi penerapannya dalam praktik. Dalam makalah ini, pendekatan dua tahap yang efisien diusulkan untuk menyelesaikan kelas teknik pengurangan dimensi, termasuk Analisis Korelasi Canonical, Orthonormal Partial Least Squares, linear Discriminant Analysis, dan Hypergraph Spectral Learning. Pendekatan dua peringkat yang dicadangkan skala secara linear dari segi ukuran sampel dan dimensi data. Sumbangan utama makalah ini merangkumi 1 kami dengan tegas menjalin hubungan kesetaraan antara pendekatan dua peringkat yang dicadangkan dan rumusan asalnya tanpa anggapan; dan 2 kami menunjukkan bahawa hubungan kesetaraan masih berlaku dalam pengaturan regularisasi. Kami telah melakukan eksperimen yang luas menggunakan kedua-dua kumpulan data sintetik dan dunia nyata. Hasil eksperimen kami mengesahkan hubungan kesetaraan yang dijalin dalam makalah ini. Hasil juga menunjukkan skalabilitas pendekatan dua tahap yang dicadangkan. [[EENNDD]] masalah nilai eigen umum; pengaturcaraan; pengurangan dimensi; skalabiliti; petak paling sedikit"], [{"string": "Real-time bidding algorithms for performance-based display ad allocation We describe a real-time bidding algorithm for performance-based display ad allocation . A central issue in performance display advertising is matching campaigns to ad impressions , which can be formulated as a constrained optimization problem that maximizes revenue subject to constraints such as budget limits and inventory availability . The current practice is to solve the optimization problem offline at a tractable level of impression granularity e.g. , the page level , and to serve ads online based on the precomputed static delivery scheme . Although this offline approach takes a global view to achieve optimality , it fails to scale to ad allocation at the individual impression level . Therefore , we propose a real-time bidding algorithm that enables fine-grained impression valuation e.g. , targeting users with real-time conversion data , and adjusts value-based bids according to real-time constraint snapshots e.g. , budget consumption levels . Theoretically , we show that under a linear programming LP primal-dual formulation , the simple real-time bidding algorithm is indeed an online solver to the original primal problem by taking the optimal solution to the dual problem as input . In other words , the online algorithm guarantees the offline optimality given the same level of knowledge an offline optimization would have . Empirically , we develop and experiment with two real-time bid adjustment approaches to adapting to the non-stationary nature of the marketplace : one adjusts bids against real-time constraint satisfaction levels using control-theoretic methods , and the other adjusts bids also based on the statistically modeled historical bidding landscape . Finally , we show experimental results with real-world ad delivery data that support our theoretical conclusions .", "keywords": ["linear programming", "real-time bidding", "performance display", "ad exchange", "combinatorial optimization"], "combined": "Real-time bidding algorithms for performance-based display ad allocation We describe a real-time bidding algorithm for performance-based display ad allocation . A central issue in performance display advertising is matching campaigns to ad impressions , which can be formulated as a constrained optimization problem that maximizes revenue subject to constraints such as budget limits and inventory availability . The current practice is to solve the optimization problem offline at a tractable level of impression granularity e.g. , the page level , and to serve ads online based on the precomputed static delivery scheme . Although this offline approach takes a global view to achieve optimality , it fails to scale to ad allocation at the individual impression level . Therefore , we propose a real-time bidding algorithm that enables fine-grained impression valuation e.g. , targeting users with real-time conversion data , and adjusts value-based bids according to real-time constraint snapshots e.g. , budget consumption levels . Theoretically , we show that under a linear programming LP primal-dual formulation , the simple real-time bidding algorithm is indeed an online solver to the original primal problem by taking the optimal solution to the dual problem as input . In other words , the online algorithm guarantees the offline optimality given the same level of knowledge an offline optimization would have . Empirically , we develop and experiment with two real-time bid adjustment approaches to adapting to the non-stationary nature of the marketplace : one adjusts bids against real-time constraint satisfaction levels using control-theoretic methods , and the other adjusts bids also based on the statistically modeled historical bidding landscape . Finally , we show experimental results with real-world ad delivery data that support our theoretical conclusions . [[EENNDD]] linear programming; real-time bidding; performance display; ad exchange; combinatorial optimization"}, "Algoritma pembidaan masa nyata untuk peruntukan iklan paparan berdasarkan prestasi Kami menerangkan algoritma pembidaan masa nyata untuk peruntukan iklan paparan berdasarkan prestasi. Masalah utama dalam iklan paparan prestasi adalah mencocokkan kampanye dengan tayangan iklan, yang dapat dirumuskan sebagai masalah pengoptimuman terkekang yang memaksimumkan pendapatan tertakluk pada batasan seperti had anggaran dan ketersediaan inventori. Amalan semasa adalah untuk menyelesaikan masalah pengoptimuman di luar talian pada tahap butiran kesan yang dapat disembuhkan, mis. , tahap halaman, dan untuk menayangkan iklan dalam talian berdasarkan skema penyampaian statik yang telah dikomputerkan. Walaupun pendekatan luar talian ini mengambil pandangan global untuk mencapai tahap optimum, ia gagal untuk meningkatkan peruntukan iklan pada tahap teraan individu. Oleh itu, kami mencadangkan algoritma pembidaan masa nyata yang membolehkan penilaian teraan yang halus, mis. , mensasarkan pengguna dengan data penukaran masa nyata, dan menyesuaikan tawaran berdasarkan nilai mengikut tangkapan had masa nyata, mis. , tahap penggunaan anggaran. Secara teorinya, kami menunjukkan bahawa di bawah formulasi linear primal-dual LP pengaturcaraan linier, algoritma penawaran masa nyata yang mudah memang menjadi penyelesai dalam talian untuk masalah primal yang asal dengan mengambil penyelesaian yang optimum untuk masalah ganda sebagai input. Dengan kata lain, algoritma dalam talian menjamin optimum luar talian memandangkan tahap pengetahuan yang sama dengan pengoptimuman luar talian. Secara empirikal, kami mengembangkan dan bereksperimen dengan dua pendekatan penyesuaian tawaran masa nyata untuk menyesuaikan diri dengan sifat pasar yang tidak bergerak: yang satu menyesuaikan bida terhadap tahap kepuasan kekangan masa nyata menggunakan kaedah teori-kawalan, dan yang lain menyesuaikan tawaran juga berdasarkan lanskap penawaran sejarah yang dimodelkan secara statistik. Akhirnya, kami menunjukkan hasil eksperimen dengan data penyampaian iklan di dunia nyata yang menyokong kesimpulan teori kami. [[EENNDD]] pengaturcaraan linear; pembidaan masa nyata; paparan prestasi; pertukaran iklan; pengoptimuman gabungan"], [{"string": "Summarizing itemset patterns using probabilistic models In this paper , we propose a novel probabilistic approach to summarize frequent itemset patterns . Such techniques are useful for summarization , post-processing , and end-user interpretation , particularly for problems where the resulting set of patterns are huge . In our approach items in the dataset are modeled as random variables . We then construct a Markov Random Fields MRF on these variables based on frequent itemsets and their occurrence statistics . The summarization proceeds in a level-wise iterative fashion . Occurrence statistics of itemsets at the lowest level are used to construct an initial MRF . Statistics of itemsets at the next level can then be inferred from the model . We use those patterns whose occurrence can not be accurately inferred from the model to augment the model in an iterative manner , repeating the procedure until all frequent itemsets can be modeled . The resulting MRF model affords a concise and useful representation of the original collection of itemsets . Extensive empirical study on real datasets show that the new approach can effectively summarize a large number of itemsets and typically significantly outperforms extant approaches .", "keywords": ["itemset pattern summarization", "markov random field", "probabilistic graphical model"], "combined": "Summarizing itemset patterns using probabilistic models In this paper , we propose a novel probabilistic approach to summarize frequent itemset patterns . Such techniques are useful for summarization , post-processing , and end-user interpretation , particularly for problems where the resulting set of patterns are huge . In our approach items in the dataset are modeled as random variables . We then construct a Markov Random Fields MRF on these variables based on frequent itemsets and their occurrence statistics . The summarization proceeds in a level-wise iterative fashion . Occurrence statistics of itemsets at the lowest level are used to construct an initial MRF . Statistics of itemsets at the next level can then be inferred from the model . We use those patterns whose occurrence can not be accurately inferred from the model to augment the model in an iterative manner , repeating the procedure until all frequent itemsets can be modeled . The resulting MRF model affords a concise and useful representation of the original collection of itemsets . Extensive empirical study on real datasets show that the new approach can effectively summarize a large number of itemsets and typically significantly outperforms extant approaches . [[EENNDD]] itemset pattern summarization; markov random field; probabilistic graphical model"}, "Meringkaskan corak itemet menggunakan model probabilistik Dalam makalah ini, kami mencadangkan pendekatan probabilistik baru untuk meringkaskan corak itemet yang kerap. Teknik sedemikian berguna untuk ringkasan, pemprosesan pasca, dan interpretasi pengguna akhir, terutama untuk masalah di mana set corak yang dihasilkan sangat besar. Dalam pendekatan kami item dalam set data dimodelkan sebagai pemboleh ubah rawak. Kami kemudian membina Markov Random Fields MRF pada pemboleh ubah ini berdasarkan kumpulan item yang kerap dan statistik kejadiannya. Ringkasan dilakukan dengan cara berulang secara aras. Statistik kejadian kumpulan barang pada tahap terendah digunakan untuk membina MRF awal. Statistik itemets pada tahap seterusnya kemudian dapat disimpulkan dari model. Kami menggunakan corak yang kejadiannya tidak dapat disimpulkan secara tepat dari model untuk menambah model secara berulang, mengulangi prosedur sehingga semua set item yang kerap dapat dimodelkan. Model MRF yang dihasilkan memberikan gambaran ringkas dan berguna dari koleksi item yang asal. Kajian empirikal yang luas pada kumpulan data sebenar menunjukkan bahawa pendekatan baru dapat merangkum sebilangan besar set item dengan berkesan dan biasanya secara signifikan mengatasi pendekatan yang ada. [[EENNDD]] ringkasan corak itemet; medan rawak markov; model grafik probabilistik"], [{"string": "Computer aided detection via asymmetric cascade of sparse hyperplane classifiers This paper describes a novel classification method for computer aided detection CAD that identifies structures of interest from medical images . CAD problems are challenging largely due to the following three characteristics . Typical CAD training data sets are large and extremely unbalanced between positive and negative classes . When searching for descriptive features , researchers often deploy a large set of experimental features , which consequently introduces irrelevant and redundant features . Finally , a CAD system has to satisfy stringent real-time requirements . This work is distinguished by three key contributions . The first is a cascade classification approach which is able to tackle all the above difficulties in a unified framework by employing an asymmetric cascade of sparse classifiers each trained to achieve high detection sensitivity and satisfactory false positive rates . The second is the incorporation of feature computational costs in a linear program formulation that allows the feature selection process to take into account different evaluation costs of various features . The third is a boosting algorithm derived from column generation optimization to effectively solve the proposed cascade linear programs . We apply the proposed approach to the problem of detecting lung nodules from helical multi-slice CT images . Our approach demonstrates superior performance in comparison against support vector machines , linear discriminant analysis and cascade AdaBoost . Especially , the resulting detection system is significantly sped up with our approach .", "keywords": ["computer aided detection", "cascading classification", "miscellaneous", "mathematical programming", "sparse solutions", "support vector machines"], "combined": "Computer aided detection via asymmetric cascade of sparse hyperplane classifiers This paper describes a novel classification method for computer aided detection CAD that identifies structures of interest from medical images . CAD problems are challenging largely due to the following three characteristics . Typical CAD training data sets are large and extremely unbalanced between positive and negative classes . When searching for descriptive features , researchers often deploy a large set of experimental features , which consequently introduces irrelevant and redundant features . Finally , a CAD system has to satisfy stringent real-time requirements . This work is distinguished by three key contributions . The first is a cascade classification approach which is able to tackle all the above difficulties in a unified framework by employing an asymmetric cascade of sparse classifiers each trained to achieve high detection sensitivity and satisfactory false positive rates . The second is the incorporation of feature computational costs in a linear program formulation that allows the feature selection process to take into account different evaluation costs of various features . The third is a boosting algorithm derived from column generation optimization to effectively solve the proposed cascade linear programs . We apply the proposed approach to the problem of detecting lung nodules from helical multi-slice CT images . Our approach demonstrates superior performance in comparison against support vector machines , linear discriminant analysis and cascade AdaBoost . Especially , the resulting detection system is significantly sped up with our approach . [[EENNDD]] computer aided detection; cascading classification; miscellaneous; mathematical programming; sparse solutions; support vector machines"}, "Pengesanan berbantukan komputer melalui lata asimetrik pengklasifikasi hiperplan jarang Kertas ini menerangkan kaedah klasifikasi baru untuk pengesanan CAD dibantu komputer yang mengenal pasti struktur minat dari gambar perubatan. Masalah CAD amat mencabar kerana tiga ciri berikut. Set data latihan CAD biasa besar dan sangat tidak seimbang antara kelas positif dan negatif. Semasa mencari ciri deskriptif, penyelidik sering menggunakan sebilangan besar ciri eksperimen, yang seterusnya memperkenalkan ciri yang tidak relevan dan berlebihan. Akhirnya, sistem CAD mesti memenuhi syarat masa nyata yang ketat. Karya ini dibezakan dengan tiga sumbangan utama. Yang pertama adalah pendekatan klasifikasi kaskade yang dapat mengatasi semua kesulitan di atas dalam kerangka bersatu dengan menggunakan kaskade asimetri pengklasifikasi jarang yang masing-masing dilatih untuk mencapai kepekaan pengesanan tinggi dan kadar positif palsu yang memuaskan. Yang kedua adalah penggabungan biaya komputasi fitur dalam perumusan program linier yang memungkinkan proses pemilihan fitur untuk mempertimbangkan biaya penilaian yang berbeza dari berbagai fitur. Yang ketiga adalah algoritma penambah yang berasal dari pengoptimuman penjanaan lajur untuk menyelesaikan secara berkesan program linear lata yang dicadangkan. Kami menerapkan pendekatan yang dicadangkan untuk masalah mengesan nodul paru-paru dari gambar CT heliks pelbagai heliks. Pendekatan kami menunjukkan prestasi yang unggul dibandingkan dengan mesin vektor sokongan, analisis diskriminan linear dan lata AdaBoost. Terutama, sistem pengesanan yang dihasilkan dipercepat dengan pendekatan kami. [[EENNDD]] pengesanan berbantukan komputer; klasifikasi lata; pelbagai; pengaturcaraan matematik; penyelesaian jarang; mesin vektor sokongan"], [{"string": "Redundancy based feature selection for microarray data In gene expression microarray data analysis , selecting a small number of discriminative genes from thousands of genes is an important problem for accurate classification of diseases or phenotypes . The problem becomes particularly challenging due to the large number of features genes and small sample size . Traditional gene selection methods often select the top-ranked genes according to their individual discriminative power without handling the high degree of redundancy among the genes . Latest research shows that removing redundant genes among selected ones can achieve a better representation of the characteristics of the targeted phenotypes and lead to improved classification accuracy . Hence , we study in this paper the relationship between feature relevance and redundancy and propose an efficient method that can effectively remove redundant genes . The efficiency and effectiveness of our method in comparison with representative methods has been demonstrated through an empirical study using public microarray data sets .", "keywords": ["microarray data", "feature redundancy", "gene selection"], "combined": "Redundancy based feature selection for microarray data In gene expression microarray data analysis , selecting a small number of discriminative genes from thousands of genes is an important problem for accurate classification of diseases or phenotypes . The problem becomes particularly challenging due to the large number of features genes and small sample size . Traditional gene selection methods often select the top-ranked genes according to their individual discriminative power without handling the high degree of redundancy among the genes . Latest research shows that removing redundant genes among selected ones can achieve a better representation of the characteristics of the targeted phenotypes and lead to improved classification accuracy . Hence , we study in this paper the relationship between feature relevance and redundancy and propose an efficient method that can effectively remove redundant genes . The efficiency and effectiveness of our method in comparison with representative methods has been demonstrated through an empirical study using public microarray data sets . [[EENNDD]] microarray data; feature redundancy; gene selection"}, "Pemilihan fitur berdasarkan redundansi untuk data microarray Dalam analisis data microarray ekspresi gen, memilih sebilangan kecil gen diskriminasi dari ribuan gen adalah masalah penting untuk klasifikasi penyakit atau fenotip yang tepat. Masalahnya menjadi sangat mencabar kerana sebilangan besar gen ciri dan saiz sampel yang kecil. Kaedah pemilihan gen tradisional sering memilih gen peringkat teratas mengikut kekuatan diskriminasi masing-masing tanpa menangani tahap redundansi yang tinggi di antara gen. Penyelidikan terbaru menunjukkan bahawa membuang gen yang berlebihan di antara yang terpilih dapat memperoleh gambaran yang lebih baik mengenai ciri-ciri fenotip yang disasarkan dan menghasilkan peningkatan ketepatan klasifikasi. Oleh itu, kami mengkaji dalam makalah ini hubungan antara relevansi ciri dan redundansi dan mencadangkan kaedah yang berkesan yang dapat menghilangkan gen berlebihan. Kecekapan dan keberkesanan kaedah kami dibandingkan dengan kaedah perwakilan telah ditunjukkan melalui kajian empirikal menggunakan set data mikroarray awam. [[EENNDD]] data microarray; kelebihan pekerja; pemilihan gen"], [{"string": "Mining phenotypes and informative genes from gene expression data Mining microarray gene expression data is an important research topic in bioinformatics with broad applications . While most of the previous studies focus on clustering either genes or samples , it is interesting to ask whether we can partition the complete set of samples into exclusive groups called phenotypes and find a set of informative genes that can manifest the phenotype structure . In this paper , we propose a new problem of simultaneously mining phenotypes and informative genes from gene expression data . Some statistics-based metrics are proposed to measure the quality of the mining results . Two interesting algorithms are developed : the heuristic search and the mutual reinforcing adjustment method . We present an extensive performance study on both real-world data sets and synthetic data sets . The mining results from the two proposed methods are clearly better than those from the previous methods . They are ready for the real-world applications . Between the two methods , the mutual reinforcing adjustment method is in general more scalable , more effective and with better quality of the mining results .", "keywords": ["bioinformatics", "array data", "informative genes", "phenotype"], "combined": "Mining phenotypes and informative genes from gene expression data Mining microarray gene expression data is an important research topic in bioinformatics with broad applications . While most of the previous studies focus on clustering either genes or samples , it is interesting to ask whether we can partition the complete set of samples into exclusive groups called phenotypes and find a set of informative genes that can manifest the phenotype structure . In this paper , we propose a new problem of simultaneously mining phenotypes and informative genes from gene expression data . Some statistics-based metrics are proposed to measure the quality of the mining results . Two interesting algorithms are developed : the heuristic search and the mutual reinforcing adjustment method . We present an extensive performance study on both real-world data sets and synthetic data sets . The mining results from the two proposed methods are clearly better than those from the previous methods . They are ready for the real-world applications . Between the two methods , the mutual reinforcing adjustment method is in general more scalable , more effective and with better quality of the mining results . [[EENNDD]] bioinformatics; array data; informative genes; phenotype"}, "Fenotip perlombongan dan gen maklumat dari data ekspresi gen Melombong data ekspresi gen microarray adalah topik penyelidikan penting dalam bioinformatik dengan aplikasi yang luas. Walaupun kebanyakan kajian sebelumnya menumpukan pada pengelompokan gen atau sampel, adalah menarik untuk bertanya sama ada kita dapat membahagikan kumpulan sampel yang lengkap ke dalam kumpulan eksklusif yang disebut fenotip dan mencari satu set gen bermaklumat yang dapat mewujudkan struktur fenotip. Dalam makalah ini, kami mencadangkan masalah baru untuk melombong fenotip dan gen bermaklumat secara serentak dari data ekspresi gen. Beberapa metrik berdasarkan statistik dicadangkan untuk mengukur kualiti hasil perlombongan. Dua algoritma menarik dikembangkan: carian heuristik dan kaedah penyesuaian saling memperkuat. Kami membentangkan kajian prestasi yang luas pada kumpulan data dunia nyata dan kumpulan data sintetik. Hasil perlombongan dari dua kaedah yang dicadangkan jelas lebih baik daripada kaedah sebelumnya. Mereka bersedia untuk aplikasi dunia nyata. Di antara kedua kaedah tersebut, kaedah penyesuaian saling memperkuat secara umum lebih berskala, lebih berkesan dan dengan kualiti hasil perlombongan yang lebih baik. [[EENNDD]] bioinformatik; data tatasusunan; gen bermaklumat; fenotip"], [{"string": "Partitioned logistic regression for spam filtering Naive Bayes and logistic regression perform well in different regimes . While the former is a very simple generative model which is efficient to train and performs well empirically in many applications , the latter is a discriminative model which often achieves better accuracy and can be shown to outperform naive Bayes asymptotically . In this paper , we propose a novel hybrid model , partitioned logistic regression , which has several advantages over both naive Bayes and logistic regression . This model separates the original feature space into several disjoint feature groups . Individual models on these groups of features are learned using logistic regression and their predictions are combined using the naive Bayes principle to produce a robust final estimation . We show that our model is better both theoretically and empirically . In addition , when applying it in a practical application , email spam filtering , it improves the normalized AUC score at 10 % false-positive rate by 28.8 % and 23.6 % compared to naive Bayes and logistic regression , when using the exact same training examples .", "keywords": ["logistic regression", "email spam filtering", "learning", "naive bayes"], "combined": "Partitioned logistic regression for spam filtering Naive Bayes and logistic regression perform well in different regimes . While the former is a very simple generative model which is efficient to train and performs well empirically in many applications , the latter is a discriminative model which often achieves better accuracy and can be shown to outperform naive Bayes asymptotically . In this paper , we propose a novel hybrid model , partitioned logistic regression , which has several advantages over both naive Bayes and logistic regression . This model separates the original feature space into several disjoint feature groups . Individual models on these groups of features are learned using logistic regression and their predictions are combined using the naive Bayes principle to produce a robust final estimation . We show that our model is better both theoretically and empirically . In addition , when applying it in a practical application , email spam filtering , it improves the normalized AUC score at 10 % false-positive rate by 28.8 % and 23.6 % compared to naive Bayes and logistic regression , when using the exact same training examples . [[EENNDD]] logistic regression; email spam filtering; learning; naive bayes"}, "Regresi logistik berpisah untuk penyaringan spam Naive Bayes dan regresi logistik menunjukkan prestasi yang baik dalam pelbagai rejim. Walaupun yang pertama adalah model generatif yang sangat sederhana yang cekap untuk melatih dan berkinerja dengan baik secara empirik dalam banyak aplikasi, yang kedua adalah model diskriminatif yang sering mencapai ketepatan yang lebih baik dan dapat ditunjukkan untuk mengalahkan Bayes yang naif secara asimtotik. Dalam makalah ini, kami mengusulkan model hibrid baru, regresi logistik berpisah, yang mempunyai beberapa kelebihan berbanding Bayes naif dan regresi logistik. Model ini memisahkan ruang ciri asal menjadi beberapa kumpulan ciri yang tidak disatukan. Model individu pada kumpulan ciri ini dipelajari menggunakan regresi logistik dan ramalannya digabungkan menggunakan prinsip Bayes naif untuk menghasilkan anggaran akhir yang kuat. Kami menunjukkan bahawa model kami lebih baik dari segi teori dan empirik. Di samping itu, ketika menerapkannya dalam aplikasi praktikal, penapisan spam e-mel, ia meningkatkan skor AUC yang dinormalisasi pada kadar positif-positif 10% sebanyak 28.8% dan 23.6% dibandingkan dengan Bayes naif dan regresi logistik, ketika menggunakan contoh latihan yang sama. [[EENNDD]] regresi logistik; penapisan spam e-mel; belajar; bayes naif"], [{"string": "Extracting temporal signatures for comprehending systems biology models Systems biology has made massive strides in recent years , with capabilities to model complex systems including cell division , stress response , energy metabolism , and signaling pathways . Concomitant with their improved modeling capabilities , however , such biochemical network models have also become notoriously complex for humans to comprehend . We propose network comprehension as a key problem for the KDD community , where the goal is to create explainable representations of complex biological networks . We formulate this problem as one of extracting temporal signatures from multi-variate time series data , where the signatures are composed of ordinal comparisons between time series components . We show how such signatures can be inferred by formulating the data mining problem as one of feature selection in rank-order space . We propose five new feature selection strategies for rank-order space and assess their selective superiorities . Experimental results on budding yeast cell cycle models demonstrate compelling results comparable to human interpretations of the cell cycle .", "keywords": ["rank-order spaces", "temporal signatures", "systems biology", "feature selection", "biological networks"], "combined": "Extracting temporal signatures for comprehending systems biology models Systems biology has made massive strides in recent years , with capabilities to model complex systems including cell division , stress response , energy metabolism , and signaling pathways . Concomitant with their improved modeling capabilities , however , such biochemical network models have also become notoriously complex for humans to comprehend . We propose network comprehension as a key problem for the KDD community , where the goal is to create explainable representations of complex biological networks . We formulate this problem as one of extracting temporal signatures from multi-variate time series data , where the signatures are composed of ordinal comparisons between time series components . We show how such signatures can be inferred by formulating the data mining problem as one of feature selection in rank-order space . We propose five new feature selection strategies for rank-order space and assess their selective superiorities . Experimental results on budding yeast cell cycle models demonstrate compelling results comparable to human interpretations of the cell cycle . [[EENNDD]] rank-order spaces; temporal signatures; systems biology; feature selection; biological networks"}, "Mengambil tandatangan temporal untuk memahami model sistem biologi Sistem biologi telah membuat kemajuan besar dalam beberapa tahun terakhir, dengan kemampuan untuk memodelkan sistem yang kompleks termasuk pembelahan sel, tindak balas tekanan, metabolisme tenaga, dan jalur isyarat. Seiring dengan peningkatan kemampuan pemodelan mereka, bagaimanapun, model rangkaian biokimia juga menjadi sangat rumit untuk dimengerti oleh manusia. Kami mencadangkan pemahaman rangkaian sebagai masalah utama bagi komuniti KDD, di mana tujuannya adalah untuk membuat gambaran yang jelas mengenai rangkaian biologi yang kompleks. Kami merumuskan masalah ini sebagai salah satu pengekstrakan tanda tangan temporal dari data siri waktu yang bervariasi, di mana tandatangan terdiri dari perbandingan ordinal antara komponen siri masa. Kami menunjukkan bagaimana tandatangan seperti itu dapat disimpulkan dengan merumuskan masalah perlombongan data sebagai salah satu pilihan ciri di ruang pesanan peringkat. Kami mencadangkan lima strategi pemilihan ciri baru untuk ruang pesanan peringkat dan menilai keunggulan selektif mereka. Hasil eksperimen pada model kitaran sel ragi baru menunjukkan hasil yang menarik setanding dengan tafsiran manusia mengenai kitaran sel. [[EENNDD]] ruang pesanan peringkat; tandatangan temporal; biologi sistem; pemilihan ciri; rangkaian biologi"], [{"string": "Customer lifetime value modeling and its use for customer retention planning We present and discuss the important business problem of estimating the effect of retention efforts on the Lifetime Value of a customer in the Telecommunications industry . We discuss the components of this problem , in particular customer value and length of service or tenure modeling , and present a novel segment-based approach , motivated by the segment-level view marketing analysts usually employ . We then describe how we build on this approach to estimate the effects of retention on Lifetime Value . Our solution has been successfully implemented in Amdocs ' Business Insight BI platform , and we illustrate its usefulness in real-world scenarios .", "keywords": ["incentive allocation", "churn modeling", "length of service", "retention campaign", "lifetime value", "number-theoretic computations"], "combined": "Customer lifetime value modeling and its use for customer retention planning We present and discuss the important business problem of estimating the effect of retention efforts on the Lifetime Value of a customer in the Telecommunications industry . We discuss the components of this problem , in particular customer value and length of service or tenure modeling , and present a novel segment-based approach , motivated by the segment-level view marketing analysts usually employ . We then describe how we build on this approach to estimate the effects of retention on Lifetime Value . Our solution has been successfully implemented in Amdocs ' Business Insight BI platform , and we illustrate its usefulness in real-world scenarios . [[EENNDD]] incentive allocation; churn modeling; length of service; retention campaign; lifetime value; number-theoretic computations"}, "Pemodelan nilai seumur hidup pelanggan dan penggunaannya untuk perancangan pengekalan pelanggan Kami menyajikan dan membincangkan masalah perniagaan penting dalam menganggarkan pengaruh usaha pengekalan terhadap Nilai Seumur Hidup pelanggan dalam industri Telekomunikasi. Kami membincangkan komponen masalah ini, khususnya nilai pelanggan dan lama perkhidmatan atau pemodelan tenurial, dan menyajikan pendekatan baru berdasarkan segmen, yang dimotivasi oleh pandangan segmen yang biasanya digunakan oleh analis pemasaran. Kami kemudian menerangkan bagaimana kami menggunakan pendekatan ini untuk menganggar kesan pengekalan terhadap Nilai Seumur Hidup. Penyelesaian kami berjaya dilaksanakan di platform Amdocs 'Business Insight BI, dan kami menggambarkan kegunaannya dalam senario dunia nyata. [[EENNDD]] peruntukan insentif; pemodelan churn; tempoh perkhidmatan; kempen pengekalan; nilai seumur hidup; pengiraan nombor-teori"], [{"string": "Mining IC test data to optimize VLSI testing", "keywords": ["em algorithm", "ic test", "decision support", "vlsi", "real-time control", "unsupervised learning", "belief networks", "decision theory"], "combined": "Mining IC test data to optimize VLSI testing [[EENNDD]] em algorithm; ic test; decision support; vlsi; real-time control; unsupervised learning; belief networks; decision theory"}, "Perlombongan data ujian IC untuk mengoptimumkan algoritma pengujian VLSI [[EENNDD]]; ujian ic; sokongan keputusan; vlsi; kawalan masa nyata; pembelajaran tanpa pengawasan; rangkaian kepercayaan; teori keputusan"], [{"string": "Construct robust rule sets for classification We study the problem of computing classification rule sets from relational databases so that accurate predictions can be made on test data with missing attribute values . Traditional classifiers perform badly when test data are not as complete as the training data because they tailor a training database too much . We introduce the concept of one rule set being more robust than another , that is , able to make more accurate predictions on test data with missing attribute values . We show that the optimal class association rule set is as robust as the complete class association rule set . We then introduce the k-optimal rule set , which provides predictions exactly the same as the optimal class association rule set on test data with up to k missing attribute values . This leads to a hierarchy of k-optimal rule sets in which decreasing size corresponds to decreasing robustness , and they all more robust than a traditional classification rule set . We introduce two methods to find k-optimal rule sets , i.e. an optimal association rule mining approach and a heuristic approximate approach . We show experimentally that a k-optimal rule set generated by the optimal association rule mining approach performs better than that by the heuristic approximate approach and both rule sets perform significantly better than a typical classification rule set C4 .5 Rules on incomplete test data .", "keywords": ["deduction", "classification rule", "association rule"], "combined": "Construct robust rule sets for classification We study the problem of computing classification rule sets from relational databases so that accurate predictions can be made on test data with missing attribute values . Traditional classifiers perform badly when test data are not as complete as the training data because they tailor a training database too much . We introduce the concept of one rule set being more robust than another , that is , able to make more accurate predictions on test data with missing attribute values . We show that the optimal class association rule set is as robust as the complete class association rule set . We then introduce the k-optimal rule set , which provides predictions exactly the same as the optimal class association rule set on test data with up to k missing attribute values . This leads to a hierarchy of k-optimal rule sets in which decreasing size corresponds to decreasing robustness , and they all more robust than a traditional classification rule set . We introduce two methods to find k-optimal rule sets , i.e. an optimal association rule mining approach and a heuristic approximate approach . We show experimentally that a k-optimal rule set generated by the optimal association rule mining approach performs better than that by the heuristic approximate approach and both rule sets perform significantly better than a typical classification rule set C4 .5 Rules on incomplete test data . [[EENNDD]] deduction; classification rule; association rule"}, "Bina set peraturan yang kuat untuk klasifikasi Kami mengkaji masalah pengkomputeran set peraturan klasifikasi dari pangkalan data relasional sehingga ramalan yang tepat dapat dibuat pada data ujian dengan nilai atribut yang hilang. Pengelasan tradisional berprestasi buruk apabila data ujian tidak lengkap dengan data latihan kerana terlalu banyak menyesuaikan pangkalan data latihan. Kami memperkenalkan konsep satu set peraturan yang lebih mantap daripada yang lain, iaitu, dapat membuat ramalan yang lebih tepat pada data ujian dengan nilai atribut yang hilang. Kami menunjukkan bahawa set peraturan persatuan kelas yang optimum sama kuatnya dengan set peraturan persatuan kelas yang lengkap. Kami kemudian memperkenalkan set peraturan k-optimal, yang memberikan ramalan yang sama persis dengan aturan pergaulan kelas optimum yang ditetapkan pada data ujian dengan nilai atribut hingga k yang hilang. Ini membawa kepada hierarki set peraturan k-optimal di mana penurunan ukuran sesuai dengan penurunan kekuatan, dan semuanya lebih kuat daripada set peraturan klasifikasi tradisional. Kami memperkenalkan dua kaedah untuk mencari set peraturan k-optimal, iaitu pendekatan perlombongan peraturan persatuan yang optimum dan pendekatan perkiraan heuristik. Kami menunjukkan secara eksperimen bahawa satu set peraturan k-optimum yang dihasilkan oleh pendekatan perlombongan peraturan pergaulan yang optimum berkinerja lebih baik daripada itu dengan pendekatan perkiraan heuristik dan kedua-dua set peraturan menunjukkan prestasi yang lebih baik daripada peraturan klasifikasi khas yang ditetapkan C4 .5 Peraturan mengenai data ujian yang tidak lengkap. [[EENNDD]] pemotongan; peraturan pengelasan; peraturan persatuan"], [{"string": "Visualizing changes in the structure of data for exploratory feature selection Using visualization techniques to explore and understand high-dimensional data is an efficient way to combine human intelligence with the immense brute force computation power available nowadays . Several visualization techniques have been developed to study the cluster structure of data , i.e. , the existence of distinctive groups in the data and how these clusters are related to each other . However , only few of these techniques lend themselves to studying how this structure changes if the features describing the data are changed . Understanding this relationship between the features and the cluster structure means understanding the features themselves and is thus a useful tool in the feature extraction phase . In this paper we present a novel approach to visualizing how modification of the features with respect to weighting or normalization changes the cluster structure . We demonstrate the application of our approach in two music related data mining projects .", "keywords": ["interactive data mining", "high-dimensional data"], "combined": "Visualizing changes in the structure of data for exploratory feature selection Using visualization techniques to explore and understand high-dimensional data is an efficient way to combine human intelligence with the immense brute force computation power available nowadays . Several visualization techniques have been developed to study the cluster structure of data , i.e. , the existence of distinctive groups in the data and how these clusters are related to each other . However , only few of these techniques lend themselves to studying how this structure changes if the features describing the data are changed . Understanding this relationship between the features and the cluster structure means understanding the features themselves and is thus a useful tool in the feature extraction phase . In this paper we present a novel approach to visualizing how modification of the features with respect to weighting or normalization changes the cluster structure . We demonstrate the application of our approach in two music related data mining projects . [[EENNDD]] interactive data mining; high-dimensional data"}, "Memvisualisasikan perubahan dalam struktur data untuk pemilihan ciri penerokaan Menggunakan teknik visualisasi untuk meneroka dan memahami data dimensi tinggi adalah kaedah yang berkesan untuk menggabungkan kecerdasan manusia dengan kekuatan pengiraan brute force yang sangat besar yang ada sekarang. Beberapa teknik visualisasi telah dikembangkan untuk mengkaji struktur kluster data, iaitu adanya kelompok khas dalam data dan bagaimana kelompok ini saling berkaitan satu sama lain. Walau bagaimanapun, hanya sebilangan kecil teknik ini yang mampu mengkaji bagaimana struktur ini berubah sekiranya ciri-ciri yang menggambarkan data diubah. Memahami hubungan ini antara ciri dan struktur kluster bermaksud memahami ciri itu sendiri dan dengan itu menjadi alat yang berguna dalam fasa pengekstrakan ciri. Dalam makalah ini kami menyajikan pendekatan baru untuk memvisualisasikan bagaimana pengubahsuaian ciri berkenaan dengan pemberat atau normalisasi mengubah struktur kluster. Kami menunjukkan penerapan pendekatan kami dalam dua projek perlombongan data berkaitan muzik. [[EENNDD]] perlombongan data interaktif; data dimensi tinggi"], [{"string": "Mining high dimensional data for classifier knowledge We present in this paper the problem of discovering sets of attribute-value pairs in high dimensional data sets that are of interest not because of co-occurrence alone , but due to their value in serving as cores for potential classifiers of clusters . We present our algorithm in the context of a gene-expression dataset . Gene expression data , in most situations , is insufficient for clustering algorithms and any statistical inference because for 6000 + genes , typically only 10s and at most 100s of data points become available . It is difficult to use statistical techniques to design a classifier for such immensely under-specified data . The observed data , though statistically , insufficient contains some information about the domain . Our goal is to discover as much information about all potential classifiers as possible from the data and then summarize this knowledge . This summarization provides insights into the composition of potential classifiers . We present here algorithms and methods for mining a high dimensional data set , exemplified by a gene expression data set , for mining such information .", "keywords": ["design methodology", "pattern recognition", "high dimensional data"], "combined": "Mining high dimensional data for classifier knowledge We present in this paper the problem of discovering sets of attribute-value pairs in high dimensional data sets that are of interest not because of co-occurrence alone , but due to their value in serving as cores for potential classifiers of clusters . We present our algorithm in the context of a gene-expression dataset . Gene expression data , in most situations , is insufficient for clustering algorithms and any statistical inference because for 6000 + genes , typically only 10s and at most 100s of data points become available . It is difficult to use statistical techniques to design a classifier for such immensely under-specified data . The observed data , though statistically , insufficient contains some information about the domain . Our goal is to discover as much information about all potential classifiers as possible from the data and then summarize this knowledge . This summarization provides insights into the composition of potential classifiers . We present here algorithms and methods for mining a high dimensional data set , exemplified by a gene expression data set , for mining such information . [[EENNDD]] design methodology; pattern recognition; high dimensional data"}, "Perlombongan data dimensi tinggi untuk pengetahuan pengelasan Kami mengemukakan dalam makalah ini masalah menemui set pasangan atribut-nilai dalam set data dimensi tinggi yang menarik bukan kerana kejadian bersama sahaja, tetapi kerana nilai mereka berfungsi sebagai teras potensi pengelasan kluster. Kami memaparkan algoritma kami dalam konteks kumpulan data ekspresi gen. Data ekspresi gen, dalam kebanyakan situasi, tidak mencukupi untuk algoritma pengelompokan dan sebarang inferensi statistik kerana untuk 6000 + gen, biasanya hanya 10-an dan paling banyak 100-an titik data tersedia. Sukar untuk menggunakan teknik statistik untuk merancang pengkelasan bagi data yang sangat kurang ditentukan. Data yang diperhatikan, walaupun secara statistik, tidak mencukupi mengandungi beberapa maklumat mengenai domain tersebut. Matlamat kami adalah untuk menemui sebanyak mungkin maklumat mengenai semua pengklasifikasi yang berpotensi dari data dan kemudian meringkaskan pengetahuan ini. Ringkasan ini memberikan pandangan mengenai komposisi pengklasifikasi yang berpotensi. Kami menunjukkan di sini algoritma dan kaedah untuk melombong set data dimensi tinggi, yang dicontohkan oleh kumpulan data ekspresi gen, untuk melombong maklumat tersebut. [[EENNDD]] metodologi reka bentuk; pengecaman corak; data dimensi tinggi"], [{"string": "Knowledge base maintenance using knowledge gap analysis As the web and e-business have proliferated , the practice of using customer facing knowledge bases to augment customer service and support operations has increased . This can be a very efficient , scalable and cost effective way to share knowledge . The effectiveness and cost savings are proportional to the utility of the information within the knowledge base and inversely proportional to the amount of labor required in maintaining the knowledge . To address this issue , we have developed an algorithm and methodology to increase the utility of the information within a knowledge base while greatly reducing the labor required . In this paper , we describe an implementation of an algorithm and methodology for comparing a knowledge base to a set of problem tickets to determine which categories and subcategories are not well addressed within the knowledge base . We utilize text clustering on problem ticket text to determine a set of problem categories . We then compare each knowledge base solution document to each problem category centroid using a cosine distance metric . The distance between the `` closest '' solution document and the corresponding centroid becomes the basis of that problem category 's `` knowledge gap '' . Our claim is that this gap metric serves as a useful method for quickly and automatically determining which problem categories have no relevant solutions in a knowledge base . We have implemented our approach , and we present the results of performing a knowledge gap analysis on a set of support center problem tickets .", "keywords": ["text mining", "information search and retrieval", "knowledge management", "database applications", "gap analysis", "clustering"], "combined": "Knowledge base maintenance using knowledge gap analysis As the web and e-business have proliferated , the practice of using customer facing knowledge bases to augment customer service and support operations has increased . This can be a very efficient , scalable and cost effective way to share knowledge . The effectiveness and cost savings are proportional to the utility of the information within the knowledge base and inversely proportional to the amount of labor required in maintaining the knowledge . To address this issue , we have developed an algorithm and methodology to increase the utility of the information within a knowledge base while greatly reducing the labor required . In this paper , we describe an implementation of an algorithm and methodology for comparing a knowledge base to a set of problem tickets to determine which categories and subcategories are not well addressed within the knowledge base . We utilize text clustering on problem ticket text to determine a set of problem categories . We then compare each knowledge base solution document to each problem category centroid using a cosine distance metric . The distance between the `` closest '' solution document and the corresponding centroid becomes the basis of that problem category 's `` knowledge gap '' . Our claim is that this gap metric serves as a useful method for quickly and automatically determining which problem categories have no relevant solutions in a knowledge base . We have implemented our approach , and we present the results of performing a knowledge gap analysis on a set of support center problem tickets . [[EENNDD]] text mining; information search and retrieval; knowledge management; database applications; gap analysis; clustering"}, "Penyelenggaraan asas pengetahuan menggunakan analisis jurang pengetahuan Seiring berkembangnya web dan e-perniagaan, amalan menggunakan asas pengetahuan yang dihadapi pelanggan untuk meningkatkan perkhidmatan pelanggan dan operasi sokongan telah meningkat. Ini boleh menjadi kaedah yang sangat berkesan, berskala dan menjimatkan untuk berkongsi pengetahuan. Keberkesanan dan penjimatan kos adalah sebanding dengan utiliti maklumat dalam pangkalan pengetahuan dan berbanding terbalik dengan jumlah tenaga kerja yang diperlukan dalam mengekalkan pengetahuan. Untuk mengatasi masalah ini, kami telah mengembangkan algoritma dan metodologi untuk meningkatkan utiliti maklumat dalam pangkalan pengetahuan sambil mengurangkan tenaga kerja yang diperlukan. Dalam makalah ini, kami menjelaskan pelaksanaan algoritma dan metodologi untuk membandingkan asas pengetahuan dengan sekumpulan tiket bermasalah untuk menentukan kategori dan subkategori mana yang tidak ditangani dengan baik dalam pangkalan pengetahuan. Kami menggunakan pengelompokan teks pada teks tiket bermasalah untuk menentukan sekumpulan kategori masalah. Kami kemudian membandingkan setiap dokumen penyelesaian asas pengetahuan dengan setiap centroid kategori masalah menggunakan metrik jarak kosinus. Jarak antara dokumen penyelesaian \"terdekat\" dan centroid yang sesuai menjadi asas \"jurang pengetahuan\" kategori masalah itu. Tuntutan kami adalah bahawa metrik jurang ini berfungsi sebagai kaedah yang berguna untuk menentukan dengan cepat dan automatik kategori masalah mana yang tidak mempunyai penyelesaian yang relevan dalam pangkalan pengetahuan. Kami telah melaksanakan pendekatan kami, dan kami membentangkan hasil melakukan analisis jurang pengetahuan pada satu set tiket masalah pusat sokongan. [[EENNDD]] perlombongan teks; carian dan pengambilan maklumat; pengurusan pengetahuan; aplikasi pangkalan data; analisis jurang; pengelompokan"], [{"string": "Generalizing the notion of support The goal of this paper is to show that generalizing the notion of support can be useful in extending association analysis to non-traditional types of patterns and non-binary data . To that end , we describe a framework for generalizing support that is based on the simple , but useful observation that support can be viewed as the composition of two functions : a function that evaluates the strength or presence of a pattern in each object transaction and a function that summarizes these evaluations with a single number . A key goal of any framework is to allow people to more easily express , explore , and communicate ideas , and hence , we illustrate how our support framework can be used to describe support for a variety of commonly used association patterns , such as frequent itemsets , general Boolean patterns , and error-tolerant itemsets . We also present two examples of the practical usefulness of generalized support . One example shows the usefulness of support functions for continuous data . Another example shows how the hyperclique pattern -- an association pattern originally defined for binary data -- can be extended to continuous data by generalizing a support function .", "keywords": ["association analysis", "support", "hyperclique"], "combined": "Generalizing the notion of support The goal of this paper is to show that generalizing the notion of support can be useful in extending association analysis to non-traditional types of patterns and non-binary data . To that end , we describe a framework for generalizing support that is based on the simple , but useful observation that support can be viewed as the composition of two functions : a function that evaluates the strength or presence of a pattern in each object transaction and a function that summarizes these evaluations with a single number . A key goal of any framework is to allow people to more easily express , explore , and communicate ideas , and hence , we illustrate how our support framework can be used to describe support for a variety of commonly used association patterns , such as frequent itemsets , general Boolean patterns , and error-tolerant itemsets . We also present two examples of the practical usefulness of generalized support . One example shows the usefulness of support functions for continuous data . Another example shows how the hyperclique pattern -- an association pattern originally defined for binary data -- can be extended to continuous data by generalizing a support function . [[EENNDD]] association analysis; support; hyperclique"}, "Menyamaratakan gagasan sokongan Matlamat makalah ini adalah untuk menunjukkan bahawa menggeneralisasikan gagasan sokongan boleh berguna dalam memperluas analisis perkaitan ke jenis corak dan data bukan perduaan bukan tradisional. Untuk itu, kami menerangkan kerangka untuk menggeneralisasikan sokongan yang berdasarkan pada pemerhatian yang sederhana namun berguna bahawa sokongan dapat dilihat sebagai komposisi dua fungsi: fungsi yang menilai kekuatan atau keberadaan pola dalam setiap transaksi objek dan fungsi yang merangkum penilaian ini dengan satu nombor. Matlamat utama kerangka kerja apa pun adalah untuk membolehkan orang mengekspresikan, meneroka, dan menyampaikan idea dengan lebih mudah, dan oleh itu, kami menggambarkan bagaimana kerangka sokongan kami dapat digunakan untuk menggambarkan sokongan untuk pelbagai corak pergaulan yang biasa digunakan, seperti kumpulan barang yang kerap, corak Boolean umum, dan set item yang bertolak ansur. Kami juga mengemukakan dua contoh kegunaan praktikal sokongan umum. Satu contoh menunjukkan kegunaan fungsi sokongan untuk data berterusan. Contoh lain menunjukkan bagaimana pola hiperklik - pola persatuan yang awalnya ditakrifkan untuk data binari - dapat diperluas ke data berterusan dengan menggeneralisasi fungsi sokongan. [[EENNDD]] analisis persatuan; sokongan; hiperklik"], [{"string": "Mining top-k frequent items in a data stream with flexible sliding windows We study the problem of finding the k most frequent items in a stream of items for the recently proposed max-frequency measure . Based on the properties of an item , the max-frequency of an item is counted over a sliding window of which the length changes dynamically . Besides being parameterless , this way of measuring the support of items was shown to have the advantage of a faster detection of bursts in a stream , especially if the set of items is heterogeneous . The algorithm that was proposed for maintaining all frequent items , however , scales poorly when the number of items becomes large . Therefore , in this paper we propose , instead of reporting all frequent items , to only mine the top-k most frequent ones . First we prove that in order to solve this problem exactly , we still need a prohibitive amount of memory at least linear in the number of items . Yet , under some reasonable conditions , we show both theoretically and empirically that a memory-efficient algorithm exists . A prototype of this algorithm is implemented and we present its performance w.r.t. memory-efficiency on real-life data and in controlled experiments with synthetic data .", "keywords": ["data stream mining", "top-k frequent items"], "combined": "Mining top-k frequent items in a data stream with flexible sliding windows We study the problem of finding the k most frequent items in a stream of items for the recently proposed max-frequency measure . Based on the properties of an item , the max-frequency of an item is counted over a sliding window of which the length changes dynamically . Besides being parameterless , this way of measuring the support of items was shown to have the advantage of a faster detection of bursts in a stream , especially if the set of items is heterogeneous . The algorithm that was proposed for maintaining all frequent items , however , scales poorly when the number of items becomes large . Therefore , in this paper we propose , instead of reporting all frequent items , to only mine the top-k most frequent ones . First we prove that in order to solve this problem exactly , we still need a prohibitive amount of memory at least linear in the number of items . Yet , under some reasonable conditions , we show both theoretically and empirically that a memory-efficient algorithm exists . A prototype of this algorithm is implemented and we present its performance w.r.t. memory-efficiency on real-life data and in controlled experiments with synthetic data . [[EENNDD]] data stream mining; top-k frequent items"}, "Melombong item paling kerap k dalam aliran data dengan tingkap gelangsar yang fleksibel Kami mengkaji masalah mencari item k paling kerap dalam aliran item untuk ukuran frekuensi maksimum yang baru-baru ini dicadangkan. Berdasarkan sifat item, frekuensi maksimum item dikira melalui tetingkap gelongsor yang panjangnya berubah secara dinamik. Selain tanpa parameter, cara mengukur sokongan item terbukti memiliki kelebihan pengesanan ledakan yang lebih cepat dalam aliran, terutama jika set item itu heterogen. Algoritma yang dicadangkan untuk mengekalkan semua item yang kerap, bagaimanapun, skala kurang baik apabila jumlah item menjadi besar. Oleh itu, dalam makalah ini kami mencadangkan, dan bukannya melaporkan semua item yang kerap, hanya melombong barang-barang teratas yang paling kerap. Mula-mula kita membuktikan bahawa untuk menyelesaikan masalah ini dengan tepat, kita masih memerlukan jumlah memori yang melarang sekurang-kurangnya linear dalam jumlah item. Namun, dalam beberapa keadaan yang munasabah, kami menunjukkan secara teori dan empirik bahawa algoritma cekap memori wujud. Prototaip algoritma ini dilaksanakan dan kami menunjukkan prestasinya w.r.t. kecekapan memori pada data kehidupan sebenar dan dalam eksperimen terkawal dengan data sintetik. [[EENNDD]] perlombongan aliran data; barang kerap top-k"], [{"string": "Improving predictions using aggregate information In domains such as consumer products or manufacturing amongst others , we have problems that warrant the prediction of a continuous target . Besides the usual set of explanatory attributes we may also have exact or approximate estimates of aggregated targets , which are the sums of disjoint sets of individual targets that we are trying to predict . Hence , the question now becomes can we use these aggregated targets , which are a coarser piece of information , to improve the quality of predictions of the individual targets ? In this paper , we provide a simple yet provable way of accomplishing this . In particular , given predictions from any regression model of the target on the test data , we elucidate a provable method for improving these predictions in terms of mean squared error , given exact or accurate enough information of the aggregated targets . These estimates of the aggregated targets may be readily available or obtained -- through multilevel regression -- at different levels of granularity . Based on the proof of our method we suggest a criterion for choosing the appropriate level . Moreover , in addition to estimates of the aggregated targets , if we have exact or approximate estimates of the mean and variance of the target distribution , then based on our general strategy we provide an optimal way of incorporating this information so as to further improve the quality of predictions of the individual targets . We then validate the results and our claims by conducting experiments on synthetic and real industrial data obtained from diverse domains .", "keywords": ["regression", "general", "hierarchical", "coarse to fine"], "combined": "Improving predictions using aggregate information In domains such as consumer products or manufacturing amongst others , we have problems that warrant the prediction of a continuous target . Besides the usual set of explanatory attributes we may also have exact or approximate estimates of aggregated targets , which are the sums of disjoint sets of individual targets that we are trying to predict . Hence , the question now becomes can we use these aggregated targets , which are a coarser piece of information , to improve the quality of predictions of the individual targets ? In this paper , we provide a simple yet provable way of accomplishing this . In particular , given predictions from any regression model of the target on the test data , we elucidate a provable method for improving these predictions in terms of mean squared error , given exact or accurate enough information of the aggregated targets . These estimates of the aggregated targets may be readily available or obtained -- through multilevel regression -- at different levels of granularity . Based on the proof of our method we suggest a criterion for choosing the appropriate level . Moreover , in addition to estimates of the aggregated targets , if we have exact or approximate estimates of the mean and variance of the target distribution , then based on our general strategy we provide an optimal way of incorporating this information so as to further improve the quality of predictions of the individual targets . We then validate the results and our claims by conducting experiments on synthetic and real industrial data obtained from diverse domains . [[EENNDD]] regression; general; hierarchical; coarse to fine"}, "Memperbaiki ramalan menggunakan maklumat agregat Dalam domain seperti produk pengguna atau pembuatan antara lain, kita mempunyai masalah yang memerlukan ramalan sasaran berterusan. Selain set atribut penjelasan yang biasa, kami mungkin juga mempunyai anggaran tepat atau anggaran sasaran agregat, yang merupakan jumlah set taksiran bagi setiap sasaran yang ingin kami ramalkan. Oleh itu, persoalannya sekarang dapatkah kita menggunakan sasaran gabungan ini, yang merupakan maklumat yang lebih kasar, untuk meningkatkan kualiti ramalan sasaran individu? Dalam makalah ini, kami menyediakan cara yang mudah namun dapat dibuktikan untuk mencapai ini. Khususnya, berdasarkan ramalan dari model regresi sasaran pada data ujian, kami menjelaskan kaedah yang dapat dibuktikan untuk memperbaiki ramalan ini dari segi ralat kuadrat rata, diberikan maklumat yang cukup tepat atau tepat mengenai agregat sasaran. Anggaran sasaran agregat ini mungkin tersedia atau diperoleh - melalui regresi bertingkat - pada tahap butiran yang berbeza. Berdasarkan bukti kaedah kami, kami mencadangkan kriteria untuk memilih tahap yang sesuai. Lebih-lebih lagi, sebagai tambahan kepada anggaran sasaran yang digabungkan, jika kita mempunyai anggaran yang tepat atau tepat mengenai min dan varians pengagihan sasaran, maka berdasarkan strategi umum kami, kami menyediakan cara yang optimum untuk memasukkan maklumat ini sehingga dapat meningkatkan lagi kualiti ramalan sasaran individu. Kami kemudian mengesahkan hasil dan tuntutan kami dengan melakukan eksperimen pada data industri sintetik dan sebenar yang diperoleh dari pelbagai domain. [[EENNDD]] regresi; umum; hierarki; kasar hingga halus"], [{"string": "Domain-constrained semi-supervised mining of tracking models in sensor networks Accurate localization of mobile objects is a major research problem in sensor networks and an important data mining application . Specifically , the localization problem is to determine the location of a client device accurately given the radio signal strength values received at the client device from multiple beacon sensors or access points . Conventional data mining and machine learning methods can be applied to solve this problem . However , all of them require large amounts of labeled training data , which can be quite expensive . In this paper , we propose a probabilistic semi supervised learning approach to reduce the calibration effort and increase the tracking accuracy . Our method is based on semi-supervised conditional random fields which can enhance the learned model from a small set of training data with abundant unlabeled data effectively . To make our method more efficient , we exploit a Generalized EM algorithm coupled with domain constraints . We validate our method through extensive experiments in a real sensor network using Crossbow MICA2 sensors . The results demonstrate the advantages of methods compared to other state-of-the-art object-tracking algorithms .", "keywords": ["crf", "localization", "learning", "sensor networks", "em", "tracking", "calibration"], "combined": "Domain-constrained semi-supervised mining of tracking models in sensor networks Accurate localization of mobile objects is a major research problem in sensor networks and an important data mining application . Specifically , the localization problem is to determine the location of a client device accurately given the radio signal strength values received at the client device from multiple beacon sensors or access points . Conventional data mining and machine learning methods can be applied to solve this problem . However , all of them require large amounts of labeled training data , which can be quite expensive . In this paper , we propose a probabilistic semi supervised learning approach to reduce the calibration effort and increase the tracking accuracy . Our method is based on semi-supervised conditional random fields which can enhance the learned model from a small set of training data with abundant unlabeled data effectively . To make our method more efficient , we exploit a Generalized EM algorithm coupled with domain constraints . We validate our method through extensive experiments in a real sensor network using Crossbow MICA2 sensors . The results demonstrate the advantages of methods compared to other state-of-the-art object-tracking algorithms . [[EENNDD]] crf; localization; learning; sensor networks; em; tracking; calibration"}, "Perlombongan separa diawasi oleh domain pelacakan di rangkaian sensor Penyetempatan objek mudah alih yang tepat adalah masalah penyelidikan utama dalam rangkaian sensor dan aplikasi perlombongan data yang penting. Secara khusus, masalah penyetempatan adalah untuk menentukan lokasi peranti klien dengan tepat mengingat nilai kekuatan isyarat radio yang diterima pada peranti klien dari beberapa sensor suar atau titik akses. Kaedah perlombongan data konvensional dan pembelajaran mesin dapat diaplikasikan untuk menyelesaikan masalah ini. Walau bagaimanapun, semuanya memerlukan sejumlah besar data latihan berlabel, yang mungkin agak mahal. Dalam makalah ini, kami mencadangkan pendekatan pembelajaran semi-diawasi probabilistik untuk mengurangkan usaha penentukuran dan meningkatkan ketepatan penjejakan. Kaedah kami didasarkan pada bidang rawak bersyarat separa penyeliaan yang dapat meningkatkan model yang dipelajari dari sekumpulan kecil data latihan dengan data tidak berlabel yang banyak dengan berkesan. Untuk menjadikan kaedah kami lebih cekap, kami mengeksploitasi algoritma EM Umum digabungkan dengan batasan domain. Kami mengesahkan kaedah kami melalui eksperimen yang luas dalam rangkaian sensor sebenar menggunakan sensor Crossbow MICA2. Hasilnya menunjukkan kelebihan kaedah berbanding dengan algoritma penjejakan objek canggih yang lain. [[EENNDD]] crf; penyetempatan; belajar; rangkaian sensor; em; Penjejakan; penentukuran"], [{"string": "Fragments of order High-dimensional collections of 0 -- 1 data occur in many applications . The attributes in such data sets are typically considered to be unordered . However , in many cases there is a natural total or partial order \u227a underlying the variables of the data set . Examples of variables for which such orders exist include terms in documents , courses in enrollment data , and paleontological sites in fossil data collections . The observations in such applications are flat , unordered sets ; however , the data sets respect the underlying ordering of the variables . By this we mean that if A \u227a B \u227a C are three variables respecting the underlying ordering \u227a , and both of variables A and C appear in an observation , then , up to noise levels , variable B also appears in this observation . Similarly , if A1 \u227a A2 \u227a ... \u227a Al-1 \u227a Ai is a longer sequence of variables , we do not expect to see many observations for which there are indices i j k such that Ai and Ak occur in the observation but Aj does not . In this paper we study the problem of discovering fragments of orders of variables implicit in collections of unordered observations . We define measures that capture how well a given order agrees with the observed data . We describe a simple and efficient algorithm for finding all the fragments that satisfy certain conditions . We also discuss the sometimes necessary postprocessing for selecting only the best fragments of order . Also , we relate our method with a sequencing approach that uses a spectral algorithm , and with the consecutive ones problem . We present experimental results on some real data sets author lists of database papers , exam results data , and paleontological data .", "keywords": ["discovering hidden orderings", "consecutive ones property", "nonnumerical algorithms and problems", "novel data mining algorithms", "spectral analysis of data"], "combined": "Fragments of order High-dimensional collections of 0 -- 1 data occur in many applications . The attributes in such data sets are typically considered to be unordered . However , in many cases there is a natural total or partial order \u227a underlying the variables of the data set . Examples of variables for which such orders exist include terms in documents , courses in enrollment data , and paleontological sites in fossil data collections . The observations in such applications are flat , unordered sets ; however , the data sets respect the underlying ordering of the variables . By this we mean that if A \u227a B \u227a C are three variables respecting the underlying ordering \u227a , and both of variables A and C appear in an observation , then , up to noise levels , variable B also appears in this observation . Similarly , if A1 \u227a A2 \u227a ... \u227a Al-1 \u227a Ai is a longer sequence of variables , we do not expect to see many observations for which there are indices i j k such that Ai and Ak occur in the observation but Aj does not . In this paper we study the problem of discovering fragments of orders of variables implicit in collections of unordered observations . We define measures that capture how well a given order agrees with the observed data . We describe a simple and efficient algorithm for finding all the fragments that satisfy certain conditions . We also discuss the sometimes necessary postprocessing for selecting only the best fragments of order . Also , we relate our method with a sequencing approach that uses a spectral algorithm , and with the consecutive ones problem . We present experimental results on some real data sets author lists of database papers , exam results data , and paleontological data . [[EENNDD]] discovering hidden orderings; consecutive ones property; nonnumerical algorithms and problems; novel data mining algorithms; spectral analysis of data"}, "Fragmen pesanan Koleksi dimensi tinggi data 0 - 1 berlaku dalam banyak aplikasi. Atribut dalam set data tersebut biasanya dianggap tidak tersusun. Walau bagaimanapun, dalam banyak kes terdapat urutan total atau separa semula jadi \u227a yang mendasari pemboleh ubah set data. Contoh pemboleh ubah yang pesanannya ada termasuk istilah dalam dokumen, kursus dalam data pendaftaran, dan laman paleontologi dalam pengumpulan data fosil. Pemerhatian dalam aplikasi tersebut adalah set yang rata dan tidak tersusun; namun, set data berkenaan dengan susunan yang mendasari pemboleh ubah. Dengan ini kita bermaksud bahawa jika A \u227a B \u227a C adalah tiga pemboleh ubah yang menghormati susunan yang mendasari \u227a, dan kedua-dua pemboleh ubah A dan C muncul dalam pemerhatian, maka, hingga tahap kebisingan, pemboleh ubah B juga muncul dalam pemerhatian ini. Begitu juga, jika A1 \u227a A2 \u227a ... \u227a Al-1 \u227a Ai adalah urutan pemboleh ubah yang lebih panjang, kami tidak menjangka dapat melihat banyak pemerhatian yang mana terdapat indeks ijk sehingga Ai dan Ak berlaku dalam pemerhatian tetapi Aj tidak . Dalam makalah ini kita mengkaji masalah menemui serpihan pesanan pemboleh ubah yang tersirat dalam koleksi pemerhatian yang tidak tersusun. Kami menentukan langkah-langkah yang menunjukkan seberapa baik pesanan yang diberikan sesuai dengan data yang diperhatikan. Kami menerangkan algoritma yang mudah dan berkesan untuk mencari semua serpihan yang memenuhi syarat tertentu. Kami juga membincangkan proses pasca pemprosesan yang kadang-kadang diperlukan untuk memilih bahagian pesanan yang terbaik sahaja. Juga, kami mengaitkan kaedah kami dengan pendekatan penjujukan yang menggunakan algoritma spektrum, dan dengan masalah yang berturut-turut. Kami membentangkan hasil eksperimen pada beberapa kumpulan data nyata senarai penulis kertas pangkalan data, data hasil peperiksaan, dan data paleontologi. [[EENNDD]] menemui pesanan tersembunyi; harta yang berturut-turut; algoritma dan masalah bukan berangka; algoritma perlombongan data novel; analisis data spektrum"], [{"string": "Density-based clustering for real-time stream data Existing data-stream clustering algorithms such as CluStream arebased on k-means . These clustering algorithms are incompetent tofind clusters of arbitrary shapes and can not handle outliers . Further , they require the knowledge of k and user-specified time window . To address these issues , this paper proposes D-Stream , a framework for clustering stream data using adensity-based approach . The algorithm uses an online component which maps each input data record into a grid and an offline component which computes the grid density and clusters the grids based on the density . The algorithm adopts a density decaying technique to capture the dynamic changes of a data stream . Exploiting the intricate relationships between the decay factor , data density and cluster structure , our algorithm can efficiently and effectively generate and adjust the clusters in real time . Further , a theoretically sound technique is developed to detect and remove sporadic grids mapped to by outliers in order to dramatically improve the space and time efficiency of the system . The technique makes high-speed data stream clustering feasible without degrading the clustering quality . The experimental results show that our algorithm has superior quality and efficiency , can find clusters of arbitrary shapes , and can accurately recognize the evolving behaviors of real-time data streams .", "keywords": ["sporadic grids", "d-stream", "density-based clustering", "stream data mining"], "combined": "Density-based clustering for real-time stream data Existing data-stream clustering algorithms such as CluStream arebased on k-means . These clustering algorithms are incompetent tofind clusters of arbitrary shapes and can not handle outliers . Further , they require the knowledge of k and user-specified time window . To address these issues , this paper proposes D-Stream , a framework for clustering stream data using adensity-based approach . The algorithm uses an online component which maps each input data record into a grid and an offline component which computes the grid density and clusters the grids based on the density . The algorithm adopts a density decaying technique to capture the dynamic changes of a data stream . Exploiting the intricate relationships between the decay factor , data density and cluster structure , our algorithm can efficiently and effectively generate and adjust the clusters in real time . Further , a theoretically sound technique is developed to detect and remove sporadic grids mapped to by outliers in order to dramatically improve the space and time efficiency of the system . The technique makes high-speed data stream clustering feasible without degrading the clustering quality . The experimental results show that our algorithm has superior quality and efficiency , can find clusters of arbitrary shapes , and can accurately recognize the evolving behaviors of real-time data streams . [[EENNDD]] sporadic grids; d-stream; density-based clustering; stream data mining"}, "Pengelompokan berdasarkan kepadatan untuk data aliran masa nyata Algoritma pengelompokan aliran data yang ada seperti CluStream berdasarkan k-cara. Algoritma pengelompokan ini tidak cekap untuk mencari kumpulan bentuk sewenang-wenangnya dan tidak dapat menangani garis besar. Selanjutnya, mereka memerlukan pengetahuan tentang k dan tetingkap waktu yang ditentukan pengguna. Untuk mengatasi masalah ini, makalah ini mengusulkan D-Stream, kerangka kerja untuk mengumpulkan data aliran menggunakan pendekatan berdasarkan adensity. Algoritma menggunakan komponen dalam talian yang memetakan setiap catatan data input ke dalam grid dan komponen luar talian yang mengira kepadatan grid dan mengumpulkan grid berdasarkan kepadatan. Algoritma menggunakan teknik pereputan kepadatan untuk menangkap perubahan dinamik aliran data. Mengeksploitasi hubungan yang rumit antara faktor pereputan, ketumpatan data dan struktur kluster, algoritma kami dapat menjana dan menyesuaikan kluster secara berkesan dan berkesan dalam masa nyata. Selanjutnya, teknik suara secara teori dikembangkan untuk mengesan dan menghilangkan grid sporadis yang dipetakan oleh outliers untuk meningkatkan ruang dan masa kecekapan sistem secara dramatik. Teknik ini menjadikan pengelompokan aliran data berkelajuan tinggi dapat dilaksanakan tanpa menurunkan kualiti pengelompokan. Hasil eksperimen menunjukkan bahawa algoritma kami mempunyai kualiti dan kecekapan yang unggul, dapat mencari kumpulan bentuk sewenang-wenangnya, dan dapat mengenali tingkah laku aliran data masa nyata dengan tepat. [[EENNDD]] grid sporadis; d-aliran; pengelompokan berdasarkan ketumpatan; melombong data aliran"], [{"string": "A general probabilistic framework for clustering individuals and objects", "keywords": ["em algorithm", "mixture models"], "combined": "A general probabilistic framework for clustering individuals and objects [[EENNDD]] em algorithm; mixture models"}, "Kerangka probabilistik umum untuk mengumpulkan algoritma dan objek [[EENNDD]] em; model campuran"], [{"string": "The IOC algorithm : efficient many-class non-parametric classification for high-dimensional data This paper is about a variant of k nearest neighbor classification on large many-class high dimensional datasets . K nearest neighbor remains a popular classification technique , especially in areas such as computer vision , drug activity prediction and astrophysics . Furthermore , many more modern classifiers , such as kernel-based Bayes classifiers or the prediction phase of SVMs , require computational regimes similar to k-NN . We believe that tractable k-NN algorithms therefore continue to be important . This paper relies on the insight that even with many classes , the task of finding the majority class among the k nearest neighbors of a query need not require us to explicitly find those k nearest neighbors . This insight was previously used in Liu et al. , 2003 in two algorithms called KNS2 and KNS3 which dealt with fast classification in the case of two classes . In this paper we show how a different approach , IOC standing for the International Olympic Committee can apply to the case of n classes where n 2 . IOC assumes a slightly different processing of the datapoints in the neighborhood of the query . This allows it to search a set of metric trees , one for each class . During the searches it is possible to quickly prune away classes that can not possibly be the majority . We give experimental results on datasets of up to 5.8 x 105 records and 1.5 x 103 attributes , frequently showing an order of magnitude acceleration compared with each of i conventional linear scan , ii a well-known independent SR-tree implementation of conventional k-NN and iii a highly optimized conventional k-NN metric tree search .", "keywords": ["high dimension", "classification", "learning", "k nearest neighbor", "metric tree"], "combined": "The IOC algorithm : efficient many-class non-parametric classification for high-dimensional data This paper is about a variant of k nearest neighbor classification on large many-class high dimensional datasets . K nearest neighbor remains a popular classification technique , especially in areas such as computer vision , drug activity prediction and astrophysics . Furthermore , many more modern classifiers , such as kernel-based Bayes classifiers or the prediction phase of SVMs , require computational regimes similar to k-NN . We believe that tractable k-NN algorithms therefore continue to be important . This paper relies on the insight that even with many classes , the task of finding the majority class among the k nearest neighbors of a query need not require us to explicitly find those k nearest neighbors . This insight was previously used in Liu et al. , 2003 in two algorithms called KNS2 and KNS3 which dealt with fast classification in the case of two classes . In this paper we show how a different approach , IOC standing for the International Olympic Committee can apply to the case of n classes where n 2 . IOC assumes a slightly different processing of the datapoints in the neighborhood of the query . This allows it to search a set of metric trees , one for each class . During the searches it is possible to quickly prune away classes that can not possibly be the majority . We give experimental results on datasets of up to 5.8 x 105 records and 1.5 x 103 attributes , frequently showing an order of magnitude acceleration compared with each of i conventional linear scan , ii a well-known independent SR-tree implementation of conventional k-NN and iii a highly optimized conventional k-NN metric tree search . [[EENNDD]] high dimension; classification; learning; k nearest neighbor; metric tree"}, "Algoritma IOC: pengkelasan non-parametrik kelas banyak yang cekap untuk data dimensi tinggi Kertas ini adalah mengenai varian klasifikasi jiran terdekat pada set data dimensi tinggi kelas banyak yang besar. Jiran terdekat adalah teknik klasifikasi yang popular, terutamanya dalam bidang seperti penglihatan komputer, ramalan aktiviti dadah dan astrofizik. Tambahan pula, banyak pengklasifikasi moden, seperti pengkelasan Bayes berasaskan kernel atau fasa ramalan SVM, memerlukan rejim komputasi yang serupa dengan k-NN. Oleh itu, kami percaya bahawa algoritma k-NN yang dapat disusun terus menjadi penting. Makalah ini bergantung pada pandangan bahawa walaupun dengan banyak kelas, tugas untuk mencari kelas majoriti di antara jiran terdekat yang berdekatan dengan pertanyaan tidak memerlukan kita untuk mencari jiran terdekat tersebut secara eksplisit. Wawasan ini sebelum ini digunakan dalam Liu et al. , 2003 dalam dua algoritma yang disebut KNS2 dan KNS3 yang menangani klasifikasi pantas dalam kes dua kelas. Dalam makalah ini kami menunjukkan bagaimana pendekatan yang berbeza, IOC yang mewakili Jawatankuasa Olimpik Antarabangsa dapat berlaku untuk kelas n di mana n 2. IOC menganggap pemprosesan titik data yang sedikit berbeza di kawasan sekitar pertanyaan. Ini membolehkannya mencari sekumpulan pokok metrik, satu untuk setiap kelas. Semasa carian adalah mungkin untuk menjauhkan kelas dengan cepat yang tidak mungkin menjadi majoriti. Kami memberikan hasil eksperimen pada set data hingga 5,8 x 105 rekod dan atribut 1,5 x 103, sering menunjukkan urutan pecutan magnitud berbanding dengan setiap imbasan linier konvensional, ii implementasi SR-pohon bebas terkenal k-NN konvensional dan iii carian pokok metrik k-NN konvensional yang sangat dioptimumkan. [[EENNDD]] dimensi tinggi; pengelasan; belajar; k jiran terdekat; pokok metrik"], [{"string": "Efficient influence maximization in social networks Influence maximization is the problem of finding a small subset of nodes seed nodes in a social network that could maximize the spread of influence . In this paper , we study the efficient influence maximization from two complementary directions . One is to improve the original greedy algorithm of 5 and its improvement 7 to further reduce its running time , and the second is to propose new degree discount heuristics that improves influence spread . We evaluate our algorithms by experiments on two large academic collaboration graphs obtained from the online archival database arXiv.org . Our experimental results show that a our improved greedy algorithm achieves better running time comparing with the improvement of 7 with matching influence spread , b our degree discount heuristics achieve much better influence spread than classic degree and centrality-based heuristics , and when tuned for a specific influence cascade model , it achieves almost matching influence thread with the greedy algorithm , and more importantly c the degree discount heuristics run only in milliseconds while even the improved greedy algorithms run in hours in our experiment graphs with a few tens of thousands of nodes . Based on our results , we believe that fine-tuned heuristics may provide truly scalable solutions to the influence maximization problem with satisfying influence spread and blazingly fast running time . Therefore , contrary to what implied by the conclusion of 5 that traditional heuristics are outperformed by the greedy approximation algorithm , our results shed new lights on the research of heuristic algorithms .", "keywords": ["influence maximization", "heuristic algorithms", "social networks", "nonnumerical algorithms and problems"], "combined": "Efficient influence maximization in social networks Influence maximization is the problem of finding a small subset of nodes seed nodes in a social network that could maximize the spread of influence . In this paper , we study the efficient influence maximization from two complementary directions . One is to improve the original greedy algorithm of 5 and its improvement 7 to further reduce its running time , and the second is to propose new degree discount heuristics that improves influence spread . We evaluate our algorithms by experiments on two large academic collaboration graphs obtained from the online archival database arXiv.org . Our experimental results show that a our improved greedy algorithm achieves better running time comparing with the improvement of 7 with matching influence spread , b our degree discount heuristics achieve much better influence spread than classic degree and centrality-based heuristics , and when tuned for a specific influence cascade model , it achieves almost matching influence thread with the greedy algorithm , and more importantly c the degree discount heuristics run only in milliseconds while even the improved greedy algorithms run in hours in our experiment graphs with a few tens of thousands of nodes . Based on our results , we believe that fine-tuned heuristics may provide truly scalable solutions to the influence maximization problem with satisfying influence spread and blazingly fast running time . Therefore , contrary to what implied by the conclusion of 5 that traditional heuristics are outperformed by the greedy approximation algorithm , our results shed new lights on the research of heuristic algorithms . [[EENNDD]] influence maximization; heuristic algorithms; social networks; nonnumerical algorithms and problems"}, "Pemaksimalan pengaruh yang cekap dalam rangkaian sosial Pemaksimum pengaruh adalah masalah mencari subset kecil node benih dalam rangkaian sosial yang dapat memaksimumkan penyebaran pengaruh. Dalam makalah ini, kami mengkaji pemaksimalan pengaruh yang cekap dari dua arah pelengkap. Salah satunya ialah meningkatkan algoritma rakus asli 5 dan peningkatan 7 untuk mengurangkan lagi masa berjalannya, dan yang kedua adalah mencadangkan heuristik diskaun darjah baru yang meningkatkan penyebaran pengaruh. Kami menilai algoritma kami dengan melakukan eksperimen pada dua grafik kolaborasi akademik yang besar yang diperolehi dari pangkalan data arkib dalam talian arXiv.org. Hasil eksperimen kami menunjukkan bahawa algoritma tamak yang ditingkatkan mencapai masa berjalan lebih baik berbanding dengan peningkatan 7 dengan spread pengaruh yang sepadan, b heuristik diskaun darjah kami mencapai penyebaran pengaruh yang jauh lebih baik daripada heuristik berasaskan darjah dan sentraliti klasik, dan ketika ditala untuk spesifik mempengaruhi model kaskade, ia mencapai untaian pengaruh yang hampir sama dengan algoritma tamak, dan yang lebih penting ialah heuristik diskaun darjah hanya berjalan dalam milisaat sementara algoritma tamak yang ditingkatkan berjalan dalam beberapa jam dalam grafik eksperimen kami dengan beberapa puluhan ribu nod. Berdasarkan hasil kami, kami percaya bahawa heuristik yang diselaraskan dapat memberikan penyelesaian yang benar-benar berskala untuk masalah memaksimalkan pengaruh dengan penyebaran pengaruh yang memuaskan dan masa berjalan yang sangat cepat. Oleh itu, bertentangan dengan yang disiratkan oleh kesimpulan 5 bahawa heuristik tradisional lebih baik daripada algoritma pendekatan tamak, hasil kami memberi penerangan baru mengenai penyelidikan algoritma heuristik. [[EENNDD]] mempengaruhi pemaksaan; algoritma heuristik; rangkaian sosial; algoritma dan masalah bukan berangka"], [{"string": "Interestingness of frequent itemsets using Bayesian networks as background knowledge The paper presents a method for pruning frequent itemsets based on background knowledge represented by a Bayesian network . The interestingness of an itemset is defined as the absolute difference between its support estimated from data and from the Bayesian network . Efficient algorithms are presented for finding interestingness of a collection of frequent itemsets , and for finding all attribute sets with a given minimum interestingness . Practical usefulness of the algorithms and their efficiency have been verified experimentally .", "keywords": ["knowledge", "bayesian network", "association rule", "frequent itemset", "background", "interestingness"], "combined": "Interestingness of frequent itemsets using Bayesian networks as background knowledge The paper presents a method for pruning frequent itemsets based on background knowledge represented by a Bayesian network . The interestingness of an itemset is defined as the absolute difference between its support estimated from data and from the Bayesian network . Efficient algorithms are presented for finding interestingness of a collection of frequent itemsets , and for finding all attribute sets with a given minimum interestingness . Practical usefulness of the algorithms and their efficiency have been verified experimentally . [[EENNDD]] knowledge; bayesian network; association rule; frequent itemset; background; interestingness"}, "Minat kumpulan barang yang kerap menggunakan rangkaian Bayesian sebagai pengetahuan latar Makalah ini menyajikan kaedah untuk memangkas set item yang kerap berdasarkan pengetahuan latar yang diwakili oleh rangkaian Bayesian. Keseronokan suatu item ditakrifkan sebagai perbezaan mutlak antara sokongannya yang dianggarkan dari data dan dari rangkaian Bayesian. Algoritma yang cekap disajikan untuk mencari keseronokan koleksi kumpulan barang yang kerap, dan untuk mencari semua set atribut dengan minat minimum yang diberikan. Kegunaan praktikal algoritma dan kecekapannya telah disahkan secara eksperimen. [[EENNDD]] pengetahuan; rangkaian bayesian; peraturan persatuan; itemet yang kerap; latar belakang; keseronokan"], [{"string": "Translation-invariant mixture models for curve clustering In this paper we present a family of algorithms that can simultaneously align and cluster sets of multidimensional curves defined on a discrete time grid . Our approach uses the Expectation-Maximization EM algorithm to recover both the mean curve shapes for each cluster , and the most likely shifts , offsets , and cluster memberships for each curve . We demonstrate how Bayesian estimation methods can improve the results for small sample sizes by enforcing smoothness in the cluster mean curves . We evaluate the methodology on two real-world data sets , time-course gene expression data and storm trajectory data . Experimental results show that models that incorporate curve alignment systematically provide improvements in predictive power and within-cluster variance on test data sets . The proposed approach provides a non-parametric , computationally efficient , and robust methodology for clustering broad classes of curve data .", "keywords": ["mixture model", "learning", "em", "curve clustering", "alignment", "transformation invariance"], "combined": "Translation-invariant mixture models for curve clustering In this paper we present a family of algorithms that can simultaneously align and cluster sets of multidimensional curves defined on a discrete time grid . Our approach uses the Expectation-Maximization EM algorithm to recover both the mean curve shapes for each cluster , and the most likely shifts , offsets , and cluster memberships for each curve . We demonstrate how Bayesian estimation methods can improve the results for small sample sizes by enforcing smoothness in the cluster mean curves . We evaluate the methodology on two real-world data sets , time-course gene expression data and storm trajectory data . Experimental results show that models that incorporate curve alignment systematically provide improvements in predictive power and within-cluster variance on test data sets . The proposed approach provides a non-parametric , computationally efficient , and robust methodology for clustering broad classes of curve data . [[EENNDD]] mixture model; learning; em; curve clustering; alignment; transformation invariance"}, "Model campuran terjemahan-invarian untuk pengelompokan kurva Dalam makalah ini kami menyajikan sekumpulan algoritma yang dapat secara serentak menyelaraskan dan menyusun kumpulan lengkung multidimensi yang ditentukan pada grid masa diskrit. Pendekatan kami menggunakan algoritma EM Ekspektasi-Maksimum untuk memulihkan kedua-dua bentuk kurva min untuk setiap kelompok, dan kemungkinan perubahan, offset, dan keanggotaan kluster untuk setiap kurva. Kami menunjukkan bagaimana kaedah anggaran Bayesian dapat meningkatkan hasil untuk ukuran sampel kecil dengan menegakkan kelancaran pada kurva min kelompok. Kami menilai metodologi pada dua kumpulan data dunia nyata, data ekspresi gen kursus waktu dan data lintasan ribut. Hasil eksperimen menunjukkan bahawa model yang menggabungkan penjajaran kurva secara sistematik memberikan peningkatan daya ramalan dan varians dalam kelompok pada set data ujian. Pendekatan yang dicadangkan menyediakan metodologi non-parametrik, efisien secara komputasi, dan mantap untuk mengumpulkan data lengkung kelas yang luas. [[EENNDD]] model campuran; belajar; em; pengelompokan keluk; penjajaran; invarians transformasi"], [{"string": "A generalized framework for mining spatio-temporal patterns in scientific data In this paper , we present a general framework to discover spatial associations and spatio-temporal episodes for scientific datasets . In contrast to previous work in this area , features are modeled as geometric objects rather than points . We define multiple distance metrics that take into account objects ' extent and thus are more robust in capturing the influence of an object on other objects in spatial neighborhood . We have developed algorithms to discover four different types of spatial object interaction association patterns . We also extend our approach to accommodate temporal information and propose a simple algorithm to derive spatio-temporal episodes . We show that such episodes can be used to reason about critical events . We evaluate our framework on real datasets to demonstrate its efficacy . The datasets originate from two different areas : Computational Molecular Dynamics and Computational Fluid Flow . We present results highlighting the importance of the identified patterns and episodes by using knowledge from the underlying domains . We also show that the proposed algorithms scale linearly with respect to the dataset size .", "keywords": ["scientific data", "spatial object association", "spatio-temporal association/episode"], "combined": "A generalized framework for mining spatio-temporal patterns in scientific data In this paper , we present a general framework to discover spatial associations and spatio-temporal episodes for scientific datasets . In contrast to previous work in this area , features are modeled as geometric objects rather than points . We define multiple distance metrics that take into account objects ' extent and thus are more robust in capturing the influence of an object on other objects in spatial neighborhood . We have developed algorithms to discover four different types of spatial object interaction association patterns . We also extend our approach to accommodate temporal information and propose a simple algorithm to derive spatio-temporal episodes . We show that such episodes can be used to reason about critical events . We evaluate our framework on real datasets to demonstrate its efficacy . The datasets originate from two different areas : Computational Molecular Dynamics and Computational Fluid Flow . We present results highlighting the importance of the identified patterns and episodes by using knowledge from the underlying domains . We also show that the proposed algorithms scale linearly with respect to the dataset size . [[EENNDD]] scientific data; spatial object association; spatio-temporal association/episode"}, "Kerangka umum untuk melombong corak spatio-temporal dalam data saintifik Dalam makalah ini, kami menyajikan kerangka umum untuk mengetahui hubungan spasial dan episod spatio-temporal untuk kumpulan data saintifik. Berbeza dengan karya sebelumnya di kawasan ini, ciri dimodelkan sebagai objek geometri dan bukannya titik. Kami menentukan metrik jarak jauh yang mengambil kira sejauh mana objek dan dengan demikian lebih mantap dalam menangkap pengaruh objek pada objek lain dalam lingkungan ruang. Kami telah mengembangkan algoritma untuk menemui empat jenis corak hubungan interaksi objek spatial. Kami juga memperluas pendekatan kami untuk menampung maklumat temporal dan mencadangkan algoritma mudah untuk mendapatkan episod spatio-temporal. Kami menunjukkan bahawa episod seperti itu dapat digunakan untuk memberi alasan mengenai peristiwa kritikal. Kami menilai kerangka kerja kami pada set data sebenar untuk menunjukkan keberkesanannya. Set data berasal dari dua bidang yang berbeza: Dinamika Molekul Komputasi dan Aliran Cecair Komputasi. Kami membentangkan hasil yang menekankan pentingnya corak dan episod yang dikenal pasti dengan menggunakan pengetahuan dari domain yang mendasari. Kami juga menunjukkan bahawa algoritma yang dicadangkan skala secara linear sehubungan dengan ukuran set data. [[EENNDD]] data saintifik; persatuan objek spatial; perkaitan / episod spatio-temporal"], [{"string": "Query-time entity resolution The goal of entity resolution is to reconcile database references corresponding to the same real-world entities . Given the abundance of publicly available databases where entities are not resolved , we motivate the problem of quickly processing queries that require resolved entities from such ` unclean ' databases . We propose a two-stage collective resolution strategy for processing queries . We then show how it can be performed on-the-fly by adaptively extracting and resolving those database references that are the most helpful for resolving the query . We validate our approach on two large real-world publication databases where we show the usefulness of collective resolution and at the same time demonstrate the need for adaptive strategies for query processing . We then show how the same queries can be answered in real time using our adaptive approach while preserving the gains of collective resolution .", "keywords": ["adaptive", "entity resolution", "relations", "query"], "combined": "Query-time entity resolution The goal of entity resolution is to reconcile database references corresponding to the same real-world entities . Given the abundance of publicly available databases where entities are not resolved , we motivate the problem of quickly processing queries that require resolved entities from such ` unclean ' databases . We propose a two-stage collective resolution strategy for processing queries . We then show how it can be performed on-the-fly by adaptively extracting and resolving those database references that are the most helpful for resolving the query . We validate our approach on two large real-world publication databases where we show the usefulness of collective resolution and at the same time demonstrate the need for adaptive strategies for query processing . We then show how the same queries can be answered in real time using our adaptive approach while preserving the gains of collective resolution . [[EENNDD]] adaptive; entity resolution; relations; query"}, "Penyelesaian entiti masa pertanyaan Matlamat penyelesaian entiti adalah untuk mendamaikan rujukan pangkalan data yang sesuai dengan entiti dunia nyata yang sama. Memandangkan banyak pangkalan data yang tersedia untuk umum di mana entiti tidak dapat diselesaikan, kami memotivasi masalah memproses pertanyaan dengan cepat yang memerlukan entiti yang diselesaikan dari pangkalan data \"tidak bersih\" tersebut. Kami mencadangkan strategi penyelesaian kolektif dua peringkat untuk memproses pertanyaan. Kami kemudian menunjukkan bagaimana ia dapat dilakukan secara cepat dengan mengekstrak dan menyelesaikan rujukan pangkalan data tersebut yang paling berguna untuk menyelesaikan pertanyaan. Kami mengesahkan pendekatan kami pada dua pangkalan data penerbitan dunia nyata yang besar di mana kami menunjukkan kegunaan resolusi kolektif dan pada masa yang sama menunjukkan perlunya strategi adaptif untuk pemprosesan pertanyaan. Kami kemudian menunjukkan bagaimana pertanyaan yang sama dapat dijawab dalam masa nyata menggunakan pendekatan adaptif kami sambil mengekalkan keuntungan penyelesaian bersama. [[EENNDD]] adaptif; ketetapan entiti; hubungan; pertanyaan"], [{"string": "A framework for community identification in dynamic social networks We propose frameworks and algorithms for identifying communities in social networks that change over time . Communities are intuitively characterized as `` unusually densely knit '' subsets of a social network . This notion becomes more problematic if the social interactions change over time . Aggregating social networks over time can radically misrepresent the existing and changing community structure . Instead , we propose an optimization-based approach for modeling dynamic community structure . We prove that finding the most explanatory community structure is NP-hard and APX-hard , and propose algorithms based on dynamic programming , exhaustive search , maximum matching , and greedy heuristics . We demonstrate empirically that the heuristics trace developments of community structure accurately for several synthetic and real-world examples .", "keywords": ["community identification", "dynamic social networks", "model development"], "combined": "A framework for community identification in dynamic social networks We propose frameworks and algorithms for identifying communities in social networks that change over time . Communities are intuitively characterized as `` unusually densely knit '' subsets of a social network . This notion becomes more problematic if the social interactions change over time . Aggregating social networks over time can radically misrepresent the existing and changing community structure . Instead , we propose an optimization-based approach for modeling dynamic community structure . We prove that finding the most explanatory community structure is NP-hard and APX-hard , and propose algorithms based on dynamic programming , exhaustive search , maximum matching , and greedy heuristics . We demonstrate empirically that the heuristics trace developments of community structure accurately for several synthetic and real-world examples . [[EENNDD]] community identification; dynamic social networks; model development"}, "Kerangka kerja untuk mengenal pasti komuniti dalam rangkaian sosial yang dinamik Kami mencadangkan kerangka kerja dan algoritma untuk mengenal pasti komuniti dalam rangkaian sosial yang berubah dari masa ke masa. Komuniti secara intuitif dicirikan sebagai subkumpulan jaringan sosial yang \"sangat rajin\". Gagasan ini menjadi lebih bermasalah sekiranya interaksi sosial berubah dari masa ke masa. Gabungan rangkaian sosial dari masa ke masa secara radikal dapat memberi gambaran yang salah mengenai struktur masyarakat yang ada dan berubah. Sebaliknya, kami mencadangkan pendekatan berasaskan pengoptimuman untuk pemodelan struktur komuniti yang dinamik. Kami membuktikan bahawa mencari struktur komuniti yang paling jelas adalah NP-hard dan APX-hard, dan mencadangkan algoritma berdasarkan pengaturcaraan dinamik, carian lengkap, padanan maksimum, dan heuristik tamak. Kami menunjukkan secara empirik bahawa heuristik mengesan perkembangan struktur masyarakat dengan tepat untuk beberapa contoh sintetik dan dunia nyata. [[EENNDD]] pengenalan masyarakat; rangkaian sosial yang dinamik; pembangunan model"], [{"string": "An iterative method for multi-class cost-sensitive learning Cost-sensitive learning addresses the issue of classification in the presence of varying costs associated with different types of misclassification . In this paper , we present a method for solving multi-class cost-sensitive learning problems using any binary classification algorithm . This algorithm is derived using hree key ideas : 1 iterative weighting ; 2 expanding data space ; and 3 gradient boosting with stochastic ensembles . We establish some theoretical guarantees concerning the performance of this method . In particular , we show that a certain variant possesses the boosting property , given a form of weak learning assumption on the component binary classifier . We also empirically evaluate the performance of the proposed method using benchmark data sets and verify that our method generally achieves better results than representative methods for cost-sensitive learning , in terms of predictive performance cost minimization and , in many cases , computational efficiency .", "keywords": ["boosting", "multi-class classification", "cost-sensitive learning", "learning"], "combined": "An iterative method for multi-class cost-sensitive learning Cost-sensitive learning addresses the issue of classification in the presence of varying costs associated with different types of misclassification . In this paper , we present a method for solving multi-class cost-sensitive learning problems using any binary classification algorithm . This algorithm is derived using hree key ideas : 1 iterative weighting ; 2 expanding data space ; and 3 gradient boosting with stochastic ensembles . We establish some theoretical guarantees concerning the performance of this method . In particular , we show that a certain variant possesses the boosting property , given a form of weak learning assumption on the component binary classifier . We also empirically evaluate the performance of the proposed method using benchmark data sets and verify that our method generally achieves better results than representative methods for cost-sensitive learning , in terms of predictive performance cost minimization and , in many cases , computational efficiency . [[EENNDD]] boosting; multi-class classification; cost-sensitive learning; learning"}, "Kaedah berulang untuk pembelajaran sensitif kos berbilang kelas Pembelajaran sensitif kos menangani masalah klasifikasi dengan adanya pelbagai kos yang berkaitan dengan pelbagai jenis salah klasifikasi. Dalam makalah ini, kami menyajikan kaedah untuk menyelesaikan masalah pembelajaran sensitif kos berbilang kelas dengan menggunakan algoritma klasifikasi binari. Algoritma ini dihasilkan menggunakan idea utama hree: 1 iterative weighting; 2 memperluas ruang data; dan 3 peningkatan kecerunan dengan ensemble stokastik. Kami menetapkan beberapa jaminan teori mengenai prestasi kaedah ini. Secara khusus, kami menunjukkan bahawa varian tertentu memiliki sifat penambahbaikan, memandangkan bentuk anggapan pembelajaran yang lemah pada komponen pengkelasan binari. Kami juga secara empirikal menilai prestasi kaedah yang dicadangkan menggunakan set data penanda aras dan mengesahkan bahawa kaedah kami secara amnya memperoleh hasil yang lebih baik daripada kaedah representatif untuk pembelajaran sensitif kos, dari segi pengurangan kos prestasi ramalan dan, dalam banyak kes, kecekapan komputasi. [[EENNDD]] meningkatkan; klasifikasi pelbagai kelas; pembelajaran sensitif kos; belajar"], [{"string": "Learning , indexing , and diagnosing network faults Modern communication networks generate massive volume of operational event data , e.g. , alarm , alert , and metrics , which can be used by a network management system NMS to diagnose potential faults . In this work , we introduce a new class of indexable fault signatures that encode temporal evolution of events generated by a network fault as well as topological relationships among the nodes where these events occur . We present an efficient learning algorithm to extract such fault signatures from noisy historical event data , and with the help of novel space-time indexing structures , we show how to perform efficient , online signature matching . We provide results from extensive experimental studies to explore the efficacy of our approach and point out potential applications of such signatures for many different types of networks including social and information networks .", "keywords": ["online diagnosis", "network topology", "fault signature"], "combined": "Learning , indexing , and diagnosing network faults Modern communication networks generate massive volume of operational event data , e.g. , alarm , alert , and metrics , which can be used by a network management system NMS to diagnose potential faults . In this work , we introduce a new class of indexable fault signatures that encode temporal evolution of events generated by a network fault as well as topological relationships among the nodes where these events occur . We present an efficient learning algorithm to extract such fault signatures from noisy historical event data , and with the help of novel space-time indexing structures , we show how to perform efficient , online signature matching . We provide results from extensive experimental studies to explore the efficacy of our approach and point out potential applications of such signatures for many different types of networks including social and information networks . [[EENNDD]] online diagnosis; network topology; fault signature"}, "Mempelajari, mengindeks, dan mendiagnosis kesalahan rangkaian Rangkaian komunikasi moden menghasilkan sejumlah besar data peristiwa operasi, mis. , penggera, amaran, dan metrik, yang dapat digunakan oleh sistem pengurusan rangkaian NMS untuk mendiagnosis kemungkinan kerosakan. Dalam karya ini, kami memperkenalkan kelas baru tanda tangan kesalahan yang dapat diindeks yang mengekod evolusi temporal peristiwa yang dihasilkan oleh kesalahan rangkaian serta hubungan topologi di antara nod di mana peristiwa ini berlaku. Kami menyajikan algoritma pembelajaran yang cekap untuk mengekstrak tanda tangan kesalahan dari data peristiwa bersejarah yang bising, dan dengan bantuan struktur pengindeksan ruang-waktu yang baru, kami menunjukkan bagaimana melakukan pemadanan tandatangan dalam talian yang cekap. Kami memberikan hasil dari kajian eksperimen yang luas untuk meneroka keberkesanan pendekatan kami dan menunjukkan potensi aplikasi tanda tangan tersebut untuk pelbagai jenis rangkaian termasuk rangkaian sosial dan maklumat. [[EENNDD]] diagnosis dalam talian; topologi rangkaian; tandatangan kesalahan"], [{"string": "A sequential sampling algorithm for a general class of utility criteria", "keywords": ["learning"], "combined": "A sequential sampling algorithm for a general class of utility criteria [[EENNDD]] learning"}, "Algoritma pensampelan berurutan untuk kelas umum kriteria utiliti [[EENNDD]] pembelajaran"], [{"string": "Nonlinear adaptive distance metric learning for clustering A good distance metric is crucial for many data mining tasks . To learn a metric in the unsupervised setting , most metric learning algorithms project observed data to a low-dimensional manifold , where geometric relationships such as pairwise distances are preserved . It can be extended to the nonlinear case by applying the kernel trick , which embeds the data into a feature space by specifying the kernel function that computes the dot products between data points in the feature space . In this paper , we propose a novel unsupervised Nonlinear Adaptive Metric Learning algorithm , called NAML , which performs clustering and distance metric learning simultaneously . NAML firstmaps the data to a high-dimensional space through a kernel function ; then applies a linear projection to find a low-dimensional manifold where the separability of the data is maximized ; and finally performs clustering in the low-dimensional space . The performance of NAML depends on the selection of the kernel function and the projection . We show that the joint kernel learning , dimensionality reduction , and clustering can be formulated as a trace maximization problem , which can be solved via an iterative procedure in the EM framework . Experimental results demonstrated the efficacy of the proposed algorithm .", "keywords": ["convex programming", "kernel", "clustering", "distance metric"], "combined": "Nonlinear adaptive distance metric learning for clustering A good distance metric is crucial for many data mining tasks . To learn a metric in the unsupervised setting , most metric learning algorithms project observed data to a low-dimensional manifold , where geometric relationships such as pairwise distances are preserved . It can be extended to the nonlinear case by applying the kernel trick , which embeds the data into a feature space by specifying the kernel function that computes the dot products between data points in the feature space . In this paper , we propose a novel unsupervised Nonlinear Adaptive Metric Learning algorithm , called NAML , which performs clustering and distance metric learning simultaneously . NAML firstmaps the data to a high-dimensional space through a kernel function ; then applies a linear projection to find a low-dimensional manifold where the separability of the data is maximized ; and finally performs clustering in the low-dimensional space . The performance of NAML depends on the selection of the kernel function and the projection . We show that the joint kernel learning , dimensionality reduction , and clustering can be formulated as a trace maximization problem , which can be solved via an iterative procedure in the EM framework . Experimental results demonstrated the efficacy of the proposed algorithm . [[EENNDD]] convex programming; kernel; clustering; distance metric"}, "Pembelajaran metrik jarak adaptif tidak linier untuk pengelompokan Metrik jarak yang baik sangat penting untuk banyak tugas perlombongan data. Untuk mempelajari metrik dalam tetapan tanpa pengawasan, kebanyakan algoritma pembelajaran metrik memproyeksikan data ke manifold dimensi rendah, di mana hubungan geometri seperti jarak berpasangan dipelihara. Ini dapat diperluas ke kasus nonlinear dengan menerapkan trik kernel, yang memasukkan data ke dalam ruang fitur dengan menentukan fungsi kernel yang menghitung produk titik antara titik data di ruang fitur. Dalam makalah ini, kami mengusulkan sebuah novel algoritma Pembelajaran Metrik Adaptive Nonlinear tanpa pengawasan, yang disebut NAML, yang melakukan pembelajaran metrik pengelompokan dan jarak jauh secara serentak. NAML memetakan data ke ruang dimensi tinggi melalui fungsi kernel; kemudian menerapkan unjuran linear untuk mencari manifold dimensi rendah di mana keterpisahan data dimaksimumkan; dan akhirnya melakukan pengelompokan di ruang dimensi rendah. Prestasi NAML bergantung pada pemilihan fungsi kernel dan unjuran. Kami menunjukkan bahawa pembelajaran kernel bersama, pengurangan dimensi, dan pengelompokan dapat dirumuskan sebagai masalah pemaksaan jejak, yang dapat diselesaikan melalui prosedur berulang dalam kerangka EM. Hasil eksperimen menunjukkan keberkesanan algoritma yang dicadangkan. [[EENNDD]] pengaturcaraan cembung; kernel; pengelompokan; sukatan jarak"], [{"string": "Identifying prospective customers", "keywords": ["customer prospecting"], "combined": "Identifying prospective customers [[EENNDD]] customer prospecting"}, "Mengenal calon pelanggan [[EENNDD]] prospek pelanggan"], [{"string": "Partial least squares regression for graph mining Attributed graphs are increasingly more common in many application domains such as chemistry , biology and text processing . A central issue in graph mining is how to collect informative subgraph patterns for a given learning task . We propose an iterative mining method based on partial least squares regression PLS . To apply PLS to graph data , a sparse version of PLS is developed first and then it is combined with a weighted pattern mining algorithm . The mining algorithm is iteratively called with different weight vectors , creating one latent component per one mining call . Our method , graph PLS , is efficient and easy to implement , because the weight vector is updated with elementary matrix calculations . In experiments , our graph PLS algorithm showed competitive prediction accuracies in many chemical datasets and its efficiency was significantly superior to graph boosting gBoost and the naive method based on frequent graph mining .", "keywords": ["partial least squares regression", "chemoinformatics", "graph boosting", "graph mining"], "combined": "Partial least squares regression for graph mining Attributed graphs are increasingly more common in many application domains such as chemistry , biology and text processing . A central issue in graph mining is how to collect informative subgraph patterns for a given learning task . We propose an iterative mining method based on partial least squares regression PLS . To apply PLS to graph data , a sparse version of PLS is developed first and then it is combined with a weighted pattern mining algorithm . The mining algorithm is iteratively called with different weight vectors , creating one latent component per one mining call . Our method , graph PLS , is efficient and easy to implement , because the weight vector is updated with elementary matrix calculations . In experiments , our graph PLS algorithm showed competitive prediction accuracies in many chemical datasets and its efficiency was significantly superior to graph boosting gBoost and the naive method based on frequent graph mining . [[EENNDD]] partial least squares regression; chemoinformatics; graph boosting; graph mining"}, "Regresi kuasa dua separa untuk perlombongan grafik Grafik atribut semakin kerap berlaku di banyak domain aplikasi seperti kimia, biologi dan pemprosesan teks. Masalah utama dalam perlombongan grafik adalah bagaimana mengumpulkan corak subgraf maklumat untuk tugas pembelajaran yang diberikan. Kami mencadangkan kaedah perlombongan berulang berdasarkan PLS regresi kuadrat separa terkecil. Untuk menerapkan PLS ke data grafik, versi jarang PLS dikembangkan terlebih dahulu dan kemudian digabungkan dengan algoritma perlombongan corak berwajaran. Algoritma perlombongan dipanggil secara berulang dengan vektor berat yang berbeza, mewujudkan satu komponen pendam setiap panggilan perlombongan. Kaedah kami, grafik PLS, cekap dan mudah dilaksanakan, kerana vektor berat dikemas kini dengan pengiraan matriks dasar. Dalam eksperimen, algoritma grafik PLS kami menunjukkan ketepatan ramalan kompetitif dalam banyak kumpulan data kimia dan kecekapannya jauh lebih tinggi daripada peningkatan grafik gBoost dan kaedah naif berdasarkan penambangan grafik yang kerap. [[EENNDD]] regresi kuasa dua separa; chemoinformatics; peningkatan grafik; perlombongan grafik"], [{"string": "Accurate decision trees for mining high-speed data streams In this paper we study the problem of constructing accurate decision tree models from data streams . Data streams are incremental tasks that require incremental , online , and any-time learning algorithms . One of the most successful algorithms for mining data streams is VFDT . In this paper we extend the VFDT system in two directions : the ability to deal with continuous data and the use of more powerful classification techniques at tree leaves . The proposed system , VFDTc , can incorporate and classify new information online , with a single scan of the data , in time constant per example . The most relevant property of our system is the ability to obtain a performance similar to a standard decision tree algorithm even for medium size datasets . This is relevant due to the any-time property . We study the behavior of VFDTc in different problems and demonstrate its utility in large and medium data sets . Under a bias-variance analysis we observe that VFDTc in comparison to C4 .5 is able to reduce the variance component .", "keywords": ["decision trees", "subsampling", "decision support", "disk-based algorithms", "hoeffding bounds", "incremental learning"], "combined": "Accurate decision trees for mining high-speed data streams In this paper we study the problem of constructing accurate decision tree models from data streams . Data streams are incremental tasks that require incremental , online , and any-time learning algorithms . One of the most successful algorithms for mining data streams is VFDT . In this paper we extend the VFDT system in two directions : the ability to deal with continuous data and the use of more powerful classification techniques at tree leaves . The proposed system , VFDTc , can incorporate and classify new information online , with a single scan of the data , in time constant per example . The most relevant property of our system is the ability to obtain a performance similar to a standard decision tree algorithm even for medium size datasets . This is relevant due to the any-time property . We study the behavior of VFDTc in different problems and demonstrate its utility in large and medium data sets . Under a bias-variance analysis we observe that VFDTc in comparison to C4 .5 is able to reduce the variance component . [[EENNDD]] decision trees; subsampling; decision support; disk-based algorithms; hoeffding bounds; incremental learning"}, "Pokok keputusan yang tepat untuk melombong aliran data berkelajuan tinggi Dalam makalah ini kami mengkaji masalah pembinaan model keputusan yang tepat dari aliran data. Aliran data adalah tugas tambahan yang memerlukan algoritma pembelajaran tambahan, dalam talian, dan bila-bila masa. Salah satu algoritma yang paling berjaya untuk aliran data perlombongan adalah VFDT. Dalam makalah ini kami memperluas sistem VFDT dalam dua arah: kemampuan untuk menangani data berterusan dan penggunaan teknik klasifikasi yang lebih kuat pada daun pokok. Sistem yang dicadangkan, VFDTc, dapat menggabungkan dan mengklasifikasikan maklumat baru dalam talian, dengan satu imbasan data, dalam pemalar masa setiap contoh. Harta yang paling relevan dari sistem kami adalah keupayaan untuk memperoleh prestasi yang serupa dengan algoritma pokok keputusan standard walaupun untuk set data bersaiz sederhana. Ini berkaitan kerana harta tanah pada bila-bila masa. Kami mengkaji tingkah laku VFDTc dalam masalah yang berbeza dan menunjukkan kegunaannya dalam kumpulan data besar dan sederhana. Di bawah analisis bias-varians kita melihat bahawa VFDTc dibandingkan dengan C4 .5 mampu mengurangkan komponen varians. [[EENNDD]] pokok keputusan; pengambilan sampel; sokongan keputusan; algoritma berasaskan cakera; had cangkul; pembelajaran tambahan"], [{"string": "Estimating the global pagerank of web communities Localized search engines are small-scale systems that index a particular community on the web . They offer several benefits over their large-scale counterparts in that they are relatively inexpensive to build , and can provide more precise and complete search capability over their relevant domains . One disadvantage such systems have over large-scale search engines is the lack of global PageRank values . Such information is needed to assess the value of pages in the localized search domain within the context of the web as a whole . In this paper , we present well-motivated algorithms to estimate the global PageRank values of a local domain . The algorithms are all highly scalable in that , given a local domain of size n , they use O n resources that include computation time , bandwidth , and storage . We test our methods across a variety of localized domains , including site-specific domains and topic-specific domains . We demonstrate that by crawling as few as n or 2n additional pages , our methods can give excellent global PageRank estimates .", "keywords": ["numerical linear algebra", "stochastic complementation", "information search and retrieval", "page rank", "markov chain"], "combined": "Estimating the global pagerank of web communities Localized search engines are small-scale systems that index a particular community on the web . They offer several benefits over their large-scale counterparts in that they are relatively inexpensive to build , and can provide more precise and complete search capability over their relevant domains . One disadvantage such systems have over large-scale search engines is the lack of global PageRank values . Such information is needed to assess the value of pages in the localized search domain within the context of the web as a whole . In this paper , we present well-motivated algorithms to estimate the global PageRank values of a local domain . The algorithms are all highly scalable in that , given a local domain of size n , they use O n resources that include computation time , bandwidth , and storage . We test our methods across a variety of localized domains , including site-specific domains and topic-specific domains . We demonstrate that by crawling as few as n or 2n additional pages , our methods can give excellent global PageRank estimates . [[EENNDD]] numerical linear algebra; stochastic complementation; information search and retrieval; page rank; markov chain"}, "Menganggarkan pagerank global komuniti web Enjin carian tempatan adalah sistem skala kecil yang mengindeks komuniti tertentu di web. Mereka menawarkan beberapa faedah berbanding rakan mereka yang berskala besar kerana ia agak murah untuk dibina, dan dapat memberikan kemampuan pencarian yang lebih tepat dan lengkap di atas domain yang relevan. Satu kelemahan sistem seperti mesin pencari skala besar adalah kurangnya nilai PageRank global. Maklumat sedemikian diperlukan untuk menilai nilai halaman dalam domain carian yang dilokalkan dalam konteks web secara keseluruhan. Dalam makalah ini, kami menyajikan algoritma yang bermotivasi dengan baik untuk memperkirakan nilai-nilai PageRank global dari domain tempatan. Algoritma semuanya sangat berskala kerana, memandangkan domain lokal berukuran n, mereka menggunakan sumber daya O n yang meliputi waktu pengiraan, lebar jalur, dan penyimpanan. Kami menguji kaedah kami di pelbagai domain yang dilokalkan, termasuk domain khusus laman web dan domain khusus topik. Kami menunjukkan bahawa dengan merangkak hanya beberapa halaman n atau 2n, kaedah kami dapat memberikan anggaran PageRank global yang sangat baik. [[EENNDD]] aljabar linear berangka; pelengkap stokastik; carian dan pengambilan maklumat; kedudukan halaman; rantaian markov"], [{"string": "A Bayesian network framework for reject inference Most learning methods assume that the training set is drawn randomly from the population to which the learned model is to be applied . However in many applications this assumption is invalid . For example , lending institutions create models of who is likely to repay a loan from training sets consisting of people in their records to whom loans were given in the past ; however , the institution approved loan applications previously based on who was thought unlikely to default . Learning from only approved loans yields an incorrect model because the training set is a biased sample of the general population of applicants . The issue of including rejected samples in the learning process , or alternatively using rejected samples to adjust a model learned from accepted samples only , is called reject inference . The main contribution of this paper is a systematic analysis of different cases that arise in reject inference , with explanations of which cases arise in various real-world situations . We use Bayesian networks to formalize each case as a set of conditional independence relationships and identify eight cases , including the familiar missing completely at random MCAR , missing at random MAR , and missing not at random MNAR cases . For each case we present an overview of available learning algorithms . These algorithms have been published in separate fields of research , including epidemiology , econometrics , clinical trial evaluation , sociology , and credit scoring ; our second major contribution is to describe these algorithms in a common framework .", "keywords": ["reject inference", "bayesian networks", "propensity scores", "heckman estimator", "sample selection bias", "expectation-maximization"], "combined": "A Bayesian network framework for reject inference Most learning methods assume that the training set is drawn randomly from the population to which the learned model is to be applied . However in many applications this assumption is invalid . For example , lending institutions create models of who is likely to repay a loan from training sets consisting of people in their records to whom loans were given in the past ; however , the institution approved loan applications previously based on who was thought unlikely to default . Learning from only approved loans yields an incorrect model because the training set is a biased sample of the general population of applicants . The issue of including rejected samples in the learning process , or alternatively using rejected samples to adjust a model learned from accepted samples only , is called reject inference . The main contribution of this paper is a systematic analysis of different cases that arise in reject inference , with explanations of which cases arise in various real-world situations . We use Bayesian networks to formalize each case as a set of conditional independence relationships and identify eight cases , including the familiar missing completely at random MCAR , missing at random MAR , and missing not at random MNAR cases . For each case we present an overview of available learning algorithms . These algorithms have been published in separate fields of research , including epidemiology , econometrics , clinical trial evaluation , sociology , and credit scoring ; our second major contribution is to describe these algorithms in a common framework . [[EENNDD]] reject inference; bayesian networks; propensity scores; heckman estimator; sample selection bias; expectation-maximization"}, "Rangka kerja Bayesian untuk menolak inferens Sebilangan besar kaedah pembelajaran menganggap bahawa set latihan diambil secara rawak dari populasi yang menjadi model pembelajaran yang akan diterapkan. Namun dalam banyak aplikasi anggapan ini tidak sah. Sebagai contoh, institusi pinjaman membuat model siapa yang mungkin membayar balik pinjaman dari kumpulan latihan yang terdiri daripada orang-orang yang ada dalam catatan mereka kepada siapa pinjaman diberikan pada masa lalu; namun, institusi tersebut meluluskan permohonan pinjaman yang sebelumnya berdasarkan kepada siapa yang dianggap tidak boleh ingkar. Belajar dari pinjaman yang diluluskan hanya menghasilkan model yang salah kerana set latihan adalah sampel berat sebelah dari populasi umum pemohon. Masalah memasukkan sampel yang ditolak dalam proses pembelajaran, atau sebagai alternatif menggunakan sampel yang ditolak untuk menyesuaikan model yang dipelajari dari sampel yang diterima sahaja, disebut inferensi penolakan. Sumbangan utama makalah ini adalah analisis sistematik terhadap kes-kes yang berbeza yang timbul dalam kesimpulan penolakan, dengan penjelasan mengenai kes-kes yang timbul dalam pelbagai situasi di dunia nyata. Kami menggunakan rangkaian Bayes untuk memformalkan setiap kes sebagai satu set hubungan kemerdekaan bersyarat dan mengenal pasti lapan kes, termasuk yang hilang sepenuhnya pada MCAR rawak, hilang pada MAR rawak, dan tidak hilang pada kes MNAR rawak. Untuk setiap kes kami menyajikan gambaran keseluruhan algoritma pembelajaran yang ada. Algoritma ini telah diterbitkan dalam bidang penyelidikan yang berasingan, termasuk epidemiologi, ekonometrik, penilaian percubaan klinikal, sosiologi, dan pemarkahan kredit; sumbangan utama kedua kami adalah untuk menerangkan algoritma ini dalam kerangka yang sama. [[EENNDD]] menolak inferens; rangkaian bayesian; skor kecenderungan; penganggar heckman; bias pemilihan sampel; jangkaan-maksimum"], [{"string": "The distributed boosting algorithm In this paper , we propose a general framework for distributed boosting intended for efficient integrating specialized classifiers learned over very large and distributed homogeneous databases that can not be merged at a single location . Our distributed boosting algorithm can also be used as a parallel classification technique , where a massive database that can not fit into main computer memory is partitioned into disjoint subsets for a more efficient analysis . In the proposed method , at each boosting round the classifiers are first learned from disjoint datasets and then exchanged amongst the sites . Finally the classifiers are combined into a weighted voting ensemble on each disjoint data set . The ensemble that is applied to an unseen test set represents an ensemble of ensembles built on all distributed sites . In experiments performed on four large data sets the proposed distributed boosting method achieved classification accuracy comparable or even slightly better than the standard boosting algorithm while requiring less memory and less computational time . In addition , the communication overhead of the distributed boosting algorithm is very small making it a viable alternative to the standard boosting for large-scale databases .", "keywords": ["boosting", "classifier ensembles", "distributed learning"], "combined": "The distributed boosting algorithm In this paper , we propose a general framework for distributed boosting intended for efficient integrating specialized classifiers learned over very large and distributed homogeneous databases that can not be merged at a single location . Our distributed boosting algorithm can also be used as a parallel classification technique , where a massive database that can not fit into main computer memory is partitioned into disjoint subsets for a more efficient analysis . In the proposed method , at each boosting round the classifiers are first learned from disjoint datasets and then exchanged amongst the sites . Finally the classifiers are combined into a weighted voting ensemble on each disjoint data set . The ensemble that is applied to an unseen test set represents an ensemble of ensembles built on all distributed sites . In experiments performed on four large data sets the proposed distributed boosting method achieved classification accuracy comparable or even slightly better than the standard boosting algorithm while requiring less memory and less computational time . In addition , the communication overhead of the distributed boosting algorithm is very small making it a viable alternative to the standard boosting for large-scale databases . [[EENNDD]] boosting; classifier ensembles; distributed learning"}, "Algoritma peningkatan didistribusikan Dalam makalah ini, kami mencadangkan kerangka umum untuk meningkatkan didistribusikan yang bertujuan untuk mengintegrasikan pengklasifikasi khusus yang dipelajari melalui pangkalan data homogen yang sangat besar dan diedarkan yang tidak dapat digabungkan di satu lokasi. Algoritma peningkatan yang diedarkan kami juga dapat digunakan sebagai teknik klasifikasi paralel, di mana pangkalan data besar yang tidak dapat dimasukkan ke dalam memori komputer utama dibahagikan kepada subset gabungan untuk analisis yang lebih cekap. Dalam kaedah yang dicadangkan, pada setiap pusingan peningkatan, pengkelasan pertama kali dipelajari dari kumpulan data yang tidak disatukan dan kemudian ditukar di antara laman web. Akhirnya pengklasifikasi digabungkan menjadi ensembel pengundian berwajaran pada setiap set data yang tidak terikat. Ensemble yang diterapkan pada set ujian yang tidak dapat dilihat mewakili kumpulan ensemble yang dibina di semua laman web yang diedarkan. Dalam eksperimen yang dilakukan pada empat kumpulan data yang besar, kaedah penambahan didistribusikan yang dicadangkan mencapai ketepatan klasifikasi yang setanding atau bahkan sedikit lebih baik daripada algoritma peningkatan piawai sambil memerlukan lebih sedikit memori dan lebih sedikit masa pengiraan. Sebagai tambahan, overhead komunikasi algoritma peningkatan yang diedarkan sangat kecil menjadikannya alternatif yang layak untuk peningkatan standard untuk pangkalan data berskala besar. [[EENNDD]] meningkatkan; ensembel pengkelasan; pembelajaran yang diedarkan"], [{"string": "Cancer genomics Throughout life , the cells in every individual accumulate many changes in the DNA inherited from his or her parents . Certain combinations of changes lead to cancer . During the last decade , the cost of DNA sequencing has been dropping by a factor of 10 every two years , making it now possible to read most of the three billion base genome from a patient 's cancer tumor , and to try to determine all of the thousands of DNA changes in it . Under the auspices of NCI 's Cancer Genome Atlas Project , 10,000 tumors will be sequenced in this manner in the next few years . Soon cancer genome sequencing will be a widespread clinical practice , and millions of tumors will be sequenced . A massive computational problem looms in interpreting these data . First , because we can only read short pieces of DNA , we have the enormous problem of assembling a coherent and reliable representation of the tumor genome from massive amounts of incomplete and error-prone evidence . This is the first challenge . Second , every human genome is unique from birth , and every tumor a unique variant . There is no single route to cancer . We must learn to read the varied signatures of cancer within the tumor genome and associate these with optimal treatments . Already there are hundreds of molecularly targeted treatments for cancer available , each known to be more or less effective depending on specific genetic variants . However , targeting a single gene with one treatment rarely works . The second challenge is to tackle the combinatorics of personalized , targeted , combination therapy in cancer .", "keywords": ["cancer"], "combined": "Cancer genomics Throughout life , the cells in every individual accumulate many changes in the DNA inherited from his or her parents . Certain combinations of changes lead to cancer . During the last decade , the cost of DNA sequencing has been dropping by a factor of 10 every two years , making it now possible to read most of the three billion base genome from a patient 's cancer tumor , and to try to determine all of the thousands of DNA changes in it . Under the auspices of NCI 's Cancer Genome Atlas Project , 10,000 tumors will be sequenced in this manner in the next few years . Soon cancer genome sequencing will be a widespread clinical practice , and millions of tumors will be sequenced . A massive computational problem looms in interpreting these data . First , because we can only read short pieces of DNA , we have the enormous problem of assembling a coherent and reliable representation of the tumor genome from massive amounts of incomplete and error-prone evidence . This is the first challenge . Second , every human genome is unique from birth , and every tumor a unique variant . There is no single route to cancer . We must learn to read the varied signatures of cancer within the tumor genome and associate these with optimal treatments . Already there are hundreds of molecularly targeted treatments for cancer available , each known to be more or less effective depending on specific genetic variants . However , targeting a single gene with one treatment rarely works . The second challenge is to tackle the combinatorics of personalized , targeted , combination therapy in cancer . [[EENNDD]] cancer"}, "Genomik barah Sepanjang hayat, sel-sel pada setiap individu mengumpulkan banyak perubahan dalam DNA yang diwarisi dari ibu bapanya. Kombinasi perubahan tertentu membawa kepada barah. Selama dekad yang lalu, kos penjujukan DNA telah menurun dengan faktor 10 setiap dua tahun, sehingga sekarang memungkinkan untuk membaca sebagian besar tiga miliar genom asas dari tumor barah pesakit, dan untuk mencoba menentukan semua ribuan DNA berubah di dalamnya. Di bawah naungan Projek Atlas Genom Kanser NCI, 10,000 tumor akan diuruskan dengan cara ini dalam beberapa tahun ke depan. Tidak lama lagi penjujukan genom barah akan menjadi amalan klinikal yang meluas, dan berjuta-juta tumor akan dijelaskan. Masalah komputasi besar muncul dalam mentafsirkan data ini. Pertama, kerana kita hanya dapat membaca kepingan DNA yang pendek, kita mempunyai masalah besar untuk mengumpulkan perwakilan genom tumor yang koheren dan boleh dipercayai dari sejumlah besar bukti yang tidak lengkap dan rawan kesalahan. Ini adalah cabaran pertama. Kedua, setiap genom manusia unik sejak lahir, dan setiap tumor adalah varian unik. Tidak ada satu pun jalan menuju barah. Kita mesti belajar membaca pelbagai tanda barah dalam genom tumor dan mengaitkannya dengan rawatan yang optimum. Sudah ada beratus-ratus rawatan sasaran molekul untuk kanser yang tersedia, masing-masing diketahui lebih kurang berkesan bergantung pada varian genetik tertentu. Walau bagaimanapun, menyasarkan gen tunggal dengan satu rawatan jarang berlaku. Cabaran kedua adalah mengatasi kombinasi terapi kombinasi yang diperibadikan, disasarkan, dalam barah. [[EENNDD]] barah"], [{"string": "Stable feature selection via dense feature groups Many feature selection algorithms have been proposed in the past focusing on improving classification accuracy . In this work , we point out the importance of stable feature selection for knowledge discovery from high-dimensional data , and identify two causes of instability of feature selection algorithms : selection of a minimum subset without redundant features and small sample size . We propose a general framework for stable feature selection which emphasizes both good generalization and stability of feature selection results . The framework identifies dense feature groups based on kernel density estimation and treats features in each dense group as a coherent entity for feature selection . An efficient algorithm DRAGS Dense Relevant Attribute Group Selector is developed under this framework . We also introduce a general measure for assessing the stability of feature selection algorithms . Our empirical study based on microarray data verifies that dense feature groups remain stable under random sample hold out , and the DRAGS algorithm is effective in identifying a set of feature groups which exhibit both high classification accuracy and stability .", "keywords": ["high-dimensional data", "classification", "kernel density estimation", "learning", "feature selection", "stability"], "combined": "Stable feature selection via dense feature groups Many feature selection algorithms have been proposed in the past focusing on improving classification accuracy . In this work , we point out the importance of stable feature selection for knowledge discovery from high-dimensional data , and identify two causes of instability of feature selection algorithms : selection of a minimum subset without redundant features and small sample size . We propose a general framework for stable feature selection which emphasizes both good generalization and stability of feature selection results . The framework identifies dense feature groups based on kernel density estimation and treats features in each dense group as a coherent entity for feature selection . An efficient algorithm DRAGS Dense Relevant Attribute Group Selector is developed under this framework . We also introduce a general measure for assessing the stability of feature selection algorithms . Our empirical study based on microarray data verifies that dense feature groups remain stable under random sample hold out , and the DRAGS algorithm is effective in identifying a set of feature groups which exhibit both high classification accuracy and stability . [[EENNDD]] high-dimensional data; classification; kernel density estimation; learning; feature selection; stability"}, "Pemilihan ciri stabil melalui kumpulan ciri padat Banyak algoritma pemilihan ciri telah dicadangkan pada masa lalu yang memfokuskan pada peningkatan ketepatan klasifikasi. Dalam karya ini, kami menunjukkan pentingnya pemilihan ciri stabil untuk penemuan pengetahuan dari data dimensi tinggi, dan mengenal pasti dua penyebab ketidakstabilan algoritma pemilihan ciri: pemilihan subset minimum tanpa ciri berlebihan dan ukuran sampel kecil. Kami mencadangkan kerangka umum untuk pemilihan ciri stabil yang menekankan baik generalisasi dan kestabilan hasil pemilihan ciri. Rangka kerja mengenal pasti kumpulan ciri padat berdasarkan perkiraan kepadatan kernel dan memperlakukan ciri dalam setiap kumpulan padat sebagai entiti koheren untuk pemilihan ciri. Algoritma yang cekap DRAGS Dense Relevant Attribute Group Selector dikembangkan di bawah kerangka ini. Kami juga memperkenalkan ukuran umum untuk menilai kestabilan algoritma pemilihan ciri. Kajian empirik kami berdasarkan data microarray mengesahkan bahawa kumpulan ciri padat tetap stabil di bawah sampel rawak, dan algoritma DRAGS berkesan dalam mengenal pasti sekumpulan kumpulan ciri yang menunjukkan ketepatan dan kestabilan klasifikasi tinggi. [[EENNDD]] data dimensi tinggi; pengelasan; anggaran ketumpatan kernel; belajar; pemilihan ciri; kestabilan"], [{"string": "Na\u00efve filterbots for robust cold-start recommendations The goal of a recommender system is to suggest items of interest to a user based on historical behavior of a community of users . Given detailed enough history , item-based collaborative filtering CF often performs as well or better than almost any other recommendation method . However , in cold-start situations - where a user , an item , or the entire system is new - simple non-personalized recommendations often fare better . We improve the scalability and performance of a previous approach to handling cold-start situations that uses filterbots , or surrogate users that rate items based only on user or item attributes . We show that introducing a very small number of simple filterbots helps make CF algorithms more robust . In particular , adding just seven global filterbots improves both user-based and item-based CF in cold-start user , cold-start item , and cold-start system settings . Performance is better when data is scarce , performance is no worse when data is plentiful , and algorithm efficiency is negligibly affected . We systematically compare a non-personalized baseline , user-based CF , item-based CF , and our bot-augmented user - and item-based CF algorithms using three data sets Yahoo ! Movies , MovieLens , and EachMovie with the normalized MAE metric in three types of cold-start situations . The advantage of our `` na\u00efve filterbot '' approach is most pronounced for the Yahoo ! data , the sparsest of the three data sets .", "keywords": ["hybrid content and collaborative filtering", "collaborative filtering", "cold start", "robustness", "learning", "performance analysis", "na\u00efve filterbots", "recommender systems"], "combined": "Na\u00efve filterbots for robust cold-start recommendations The goal of a recommender system is to suggest items of interest to a user based on historical behavior of a community of users . Given detailed enough history , item-based collaborative filtering CF often performs as well or better than almost any other recommendation method . However , in cold-start situations - where a user , an item , or the entire system is new - simple non-personalized recommendations often fare better . We improve the scalability and performance of a previous approach to handling cold-start situations that uses filterbots , or surrogate users that rate items based only on user or item attributes . We show that introducing a very small number of simple filterbots helps make CF algorithms more robust . In particular , adding just seven global filterbots improves both user-based and item-based CF in cold-start user , cold-start item , and cold-start system settings . Performance is better when data is scarce , performance is no worse when data is plentiful , and algorithm efficiency is negligibly affected . We systematically compare a non-personalized baseline , user-based CF , item-based CF , and our bot-augmented user - and item-based CF algorithms using three data sets Yahoo ! Movies , MovieLens , and EachMovie with the normalized MAE metric in three types of cold-start situations . The advantage of our `` na\u00efve filterbot '' approach is most pronounced for the Yahoo ! data , the sparsest of the three data sets . [[EENNDD]] hybrid content and collaborative filtering; collaborative filtering; cold start; robustness; learning; performance analysis; na\u00efve filterbots; recommender systems"}, "Botol penapis naif untuk cadangan permulaan yang baik Matlamat sistem pengesyorkan adalah untuk mencadangkan item yang menarik kepada pengguna berdasarkan tingkah laku sejarah komuniti pengguna. Memandangkan sejarah yang cukup terperinci, penapisan kolaboratif berasaskan item sering dilakukan dengan baik atau lebih baik daripada kaedah cadangan lain. Walau bagaimanapun, dalam situasi permulaan yang sejuk - di mana pengguna, item, atau keseluruhan sistem baru - cadangan mudah yang tidak diperibadikan sering kali lebih baik. Kami meningkatkan skalabilitas dan prestasi pendekatan sebelumnya untuk menangani situasi mulai sejuk yang menggunakan bot filter, atau pengganti pengguna yang menilai item hanya berdasarkan atribut pengguna atau item. Kami menunjukkan bahawa memperkenalkan sebilangan kecil bot filter mudah membantu menjadikan algoritma CF lebih mantap. Khususnya, menambahkan hanya tujuh bot filter global meningkatkan CF berdasarkan pengguna dan item pada tetapan awal pengguna, item permulaan sejuk, dan tetapan sistem permulaan sejuk. Prestasi lebih baik apabila data kekurangan, prestasi tidak lebih buruk apabila data banyak, dan kecekapan algoritma terjejas. Kami secara sistematik membandingkan garis dasar yang tidak diperibadikan, CF berdasarkan pengguna, CF berasaskan item, dan pengguna bot kami yang ditambah - dan algoritma CF berasaskan item menggunakan tiga set data Yahoo! Filem, MovieLens, dan EveryMovie dengan metrik MAE yang dinormalisasi dalam tiga jenis situasi permulaan yang sejuk. Kelebihan pendekatan \"naif filterbot\" kami paling ketara untuk Yahoo! data, paling jarang dari tiga set data. [[EENNDD]] kandungan hibrid dan penapisan kolaboratif; penapisan kolaboratif; permulaan sejuk; ketahanan; belajar; analisis prestasi; bot penapis naif; sistem cadangan"], [{"string": "Evaluating the novelty of text-mined rules using lexical knowledge In this paper , we present a new method of estimating the novelty of rules discovered by data-mining methods using WordNet , a lexical knowledge-base of English words . We assess the novelty of a rule by the average semantic distance in a knowledge hierarchy between the words in the antecedent and the consequent of the rule - the more the average distance , more is the novelty of the rule . The novelty of rules extracted by the DiscoTEX text-mining system on Amazon.com book descriptions were evaluated by both human subjects and by our algorithm . By computing correlation coefficients between pairs of human ratings and between human and automatic ratings , we found that the automatic scoring of rules based on our novelty measure correlates with human judgments about as well as human judgments correlate with one another . @Text mining", "keywords": ["interesting rules", "knowledge hierarchy", "novelty", "semantic distance", "wordnet"], "combined": "Evaluating the novelty of text-mined rules using lexical knowledge In this paper , we present a new method of estimating the novelty of rules discovered by data-mining methods using WordNet , a lexical knowledge-base of English words . We assess the novelty of a rule by the average semantic distance in a knowledge hierarchy between the words in the antecedent and the consequent of the rule - the more the average distance , more is the novelty of the rule . The novelty of rules extracted by the DiscoTEX text-mining system on Amazon.com book descriptions were evaluated by both human subjects and by our algorithm . By computing correlation coefficients between pairs of human ratings and between human and automatic ratings , we found that the automatic scoring of rules based on our novelty measure correlates with human judgments about as well as human judgments correlate with one another . @Text mining [[EENNDD]] interesting rules; knowledge hierarchy; novelty; semantic distance; wordnet"}, "Mengevaluasi kebaruan peraturan yang dilampirkan teks menggunakan pengetahuan leksikal Dalam makalah ini, kami menyajikan kaedah baru untuk menganggarkan kebaruan peraturan yang dijumpai dengan kaedah perlombongan data menggunakan WordNet, pangkalan pengetahuan leksikal kata-kata bahasa Inggeris. Kami menilai kebaruan peraturan dengan jarak semantik rata-rata dalam hierarki pengetahuan antara kata-kata di anteseden dan akibat peraturan - semakin jauh jarak rata-rata, semakin banyak kebaruan peraturan. Kebaruan peraturan yang diekstrak oleh sistem perlombongan teks DiscoTEX pada keterangan buku Amazon.com dinilai oleh subjek manusia dan juga algoritma kami. Dengan mengira pekali korelasi antara pasangan penilaian manusia dan antara penilaian manusia dan automatik, kami mendapati bahawa skor automatik peraturan berdasarkan ukuran kebaruan kami berkorelasi dengan penilaian manusia tentang dan penilaian manusia saling berkaitan antara satu sama lain. @Teks perlombongan [[EENNDD]] peraturan menarik; hierarki pengetahuan; kebaharuan; jarak semantik; jaring perkataan"], [{"string": "Single-pass online learning : performance , voting schemes and online feature selection To learn concepts over massive data streams , it is essential to design inference and learning methods that operate in real time with limited memory . Online learning methods such as perceptron or Winnow are naturally suited to stream processing ; however , in practice multiple passes over the same training data are required to achieve accuracy comparable to state-of-the-art batch learners . In the current work we address the problem of training an on-line learner with a single passover the data . We evaluate several existing methods , and also propose a new modification of Margin Balanced Winnow , which has performance comparable to linear SVM . We also explore the effect of averaging , a.k.a. voting , on online learning . Finally , we describe how the new Modified Margin Balanced Winnow algorithm can be naturally adapted to perform feature selection . This scheme performs comparably to widely-used batch feature selection methods like information gain or Chi-square , with the advantage of being able to select features on-the-fly . Taken together , these techniques allow single-pass online learning to be competitive with batch techniques , and still maintain the advantages of on-line learning .", "keywords": ["learning", "winnow", "averaging", "online learning", "voting", "models"], "combined": "Single-pass online learning : performance , voting schemes and online feature selection To learn concepts over massive data streams , it is essential to design inference and learning methods that operate in real time with limited memory . Online learning methods such as perceptron or Winnow are naturally suited to stream processing ; however , in practice multiple passes over the same training data are required to achieve accuracy comparable to state-of-the-art batch learners . In the current work we address the problem of training an on-line learner with a single passover the data . We evaluate several existing methods , and also propose a new modification of Margin Balanced Winnow , which has performance comparable to linear SVM . We also explore the effect of averaging , a.k.a. voting , on online learning . Finally , we describe how the new Modified Margin Balanced Winnow algorithm can be naturally adapted to perform feature selection . This scheme performs comparably to widely-used batch feature selection methods like information gain or Chi-square , with the advantage of being able to select features on-the-fly . Taken together , these techniques allow single-pass online learning to be competitive with batch techniques , and still maintain the advantages of on-line learning . [[EENNDD]] learning; winnow; averaging; online learning; voting; models"}, "Pembelajaran dalam talian pas tunggal: prestasi, skema pengundian dan pemilihan ciri dalam talian Untuk mempelajari konsep melalui aliran data yang besar, sangat mustahak untuk merancang inferensi dan kaedah pembelajaran yang beroperasi dalam masa nyata dengan memori terhad. Kaedah pembelajaran dalam talian seperti perceptron atau Winnow secara semula jadi sesuai untuk memproses aliran; namun, dalam praktiknya banyak lulus data latihan yang sama diperlukan untuk mencapai ketepatan yang setanding dengan pelajar kumpulan canggih. Dalam karya semasa kami menangani masalah melatih pelajar on-line dengan satu data paskah. Kami menilai beberapa kaedah yang ada, dan juga mencadangkan modifikasi baru Margin Balanced Winnow, yang mempunyai prestasi setanding dengan SVM linear. Kami juga meneroka kesan rata-rata, sebilangan suara, terhadap pembelajaran dalam talian. Akhirnya, kami menerangkan bagaimana algoritma Modified Margin Balanced Winnow yang baru dapat disesuaikan secara semula jadi untuk melakukan pemilihan ciri. Skema ini berfungsi setanding dengan kaedah pemilihan ciri kumpulan yang banyak digunakan seperti perolehan maklumat atau Chi-square, dengan kelebihan dapat memilih fitur dalam perjalanan. Secara keseluruhan, teknik ini membolehkan pembelajaran dalam talian lulus bersaing dengan teknik kumpulan, dan masih mengekalkan kelebihan pembelajaran dalam talian. [[EENNDD]] pembelajaran; winnow; purata; pembelajaran dalam talian; mengundi; model"], [{"string": "Unsupervised learning on k-partite graphs Various data mining applications involve data objects of multiple types that are related to each other , which can be naturally formulated as a k-partite graph . However , the research on mining the hidden structures from a k-partite graph is still limited and preliminary . In this paper , we propose a general model , the relation summary network , to find the hidden structures the local cluster structures and the global community structures from a k-partite graph . The model provides a principal framework for unsupervised learning on k-partite graphs of various structures . Under this model , we derive a novel algorithm to identify the hidden structures of a k-partite graph by constructing a relation summary network to approximate the original k-partite graph under a broad range of distortion measures . Experiments on both synthetic and real datasets demonstrate the promise and effectiveness of the proposed model and algorithm . We also establish the connections between existing clustering approaches and the proposed model to provide a unified view to the clustering approaches .", "keywords": ["k-partite graph", "bregman divergence", "unsupervised learning", "relation summary network"], "combined": "Unsupervised learning on k-partite graphs Various data mining applications involve data objects of multiple types that are related to each other , which can be naturally formulated as a k-partite graph . However , the research on mining the hidden structures from a k-partite graph is still limited and preliminary . In this paper , we propose a general model , the relation summary network , to find the hidden structures the local cluster structures and the global community structures from a k-partite graph . The model provides a principal framework for unsupervised learning on k-partite graphs of various structures . Under this model , we derive a novel algorithm to identify the hidden structures of a k-partite graph by constructing a relation summary network to approximate the original k-partite graph under a broad range of distortion measures . Experiments on both synthetic and real datasets demonstrate the promise and effectiveness of the proposed model and algorithm . We also establish the connections between existing clustering approaches and the proposed model to provide a unified view to the clustering approaches . [[EENNDD]] k-partite graph; bregman divergence; unsupervised learning; relation summary network"}, "Pembelajaran tanpa pengawasan pada grafik k-partite Pelbagai aplikasi perlombongan data melibatkan objek data dari pelbagai jenis yang saling berkaitan, yang secara semula jadi dapat dirumuskan sebagai grafik k-partite. Walau bagaimanapun, penyelidikan untuk melombong struktur tersembunyi dari graf k-partite masih terhad dan awal. Dalam makalah ini, kami mencadangkan model umum, rangkaian ringkasan hubungan, untuk mencari struktur tersembunyi struktur kluster tempatan dan struktur komuniti global dari grafik k-partite. Model ini menyediakan kerangka utama untuk pembelajaran tanpa pengawasan pada grafik k-partite dari pelbagai struktur. Di bawah model ini, kami memperoleh algoritma baru untuk mengenal pasti struktur grafik k-partite yang tersembunyi dengan membina rangkaian ringkasan hubungan untuk menghampiri grafik k-partit yang asal di bawah pelbagai langkah penyelewengan. Eksperimen pada kedua-dua kumpulan data sintetik dan nyata menunjukkan janji dan keberkesanan model dan algoritma yang dicadangkan. Kami juga menjalin hubungan antara pendekatan pengelompokan yang ada dan model yang diusulkan untuk memberikan pandangan terpadu untuk pendekatan pengelompokan. [[EENNDD]] graf k-partite; perbezaan bregman; pembelajaran tanpa pengawasan; rangkaian ringkasan hubungan"], [{"string": "PET : a statistical model for popular events tracking in social communities User generated information in online communities has been characterized with the mixture of a text stream and a network structure both changing over time . A good example is a web-blogging community with the daily blog posts and a social network of bloggers . An important task of analyzing an online community is to observe and track the popular events , or topics that evolve over time in the community . Existing approaches usually focus on either the burstiness of topics or the evolution of networks , but ignoring the interplay between textual topics and network structures . In this paper , we formally define the problem of popular event tracking in online communities PET , focusing on the interplay between texts and networks . We propose a novel statistical method that models the the popularity of events over time , taking into consideration the burstiness of user interest , information diffusion on the network structure , and the evolution of textual topics . Specifically , a Gibbs Random Field is defined to model the influence of historic status and the dependency relationships in the graph ; thereafter a topic model generates the words in text content of the event , regularized by the Gibbs Random Field . We prove that two classic models in information diffusion and text burstiness are special cases of our model under certain situations . Empirical experiments with two different communities and datasets i.e. , Twitter and DBLP show that our approach is effective and outperforms existing approaches .", "keywords": ["topic modeling", "social communities", "pet", "popular events tracking"], "combined": "PET : a statistical model for popular events tracking in social communities User generated information in online communities has been characterized with the mixture of a text stream and a network structure both changing over time . A good example is a web-blogging community with the daily blog posts and a social network of bloggers . An important task of analyzing an online community is to observe and track the popular events , or topics that evolve over time in the community . Existing approaches usually focus on either the burstiness of topics or the evolution of networks , but ignoring the interplay between textual topics and network structures . In this paper , we formally define the problem of popular event tracking in online communities PET , focusing on the interplay between texts and networks . We propose a novel statistical method that models the the popularity of events over time , taking into consideration the burstiness of user interest , information diffusion on the network structure , and the evolution of textual topics . Specifically , a Gibbs Random Field is defined to model the influence of historic status and the dependency relationships in the graph ; thereafter a topic model generates the words in text content of the event , regularized by the Gibbs Random Field . We prove that two classic models in information diffusion and text burstiness are special cases of our model under certain situations . Empirical experiments with two different communities and datasets i.e. , Twitter and DBLP show that our approach is effective and outperforms existing approaches . [[EENNDD]] topic modeling; social communities; pet; popular events tracking"}, "PET: model statistik untuk penjejakan peristiwa popular di komuniti sosial Maklumat yang dihasilkan pengguna dalam komuniti dalam talian telah dicirikan dengan campuran aliran teks dan struktur rangkaian yang kedua-duanya berubah dari masa ke masa. Contoh yang baik adalah komuniti blog-blog dengan catatan blog harian dan rangkaian sosial blogger. Tugas penting untuk menganalisis komuniti dalam talian adalah untuk memerhatikan dan mengesan peristiwa popular, atau topik yang berkembang dari masa ke masa dalam komuniti. Pendekatan yang ada biasanya memusatkan perhatian pada topik atau evolusi rangkaian, tetapi mengabaikan interaksi antara topik teks dan struktur rangkaian. Dalam makalah ini, kami secara formal mendefinisikan masalah pengesanan acara popular di komuniti dalam talian PET, dengan fokus pada interaksi antara teks dan rangkaian. Kami mencadangkan kaedah statistik baru yang memodelkan populariti peristiwa dari masa ke masa, dengan mempertimbangkan banyaknya minat pengguna, penyebaran maklumat mengenai struktur rangkaian, dan evolusi topik teks. Secara khusus, Medan Rawak Gibbs didefinisikan untuk memodelkan pengaruh status bersejarah dan hubungan ketergantungan dalam grafik; selepas itu model topik menghasilkan kata-kata dalam kandungan teks acara tersebut, yang diatur oleh Gibbs Random Field. Kami membuktikan bahawa dua model klasik dalam penyebaran maklumat dan kekaburan teks adalah kes khas model kami dalam situasi tertentu. Eksperimen empirikal dengan dua komuniti dan kumpulan data yang berbeza, iaitu Twitter dan DBLP menunjukkan bahawa pendekatan kami berkesan dan mengatasi pendekatan yang ada. [[EENNDD]] pemodelan topik; komuniti sosial; haiwan peliharaan; penjejakan acara popular"], [{"string": "Knowledge-based data mining We describe techniques for combining two types of knowledge systems : expert and machine learning . Both the expert system and the learning system represent information by logical decision rules or trees . Unlike the classical views of knowledge-base evaluation or refinement , our view accepts the contents of the knowledge base as completely correct . The knowledge base and the results of its stored cases will provide direction for the discovery of new relationships in the form of newly induced decision rules . An expert system called SEAS was built to discover sales leads for computer products and solutions . The system interviews executives by asking questions , and based on the responses , recommends products that may improve a business ' operations . Leveraging this expert system , we record the results of the interviews and the program 's recommendations . The very same data stored by the expert system is used to find new predictive rules . Among the potential advantages of this approach are a the capability to spot new sales trends and b the substitution of less expensive probabilistic rules that use database data instead of interviews .", "keywords": ["decision rule induction", "expert systems", "database applications", "sales leads"], "combined": "Knowledge-based data mining We describe techniques for combining two types of knowledge systems : expert and machine learning . Both the expert system and the learning system represent information by logical decision rules or trees . Unlike the classical views of knowledge-base evaluation or refinement , our view accepts the contents of the knowledge base as completely correct . The knowledge base and the results of its stored cases will provide direction for the discovery of new relationships in the form of newly induced decision rules . An expert system called SEAS was built to discover sales leads for computer products and solutions . The system interviews executives by asking questions , and based on the responses , recommends products that may improve a business ' operations . Leveraging this expert system , we record the results of the interviews and the program 's recommendations . The very same data stored by the expert system is used to find new predictive rules . Among the potential advantages of this approach are a the capability to spot new sales trends and b the substitution of less expensive probabilistic rules that use database data instead of interviews . [[EENNDD]] decision rule induction; expert systems; database applications; sales leads"}, "Perlombongan data berasaskan pengetahuan Kami menerangkan teknik untuk menggabungkan dua jenis sistem pengetahuan: pembelajaran pakar dan mesin. Kedua-dua sistem pakar dan sistem pembelajaran mewakili maklumat dengan peraturan atau pokok keputusan yang logik. Tidak seperti pandangan klasik penilaian atau penyempurnaan pangkalan pengetahuan, pandangan kami menerima isi pangkalan pengetahuan sebagai benar. Pangkalan pengetahuan dan hasil kes-kes yang tersimpan akan memberikan petunjuk untuk penemuan hubungan baru dalam bentuk peraturan keputusan yang baru diinduksi. Sistem pakar yang disebut SEAS dibina untuk mencari petunjuk penjualan produk dan penyelesaian komputer. Sistem ini mewawancarai para eksekutif dengan mengajukan pertanyaan, dan berdasarkan respons, mengesyorkan produk yang dapat meningkatkan operasi perniagaan. Dengan memanfaatkan sistem pakar ini, kami mencatat hasil wawancara dan cadangan program. Data yang sama yang disimpan oleh sistem pakar digunakan untuk mencari peraturan ramalan baru. Antara kelebihan potensi pendekatan ini adalah kemampuan untuk melihat trend penjualan baru dan penggantian peraturan probabilistik yang lebih murah yang menggunakan data pangkalan data dan bukannya wawancara. [[EENNDD]] arahan keputusan keputusan; sistem pakar; aplikasi pangkalan data; petunjuk penjualan"], [{"string": "XRules : an effective structural classifier for XML data XML documents have recently become ubiquitous because of their varied applicability in a number of applications . Classification is an important problem in the data mining domain , but current classification methods for XML documents use IR-based methods in which each document is treated as a bag of words . Such techniques ignore a significant amount of information hidden inside the documents . In this paper we discuss the problem of rule based classification of XML data by using frequent discriminatory substructures within XML documents . Such a technique is more capable of finding the classification characteristics of documents . In addition , the technique can also be extended to cost sensitive classification . We show the effectiveness of the method with respect to other classifiers . We note that the methodology discussed in this paper is applicable to any kind of semi-structured data .", "keywords": ["xml/semi-structured data", "database applications", "tree mining", "classification"], "combined": "XRules : an effective structural classifier for XML data XML documents have recently become ubiquitous because of their varied applicability in a number of applications . Classification is an important problem in the data mining domain , but current classification methods for XML documents use IR-based methods in which each document is treated as a bag of words . Such techniques ignore a significant amount of information hidden inside the documents . In this paper we discuss the problem of rule based classification of XML data by using frequent discriminatory substructures within XML documents . Such a technique is more capable of finding the classification characteristics of documents . In addition , the technique can also be extended to cost sensitive classification . We show the effectiveness of the method with respect to other classifiers . We note that the methodology discussed in this paper is applicable to any kind of semi-structured data . [[EENNDD]] xml/semi-structured data; database applications; tree mining; classification"}, "XRules: pengklasifikasi struktur yang berkesan untuk data XML Dokumen XML baru-baru ini menjadi banyak di mana-mana kerana kebolehgunaannya yang bervariasi dalam sejumlah aplikasi. Klasifikasi adalah masalah penting dalam domain perlombongan data, tetapi kaedah klasifikasi semasa untuk dokumen XML menggunakan kaedah berdasarkan IR di mana setiap dokumen dianggap sebagai beg kata. Teknik sedemikian mengabaikan sebilangan besar maklumat yang tersembunyi di dalam dokumen. Dalam makalah ini kita membincangkan masalah pengkelasan data XML berdasarkan peraturan dengan menggunakan substruktur diskriminasi yang kerap dalam dokumen XML. Teknik sedemikian lebih mampu mencari ciri klasifikasi dokumen. Selain itu, teknik ini juga dapat diperluas ke klasifikasi sensitif terhadap kos. Kami menunjukkan keberkesanan kaedah berkenaan dengan pengelasan lain. Kami perhatikan bahawa metodologi yang dibincangkan dalam makalah ini berlaku untuk semua jenis data separa berstruktur. [[EENNDD]] xml / separa berstruktur data; aplikasi pangkalan data; perlombongan pokok; pengelasan"], [{"string": "Fast mining of high dimensional expressive contrast patterns using zero-suppressed binary decision diagrams Patterns of contrast are a very important way of comparing multi-dimensional datasets . Such patterns are able to capture regions of high difference between two classes of data , and are useful for human experts and the construction of classifiers . However , mining such patterns is particularly challenging when the number of dimensions is large . This paper describes a new technique for mining several varieties of contrast pattern , based on the use of Zero-Suppressed Binary Decision Diagrams ZBDDs , a powerful data structure for manipulating sparse data . We study the mining of both simple contrast patterns , such as emerging patterns , and more novel and complex contrasts , which we call disjunctive emerging patterns . A performance study demonstrates our ZBDD technique is highly scalable , substantially improves on state of the art mining for emerging patterns and can be effective for discovering complex contrasts from datasets with thousands of attributes .", "keywords": ["zero-suppressed binary decision diagrams", "contrast patterns", "disjunctive emerging patterns"], "combined": "Fast mining of high dimensional expressive contrast patterns using zero-suppressed binary decision diagrams Patterns of contrast are a very important way of comparing multi-dimensional datasets . Such patterns are able to capture regions of high difference between two classes of data , and are useful for human experts and the construction of classifiers . However , mining such patterns is particularly challenging when the number of dimensions is large . This paper describes a new technique for mining several varieties of contrast pattern , based on the use of Zero-Suppressed Binary Decision Diagrams ZBDDs , a powerful data structure for manipulating sparse data . We study the mining of both simple contrast patterns , such as emerging patterns , and more novel and complex contrasts , which we call disjunctive emerging patterns . A performance study demonstrates our ZBDD technique is highly scalable , substantially improves on state of the art mining for emerging patterns and can be effective for discovering complex contrasts from datasets with thousands of attributes . [[EENNDD]] zero-suppressed binary decision diagrams; contrast patterns; disjunctive emerging patterns"}, "Perlombongan pantas corak kontras ekspresif dimensi tinggi menggunakan gambar rajah keputusan binari yang ditindas sifar Pola kontras adalah kaedah yang sangat penting untuk membandingkan set data pelbagai dimensi. Corak sedemikian dapat menangkap kawasan yang mempunyai perbezaan tinggi antara dua kelas data, dan berguna untuk pakar manusia dan pembinaan pengkelasan. Walau bagaimanapun, perlombongan corak seperti ini sangat mencabar apabila jumlah dimensi besar. Makalah ini menerangkan teknik baru untuk melombong beberapa jenis corak kontras, berdasarkan penggunaan Diagram Keputusan Perduaan Sifar ZBDD, struktur data yang kuat untuk memanipulasi data yang jarang. Kami mengkaji perlombongan kedua-dua corak kontras sederhana, seperti corak baru muncul, dan lebih banyak kontras yang baru dan kompleks, yang kami sebut sebagai pola muncul yang tidak berfungsi. Satu kajian prestasi menunjukkan teknik ZBDD kami sangat berskala, meningkatkan penambangan terkini untuk corak yang baru muncul dan boleh berkesan untuk menemui perbezaan yang kompleks dari kumpulan data dengan ribuan atribut. [[EENNDD]] gambar rajah keputusan binari sifar; corak kontras; corak muncul yang tidak berfungsi"], [{"string": "Efficient kernel feature extraction for massive data sets Maximum margin discriminant analysis MMDA was proposed that uses the margin idea for feature extraction . It often outperforms traditional methods like kernel principal component analysis KPCA and kernel Fisher discriminant analysis KFD . However , as in other kernel methods , its time complexity is cubic in the number of training points m , and is thus computationally inefficient on massive data sets . In this paper , we propose an 1 + \u03b5 2-approximation algorithm for obtaining the MMDA features by extending the core vector machines . The resultant time complexity is only linear in m , while its space complexity is independent of m. Extensive comparisons with the original MMDA , KPCA , and KFD on a number of large data sets show that the proposed feature extractor can improve classification accuracy , and is also faster than these kernel-based methods by more than an order of magnitude .", "keywords": ["kernel feature", "svm", "learning", "scalability", "design methodology", "extraction"], "combined": "Efficient kernel feature extraction for massive data sets Maximum margin discriminant analysis MMDA was proposed that uses the margin idea for feature extraction . It often outperforms traditional methods like kernel principal component analysis KPCA and kernel Fisher discriminant analysis KFD . However , as in other kernel methods , its time complexity is cubic in the number of training points m , and is thus computationally inefficient on massive data sets . In this paper , we propose an 1 + \u03b5 2-approximation algorithm for obtaining the MMDA features by extending the core vector machines . The resultant time complexity is only linear in m , while its space complexity is independent of m. Extensive comparisons with the original MMDA , KPCA , and KFD on a number of large data sets show that the proposed feature extractor can improve classification accuracy , and is also faster than these kernel-based methods by more than an order of magnitude . [[EENNDD]] kernel feature; svm; learning; scalability; design methodology; extraction"}, "Pengekstrakan ciri kernel yang cekap untuk kumpulan data besar Analisis diskriminasi margin maksimum MMDA dicadangkan yang menggunakan idea margin untuk pengekstrakan ciri. Selalunya mengungguli kaedah tradisional seperti analisis komponen utama kernel KPCA dan analisis diskriminan kernel Fisher KFD. Namun, seperti dalam kaedah kernel lain, kerumitan waktunya adalah kubik dalam jumlah titik latihan m, dan dengan itu tidak efisien secara komputasi pada set data yang besar. Dalam makalah ini, kami mencadangkan algoritma penghampiran 1 + \u03b5 2 untuk memperoleh ciri MMDA dengan memperluas mesin vektor teras. Kerumitan masa yang dihasilkan hanya linear dalam m, sementara kerumitan ruangnya tidak bergantung pada m. Perbandingan yang luas dengan MMDA, KPCA, dan KFD yang asli pada sejumlah set data yang besar menunjukkan bahawa pengekstrak ciri yang dicadangkan dapat meningkatkan ketepatan klasifikasi, dan juga lebih cepat daripada kaedah berasaskan kernel ini dengan lebih dari satu urutan besarnya. [[EENNDD]] ciri kernel; svm; belajar; skalabiliti; metodologi reka bentuk; pengekstrakan"], [{"string": "Mining uncertain data with probabilistic guarantees Data uncertainty is inherent in applications such as sensor monitoring systems , location-based services , and biological databases . To manage this vast amount of imprecise information , probabilistic databases have been recently developed . In this paper , we study the discovery of frequent patterns and association rules from probabilistic data under the Possible World Semantics . This is technically challenging , since a probabilistic database can have an exponential number of possible worlds . We propose two effcient algorithms , which discover frequent patterns in bottom-up and top-down manners . Both algorithms can be easily extended to discover maximal frequent patterns . We also explain how to use these patterns to generate association rules . Extensive experiments , using real and synthetic datasets , were conducted to validate the performance of our methods .", "keywords": ["uncertain data", "frequent pattern", "association rule"], "combined": "Mining uncertain data with probabilistic guarantees Data uncertainty is inherent in applications such as sensor monitoring systems , location-based services , and biological databases . To manage this vast amount of imprecise information , probabilistic databases have been recently developed . In this paper , we study the discovery of frequent patterns and association rules from probabilistic data under the Possible World Semantics . This is technically challenging , since a probabilistic database can have an exponential number of possible worlds . We propose two effcient algorithms , which discover frequent patterns in bottom-up and top-down manners . Both algorithms can be easily extended to discover maximal frequent patterns . We also explain how to use these patterns to generate association rules . Extensive experiments , using real and synthetic datasets , were conducted to validate the performance of our methods . [[EENNDD]] uncertain data; frequent pattern; association rule"}, "Melombong data tidak pasti dengan jaminan probabilistik Ketidakpastian data wujud dalam aplikasi seperti sistem pemantauan sensor, perkhidmatan berdasarkan lokasi, dan pangkalan data biologi. Untuk menguruskan sejumlah besar maklumat tidak tepat ini, pangkalan data probabilistik baru-baru ini dikembangkan. Dalam makalah ini, kami mengkaji penemuan corak dan peraturan pergaulan yang kerap dari data probabilistik di bawah Kemungkinan Dunia Semantik. Ini secara teknikalnya mencabar, kerana pangkalan data probabilistik dapat memiliki sejumlah dunia yang mungkin. Kami mencadangkan dua algoritma berkesan, yang menemui corak yang kerap dengan cara bawah-atas dan atas-bawah. Kedua-dua algoritma dapat diperluas dengan mudah untuk menemui corak kekerapan maksimum. Kami juga menerangkan bagaimana menggunakan corak ini untuk menghasilkan peraturan pergaulan. Eksperimen ekstensif, menggunakan set data nyata dan sintetik, dilakukan untuk mengesahkan prestasi kaedah kami. [[EENNDD]] data tidak pasti; corak kerap; peraturan persatuan"], [{"string": "Combining proactive and reactive predictions for data streams Mining data streams is important in both science and commerce . Two major challenges are 1 the data may grow without limit so that it is difficult to retain a long history ; and 2 the underlying concept of the data may change over time . Different from common practice that keeps recent raw data , this paper uses a measure of conceptual equivalence to organize the data history into a history of concepts . Along the journey of concept change , it identifies new concepts as well as re-appearing ones , and learns transition patterns among concepts to help prediction . Different from conventional methodology that passively waits until the concept changes , this paper incorporates proactive and reactive predictions . In a proactive mode , it anticipates what the new concept will be if a future concept change takes place , and prepares prediction strategies in advance . If the anticipation turns out to be correct , a proper prediction model can be launched instantly upon the concept change . If not , it promptly resorts to a reactive mode : adapting a prediction model to the new data . A system RePro is proposed to implement these new ideas . Experiments compare the system with representative existing prediction methods on various benchmark data sets that represent diversified scenarios of concept change . Empirical evidence demonstrates that the proposed methodology is an effective and efficient solution to prediction for data streams .", "keywords": ["conceptual equivalence", "proactive learning", "data stream"], "combined": "Combining proactive and reactive predictions for data streams Mining data streams is important in both science and commerce . Two major challenges are 1 the data may grow without limit so that it is difficult to retain a long history ; and 2 the underlying concept of the data may change over time . Different from common practice that keeps recent raw data , this paper uses a measure of conceptual equivalence to organize the data history into a history of concepts . Along the journey of concept change , it identifies new concepts as well as re-appearing ones , and learns transition patterns among concepts to help prediction . Different from conventional methodology that passively waits until the concept changes , this paper incorporates proactive and reactive predictions . In a proactive mode , it anticipates what the new concept will be if a future concept change takes place , and prepares prediction strategies in advance . If the anticipation turns out to be correct , a proper prediction model can be launched instantly upon the concept change . If not , it promptly resorts to a reactive mode : adapting a prediction model to the new data . A system RePro is proposed to implement these new ideas . Experiments compare the system with representative existing prediction methods on various benchmark data sets that represent diversified scenarios of concept change . Empirical evidence demonstrates that the proposed methodology is an effective and efficient solution to prediction for data streams . [[EENNDD]] conceptual equivalence; proactive learning; data stream"}, "Menggabungkan ramalan proaktif dan reaktif untuk aliran data Penambangan aliran data penting dalam sains dan perdagangan. Dua cabaran utama adalah 1 data dapat berkembang tanpa had sehingga sukar untuk mengekalkan sejarah yang panjang; dan 2 konsep data yang mendasari mungkin berubah dari masa ke masa. Berlainan dengan amalan biasa yang menyimpan data mentah baru-baru ini, makalah ini menggunakan ukuran kesetaraan konsep untuk mengatur sejarah data menjadi sejarah konsep. Sepanjang perjalanan perubahan konsep, ia mengenal pasti konsep baru dan juga muncul kembali, dan mempelajari pola peralihan antara konsep untuk membantu ramalan. Berbeza dengan metodologi konvensional yang secara pasif menunggu sehingga konsep berubah, makalah ini menggabungkan ramalan proaktif dan reaktif. Dalam mod proaktif, ia mengantisipasi konsep baru jika perubahan konsep di masa depan berlaku, dan menyiapkan strategi ramalan terlebih dahulu. Sekiranya jangkaan itu benar, model ramalan yang tepat dapat dilancarkan serta-merta setelah perubahan konsep. Sekiranya tidak, ia segera beralih ke mod reaktif: menyesuaikan model ramalan dengan data baru. Sistem RePro dicadangkan untuk menerapkan idea-idea baru ini. Eksperimen membandingkan sistem dengan kaedah ramalan sedia ada yang representatif pada pelbagai set data penanda aras yang mewakili pelbagai senario perubahan konsep. Bukti empirikal menunjukkan bahawa metodologi yang dicadangkan adalah penyelesaian yang berkesan dan efisien untuk ramalan untuk aliran data. [[EENNDD]] kesetaraan konsep; pembelajaran proaktif; aliran data"], [{"string": "Exploiting unlabeled data in ensemble methods An adaptive semi-supervised ensemble method , ASSEMBLE , is proposed that constructs classification ensembles based on both labeled and unlabeled data . ASSEMBLE alternates between assigning `` pseudo-classes '' to the unlabeled data using the existing ensemble and constructing the next base classifier using both the labeled and pseudolabeled data . Mathematically , this intuitive algorithm corresponds to maximizing the classification margin in hypothesis space as measured on both the labeled and unlabeled of data . Unlike alternative approaches , ASSEMBLE does not require a semi-supervised learning method for the base classifier . ASSEMBLE can be used in conjunction with any cost-sensitive classification algorithm for both two-class and multi-class problems . ASSEMBLE using decision trees won the NIPS 2001 Unlabeled Data Competition . In addition , strong results on several benchmark datasets using both decision trees and neural networks support the proposed method .", "keywords": ["classification", "ensemble learning", "self-modifying machines", "boosting", "semi-supervised learning"], "combined": "Exploiting unlabeled data in ensemble methods An adaptive semi-supervised ensemble method , ASSEMBLE , is proposed that constructs classification ensembles based on both labeled and unlabeled data . ASSEMBLE alternates between assigning `` pseudo-classes '' to the unlabeled data using the existing ensemble and constructing the next base classifier using both the labeled and pseudolabeled data . Mathematically , this intuitive algorithm corresponds to maximizing the classification margin in hypothesis space as measured on both the labeled and unlabeled of data . Unlike alternative approaches , ASSEMBLE does not require a semi-supervised learning method for the base classifier . ASSEMBLE can be used in conjunction with any cost-sensitive classification algorithm for both two-class and multi-class problems . ASSEMBLE using decision trees won the NIPS 2001 Unlabeled Data Competition . In addition , strong results on several benchmark datasets using both decision trees and neural networks support the proposed method . [[EENNDD]] classification; ensemble learning; self-modifying machines; boosting; semi-supervised learning"}, "Mengeksploitasi data tanpa label dalam kaedah ensemble Kaedah ensemble separa penyeliaan adaptif, ASSEMBLE, dicadangkan untuk membina ensembel klasifikasi berdasarkan kedua-dua data berlabel dan tidak berlabel. ASSEMBLE bergantian antara menetapkan \"kelas pseudo\" kepada data yang tidak berlabel menggunakan ensemble yang ada dan membina pengkelasan asas seterusnya menggunakan data berlabel dan pseudolabeled. Secara matematik, algoritma intuitif ini sesuai dengan memaksimumkan margin klasifikasi di ruang hipotesis seperti yang diukur pada data berlabel dan tidak berlabel. Tidak seperti pendekatan alternatif, PERSATUAN tidak memerlukan kaedah pembelajaran separa penyeliaan untuk pengkelasan asas. ASSEMBLE boleh digunakan bersama dengan mana-mana algoritma klasifikasi sensitif kos untuk masalah dua kelas dan pelbagai kelas. PERSATUAN menggunakan keputusan keputusan memenangi Pertandingan Data Tanpa Label NIPS 2001. Di samping itu, hasil yang kuat pada beberapa set data penanda aras yang menggunakan kedua-dua pohon keputusan dan rangkaian saraf menyokong kaedah yang dicadangkan. [[EENNDD]] klasifikasi; pembelajaran ensemble; mesin ubahsuai sendiri; meningkatkan; pembelajaran separa penyeliaan"], [{"string": "Mining preferences from superior and inferior examples Mining user preferences plays a critical role in many important applications such as customer relationship management CRM , product and service recommendation , and marketing campaigns . In this paper , we identify an interesting and practical problem of mining user preferences : in a multidimensional space where the user preferences on some categorical attributes are unknown , from some superior and inferior examples provided by a user , can we learn about the user 's preferences on those categorical attributes ? We model the problem systematically and show that mining user preferences from superior and inferior examples is challenging . Although the problem has great potential in practice , to the best of our knowledge , it has not been explored systematically before . As the first attempt to tackle the problem , we propose a greedy method and show that our method is practical using real data sets and synthetic data sets .", "keywords": ["skyline", "preferences", "inferior examples", "superior examples"], "combined": "Mining preferences from superior and inferior examples Mining user preferences plays a critical role in many important applications such as customer relationship management CRM , product and service recommendation , and marketing campaigns . In this paper , we identify an interesting and practical problem of mining user preferences : in a multidimensional space where the user preferences on some categorical attributes are unknown , from some superior and inferior examples provided by a user , can we learn about the user 's preferences on those categorical attributes ? We model the problem systematically and show that mining user preferences from superior and inferior examples is challenging . Although the problem has great potential in practice , to the best of our knowledge , it has not been explored systematically before . As the first attempt to tackle the problem , we propose a greedy method and show that our method is practical using real data sets and synthetic data sets . [[EENNDD]] skyline; preferences; inferior examples; superior examples"}, "Pilihan perlombongan dari contoh unggul dan rendah diri Pilihan pengguna perlombongan memainkan peranan penting dalam banyak aplikasi penting seperti CRM pengurusan hubungan pelanggan, cadangan produk dan perkhidmatan, dan kempen pemasaran. Dalam makalah ini, kami mengenal pasti masalah menarik dan praktikal pilihan pengguna perlombongan: di ruang multidimensi di mana pilihan pengguna terhadap beberapa atribut kategori tidak diketahui, dari beberapa contoh unggul dan rendah yang diberikan oleh pengguna, dapatkah kita belajar tentang pengguna keutamaan pada sifat-sifat kategori itu? Kami memodelkan masalah secara sistematik dan menunjukkan bahawa pilihan pengguna perlombongan daripada contoh yang unggul dan rendah adalah mencabar. Walaupun masalah ini berpotensi besar dalam praktiknya, sepanjang pengetahuan kita, ia belum pernah diterokai secara sistematik sebelumnya. Sebagai percubaan pertama untuk mengatasi masalah tersebut, kami mencadangkan kaedah tamak dan menunjukkan bahawa kaedah kami praktikal menggunakan set data sebenar dan kumpulan data sintetik. [[EENNDD]] latar langit; pilihan; contoh rendah; contoh unggul"], [{"string": "Truth discovery with multiple conflicting information providers on the web The world-wide web has become the most important information source for most of us . Unfortunately , there is no guarantee for the correctness of information on the web . Moreover , different web sites often provide conflicting information on a subject , such as different specifications for the same product . In this paper we propose a new problem called Veracity , i.e. , conformity to truth , which studies how to find true facts from a large amount of conflicting information on many subjects that is provided by various web sites . We design a general framework for the Veracity problem , and invent an algorithm called TruthFinder , which utilizes the relationships between web sites and their information , i.e. , a web site is trustworthy if it provides many pieces of true information , and a piece of information is likely to be true if it is provided by many trustworthy web sites . Our experiments show that TruthFinder successfully finds true facts among conflicting information , and identifies trustworthy web sites better than the popular search engines .", "keywords": ["web mining", "data quality", "link analysis"], "combined": "Truth discovery with multiple conflicting information providers on the web The world-wide web has become the most important information source for most of us . Unfortunately , there is no guarantee for the correctness of information on the web . Moreover , different web sites often provide conflicting information on a subject , such as different specifications for the same product . In this paper we propose a new problem called Veracity , i.e. , conformity to truth , which studies how to find true facts from a large amount of conflicting information on many subjects that is provided by various web sites . We design a general framework for the Veracity problem , and invent an algorithm called TruthFinder , which utilizes the relationships between web sites and their information , i.e. , a web site is trustworthy if it provides many pieces of true information , and a piece of information is likely to be true if it is provided by many trustworthy web sites . Our experiments show that TruthFinder successfully finds true facts among conflicting information , and identifies trustworthy web sites better than the popular search engines . [[EENNDD]] web mining; data quality; link analysis"}, "Penemuan kebenaran dengan pelbagai penyedia maklumat yang bertentangan di web Web di seluruh dunia telah menjadi sumber maklumat yang paling penting bagi kebanyakan kita. Malangnya, tidak ada jaminan untuk kebenaran maklumat di web. Lebih-lebih lagi, laman web yang berlainan sering memberikan maklumat yang bertentangan mengenai sesuatu perkara, seperti spesifikasi yang berbeza untuk produk yang sama. Dalam makalah ini kami mencadangkan masalah baru yang disebut Veracity, iaitu kesesuaian dengan kebenaran, yang mengkaji bagaimana mencari fakta benar dari sejumlah besar maklumat bertentangan pada banyak subjek yang disediakan oleh pelbagai laman web. Kami merancang kerangka umum untuk masalah Veracity, dan mencipta algoritma yang disebut TruthFinder, yang memanfaatkan hubungan antara laman web dan maklumatnya, iaitu, laman web boleh dipercayai jika memberikan banyak maklumat yang benar, dan sebilangan maklumat adalah kemungkinan benar jika ia disediakan oleh banyak laman web yang boleh dipercayai. Eksperimen kami menunjukkan bahawa TruthFinder berjaya mencari fakta sebenar di antara maklumat yang bertentangan, dan mengenal pasti laman web yang boleh dipercayai lebih baik daripada enjin carian yang popular. [[EENNDD]] perlombongan web; kualiti data; analisis pautan"], [{"string": "Screening and interpreting multi-item associations based on log-linear modeling Association rules have received a lot of attention in the data mining community since their introduction . The classical approach to find rules whose items enjoy high support appear in a lot of the transactions in the data set is , however , filled with shortcomings . It has been shown that support can be misleading as an indicator of how interesting the rule is . Alternative measures , such as lift , have been proposed . More recently , a paper by DuMouchel et al. proposed the use of all-two-factor loglinear models to discover sets of items that can not be explained by pairwise associations between the items involved . This approach , however , has its limitations , since it stops short of considering higher order interactions other than pairwise among the items . In this paper , we propose a method that examines the parameters of the fitted loglinear models to find all the significant association patterns among the items . Since fitting loglinear models for large data sets can be computationally prohibitive , we apply graph-theoretical results to divide the original set of items into components sets of items that are statistically independent from each other . We then apply loglinear modeling to each of the components and find the interesting associations among items in them . The technique is experimentally evaluated with a real data set insurance data and a series of synthetic data sets . The results show that the technique is effective in finding interesting associations among the items involved .", "keywords": ["graphical model", "log-linear model", "database applications", "association rule"], "combined": "Screening and interpreting multi-item associations based on log-linear modeling Association rules have received a lot of attention in the data mining community since their introduction . The classical approach to find rules whose items enjoy high support appear in a lot of the transactions in the data set is , however , filled with shortcomings . It has been shown that support can be misleading as an indicator of how interesting the rule is . Alternative measures , such as lift , have been proposed . More recently , a paper by DuMouchel et al. proposed the use of all-two-factor loglinear models to discover sets of items that can not be explained by pairwise associations between the items involved . This approach , however , has its limitations , since it stops short of considering higher order interactions other than pairwise among the items . In this paper , we propose a method that examines the parameters of the fitted loglinear models to find all the significant association patterns among the items . Since fitting loglinear models for large data sets can be computationally prohibitive , we apply graph-theoretical results to divide the original set of items into components sets of items that are statistically independent from each other . We then apply loglinear modeling to each of the components and find the interesting associations among items in them . The technique is experimentally evaluated with a real data set insurance data and a series of synthetic data sets . The results show that the technique is effective in finding interesting associations among the items involved . [[EENNDD]] graphical model; log-linear model; database applications; association rule"}, "Menyaring dan menafsirkan persatuan pelbagai item berdasarkan pemodelan log-linear Peraturan persatuan telah mendapat banyak perhatian dalam komuniti perlombongan data sejak diperkenalkan. Pendekatan klasik untuk mencari peraturan yang itemnya mendapat sokongan tinggi muncul dalam banyak transaksi dalam kumpulan data, namun, dipenuhi dengan kekurangan. Telah ditunjukkan bahawa sokongan dapat menyesatkan sebagai petunjuk betapa menariknya peraturan tersebut. Langkah-langkah alternatif, seperti lif, telah dicadangkan. Baru-baru ini, sebuah makalah oleh DuMouchel et al. mencadangkan penggunaan model loglinear semua-dua faktor untuk menemui set item yang tidak dapat dijelaskan oleh perkaitan berpasangan antara item yang terlibat. Pendekatan ini, bagaimanapun, mempunyai batasannya, kerana berhenti mempertimbangkan interaksi pesanan yang lebih tinggi selain berpasangan antara item. Dalam makalah ini, kami mencadangkan kaedah yang meneliti parameter model loglinear yang dipasang untuk mencari semua corak hubungan yang signifikan antara item tersebut. Oleh kerana model loglinear yang sesuai untuk set data yang besar boleh dilarang secara komputasi, kami menerapkan hasil teori-teori untuk membahagikan set item yang asal menjadi set komponen item yang secara statistik bebas antara satu sama lain. Kami kemudian menerapkan pemodelan loglinear pada setiap komponen dan mencari perkaitan yang menarik antara item-item di dalamnya. Teknik ini dinilai secara eksperimen dengan data insurans set data sebenar dan serangkaian set data sintetik. Hasil kajian menunjukkan bahawa teknik ini berkesan dalam mencari perkaitan yang menarik antara item yang terlibat. [[EENNDD]] model grafik; model log-linear; aplikasi pangkalan data; peraturan persatuan"], [{"string": "Is there a grand challenge or X-prize for data mining ? This panel will discuss possible exciting and motivating Grand Challenge problems for Data Mining , focusing on bioinformatics , multimedia mining , link mining , text mining , and web mining .", "keywords": ["grand challenge", "text mining", "image mining", "bioinformatics", "x-prize", "multimedia mining", "video mining", "web mining", "link mining"], "combined": "Is there a grand challenge or X-prize for data mining ? This panel will discuss possible exciting and motivating Grand Challenge problems for Data Mining , focusing on bioinformatics , multimedia mining , link mining , text mining , and web mining . [[EENNDD]] grand challenge; text mining; image mining; bioinformatics; x-prize; multimedia mining; video mining; web mining; link mining"}, "Adakah terdapat cabaran besar atau hadiah X untuk perlombongan data? Panel ini akan membincangkan kemungkinan masalah Grand Challenge yang menarik dan memotivasi untuk Perlombongan Data, dengan fokus pada bioinformatik, perlombongan multimedia, perlombongan pautan, perlombongan teks, dan perlombongan web. [[EENNDD]] cabaran besar; perlombongan teks; perlombongan imej; bioinformatik; hadiah x; perlombongan multimedia; perlombongan video; perlombongan web; perlombongan pautan"], [{"string": "Aggregation-based feature invention and relational concept classes Model induction from relational data requires aggregation of the values of attributes of related entities . This paper makes three contributions to the study of relational learning . 1 It presents a hierarchy of relational concepts of increasing complexity , using relational schema characteristics such as cardinality , and derives classes of aggregation operators that are needed to learn these concepts . 2 Expanding one level of the hierarchy , it introduces new aggregation operators that model the distributions of the values to be aggregated and for classification problems the differences in these distributions by class . 3 It demonstrates empirically on a noisy business domain that more-complex aggregation methods can increase generalization performance . Constructing features using target-dependent aggregations can transform relational prediction tasks so that well-understood feature-vector-based modeling algorithms can be applied successfully .", "keywords": ["constructive induction", "relational learning", "learning", "feature construction", "propositionalization", "aggregation"], "combined": "Aggregation-based feature invention and relational concept classes Model induction from relational data requires aggregation of the values of attributes of related entities . This paper makes three contributions to the study of relational learning . 1 It presents a hierarchy of relational concepts of increasing complexity , using relational schema characteristics such as cardinality , and derives classes of aggregation operators that are needed to learn these concepts . 2 Expanding one level of the hierarchy , it introduces new aggregation operators that model the distributions of the values to be aggregated and for classification problems the differences in these distributions by class . 3 It demonstrates empirically on a noisy business domain that more-complex aggregation methods can increase generalization performance . Constructing features using target-dependent aggregations can transform relational prediction tasks so that well-understood feature-vector-based modeling algorithms can be applied successfully . [[EENNDD]] constructive induction; relational learning; learning; feature construction; propositionalization; aggregation"}, "Penciptaan ciri berasaskan agregasi dan kelas konsep relasional Induksi model dari data hubungan memerlukan penggabungan nilai atribut entiti berkaitan. Makalah ini memberikan tiga sumbangan untuk kajian pembelajaran relasional. 1 Ini menyajikan hierarki konsep relasional peningkatan kerumitan, menggunakan ciri-ciri skema hubungan seperti kardinaliti, dan memperoleh kelas pengendali agregasi yang diperlukan untuk mempelajari konsep-konsep ini. 2 Memperluas satu tahap hierarki, ia memperkenalkan pengendali agregasi baru yang memodelkan pengagihan nilai yang akan dikumpulkan dan untuk masalah klasifikasi perbezaan pengedaran ini mengikut kelas. 3 Ini menunjukkan secara empirik pada domain perniagaan yang bising bahawa kaedah pengagregatan yang lebih kompleks dapat meningkatkan prestasi generalisasi. Membina ciri menggunakan agregasi bergantung pada sasaran dapat mengubah tugas ramalan relasional sehingga algoritma pemodelan berasaskan ciri-vektor yang difahami dengan baik dapat diterapkan dengan jayanya. [[EENNDD]] aruhan konstruktif; pembelajaran hubungan; belajar; pembinaan ciri; pencalonan; pengagregatan"], [{"string": "Efficient anonymity-preserving data collection The output of a data mining algorithm is only as good as its inputs , and individuals are often unwilling to provide accurate data about sensitive topics such as medical history and personal finance . Individuals maybe willing to share their data , but only if they are assured that it will be used in an aggregate study and that it can not be linked back to them . Protocols for anonymity-preserving data collection provide this assurance , in the absence of trusted parties , by allowing a set of mutually distrustful respondents to anonymously contribute data to an untrusted data miner . To effectively provide anonymity , a data collection protocol must be collusion resistant , which means that even if all dishonest respondents collude with a dishonest data miner in an attempt to learn the associations between honest respondents and their responses , they will be unable to do so . To achieve collusion resistance , previously proposed protocols for anonymity-preserving data collection have quadratically many communication rounds in the number of respondents , and employ sometimes incorrectly complicated cryptographic techniques such as zero-knowledge proofs . We describe a new protocol for anonymity-preserving , collusion resistant data collection . Our protocol has linearly many communication rounds , and achieves collusion resistance without relying on zero-knowledge proofs . This makes it especially suitable for data mining scenarios with a large number of respondents .", "keywords": ["anonymity", "privacy"], "combined": "Efficient anonymity-preserving data collection The output of a data mining algorithm is only as good as its inputs , and individuals are often unwilling to provide accurate data about sensitive topics such as medical history and personal finance . Individuals maybe willing to share their data , but only if they are assured that it will be used in an aggregate study and that it can not be linked back to them . Protocols for anonymity-preserving data collection provide this assurance , in the absence of trusted parties , by allowing a set of mutually distrustful respondents to anonymously contribute data to an untrusted data miner . To effectively provide anonymity , a data collection protocol must be collusion resistant , which means that even if all dishonest respondents collude with a dishonest data miner in an attempt to learn the associations between honest respondents and their responses , they will be unable to do so . To achieve collusion resistance , previously proposed protocols for anonymity-preserving data collection have quadratically many communication rounds in the number of respondents , and employ sometimes incorrectly complicated cryptographic techniques such as zero-knowledge proofs . We describe a new protocol for anonymity-preserving , collusion resistant data collection . Our protocol has linearly many communication rounds , and achieves collusion resistance without relying on zero-knowledge proofs . This makes it especially suitable for data mining scenarios with a large number of respondents . [[EENNDD]] anonymity; privacy"}, "Pengumpulan data pemeliharaan tanpa nama yang cekap Hasil daripada algoritma perlombongan data sama baiknya dengan inputnya, dan individu sering tidak mahu memberikan data yang tepat mengenai topik sensitif seperti sejarah perubatan dan kewangan peribadi. Individu mungkin bersedia untuk berkongsi data mereka, tetapi hanya jika mereka yakin bahawa ia akan digunakan dalam kajian agregat dan tidak dapat dihubungkan kembali dengan mereka. Protokol untuk pengumpulan data yang menjaga anonim memberikan jaminan ini, sekiranya tidak ada pihak yang dipercayai, dengan membenarkan sekumpulan responden yang saling tidak percaya untuk menyumbangkan data secara anonim kepada pelombong data yang tidak dipercayai. Untuk memberikan anonimiti secara berkesan, protokol pengumpulan data mestilah tahan kolusi, yang bermaksud bahawa walaupun semua responden yang tidak jujur berkomunikasi dengan pelombong data yang tidak jujur dalam usaha mempelajari perkaitan antara responden yang jujur dan respons mereka, mereka tidak akan dapat melakukannya. Untuk mencapai ketahanan kolusi, protokol yang diusulkan sebelumnya untuk pengumpulan data yang menjaga anonim telah secara berkala banyak pusingan komunikasi dalam jumlah responden, dan menggunakan teknik kriptografi yang kadang-kadang tidak rumit seperti bukti pengetahuan sifar. Kami menerangkan protokol baru untuk pengumpulan anonim, pengumpulan data tahan kolusi. Protokol kami secara linear mempunyai banyak pusingan komunikasi, dan mencapai ketahanan kolusi tanpa bergantung pada bukti pengetahuan sifar. Ini menjadikannya sangat sesuai untuk senario perlombongan data dengan sebilangan besar responden. [[EENNDD]] tanpa nama; privasi"], [{"string": "Applying collaborative filtering techniques to movie search for better ranking and browsing We propose a new ranking method , which combines recommender systems with information search tools for better search and browsing . Our method uses a collaborative filtering algorithm to generate personal item authorities for each user and combines them with item proximities for better ranking . To demonstrate our approach , we build a prototype movie search and browsing engine called MAD6 Movies , Actors and Directors ; 6 degrees of separation . We conduct offline and online tests of our ranking algorithm . For offline testing , we use Yahoo ! Search queries that resulted in a click on a Yahoo ! Movies or Internet Movie Database IMDB movie URL . Our online test involved 44 Yahoo ! employees providing subjective assessments of results quality . In both tests , our ranking methods show significantly better recall and quality than IMDB search and Yahoo ! Movies current search .", "keywords": ["information retrieval", "collaborative filtering", "search ranking", "movie search", "performance evaluation", "recommender systems"], "combined": "Applying collaborative filtering techniques to movie search for better ranking and browsing We propose a new ranking method , which combines recommender systems with information search tools for better search and browsing . Our method uses a collaborative filtering algorithm to generate personal item authorities for each user and combines them with item proximities for better ranking . To demonstrate our approach , we build a prototype movie search and browsing engine called MAD6 Movies , Actors and Directors ; 6 degrees of separation . We conduct offline and online tests of our ranking algorithm . For offline testing , we use Yahoo ! Search queries that resulted in a click on a Yahoo ! Movies or Internet Movie Database IMDB movie URL . Our online test involved 44 Yahoo ! employees providing subjective assessments of results quality . In both tests , our ranking methods show significantly better recall and quality than IMDB search and Yahoo ! Movies current search . [[EENNDD]] information retrieval; collaborative filtering; search ranking; movie search; performance evaluation; recommender systems"}, "Menerapkan teknik penapisan kolaboratif untuk pencarian filem untuk pemeringkatan dan penjelajahan yang lebih baik. Kami mencadangkan kaedah pemeringkatan baru, yang menggabungkan sistem pengesyoran dengan alat carian maklumat untuk carian dan penyemakan imbas yang lebih baik. Kaedah kami menggunakan algoritma penapisan kolaboratif untuk menghasilkan otoriti item peribadi untuk setiap pengguna dan menggabungkannya dengan jarak item untuk kedudukan yang lebih baik. Untuk menunjukkan pendekatan kami, kami membina mesin carian dan carian filem prototaip yang disebut MAD6 Movies, Actors and Director; 6 darjah pemisahan. Kami menjalankan ujian luar talian dan dalam talian algoritma peringkat kami. Untuk ujian luar talian, kami menggunakan Yahoo! Pertanyaan carian yang menghasilkan klik pada Yahoo! URL filem IMDB Filem atau Pangkalan Data Filem Internet. Ujian dalam talian kami melibatkan 44 Yahoo! pekerja memberikan penilaian subjektif terhadap kualiti hasil. Dalam kedua-dua ujian, kaedah pemeringkatan kami menunjukkan penarikan dan kualiti yang jauh lebih baik daripada carian IMDB dan Yahoo! Carian filem semasa. [[EENNDD]] pengambilan maklumat; penapisan kolaboratif; kedudukan carian; carian filem; penilaian prestasi; sistem cadangan"], [{"string": "Supervised probabilistic principal component analysis Principal component analysis PCA has been extensively applied in data mining , pattern recognition and information retrieval for unsupervised dimensionality reduction . When labels of data are available , e.g. , in a classification or regression task , PCA is however not able to use this information . The problem is more interesting if only part of the input data are labeled , i.e. , in a semi-supervised setting . In this paper we propose a supervised PCA model called SPPCA and a semi-supervised PCA model called S2PPCA , both of which are extensions of a probabilistic PCA model . The proposed models are able to incorporate the label information into the projection phase , and can naturally handle multiple outputs i.e. , in multi-task learning problems . We derive an efficient EM learning algorithm for both models , and also provide theoretical justifications of the model behaviors . SPPCA and S2PPCA are compared with other supervised projection methods on various learning tasks , and show not only promising performance but also good scalability .", "keywords": ["principal component analysis", "semi-supervised projection", "dimensionality reduction", "supervised projection"], "combined": "Supervised probabilistic principal component analysis Principal component analysis PCA has been extensively applied in data mining , pattern recognition and information retrieval for unsupervised dimensionality reduction . When labels of data are available , e.g. , in a classification or regression task , PCA is however not able to use this information . The problem is more interesting if only part of the input data are labeled , i.e. , in a semi-supervised setting . In this paper we propose a supervised PCA model called SPPCA and a semi-supervised PCA model called S2PPCA , both of which are extensions of a probabilistic PCA model . The proposed models are able to incorporate the label information into the projection phase , and can naturally handle multiple outputs i.e. , in multi-task learning problems . We derive an efficient EM learning algorithm for both models , and also provide theoretical justifications of the model behaviors . SPPCA and S2PPCA are compared with other supervised projection methods on various learning tasks , and show not only promising performance but also good scalability . [[EENNDD]] principal component analysis; semi-supervised projection; dimensionality reduction; supervised projection"}, "Analisis komponen utama probabilistik yang diawasi Analisis komponen utama PCA telah digunakan secara meluas dalam perlombongan data, pengecaman corak dan pengambilan maklumat untuk pengurangan dimensi yang tidak diawasi. Apabila label data tersedia, mis. , dalam tugas klasifikasi atau regresi, PCA bagaimanapun tidak dapat menggunakan maklumat ini. Masalahnya lebih menarik jika hanya sebahagian daripada data input yang dilabelkan, iaitu, dalam keadaan separa yang diawasi. Dalam makalah ini kami mencadangkan model PCA yang diawasi disebut SPPCA dan model PCA separa pengawasan yang disebut S2PPCA, yang keduanya merupakan perpanjangan dari model PCA probabilistik. Model yang dicadangkan dapat memasukkan maklumat label ke dalam fasa unjuran, dan secara semula jadi dapat menangani banyak output iaitu, dalam masalah pembelajaran pelbagai tugas. Kami memperoleh algoritma pembelajaran EM yang cekap untuk kedua-dua model, dan juga memberikan justifikasi teori mengenai tingkah laku model. SPPCA dan S2PPCA dibandingkan dengan kaedah unjuran lain yang diselia pada pelbagai tugas pembelajaran, dan menunjukkan tidak hanya prestasi yang menjanjikan tetapi juga skalabilitas yang baik. [[EENNDD]] analisis komponen utama; unjuran separa penyeliaan; pengurangan dimensi; unjuran diselia"], [{"string": "Extracting key-substring-group features for text classification In many text classification applications , it is appealing to take every document as a string of characters rather than a bag of words . Previous research studies in this area mostly focused on different variants of generative Markov chain models . Although discriminative machine learning methods like Support Vector Machine SVM have been quite successful in text classification with word features , it is neither effective nor efficient to apply them straightforwardly taking all substrings in the corpus as features . In this paper , we propose to partition all substrings into statistical equivalence groups , and then pick those groups which are important in the statistical sense as features named key-substring-group features for text classification . In particular , we propose a suffix tree based algorithm that can extract such features in linear time with respect to the total number of characters in the corpus . Our experiments on English , Chinese and Greek datasets show that SVM with key-substring-group features can achieve outstanding performance for various text classification tasks .", "keywords": ["text classification", "text mining", "content analysis and indexing", "suffix tree", "feature extraction", "machine learning"], "combined": "Extracting key-substring-group features for text classification In many text classification applications , it is appealing to take every document as a string of characters rather than a bag of words . Previous research studies in this area mostly focused on different variants of generative Markov chain models . Although discriminative machine learning methods like Support Vector Machine SVM have been quite successful in text classification with word features , it is neither effective nor efficient to apply them straightforwardly taking all substrings in the corpus as features . In this paper , we propose to partition all substrings into statistical equivalence groups , and then pick those groups which are important in the statistical sense as features named key-substring-group features for text classification . In particular , we propose a suffix tree based algorithm that can extract such features in linear time with respect to the total number of characters in the corpus . Our experiments on English , Chinese and Greek datasets show that SVM with key-substring-group features can achieve outstanding performance for various text classification tasks . [[EENNDD]] text classification; text mining; content analysis and indexing; suffix tree; feature extraction; machine learning"}, "Mengekstrak ciri kumpulan kunci-substring untuk klasifikasi teks Dalam banyak aplikasi klasifikasi teks, sangat menarik untuk mengambil setiap dokumen sebagai rentetan watak dan bukan sekumpulan kata. Kajian penyelidikan sebelumnya di kawasan ini kebanyakannya menumpukan pada varian model rantaian Markov generatif yang berbeza. Walaupun kaedah pembelajaran mesin diskriminatif seperti Support Vector Machine SVM telah cukup berjaya dalam klasifikasi teks dengan ciri-ciri perkataan, ia tidak berkesan atau tidak berkesan untuk menerapkannya secara langsung dengan mengambil semua substring dalam korpus sebagai ciri. Dalam makalah ini, kami mencadangkan untuk membahagikan semua substring ke dalam kumpulan kesetaraan statistik, dan kemudian memilih kumpulan-kumpulan yang penting dalam pengertian statistik sebagai ciri-ciri yang dinamakan ciri kumpulan-substring-kunci untuk klasifikasi teks. Secara khusus, kami mencadangkan algoritma berdasarkan pohon akhiran yang dapat mengekstrak ciri-ciri tersebut dalam masa linear sehubungan dengan jumlah watak dalam korpus. Eksperimen kami pada set data Bahasa Inggeris, Cina dan Yunani menunjukkan bahawa SVM dengan ciri kumpulan kunci-substring dapat mencapai prestasi yang luar biasa untuk pelbagai tugas klasifikasi teks. [[EENNDD]] pengelasan teks; perlombongan teks; analisis kandungan dan pengindeksan; pokok akhiran; pengekstrakan ciri; pembelajaran mesin"], [{"string": "Mining comparable bilingual text corpora for cross-language information integration Integrating information in multiple natural languages is a challenging task that often requires manually created linguistic resources such as a bilingual dictionary or examples of direct translations of text . In this paper , we propose a general cross-lingual text mining method that does not rely on any of these resources , but can exploit comparable bilingual text corpora to discover mappings between words and documents in different languages . Comparable text corpora are collections of text documents in different languages that are about similar topics ; such text corpora are often naturally available e.g. , news articles in different languages published in the same time period . The main idea of our method is to exploit frequency correlations of words in different languages in the comparable corpora and discover mappings between words in different languages . Such mappings can then be used to further discover mappings between documents in different languages , achieving cross-lingual information integration . Evaluation of the proposed method on a 120MB Chinese-English comparable news collection shows that the proposed method is effective for mapping words and documents in English and Chinese . Since our method only relies on naturally available comparable corpora , it is generally applicable to any language pairs as long as we have comparable corpora .", "keywords": ["cross-lingual text mining", "comparable corpora", "information search and retrieval", "document alignment", "frequency correlation"], "combined": "Mining comparable bilingual text corpora for cross-language information integration Integrating information in multiple natural languages is a challenging task that often requires manually created linguistic resources such as a bilingual dictionary or examples of direct translations of text . In this paper , we propose a general cross-lingual text mining method that does not rely on any of these resources , but can exploit comparable bilingual text corpora to discover mappings between words and documents in different languages . Comparable text corpora are collections of text documents in different languages that are about similar topics ; such text corpora are often naturally available e.g. , news articles in different languages published in the same time period . The main idea of our method is to exploit frequency correlations of words in different languages in the comparable corpora and discover mappings between words in different languages . Such mappings can then be used to further discover mappings between documents in different languages , achieving cross-lingual information integration . Evaluation of the proposed method on a 120MB Chinese-English comparable news collection shows that the proposed method is effective for mapping words and documents in English and Chinese . Since our method only relies on naturally available comparable corpora , it is generally applicable to any language pairs as long as we have comparable corpora . [[EENNDD]] cross-lingual text mining; comparable corpora; information search and retrieval; document alignment; frequency correlation"}, "Melombong korporat teks dwibahasa yang setanding untuk penyatuan maklumat silang bahasa Mengintegrasikan maklumat dalam pelbagai bahasa semula jadi adalah tugas yang mencabar yang sering memerlukan sumber linguistik yang dibuat secara manual seperti kamus dwibahasa atau contoh terjemahan teks secara langsung. Dalam makalah ini, kami mencadangkan kaedah perlombongan teks silang bahasa umum yang tidak bergantung pada sumber-sumber ini, tetapi dapat memanfaatkan korporat teks dwibahasa yang sebanding untuk menemukan pemetaan antara kata-kata dan dokumen dalam bahasa yang berbeza. Corpora teks yang setanding adalah koleksi dokumen teks dalam bahasa yang berbeza mengenai topik yang serupa; corpora teks seperti itu biasanya terdapat secara semula jadi, mis. , artikel berita dalam pelbagai bahasa yang diterbitkan dalam jangka masa yang sama. Idea utama kaedah kami adalah untuk mengeksploitasi korelasi frekuensi kata dalam bahasa yang berlainan di korporat yang sebanding dan menemui pemetaan antara kata dalam bahasa yang berbeza. Pemetaan semacam itu kemudian dapat digunakan untuk menemukan pemetaan lebih jauh antara dokumen dalam bahasa yang berbeza, mencapai integrasi maklumat lintas bahasa. Penilaian kaedah yang dicadangkan pada koleksi berita sebanding Cina-Inggeris 120MB menunjukkan bahawa kaedah yang dicadangkan berkesan untuk memetakan perkataan dan dokumen dalam Bahasa Inggeris dan Cina. Oleh kerana kaedah kami hanya bergantung pada korporat setanding yang tersedia secara semula jadi, ini biasanya berlaku untuk mana-mana pasangan bahasa selagi kita mempunyai corpora yang setanding. [[EENNDD]] perlombongan teks rentas bahasa; korporat setanding; carian dan pengambilan maklumat; penjajaran dokumen; korelasi frekuensi"], [{"string": "Co-evolution of social and affiliation networks In our work , we address the problem of modeling social network generation which explains both link and group formation . Recent studies on social network evolution propose generative models which capture the statistical properties of real-world networks related only to node-to-node link formation . We propose a novel model which captures the co-evolution of social and affiliation networks . We provide surprising insights into group formation based on observations in several real-world networks , showing that users often join groups for reasons other than their friends . Our experiments show that the model is able to capture both the newly observed and previously studied network properties . This work is the first to propose a generative model which captures the statistical properties of these complex networks . The proposed model facilitates controlled experiments which study the effect of actors ' behavior on the evolution of affiliation networks , and it allows the generation of realistic synthetic datasets .", "keywords": ["groups", "affiliation network", "graph generator", "evolution", "social network"], "combined": "Co-evolution of social and affiliation networks In our work , we address the problem of modeling social network generation which explains both link and group formation . Recent studies on social network evolution propose generative models which capture the statistical properties of real-world networks related only to node-to-node link formation . We propose a novel model which captures the co-evolution of social and affiliation networks . We provide surprising insights into group formation based on observations in several real-world networks , showing that users often join groups for reasons other than their friends . Our experiments show that the model is able to capture both the newly observed and previously studied network properties . This work is the first to propose a generative model which captures the statistical properties of these complex networks . The proposed model facilitates controlled experiments which study the effect of actors ' behavior on the evolution of affiliation networks , and it allows the generation of realistic synthetic datasets . [[EENNDD]] groups; affiliation network; graph generator; evolution; social network"}, "Evolusi bersama rangkaian sosial dan afiliasi Dalam karya kami, kami menangani masalah pemodelan penjanaan rangkaian sosial yang menerangkan hubungan dan pembentukan kumpulan. Kajian terbaru mengenai evolusi rangkaian sosial mencadangkan model generatif yang menangkap sifat statistik rangkaian dunia nyata yang hanya berkaitan dengan pembentukan pautan simpul ke simpul. Kami mencadangkan model novel yang menangkap evolusi bersama rangkaian sosial dan gabungan. Kami memberikan pandangan yang mengejutkan mengenai pembentukan kumpulan berdasarkan pemerhatian di beberapa rangkaian dunia nyata, menunjukkan bahawa pengguna sering bergabung dengan kumpulan dengan alasan selain rakan mereka. Eksperimen kami menunjukkan bahawa model tersebut dapat menangkap sifat rangkaian yang baru diperhatikan dan yang telah dikaji sebelumnya. Karya ini adalah yang pertama untuk mencadangkan model generatif yang menangkap sifat statistik rangkaian kompleks ini. Model yang dicadangkan ini memudahkan eksperimen terkawal yang mengkaji pengaruh tingkah laku pelakon terhadap evolusi rangkaian gabungan, dan ia memungkinkan penghasilan kumpulan data sintetik yang realistik. [[EENNDD]] kumpulan; rangkaian gabungan; penjana grafik; evolusi; rangkaian sosial"], [{"string": "A probabilistic framework for relational clustering Relational clustering has attracted more and more attention due to its phenomenal impact in various important applications which involve multi-type interrelated data objects , such as Web mining , search marketing , bioinformatics , citation analysis , and epidemiology . In this paper , we propose a probabilistic model for relational clustering , which also provides a principal framework to unify various important clustering tasks including traditional attributes-based clustering , semi-supervised clustering , co-clustering and graph clustering . The proposed model seeks to identify cluster structures for each type of data objects and interaction patterns between different types of objects . Under this model , we propose parametric hard and soft relational clustering algorithms under a large number of exponential family distributions . The algorithms are applicable to relational data of various structures and at the same time unifies a number of stat-of-the-art clustering algorithms : co-clustering algorithms , the k-partite graph clustering , Bregman k-means , and semi-supervised clustering based on hidden Markov random fields .", "keywords": ["relational clustering", "relational data"], "combined": "A probabilistic framework for relational clustering Relational clustering has attracted more and more attention due to its phenomenal impact in various important applications which involve multi-type interrelated data objects , such as Web mining , search marketing , bioinformatics , citation analysis , and epidemiology . In this paper , we propose a probabilistic model for relational clustering , which also provides a principal framework to unify various important clustering tasks including traditional attributes-based clustering , semi-supervised clustering , co-clustering and graph clustering . The proposed model seeks to identify cluster structures for each type of data objects and interaction patterns between different types of objects . Under this model , we propose parametric hard and soft relational clustering algorithms under a large number of exponential family distributions . The algorithms are applicable to relational data of various structures and at the same time unifies a number of stat-of-the-art clustering algorithms : co-clustering algorithms , the k-partite graph clustering , Bregman k-means , and semi-supervised clustering based on hidden Markov random fields . [[EENNDD]] relational clustering; relational data"}, "Kerangka probabilistik untuk pengelompokan relasional Pengelompokan relasional telah menarik perhatian semakin banyak kerana kesan fenomenalnya dalam pelbagai aplikasi penting yang melibatkan objek data yang saling berkaitan pelbagai jenis, seperti perlombongan Web, pemasaran carian, bioinformatik, analisis kutipan, dan epidemiologi. Dalam makalah ini, kami mengusulkan model probabilistik untuk pengelompokan relasional, yang juga menyediakan kerangka utama untuk menyatukan berbagai tugas pengelompokan penting termasuk pengelompokan berdasarkan atribut tradisional, pengelompokan semi-diawasi, pengelompokan bersama dan pengelompokan grafik. Model yang dicadangkan bertujuan untuk mengenal pasti struktur kluster untuk setiap jenis objek data dan corak interaksi antara pelbagai jenis objek. Di bawah model ini, kami mencadangkan algoritma pengelompokan hubungan keras dan lembut parametrik di bawah sebilangan besar pengagihan keluarga eksponensial. Algoritma tersebut dapat digunakan untuk data hubungan dari pelbagai struktur dan pada masa yang sama menyatukan sejumlah algoritma pengelompokan statik: algoritma kluster bersama, pengelompokan grafik k-partit, k-cara Bregman, dan separa penyeliaan pengelompokan berdasarkan medan rawak Markov yang tersembunyi. [[EENNDD]] pengelompokan hubungan; data hubungan"], [{"string": "Evolutionary algorithms in data mining : multi-objective performance modeling for direct marketing", "keywords": ["evolutionary computation", "pareto-optimal models", "database marketing", "multiple objectives"], "combined": "Evolutionary algorithms in data mining : multi-objective performance modeling for direct marketing [[EENNDD]] evolutionary computation; pareto-optimal models; database marketing; multiple objectives"}, "Algoritma evolusi dalam perlombongan data: pemodelan prestasi pelbagai objektif untuk pemasaran langsung [[EENNDD]] pengiraan evolusi; model optimum pareto; pemasaran pangkalan data; pelbagai objektif"], [{"string": "Mining coherent gene clusters from gene-sample-time microarray data Extensive studies have shown that mining microarray data sets is important in bioinformatics research and biomedical applications . In this paper , we explore a novel type of gene-sample-time microarray data sets , which records the expression levels of various genes under a set of samples during a series of time points . In particular , we propose the mining of coherent gene clusters from such data sets . Each cluster contains a subset of genes and a subset of samples such that the genes are coherent on the samples along the time series . The coherent gene clusters may identify the samples corresponding to some phenotypes e.g. , diseases , and suggest the candidate genes correlated to the phenotypes . We present two efficient algorithms , namely the Sample-Gene Search and the Gene-Sample Search , to mine the complete set of coherent gene clusters . We empirically evaluate the performance of our approaches on both a real microarray data set and synthetic data sets . The test results have shown that our approaches are both efficient and effective to find meaningful coherent gene clusters .", "keywords": ["bioinformatics", "microarray data", "clustering"], "combined": "Mining coherent gene clusters from gene-sample-time microarray data Extensive studies have shown that mining microarray data sets is important in bioinformatics research and biomedical applications . In this paper , we explore a novel type of gene-sample-time microarray data sets , which records the expression levels of various genes under a set of samples during a series of time points . In particular , we propose the mining of coherent gene clusters from such data sets . Each cluster contains a subset of genes and a subset of samples such that the genes are coherent on the samples along the time series . The coherent gene clusters may identify the samples corresponding to some phenotypes e.g. , diseases , and suggest the candidate genes correlated to the phenotypes . We present two efficient algorithms , namely the Sample-Gene Search and the Gene-Sample Search , to mine the complete set of coherent gene clusters . We empirically evaluate the performance of our approaches on both a real microarray data set and synthetic data sets . The test results have shown that our approaches are both efficient and effective to find meaningful coherent gene clusters . [[EENNDD]] bioinformatics; microarray data; clustering"}, "Perlombongan gen koheren dari data mikroarray masa-sampel gen Kajian lanjutan menunjukkan bahawa perlombongan set data mikroarray penting dalam penyelidikan bioinformatik dan aplikasi bioperubatan. Dalam makalah ini, kami meneroka jenis novel kumpulan data mikroarray-sampel-masa gen, yang mencatat tahap ekspresi pelbagai gen di bawah satu set sampel selama satu siri titik waktu. Khususnya, kami mencadangkan perlombongan kelompok gen yang koheren dari kumpulan data tersebut. Setiap kelompok mengandungi subset gen dan subset sampel sedemikian rupa sehingga gen tersebut sepadan pada sampel sepanjang siri masa. Kumpulan gen yang koheren dapat mengenal pasti sampel yang sesuai dengan beberapa fenotip, mis. , penyakit, dan menunjukkan gen calon berkorelasi dengan fenotip. Kami menyajikan dua algoritma yang cekap, iaitu Sample-Gene Search dan Gene-Sample Search, untuk menambang set lengkap kumpulan gen yang koheren. Kami secara empirikal menilai prestasi pendekatan kami pada kumpulan data microarray sebenar dan set data sintetik. Hasil ujian menunjukkan bahawa pendekatan kami adalah cekap dan berkesan untuk mencari kelompok gen yang koheren. [[EENNDD]] bioinformatik; data microarray; pengelompokan"], [{"string": "A system for real-time competitive market intelligence A method is described for real-time market intelligence and competitive analysis . News stories are collected online for a designated group of companies . The goal is to detect critical differences in the text written about a company versus the text for its competitors . A solution is found by mapping the task into a non-stationary text categorization model . The overall design consists of the following components : a a real-time crawler that monitors newswires for stories about the competitors b a conditional document retriever that selects only those documents that meet the indicated conditions c text analysis techniques that convert the documents to a numerical format d rule induction methods for finding patterns in data e presentation techniques for displaying results . The method is extended to combine text with numerical measures , such as those based on stock prices and market capitalizations , that allow for more objective evaluations and projections .", "keywords": ["deduction"], "combined": "A system for real-time competitive market intelligence A method is described for real-time market intelligence and competitive analysis . News stories are collected online for a designated group of companies . The goal is to detect critical differences in the text written about a company versus the text for its competitors . A solution is found by mapping the task into a non-stationary text categorization model . The overall design consists of the following components : a a real-time crawler that monitors newswires for stories about the competitors b a conditional document retriever that selects only those documents that meet the indicated conditions c text analysis techniques that convert the documents to a numerical format d rule induction methods for finding patterns in data e presentation techniques for displaying results . The method is extended to combine text with numerical measures , such as those based on stock prices and market capitalizations , that allow for more objective evaluations and projections . [[EENNDD]] deduction"}, "Sistem untuk kecerdasan pasaran masa nyata yang kompetitif Kaedah dijelaskan untuk kecerdasan pasaran masa nyata dan analisis persaingan. Berita berita dikumpulkan dalam talian untuk kumpulan syarikat yang ditentukan. Tujuannya adalah untuk mengesan perbezaan kritikal dalam teks yang ditulis mengenai syarikat berbanding teks untuk pesaingnya. Penyelesaian dijumpai dengan memetakan tugas menjadi model pengkategorian teks yang tidak bergerak. Reka bentuk keseluruhan terdiri daripada komponen berikut: a crawler masa nyata yang memantau kawat baru untuk mengetahui berita mengenai pesaing dan retriever dokumen bersyarat yang hanya memilih dokumen yang memenuhi syarat yang ditunjukkan c teknik analisis teks yang mengubah dokumen menjadi format berangka dan peraturan kaedah induksi untuk mencari corak dalam teknik penyampaian data dan mempamerkan hasil. Kaedah ini diperluas untuk menggabungkan teks dengan ukuran berangka, seperti yang berdasarkan pada harga saham dan permodalan pasar, yang memungkinkan penilaian dan unjuran yang lebih objektif. [[EENNDD]] pemotongan"], [{"string": "Finding surprising patterns in a time series database in linear time and space The problem of finding a specified pattern in a time series database i.e. query by content has received much attention and is now a relatively mature field . In contrast , the important problem of enumerating all surprising or interesting patterns has received far less attention . This problem requires a meaningful definition of `` surprise '' , and an efficient search technique . All previous attempts at finding surprising patterns in time series use a very limited notion of surprise , and\\/or do not scale to massive datasets . To overcome these limitations we introduce a novel technique that defines a pattern surprising if the frequency of its occurrence differs substantially from that expected by chance , given some previously seen data .", "keywords": ["anomaly detection", "time series", "markov model", "suffix tree", "novelty detection", "feature extraction"], "combined": "Finding surprising patterns in a time series database in linear time and space The problem of finding a specified pattern in a time series database i.e. query by content has received much attention and is now a relatively mature field . In contrast , the important problem of enumerating all surprising or interesting patterns has received far less attention . This problem requires a meaningful definition of `` surprise '' , and an efficient search technique . All previous attempts at finding surprising patterns in time series use a very limited notion of surprise , and\\/or do not scale to massive datasets . To overcome these limitations we introduce a novel technique that defines a pattern surprising if the frequency of its occurrence differs substantially from that expected by chance , given some previously seen data . [[EENNDD]] anomaly detection; time series; markov model; suffix tree; novelty detection; feature extraction"}, "Mencari corak mengejutkan dalam pangkalan data siri masa dalam waktu dan ruang linear Masalah mencari corak yang ditentukan dalam pangkalan data siri masa iaitu pertanyaan mengikut kandungan telah mendapat banyak perhatian dan kini merupakan bidang yang agak matang. Sebaliknya, masalah penting untuk menghitung semua corak yang mengejutkan atau menarik telah mendapat perhatian. Masalah ini memerlukan definisi \"kejutan\" yang bermakna, dan teknik pencarian yang cekap. Semua percubaan sebelumnya untuk mencari corak yang mengejutkan dalam siri masa menggunakan pengertian kejutan yang sangat terhad, dan \\ / atau tidak mengikut skala data yang besar. Untuk mengatasi keterbatasan ini, kami memperkenalkan teknik baru yang menentukan corak yang mengejutkan jika frekuensi kejadiannya jauh berbeza dari yang diharapkan secara kebetulan, mengingat beberapa data yang dilihat sebelumnya. [[EENNDD]] pengesanan anomali; siri masa; model markov; pokok akhiran; pengesanan kebaruan; pengekstrakan ciri"], [{"string": "Privacy preserving regression modelling via distributed computation Reluctance of data owners to share their possibly confidential or proprietary data with others who own related databases is a serious impediment to conducting a mutually beneficial data mining analysis . We address the case of vertically partitioned data -- multiple data owners\\/agencies each possess a few attributes of every data record . We focus on the case of the agencies wanting to conduct a linear regression analysis with complete records without disclosing values of their own attributes . This paper describes an algorithm that enables such agencies to compute the exact regression coefficients of the global regression equation and also perform some basic goodness-of-fit diagnostics while protecting the confidentiality of their data . In more general settings beyond the privacy scenario , this algorithm can also be viewed as method for the distributed computation for regression analyses .", "keywords": ["secure multi-party computation", "learning", "regression", "data integration", "data confidentiality"], "combined": "Privacy preserving regression modelling via distributed computation Reluctance of data owners to share their possibly confidential or proprietary data with others who own related databases is a serious impediment to conducting a mutually beneficial data mining analysis . We address the case of vertically partitioned data -- multiple data owners\\/agencies each possess a few attributes of every data record . We focus on the case of the agencies wanting to conduct a linear regression analysis with complete records without disclosing values of their own attributes . This paper describes an algorithm that enables such agencies to compute the exact regression coefficients of the global regression equation and also perform some basic goodness-of-fit diagnostics while protecting the confidentiality of their data . In more general settings beyond the privacy scenario , this algorithm can also be viewed as method for the distributed computation for regression analyses . [[EENNDD]] secure multi-party computation; learning; regression; data integration; data confidentiality"}, "Pemodelan regresi pelestarian privasi melalui pengiraan terdistribusi Keengganan pemilik data untuk berkongsi data sulit atau hak milik mereka dengan orang lain yang memiliki pangkalan data yang berkaitan adalah halangan serius untuk melakukan analisis perlombongan data yang saling menguntungkan. Kami menangani masalah data yang dipartisi secara menegak - pelbagai pemilik data / agensi masing-masing memiliki beberapa atribut setiap rekod data. Kami memberi tumpuan kepada kes agensi yang ingin melakukan analisis regresi linear dengan catatan lengkap tanpa mendedahkan nilai atribut mereka sendiri. Makalah ini menerangkan algoritma yang membolehkan agensi-agensi tersebut untuk menghitung pekali regresi yang tepat dari persamaan regresi global dan juga melakukan beberapa diagnostik kebaikan asas yang sesuai sambil melindungi kerahsiaan data mereka. Dalam tetapan yang lebih umum di luar senario privasi, algoritma ini juga dapat dilihat sebagai kaedah untuk pengiraan yang diedarkan untuk analisis regresi. [[EENNDD]] pengiraan pelbagai pihak yang selamat; belajar; regresi; penyatuan data; kerahsiaan data"], [{"string": "Efficient elastic burst detection in data streams Burst detection is the activity of finding abnormal aggregates in data streams . Such aggregates are based on sliding windows over data streams . In some applications , we want to monitor many sliding window sizes simultaneously and to report those windows with aggregates significantly different from other periods . We will present a general data structure for detecting interesting aggregates over such elastic windows in near linear time . We present applications of the algorithm for detecting Gamma Ray Bursts in large-scale astrophysical data . Detection of periods with high volumes of trading activities and high stock price volatility is also demonstrated using real time Trade and Quote TAQ data from the New York Stock Exchange NYSE . Our algorithm beats the direct computation approach by several orders of magnitude .", "keywords": ["elastic burst", "data stream"], "combined": "Efficient elastic burst detection in data streams Burst detection is the activity of finding abnormal aggregates in data streams . Such aggregates are based on sliding windows over data streams . In some applications , we want to monitor many sliding window sizes simultaneously and to report those windows with aggregates significantly different from other periods . We will present a general data structure for detecting interesting aggregates over such elastic windows in near linear time . We present applications of the algorithm for detecting Gamma Ray Bursts in large-scale astrophysical data . Detection of periods with high volumes of trading activities and high stock price volatility is also demonstrated using real time Trade and Quote TAQ data from the New York Stock Exchange NYSE . Our algorithm beats the direct computation approach by several orders of magnitude . [[EENNDD]] elastic burst; data stream"}, "Pengesanan pecahan elastik yang cekap dalam aliran data Pengesanan pecah adalah aktiviti mencari agregat yang tidak normal dalam aliran data. Agregat seperti itu berdasarkan tingkap gelongsor di atas aliran data. Dalam beberapa aplikasi, kami ingin memantau banyak saiz tetingkap gelongsor secara serentak dan melaporkan tingkap dengan agregat yang jauh berbeza dari tempoh lain. Kami akan membentangkan struktur data umum untuk mengesan agregat yang menarik pada tingkap elastik seperti itu dalam waktu hampir linear. Kami menyajikan aplikasi algoritma untuk mengesan Gamma Ray Bursts dalam data astrofizik skala besar. Pengesanan tempoh dengan jumlah aktiviti perdagangan yang tinggi dan turun naik harga saham yang tinggi juga ditunjukkan menggunakan data TAQ Trade and Quote dari masa nyata dari Bursa Saham New York NYSE. Algoritma kami mengalahkan pendekatan pengiraan langsung dengan beberapa susunan besar. [[EENNDD]] pecah elastik; aliran data"], [{"string": "Modeling and predicting user behavior in sponsored search Implicit user feedback , including click-through and subsequent browsing behavior , is crucial for evaluating and improving the quality of results returned by search engines . Several recent studies 1 , 2 , 3 , 13 , 25 have used post-result browsing behavior including the sites visited , the number of clicks , and the dwell time on site in order to improve the ranking of search results . In this paper , we first study user behavior on sponsored search results i.e. , the advertisements displayed by search engines next to the organic results , and compare this behavior to that of organic results . Second , to exploit post-result user behavior for better ranking of sponsored results , we focus on identifying patterns in user behavior and predict expected on-site actions in future instances . In particular , we show how post-result behavior depends on various properties of the queries , advertisement , sites , and users , and build a classifier using properties such as these to predict certain aspects of the user behavior . Additionally , we develop a generative model to mimic trends in observed user activity using a mixture of pareto distributions . We conduct experiments based on billions of real navigation trails collected by a major search engine 's browser toolbar .", "keywords": ["general", "implicit feedback", "sponsored search", "user behavior"], "combined": "Modeling and predicting user behavior in sponsored search Implicit user feedback , including click-through and subsequent browsing behavior , is crucial for evaluating and improving the quality of results returned by search engines . Several recent studies 1 , 2 , 3 , 13 , 25 have used post-result browsing behavior including the sites visited , the number of clicks , and the dwell time on site in order to improve the ranking of search results . In this paper , we first study user behavior on sponsored search results i.e. , the advertisements displayed by search engines next to the organic results , and compare this behavior to that of organic results . Second , to exploit post-result user behavior for better ranking of sponsored results , we focus on identifying patterns in user behavior and predict expected on-site actions in future instances . In particular , we show how post-result behavior depends on various properties of the queries , advertisement , sites , and users , and build a classifier using properties such as these to predict certain aspects of the user behavior . Additionally , we develop a generative model to mimic trends in observed user activity using a mixture of pareto distributions . We conduct experiments based on billions of real navigation trails collected by a major search engine 's browser toolbar . [[EENNDD]] general; implicit feedback; sponsored search; user behavior"}, "Memodelkan dan meramalkan tingkah laku pengguna dalam carian yang ditaja Maklum balas pengguna yang tersirat, termasuk tingkah laku klik-tayang dan penyemakan imbas seterusnya, sangat penting untuk menilai dan meningkatkan kualiti hasil yang dikembalikan oleh mesin carian. Beberapa kajian terbaru 1, 2, 3, 13, 25 telah menggunakan tingkah laku menyemak imbas pasca hasil termasuk laman web yang dikunjungi, jumlah klik, dan waktu tinggal di laman web untuk meningkatkan kedudukan hasil carian. Dalam makalah ini, kami pertama kali mengkaji tingkah laku pengguna pada hasil carian yang ditaja, iaitu iklan yang dipaparkan oleh mesin carian di sebelah hasil organik, dan membandingkan tingkah laku ini dengan hasil organik. Kedua, untuk mengeksploitasi tingkah laku pengguna pasca-hasil untuk mendapatkan keputusan yang lebih baik dari hasil tajaan, kami memberi tumpuan untuk mengenal pasti corak dalam tingkah laku pengguna dan meramalkan jangkaan tindakan di tempat pada masa akan datang. Secara khusus, kami menunjukkan bagaimana perilaku pasca hasil bergantung pada pelbagai sifat pertanyaan, iklan, laman web, dan pengguna, dan membina pengkelasan menggunakan sifat seperti ini untuk meramalkan aspek tertentu dari tingkah laku pengguna. Selain itu, kami mengembangkan model generatif untuk meniru tren aktiviti pengguna yang diperhatikan dengan menggunakan campuran pengedaran pareto. Kami melakukan eksperimen berdasarkan berbilion jejak navigasi sebenar yang dikumpulkan oleh bar alat penyemak imbas mesin carian utama. [[EENNDD]] umum; maklum balas tersirat; carian tajaan; tingkah laku pengguna"], [{"string": "A cross-collection mixture model for comparative text mining In this paper , we define and study a novel text mining problem , which we refer to as Comparative Text Mining CTM . Given a set of comparable text collections , the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differences of these collections along each common theme . This general problem subsumes many interesting applications , including business intelligence and opinion summarization . We propose a generative probabilistic mixture model for comparative text mining . The model simultaneously performs cross-collection clustering and within-collection clustering , and can be applied to an arbitrary set of comparable text collections . The model can be estimated efficiently using the Expectation-Maximization EM algorithm . We evaluate the model on two different text data sets i.e. , a news article data set and a laptop review data set , and compare it with a baseline clustering method also based on a mixture model . Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model .", "keywords": ["comparative text mining", "information search and retrieval", "clustering", "mixture models"], "combined": "A cross-collection mixture model for comparative text mining In this paper , we define and study a novel text mining problem , which we refer to as Comparative Text Mining CTM . Given a set of comparable text collections , the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differences of these collections along each common theme . This general problem subsumes many interesting applications , including business intelligence and opinion summarization . We propose a generative probabilistic mixture model for comparative text mining . The model simultaneously performs cross-collection clustering and within-collection clustering , and can be applied to an arbitrary set of comparable text collections . The model can be estimated efficiently using the Expectation-Maximization EM algorithm . We evaluate the model on two different text data sets i.e. , a news article data set and a laptop review data set , and compare it with a baseline clustering method also based on a mixture model . Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model . [[EENNDD]] comparative text mining; information search and retrieval; clustering; mixture models"}, "Model campuran pengumpulan silang untuk perlombongan teks perbandingan Dalam makalah ini, kami mendefinisikan dan mengkaji masalah perlombongan teks novel, yang kami sebut sebagai Perbandingan Teks Teks Perbandingan. Memandangkan sekumpulan koleksi teks yang dapat dibandingkan, tugas perlombongan teks perbandingan adalah untuk mencari tema umum terpendam di semua koleksi serta meringkaskan persamaan dan perbezaan koleksi ini di sepanjang setiap tema umum. Masalah umum ini merangkumi banyak aplikasi menarik, termasuk kecerdasan perniagaan dan ringkasan pendapat. Kami mencadangkan model campuran probabilistik generatif untuk perlombongan teks perbandingan. Model secara serentak melakukan pengelompokan koleksi silang dan pengelompokan dalam koleksi, dan dapat diterapkan pada sekumpulan koleksi teks yang sewenang-wenangnya. Model tersebut dapat dianggarkan dengan berkesan menggunakan algoritma Expectation-Maximization EM. Kami menilai model pada dua kumpulan data teks yang berbeza, iaitu kumpulan data artikel berita dan set data tinjauan komputer riba, dan membandingkannya dengan kaedah pengelompokan dasar yang juga berdasarkan model campuran. Hasil eksperimen menunjukkan bahawa model ini cukup berkesan dalam menemui tema umum terpendam di seluruh koleksi dan berkinerja lebih baik daripada model campuran asas kami. [[EENNDD]] perlombongan teks perbandingan; carian dan pengambilan maklumat; pengelompokan; model campuran"], [{"string": "Mining closed episodes with simultaneous events Sequential pattern discovery is a well-studied field in data mining . Episodes are sequential patterns describing events that often occur in the vicinity of each other . Episodes can impose restrictions to the order of the events , which makes them a versatile technique for describing complex patterns in the sequence . Most of the research on episodes deals with special cases such as serial , parallel , and injective episodes , while discovering general episodes is understudied . In this paper we extend the definition of an episode in order to be able to represent cases where events often occur simultaneously . We present an efficient and novel miner for discovering frequent and closed general episodes . Such a task presents unique challenges . Firstly , we can not define closure based on frequency . We solve this by computing a more conservative closure that we use to reduce the search space and discover the closed episodes as a postprocessing step . Secondly , episodes are traditionally presented as directed acyclic graphs . We argue that this representation has drawbacks leading to redundancy in the output . We solve these drawbacks by defining a subset relationship in such a way that allows us to remove the redundant episodes . We demonstrate the efficiency of our algorithm and the need for using closed episodes empirically on synthetic and real-world datasets .", "keywords": ["depth-first search", "frequent episodes", "closed episodes"], "combined": "Mining closed episodes with simultaneous events Sequential pattern discovery is a well-studied field in data mining . Episodes are sequential patterns describing events that often occur in the vicinity of each other . Episodes can impose restrictions to the order of the events , which makes them a versatile technique for describing complex patterns in the sequence . Most of the research on episodes deals with special cases such as serial , parallel , and injective episodes , while discovering general episodes is understudied . In this paper we extend the definition of an episode in order to be able to represent cases where events often occur simultaneously . We present an efficient and novel miner for discovering frequent and closed general episodes . Such a task presents unique challenges . Firstly , we can not define closure based on frequency . We solve this by computing a more conservative closure that we use to reduce the search space and discover the closed episodes as a postprocessing step . Secondly , episodes are traditionally presented as directed acyclic graphs . We argue that this representation has drawbacks leading to redundancy in the output . We solve these drawbacks by defining a subset relationship in such a way that allows us to remove the redundant episodes . We demonstrate the efficiency of our algorithm and the need for using closed episodes empirically on synthetic and real-world datasets . [[EENNDD]] depth-first search; frequent episodes; closed episodes"}, "Perlombongan episod tertutup dengan peristiwa serentak Penemuan corak berurutan adalah bidang yang dipelajari dengan baik dalam perlombongan data. Episod adalah corak berurutan yang menggambarkan peristiwa yang sering berlaku di sekitar satu sama lain. Episod boleh memberi sekatan kepada susunan peristiwa, yang menjadikannya teknik serba boleh untuk menggambarkan corak kompleks dalam urutan. Sebilangan besar penyelidikan mengenai episod berkaitan dengan kes-kes khas seperti episod bersiri, selari, dan suntikan, sementara mengetahui episod umum kurang dipelajari. Dalam makalah ini kami memperluas definisi episod agar dapat mewakili kes di mana peristiwa sering berlaku secara serentak. Kami mempersembahkan penambang yang cekap dan baru untuk mengetahui episod umum yang kerap dan tertutup. Tugas sedemikian memberikan cabaran yang unik. Pertama, kita tidak dapat menentukan penutupan berdasarkan kekerapan. Kami menyelesaikannya dengan mengira penutupan yang lebih konservatif yang kami gunakan untuk mengurangkan ruang carian dan menemui episod tertutup sebagai langkah pasca pemprosesan. Kedua, episod secara tradisional disajikan sebagai grafik asiklik yang diarahkan. Kami berpendapat bahawa perwakilan ini mempunyai kekurangan yang membawa kepada kelebihan keluaran. Kami menyelesaikan kelemahan ini dengan menentukan hubungan subset sedemikian rupa sehingga membolehkan kami membuang episod yang berlebihan. Kami menunjukkan kecekapan algoritma kami dan keperluan untuk menggunakan episod tertutup secara empirik pada kumpulan data sintetik dan dunia nyata. [[EENNDD]] carian pertama-mendalam; episod yang kerap; episod tertutup"], [{"string": "Bayesian analysis of massive datasets via particle filters Markov Chain Monte Carlo MCMC techniques revolutionized statistical practice in the 1990s by providing an essential toolkit for making the rigor and flexibility of Bayesian analysis computationally practical . At the same time the increasing prevalence of massive datasets and the expansion of the field of data mining has created the need to produce statistically sound methods that scale to these large problems . Except for the most trivial examples , current MCMC methods require a complete scan of the dataset for each iteration eliminating their candidacy as feasible data mining techniques . In this article we present a method for making Bayesian analysis of massive datasets computationally feasible . The algorithm simulates from a posterior distribution that conditions on a smaller , more manageable portion of the dataset . The remainder of the dataset may be incorporated by reweighting the initial draws using importance sampling . Computation of the importance weights requires a single scan of the remaining observations . While importance sampling increases efficiency in data access , it comes at the expense of estimation efficiency . A simple modification , based on the `` rejuvenation '' step used in particle filters for dynamic systems models , sidesteps the loss of efficiency with only a slight increase in the number of data accesses . To show proof-of-concept , we demonstrate the method on a mixture of transition models that has been used to model web traffic and robotics . For this example we show that estimation efficiency is not affected while offering a 95 % reduction in data accesses .", "keywords": ["probabilistic algorithms"], "combined": "Bayesian analysis of massive datasets via particle filters Markov Chain Monte Carlo MCMC techniques revolutionized statistical practice in the 1990s by providing an essential toolkit for making the rigor and flexibility of Bayesian analysis computationally practical . At the same time the increasing prevalence of massive datasets and the expansion of the field of data mining has created the need to produce statistically sound methods that scale to these large problems . Except for the most trivial examples , current MCMC methods require a complete scan of the dataset for each iteration eliminating their candidacy as feasible data mining techniques . In this article we present a method for making Bayesian analysis of massive datasets computationally feasible . The algorithm simulates from a posterior distribution that conditions on a smaller , more manageable portion of the dataset . The remainder of the dataset may be incorporated by reweighting the initial draws using importance sampling . Computation of the importance weights requires a single scan of the remaining observations . While importance sampling increases efficiency in data access , it comes at the expense of estimation efficiency . A simple modification , based on the `` rejuvenation '' step used in particle filters for dynamic systems models , sidesteps the loss of efficiency with only a slight increase in the number of data accesses . To show proof-of-concept , we demonstrate the method on a mixture of transition models that has been used to model web traffic and robotics . For this example we show that estimation efficiency is not affected while offering a 95 % reduction in data accesses . [[EENNDD]] probabilistic algorithms"}, "Analisis Bayesian dari kumpulan data besar melalui penapis partikel Teknik MCMC Markov Chain Monte Carlo merevolusikan praktik statistik pada tahun 1990-an dengan menyediakan alat yang penting untuk menjadikan ketelitian dan fleksibiliti analisis Bayesian praktikal secara komputasi. Pada masa yang sama peningkatan prevalensi kumpulan data besar-besaran dan pengembangan bidang perlombongan data telah menimbulkan keperluan untuk menghasilkan kaedah-kaedah yang stabil secara statistik yang dapat mengatasi masalah besar ini. Kecuali untuk contoh yang paling remeh, kaedah MCMC semasa memerlukan imbasan lengkap dari set data untuk setiap lelaran sehingga menghilangkan pencalonan mereka sebagai teknik perlombongan data yang layak. Dalam artikel ini kami menyajikan satu kaedah untuk membuat analisis Bayesian dari kumpulan data besar dapat dilakukan secara komputasi. Algoritma mensimulasikan dari taburan posterior yang menentukan bahagian data yang lebih kecil dan lebih terkawal. Selebihnya dari set data dapat digabungkan dengan menimbang semula undian awal menggunakan persampelan kepentingan. Pengiraan berat kepentingan memerlukan satu imbasan dari pemerhatian yang tinggal. Walaupun pentingnya pensampelan meningkatkan kecekapan dalam mengakses data, ia dilakukan dengan mengorbankan kecekapan anggaran. Pengubahsuaian sederhana, berdasarkan langkah \"peremajaan\" yang digunakan dalam penapis zarah untuk model sistem dinamik, mengesampingkan hilangnya kecekapan dengan hanya sedikit peningkatan jumlah akses data. Untuk menunjukkan bukti konsep, kami menunjukkan kaedah pada campuran model peralihan yang telah digunakan untuk memodelkan lalu lintas web dan robotik. Untuk contoh ini, kami menunjukkan bahawa kecekapan anggaran tidak terjejas sambil menawarkan pengurangan akses data sebanyak 95%. [[EENNDD]] algoritma probabilistik"], [{"string": "Using the fractal dimension to cluster datasets", "keywords": ["fractals"], "combined": "Using the fractal dimension to cluster datasets [[EENNDD]] fractals"}, "Menggunakan dimensi fraktal untuk mengumpulkan kumpulan data [[EENNDD]] fraktal"], [{"string": "Learning to detect malicious executables in the wild In this paper , we describe the development of a fielded application for detecting malicious executables in the wild . We gathered 1971 benign and 1651 malicious executables and encoded each as a training example using n-grams of byte codes as features . Such processing resulted in more than 255 million distinct n-grams . After selecting the most relevant n-grams for prediction , we evaluated a variety of inductive methods , including naive Bayes , decision trees , support vector machines , and boosting . Ultimately , boosted decision trees outperformed other methods with an area under the roc curve of 0.996 . Results also suggest that our methodology will scale to larger collections of executables . To the best of our knowledge , ours is the only fielded application for this task developed using techniques from machine learning and data mining .", "keywords": ["invasive software", "malicious software"], "combined": "Learning to detect malicious executables in the wild In this paper , we describe the development of a fielded application for detecting malicious executables in the wild . We gathered 1971 benign and 1651 malicious executables and encoded each as a training example using n-grams of byte codes as features . Such processing resulted in more than 255 million distinct n-grams . After selecting the most relevant n-grams for prediction , we evaluated a variety of inductive methods , including naive Bayes , decision trees , support vector machines , and boosting . Ultimately , boosted decision trees outperformed other methods with an area under the roc curve of 0.996 . Results also suggest that our methodology will scale to larger collections of executables . To the best of our knowledge , ours is the only fielded application for this task developed using techniques from machine learning and data mining . [[EENNDD]] invasive software; malicious software"}, "Belajar untuk mengesan eksekusi berbahaya di alam liar Dalam makalah ini, kami menerangkan tentang pengembangan aplikasi untuk mengesan pelaksanaan yang berniat jahat di alam liar. Kami mengumpulkan 1971 yang boleh dilaksanakan dengan baik dan 1651 yang boleh dilaksanakan yang jahat dan masing-masing dikodkan sebagai contoh latihan menggunakan n-gram kod bait sebagai ciri. Pemprosesan sedemikian menghasilkan lebih daripada 255 juta n-gram yang berbeza. Setelah memilih n-gram yang paling relevan untuk ramalan, kami menilai pelbagai kaedah induktif, termasuk Bayes naif, pohon keputusan, mesin vektor sokongan, dan peningkatan. Pada akhirnya, pohon keputusan yang lebih baik mengatasi kaedah lain dengan kawasan di bawah kurva roc 0,996. Hasilnya juga menunjukkan bahawa metodologi kami akan meningkat ke koleksi eksekusi yang lebih besar. Sepengetahuan kami, aplikasi kami adalah satu-satunya bidang tugas yang dikembangkan menggunakan teknik dari pembelajaran mesin dan perlombongan data. [[EENNDD]] perisian invasif; perisian berbahaya"], [{"string": "Parallel computation of high dimensional robust correlation and covariance matrices The computation of covariance and correlation matrices are critical to many data mining applications and processes . Unfortunately the classical covariance and correlation matrices are very sensitive to outliers . Robust methods , such as QC and the Maronna method , have been proposed . However , existing algorithms for QC only give acceptable performance when the dimensionality of the matrix is in the hundreds ; and the Maronna method is rarely used in practice because of its high computational cost . In this paper , we develop parallel algorithms for both QC and the Maronna method . We evaluate these parallel algorithms using a real data set of the gene expression of over 6,000 genes , giving rise to a matrix of over 18 million entries . In our experimental evaluation , we explore scalability in dimensionality and in the number of processors . We also compare the parallel behaviors of the two methods . After thorough experimentation , we conclude that for many data mining applications , both QC and Maronna are viable options . Less robust , but faster , QC is the recommended choice for small parallel platforms . On the other hand , the Maronna method is the recommended choice when a high degree of robustness is required , or when the parallel platform features a high number of processors .", "keywords": ["covariance", "correlation", "robust", "maronna", "parallel"], "combined": "Parallel computation of high dimensional robust correlation and covariance matrices The computation of covariance and correlation matrices are critical to many data mining applications and processes . Unfortunately the classical covariance and correlation matrices are very sensitive to outliers . Robust methods , such as QC and the Maronna method , have been proposed . However , existing algorithms for QC only give acceptable performance when the dimensionality of the matrix is in the hundreds ; and the Maronna method is rarely used in practice because of its high computational cost . In this paper , we develop parallel algorithms for both QC and the Maronna method . We evaluate these parallel algorithms using a real data set of the gene expression of over 6,000 genes , giving rise to a matrix of over 18 million entries . In our experimental evaluation , we explore scalability in dimensionality and in the number of processors . We also compare the parallel behaviors of the two methods . After thorough experimentation , we conclude that for many data mining applications , both QC and Maronna are viable options . Less robust , but faster , QC is the recommended choice for small parallel platforms . On the other hand , the Maronna method is the recommended choice when a high degree of robustness is required , or when the parallel platform features a high number of processors . [[EENNDD]] covariance; correlation; robust; maronna; parallel"}, "Pengiraan selari matriks korelasi dan kovarians tegas dimensi tinggi Pengiraan matriks kovarians dan korelasi sangat penting bagi banyak aplikasi dan proses perlombongan data. Malangnya matriks kovarians dan korelasi klasik sangat sensitif terhadap outlier. Kaedah yang mantap, seperti QC dan kaedah Maronna, telah dicadangkan. Walau bagaimanapun, algoritma yang ada untuk QC hanya memberikan prestasi yang dapat diterima apabila dimensi matriks berada dalam angka ratusan; dan kaedah Maronna jarang digunakan dalam praktiknya kerana kos pengiraannya yang tinggi. Dalam makalah ini, kami mengembangkan algoritma selari untuk QC dan kaedah Maronna. Kami menilai algoritma selari ini menggunakan set data sebenar ekspresi gen lebih daripada 6.000 gen, yang menghasilkan matriks lebih dari 18 juta entri. Dalam penilaian eksperimental kami, kami meneroka skalabiliti dalam dimensi dan jumlah pemproses. Kami juga membandingkan tingkah laku selari dari dua kaedah tersebut. Setelah percubaan menyeluruh, kami menyimpulkan bahawa untuk banyak aplikasi perlombongan data, QC dan Maronna adalah pilihan yang sesuai. Kurang mantap, tetapi lebih pantas, QC adalah pilihan yang disyorkan untuk platform selari kecil. Sebaliknya, kaedah Maronna adalah pilihan yang disyorkan apabila tahap ketahanan yang tinggi diperlukan, atau ketika platform selari mempunyai jumlah pemproses yang tinggi. [[EENNDD]] kovarians; korelasi; mantap; marona; selari"], [{"string": "Community evolution in dynamic multi-mode networks A multi-mode network typically consists of multiple heterogeneous social actors among which various types of interactions could occur . Identifying communities in a multi-mode network can help understand the structural properties of the network , address the data shortage and unbalanced problems , and assist tasks like targeted marketing and finding influential actors within or between groups . In general , a network and the membership of groups often evolve gradually . In a dynamic multi-mode network , both actor membership and interactions can evolve , which poses a challenging problem of identifying community evolution . In this work , we try to address this issue by employing the temporal information to analyze a multi-mode network . A spectral framework and its scalability issue are carefully studied . Experiments on both synthetic data and real-world large scale networks demonstrate the efficacy of our algorithm and suggest its generality in solving problems with complex relationships .", "keywords": ["community evolution", "dynamic heterogeneous network", "evolution", "dynamic network analysis", "multi-mode networks"], "combined": "Community evolution in dynamic multi-mode networks A multi-mode network typically consists of multiple heterogeneous social actors among which various types of interactions could occur . Identifying communities in a multi-mode network can help understand the structural properties of the network , address the data shortage and unbalanced problems , and assist tasks like targeted marketing and finding influential actors within or between groups . In general , a network and the membership of groups often evolve gradually . In a dynamic multi-mode network , both actor membership and interactions can evolve , which poses a challenging problem of identifying community evolution . In this work , we try to address this issue by employing the temporal information to analyze a multi-mode network . A spectral framework and its scalability issue are carefully studied . Experiments on both synthetic data and real-world large scale networks demonstrate the efficacy of our algorithm and suggest its generality in solving problems with complex relationships . [[EENNDD]] community evolution; dynamic heterogeneous network; evolution; dynamic network analysis; multi-mode networks"}, "Evolusi masyarakat dalam rangkaian multi-mod dinamik Rangkaian multi-mod biasanya terdiri daripada pelbagai pelaku sosial yang heterogen antaranya boleh berlaku pelbagai jenis interaksi. Mengenal pasti komuniti dalam rangkaian pelbagai mod dapat membantu memahami sifat struktur rangkaian, mengatasi kekurangan data dan masalah yang tidak seimbang, dan membantu tugas seperti pemasaran yang disasarkan dan mencari pelaku berpengaruh dalam atau antara kumpulan. Secara umum, rangkaian dan keanggotaan kumpulan sering berkembang secara beransur-ansur. Dalam rangkaian pelbagai mod yang dinamik, keahlian dan interaksi pelakon dapat berkembang, yang menimbulkan masalah yang sukar untuk mengenal pasti evolusi masyarakat. Dalam karya ini, kami berusaha mengatasi masalah ini dengan menggunakan maklumat temporal untuk menganalisis rangkaian pelbagai mod. Kerangka spektrum dan masalah skalabilitasnya dikaji dengan teliti. Eksperimen pada kedua-dua data sintetik dan rangkaian berskala besar dunia nyata menunjukkan keberkesanan algoritma kami dan menunjukkan keasliannya dalam menyelesaikan masalah dengan hubungan yang kompleks. [[EENNDD]] evolusi masyarakat; rangkaian heterogen dinamik; evolusi; analisis rangkaian dinamik; rangkaian pelbagai mod"], [{"string": "Sequential cost-sensitive decision making with reinforcement learning Recently , there has been increasing interest in the issues of cost-sensitive learning and decision making in a variety of applications of data mining . A number of approaches have been developed that are effective at optimizing cost-sensitive decisions when each decision is considered in isolation . However , the issue of sequential decision making , with the goal of maximizing total benefits accrued over a period of time instead of immediate benefits , has rarely been addressed . In the present paper , we propose a novel approach to sequential decision making based on the reinforcement learning framework . Our approach attempts to learn decision rules that optimize a sequence of cost-sensitive decisions so as to maximize the total benefits accrued over time . We use the domain of targeted ' marketing as a testbed for empirical evaluation of the proposed method . We conducted experiments using approximately two years of monthly promotion data derived from the well-known KDD Cup 1998 donation data set . The experimental results show that the proposed method for optimizing total accrued benefits out performs the usual targeted-marketing methodology of optimizing each promotion in isolation . We also analyze the behavior of the targeting rules that were obtained and discuss their appropriateness to the application domain .", "keywords": ["learning"], "combined": "Sequential cost-sensitive decision making with reinforcement learning Recently , there has been increasing interest in the issues of cost-sensitive learning and decision making in a variety of applications of data mining . A number of approaches have been developed that are effective at optimizing cost-sensitive decisions when each decision is considered in isolation . However , the issue of sequential decision making , with the goal of maximizing total benefits accrued over a period of time instead of immediate benefits , has rarely been addressed . In the present paper , we propose a novel approach to sequential decision making based on the reinforcement learning framework . Our approach attempts to learn decision rules that optimize a sequence of cost-sensitive decisions so as to maximize the total benefits accrued over time . We use the domain of targeted ' marketing as a testbed for empirical evaluation of the proposed method . We conducted experiments using approximately two years of monthly promotion data derived from the well-known KDD Cup 1998 donation data set . The experimental results show that the proposed method for optimizing total accrued benefits out performs the usual targeted-marketing methodology of optimizing each promotion in isolation . We also analyze the behavior of the targeting rules that were obtained and discuss their appropriateness to the application domain . [[EENNDD]] learning"}, "Membuat keputusan sensitif kos secara berurutan dengan pembelajaran pengukuhan Baru-baru ini, minat yang semakin meningkat terhadap isu pembelajaran sensitif kos dan pengambilan keputusan dalam pelbagai aplikasi perlombongan data. Sejumlah pendekatan telah dikembangkan yang efektif dalam mengoptimumkan keputusan yang sensitif terhadap biaya ketika setiap keputusan dipertimbangkan secara terpisah. Walau bagaimanapun, isu pengambilan keputusan secara berurutan, dengan tujuan memaksimumkan jumlah faedah yang diperoleh dalam jangka masa dan bukannya faedah segera, jarang ditangani. Dalam makalah ini, kami mengusulkan pendekatan baru untuk membuat keputusan berurutan berdasarkan kerangka pembelajaran pengukuhan. Pendekatan kami cuba mempelajari peraturan keputusan yang mengoptimumkan urutan keputusan yang sensitif terhadap kos sehingga dapat memaksimumkan jumlah faedah yang diperoleh dari masa ke masa. Kami menggunakan domain pemasaran yang disasarkan sebagai ujian untuk penilaian empirik kaedah yang dicadangkan. Kami menjalankan eksperimen menggunakan data promosi bulanan kira-kira dua tahun yang berasal dari kumpulan data sumbangan KDD Cup 1998 yang terkenal. Hasil eksperimen menunjukkan bahawa kaedah yang dicadangkan untuk mengoptimumkan jumlah faedah terakru melaksanakan metodologi pemasaran sasaran biasa untuk mengoptimumkan setiap promosi secara terpisah. Kami juga menganalisis tingkah laku peraturan penargetan yang diperoleh dan membincangkan kesesuaiannya dengan domain aplikasi. [[EENNDD]] pembelajaran"], [{"string": "Joint latent topic models for text and citations In this work , we address the problem of joint modeling of text and citations in the topic modeling framework . We present two different models called the Pairwise-Link-LDA and the Link-PLSA-LDA models . The Pairwise-Link-LDA model combines the ideas of LDA 4 and Mixed Membership Block Stochastic Models 1 and allows modeling arbitrary link structure . However , the model is computationally expensive , since it involves modeling the presence or absence of a citation link between every pair of documents . The second model solves this problem by assuming that the link structure is a bipartite graph . As the name indicates , Link-PLSA-LDA model combines the LDA and PLSA models into a single graphical model . Our experiments on a subset of Citeseer data show that both these models are able to predict unseen data better than the baseline model of Erosheva and Lafferty 8 , by capturing the notion of topical similarity between the contents of the cited and citing documents . Our experiments on two different data sets on the link prediction task show that the Link-PLSA-LDA model performs the best on the citation prediction task , while also remaining highly scalable . In addition , we also present some interesting visualizations generated by each of the models .", "keywords": ["citations", "hyperlinks", "topic models", "variational inference", "learning", "plsa", "influence", "lda"], "combined": "Joint latent topic models for text and citations In this work , we address the problem of joint modeling of text and citations in the topic modeling framework . We present two different models called the Pairwise-Link-LDA and the Link-PLSA-LDA models . The Pairwise-Link-LDA model combines the ideas of LDA 4 and Mixed Membership Block Stochastic Models 1 and allows modeling arbitrary link structure . However , the model is computationally expensive , since it involves modeling the presence or absence of a citation link between every pair of documents . The second model solves this problem by assuming that the link structure is a bipartite graph . As the name indicates , Link-PLSA-LDA model combines the LDA and PLSA models into a single graphical model . Our experiments on a subset of Citeseer data show that both these models are able to predict unseen data better than the baseline model of Erosheva and Lafferty 8 , by capturing the notion of topical similarity between the contents of the cited and citing documents . Our experiments on two different data sets on the link prediction task show that the Link-PLSA-LDA model performs the best on the citation prediction task , while also remaining highly scalable . In addition , we also present some interesting visualizations generated by each of the models . [[EENNDD]] citations; hyperlinks; topic models; variational inference; learning; plsa; influence; lda"}, "Model topik laten bersama untuk teks dan kutipan Dalam karya ini, kita menangani masalah pemodelan bersama teks dan petikan dalam kerangka pemodelan topik. Kami menyajikan dua model berbeza yang disebut model Pairwise-Link-LDA dan model Link-PLSA-LDA. Model Pairwise-Link-LDA menggabungkan idea LDA 4 dan Blok Keahlian Campuran Model Stokastik 1 dan membolehkan pemodelan struktur pautan sewenang-wenangnya. Walau bagaimanapun, model ini sangat mahal, kerana melibatkan pemodelan kehadiran atau ketiadaan tautan petikan antara setiap pasangan dokumen. Model kedua menyelesaikan masalah ini dengan menganggap bahawa struktur pautan adalah graf bipartit. Seperti namanya, model Link-PLSA-LDA menggabungkan model LDA dan PLSA menjadi satu model grafik. Eksperimen kami pada subset data Citeseer menunjukkan bahawa kedua-dua model ini dapat meramalkan data yang tidak dapat dilihat lebih baik daripada model asas Erosheva dan Lafferty 8, dengan menangkap pengertian persamaan topikal antara isi dokumen yang dikutip dan dikutip. Eksperimen kami pada dua set data yang berbeza pada tugas ramalan pautan menunjukkan bahawa model Link-PLSA-LDA menunjukkan prestasi terbaik pada tugas ramalan kutipan, dan juga tetap sangat berskala. Di samping itu, kami juga menyajikan beberapa visualisasi menarik yang dihasilkan oleh setiap model. [[EENNDD]] petikan; pautan hiper; model topik; inferens variasi; belajar; plsa; pengaruh; lda"], [{"string": "IDR\\/QR : an incremental dimension reduction algorithm via QR decomposition Dimension reduction is critical for many database and data mining applications , such as efficient storage and retrieval of high-dimensional data . In the literature , a well-known dimension reduction scheme is Linear Discriminant Analysis LDA . The common aspect of previously proposed LDA based algorithms is the use of Singular Value Decomposition SVD . Due to the difficulty of designing an incremental solution for the eigenvalue problem on the product of scatter matrices in LDA , there is little work on designing incremental LDA algorithms . In this paper , we propose an LDA based incremental dimension reduction algorithm , called IDR\\/QR , which applies QR Decomposition rather than SVD . Unlike other LDA based algorithms , this algorithm does not require the whole data matrix in main memory . This is desirable for large data sets . More importantly , with the insertion of new data items , the IDR\\/QR algorithm can constrain the computational cost by applying efficient QR-updating techniques . Finally , we evaluate the effectiveness of the IDR\\/QR algorithm in terms of classification accuracy on the reduced dimensional space . Our experiments on several real-world data sets reveal that the accuracy achieved by the IDR\\/QR algorithm is very close to the best possible accuracy achieved by other LDA based algorithms . However , the IDR\\/QR algorithm has much less computational cost , especially when new data items are dynamically inserted .", "keywords": ["dimension reduction", "incremental learning", "linear discriminant analysis", "qr decomposition"], "combined": "IDR\\/QR : an incremental dimension reduction algorithm via QR decomposition Dimension reduction is critical for many database and data mining applications , such as efficient storage and retrieval of high-dimensional data . In the literature , a well-known dimension reduction scheme is Linear Discriminant Analysis LDA . The common aspect of previously proposed LDA based algorithms is the use of Singular Value Decomposition SVD . Due to the difficulty of designing an incremental solution for the eigenvalue problem on the product of scatter matrices in LDA , there is little work on designing incremental LDA algorithms . In this paper , we propose an LDA based incremental dimension reduction algorithm , called IDR\\/QR , which applies QR Decomposition rather than SVD . Unlike other LDA based algorithms , this algorithm does not require the whole data matrix in main memory . This is desirable for large data sets . More importantly , with the insertion of new data items , the IDR\\/QR algorithm can constrain the computational cost by applying efficient QR-updating techniques . Finally , we evaluate the effectiveness of the IDR\\/QR algorithm in terms of classification accuracy on the reduced dimensional space . Our experiments on several real-world data sets reveal that the accuracy achieved by the IDR\\/QR algorithm is very close to the best possible accuracy achieved by other LDA based algorithms . However , the IDR\\/QR algorithm has much less computational cost , especially when new data items are dynamically inserted . [[EENNDD]] dimension reduction; incremental learning; linear discriminant analysis; qr decomposition"}, "IDR \\ / QR: algoritma pengurangan dimensi tambahan melalui penguraian QR Pengurangan dimensi sangat penting untuk banyak aplikasi perlombongan pangkalan data dan data, seperti penyimpanan dan pengambilan data dimensi tinggi yang cekap. Dalam literatur, skema pengurangan dimensi yang terkenal adalah Linear Discriminant Analysis LDA. Aspek umum algoritma berasaskan LDA yang dicadangkan sebelumnya adalah penggunaan Singular Value Decomposition SVD. Oleh kerana kesukaran merancang penyelesaian tambahan untuk masalah nilai eigen pada produk matriks penyebaran di LDA, ada sedikit usaha untuk merancang algoritma LDA tambahan. Dalam makalah ini, kami mencadangkan algoritma pengurangan dimensi kenaikan berdasarkan LDA, yang disebut IDR \\ / QR, yang menerapkan Penguraian QR dan bukan SVD. Tidak seperti algoritma berasaskan LDA yang lain, algoritma ini tidak memerlukan keseluruhan matriks data dalam memori utama. Ini wajar untuk set data yang besar. Lebih penting lagi, dengan memasukkan item data baru, algoritma IDR \\ / QR dapat mengekang kos pengiraan dengan menerapkan teknik mengemas kini QR yang cekap. Akhirnya, kami menilai keberkesanan algoritma IDR \\ / QR dari segi ketepatan klasifikasi pada ruang dimensi yang dikurangkan. Eksperimen kami pada beberapa set data dunia nyata menunjukkan bahawa ketepatan yang dicapai oleh algoritma IDR \\ / QR sangat hampir dengan ketepatan terbaik yang dicapai oleh algoritma berasaskan LDA yang lain. Walau bagaimanapun, algoritma IDR \\ / QR mempunyai kos pengiraan yang jauh lebih rendah, terutamanya apabila item data baru dimasukkan secara dinamik. [[EENNDD]] pengurangan dimensi; pembelajaran tambahan; analisis diskriminasi linear; penguraian qr"], [{"string": "A multinomial clustering model for fast simulation of computer architecture designs Computer architects utilize simulation tools to evaluate the merits of a new design feature . The time needed to adequately evaluate the tradeoffs associated with adding any new feature has become a critical issue . Recent work has found that by identifying execution phases present in common workloads used in simulation studies , we can apply clustering algorithms to significantly reduce the amount of time needed to complete the simulation . Our goal in this paper is to demonstrate the value of this approach when applied to the set of industry-standard benchmarks most commonly used in computer architecture studies . We also look to improve upon prior work by applying more appropriate clustering algorithms to identify phases , and to further reduce simulation time . We find that the phase clustering in computer architecture simulation has many similarities to text clustering . In prior work on clustering techniques to reduce simulation time , K-means clustering was used to identify representative program phases . In this paper we apply a mixture of multinomials to the clustering problem and show its advantages over using K-means on simulation data . We have implemented these two clustering algorithms and evaluate how well they can characterize program behavior . By adopting a mixture of multinomials model , we find that we can maintain simulation result fidelity , while greatly reducing overall simulation time . We report results for a range of applications taken from the SPEC2000 benchmark suite .", "keywords": ["k-means", "program phase", "em", "single data stream architectures", "simulation", "mixture of multinomials", "clustering"], "combined": "A multinomial clustering model for fast simulation of computer architecture designs Computer architects utilize simulation tools to evaluate the merits of a new design feature . The time needed to adequately evaluate the tradeoffs associated with adding any new feature has become a critical issue . Recent work has found that by identifying execution phases present in common workloads used in simulation studies , we can apply clustering algorithms to significantly reduce the amount of time needed to complete the simulation . Our goal in this paper is to demonstrate the value of this approach when applied to the set of industry-standard benchmarks most commonly used in computer architecture studies . We also look to improve upon prior work by applying more appropriate clustering algorithms to identify phases , and to further reduce simulation time . We find that the phase clustering in computer architecture simulation has many similarities to text clustering . In prior work on clustering techniques to reduce simulation time , K-means clustering was used to identify representative program phases . In this paper we apply a mixture of multinomials to the clustering problem and show its advantages over using K-means on simulation data . We have implemented these two clustering algorithms and evaluate how well they can characterize program behavior . By adopting a mixture of multinomials model , we find that we can maintain simulation result fidelity , while greatly reducing overall simulation time . We report results for a range of applications taken from the SPEC2000 benchmark suite . [[EENNDD]] k-means; program phase; em; single data stream architectures; simulation; mixture of multinomials; clustering"}, "Model pengelompokan multinomial untuk simulasi pantas reka bentuk seni bina komputer Arkitek komputer menggunakan alat simulasi untuk menilai kelebihan ciri reka bentuk baru. Masa yang diperlukan untuk menilai dengan tepat pertukaran yang berkaitan dengan penambahan sebarang ciri baru telah menjadi masalah penting. Hasil kerja terbaru mendapati bahawa dengan mengenal pasti fasa pelaksanaan yang terdapat dalam beban kerja yang biasa digunakan dalam kajian simulasi, kita dapat menerapkan algoritma pengelompokan untuk mengurangkan jumlah waktu yang diperlukan untuk menyelesaikan simulasi. Tujuan kami dalam makalah ini adalah untuk menunjukkan nilai pendekatan ini ketika diterapkan pada set penanda aras standard industri yang paling sering digunakan dalam kajian seni bina komputer. Kami juga ingin memperbaiki pekerjaan sebelumnya dengan menerapkan algoritma pengelompokan yang lebih tepat untuk mengenal pasti fasa, dan untuk mengurangkan masa simulasi. Kami mendapati bahawa pengelompokan fasa dalam simulasi seni bina komputer mempunyai banyak persamaan dengan pengelompokan teks. Dalam pekerjaan sebelumnya mengenai teknik pengelompokan untuk mengurangi waktu simulasi, K-berarti pengelompokan digunakan untuk mengenal pasti fasa program yang representatif. Dalam makalah ini kami menerapkan campuran multinomial untuk masalah pengelompokan dan menunjukkan kelebihannya daripada menggunakan cara-K pada data simulasi. Kami telah melaksanakan dua algoritma kluster ini dan menilai seberapa baik mereka dapat mencirikan tingkah laku program. Dengan mengadopsi campuran model multinomial, kita dapati bahawa kita dapat mengekalkan kesetiaan hasil simulasi, sekaligus mengurangkan keseluruhan waktu simulasi. Kami melaporkan hasil untuk pelbagai aplikasi yang diambil dari suite penanda aras SPEC2000. [[EENNDD]] k-bermaksud; fasa program; em; seni bina aliran data tunggal; simulasi; campuran multinomial; pengelompokan"], [{"string": "On the privacy of anonymized networks The proliferation of online social networks , and the concomitant accumulation of user data , give rise to hotly debated issues of privacy , security , and control . One specific challenge is the sharing or public release of anonymized data without accidentally leaking personally identifiable information PII . Unfortunately , it is often difficult to ascertain that sophisticated statistical techniques , potentially employing additional external data sources , are unable to break anonymity . In this paper , we consider an instance of this problem , where the object of interest is the structure of a social network , i.e. , a graph describing users and their links . Recent work demonstrates that anonymizing node identities may not be sufficient to keep the network private : the availability of node and link data from another domain , which is correlated with the anonymized network , has been used to re-identify the anonymized nodes . This paper is about conditions under which such a de-anonymization process is possible . We attempt to shed light on the following question : can we assume that a sufficiently sparse network is inherently anonymous , in the sense that even with unlimited computational power , de-anonymization is impossible ? Our approach is to introduce a random graph model for a version of the de-anonymization problem , which is parameterized by the expected node degree and a similarity parameter that controls the correlation between two graphs over the same vertex set . We find simple conditions on these parameters delineating the boundary of privacy , and show that the mean node degree need only grow slightly faster than log n with network size n for nodes to be identifiable . Our results have policy implications for sharing of anonymized network information .", "keywords": ["general", "de-anonymization", "network privacy", "graph sampling", "social networks", "random graphs"], "combined": "On the privacy of anonymized networks The proliferation of online social networks , and the concomitant accumulation of user data , give rise to hotly debated issues of privacy , security , and control . One specific challenge is the sharing or public release of anonymized data without accidentally leaking personally identifiable information PII . Unfortunately , it is often difficult to ascertain that sophisticated statistical techniques , potentially employing additional external data sources , are unable to break anonymity . In this paper , we consider an instance of this problem , where the object of interest is the structure of a social network , i.e. , a graph describing users and their links . Recent work demonstrates that anonymizing node identities may not be sufficient to keep the network private : the availability of node and link data from another domain , which is correlated with the anonymized network , has been used to re-identify the anonymized nodes . This paper is about conditions under which such a de-anonymization process is possible . We attempt to shed light on the following question : can we assume that a sufficiently sparse network is inherently anonymous , in the sense that even with unlimited computational power , de-anonymization is impossible ? Our approach is to introduce a random graph model for a version of the de-anonymization problem , which is parameterized by the expected node degree and a similarity parameter that controls the correlation between two graphs over the same vertex set . We find simple conditions on these parameters delineating the boundary of privacy , and show that the mean node degree need only grow slightly faster than log n with network size n for nodes to be identifiable . Our results have policy implications for sharing of anonymized network information . [[EENNDD]] general; de-anonymization; network privacy; graph sampling; social networks; random graphs"}, "Mengenai privasi rangkaian tanpa nama Penyebaran rangkaian sosial dalam talian, dan pengumpulan data pengguna yang bersamaan, menimbulkan isu privasi, keselamatan, dan kawalan yang diperdebatkan. Satu cabaran khusus adalah perkongsian atau pelepasan data tanpa nama secara terbuka tanpa membocorkan maklumat peribadi PII secara tidak sengaja. Malangnya, seringkali sukar untuk memastikan bahawa teknik statistik yang canggih, yang berpotensi menggunakan sumber data luaran tambahan, tidak dapat memecahkan namanya. Dalam makalah ini, kami mempertimbangkan contoh masalah ini, di mana objek yang diminati adalah struktur rangkaian sosial, iaitu grafik yang menggambarkan pengguna dan pautan mereka. Kerja terbaru menunjukkan bahawa identiti nod tanpa nama mungkin tidak mencukupi untuk menjaga privasi rangkaian: ketersediaan nod dan data pautan dari domain lain, yang berkorelasi dengan rangkaian tanpa nama, telah digunakan untuk mengenal pasti semula nod yang tidak dianonimkan. Makalah ini adalah mengenai keadaan di mana proses penghapusan anonimasi mungkin dilakukan. Kami cuba menjelaskan persoalan berikut: bolehkah kita menganggap bahawa rangkaian yang cukup jarang secara semula jadi tanpa nama, dalam arti bahawa walaupun dengan kekuatan komputasi yang tidak terhad, penghapusan anonimisasi tidak mungkin dilakukan? Pendekatan kami adalah untuk memperkenalkan model grafik rawak untuk versi dari masalah de-anonimisasi, yang diukur berdasarkan tahap node yang diharapkan dan parameter kesamaan yang mengawal korelasi antara dua grafik pada set bucu yang sama. Kami dapati syarat-syarat sederhana pada parameter-parameter ini menggambarkan batas privasi, dan menunjukkan bahawa tahap node min hanya perlu tumbuh sedikit lebih cepat daripada log n dengan ukuran rangkaian n agar node dapat dikenal pasti. Hasil kami mempunyai implikasi dasar untuk berkongsi maklumat rangkaian tanpa nama. [[EENNDD]] umum; nyah anonimisasi; privasi rangkaian; persampelan grafik; rangkaian sosial; graf rawak"], [{"string": "Detecting anomalous records in categorical datasets We consider the problem of detecting anomalies in high aritycategorical datasets . In most applications , anomalies are defined as datapoints that are `` abnormal '' . Quite often we have access to data which consists mostly of normal records , a long with a small percentage of unlabelled anomalous records . We are interested in the problem of unsupervised anomaly detection , where we use the unlabelled data for training , and detect records that do not follow the definition of normality . A standard approach is to create a model of normal data , and compare test records against it . A probabilistic approach builds a likelihood model from the training data . Records are tested for anomalies based on the complete record likelihood given the probability model . For categorical attributes , bayes nets give a standard representation of the likelihood . While this approach is good at finding outliers in the dataset , it often tends to detect records with attribute values that are rare . Sometimes , just detecting rare values of an attribute is not desired and such outliers are not considered as anomalies in that context . We present an alternative definition of anomalies , and propose an approach of comparing against marginal distribution of attribute subsets . We show that this is a more meaningful way of detecting anomalies , and has a better performance over semi-synthetic as well as real world datasets .", "keywords": ["anomaly detection", "machine learning"], "combined": "Detecting anomalous records in categorical datasets We consider the problem of detecting anomalies in high aritycategorical datasets . In most applications , anomalies are defined as datapoints that are `` abnormal '' . Quite often we have access to data which consists mostly of normal records , a long with a small percentage of unlabelled anomalous records . We are interested in the problem of unsupervised anomaly detection , where we use the unlabelled data for training , and detect records that do not follow the definition of normality . A standard approach is to create a model of normal data , and compare test records against it . A probabilistic approach builds a likelihood model from the training data . Records are tested for anomalies based on the complete record likelihood given the probability model . For categorical attributes , bayes nets give a standard representation of the likelihood . While this approach is good at finding outliers in the dataset , it often tends to detect records with attribute values that are rare . Sometimes , just detecting rare values of an attribute is not desired and such outliers are not considered as anomalies in that context . We present an alternative definition of anomalies , and propose an approach of comparing against marginal distribution of attribute subsets . We show that this is a more meaningful way of detecting anomalies , and has a better performance over semi-synthetic as well as real world datasets . [[EENNDD]] anomaly detection; machine learning"}, "Mengesan rekod anomali dalam set data kategori Kami menganggap masalah mengesan anomali dalam set data kategori tinggi. Dalam kebanyakan aplikasi, anomali didefinisikan sebagai titik data yang \"tidak normal\". Selalunya kita mempunyai akses ke data yang kebanyakannya terdiri daripada catatan biasa, panjang dengan peratusan kecil rekod anomali yang tidak berlabel. Kami berminat dengan masalah pengesanan anomali tanpa pengawasan, di mana kami menggunakan data yang tidak berlabel untuk latihan, dan mengesan rekod yang tidak mengikuti definisi normal. Pendekatan standard adalah membuat model data normal, dan membandingkan rekod ujian terhadapnya. Pendekatan probabilistik membina model kemungkinan dari data latihan. Rekod diuji untuk anomali berdasarkan kemungkinan rekod lengkap memandangkan model kebarangkalian. Untuk atribut kategori, jaring bayes memberikan gambaran standard kemungkinan. Walaupun pendekatan ini pandai mencari outliers dalam set data, ia cenderung cenderung untuk mengesan rekod dengan nilai atribut yang jarang berlaku. Kadang kala, hanya mengesan nilai langka atribut tidak diinginkan dan penyimpangan tersebut tidak dianggap sebagai anomali dalam konteks itu. Kami menyajikan definisi alternatif mengenai anomali, dan mencadangkan pendekatan membandingkan dengan pembahagian margin subset atribut. Kami menunjukkan bahawa ini adalah kaedah yang lebih bermakna untuk mengesan anomali, dan mempunyai prestasi yang lebih baik berbanding set data separa sintetik dan juga dunia nyata. [[EENNDD]] pengesanan anomali; pembelajaran mesin"], [{"string": "An integrated framework on mining logs files for computing system management Traditional approaches to system management have been largely based on domain experts through a knowledge acquisition process that translates domain knowledge into operating rules and policies . This has been well known and experienced as a cumbersome , labor intensive , and error prone process . In addition , this process is difficult to keep up with the rapidly changing environments . In this paper , we will describe our research efforts on establishing an integrated framework for mining system log files for automatic management . In particular , we apply text mining techniques to categorize messages in log files into common situations , improve categorization accuracy by considering the temporal characteristics of log messages , develop temporal mining techniques to discover the relationships between different events , and utilize visualization tools to evaluate and validate the interesting temporal patterns for system management .", "keywords": ["applications", "event relationship", "learning", "temporal pattern", "log categorization", "system management"], "combined": "An integrated framework on mining logs files for computing system management Traditional approaches to system management have been largely based on domain experts through a knowledge acquisition process that translates domain knowledge into operating rules and policies . This has been well known and experienced as a cumbersome , labor intensive , and error prone process . In addition , this process is difficult to keep up with the rapidly changing environments . In this paper , we will describe our research efforts on establishing an integrated framework for mining system log files for automatic management . In particular , we apply text mining techniques to categorize messages in log files into common situations , improve categorization accuracy by considering the temporal characteristics of log messages , develop temporal mining techniques to discover the relationships between different events , and utilize visualization tools to evaluate and validate the interesting temporal patterns for system management . [[EENNDD]] applications; event relationship; learning; temporal pattern; log categorization; system management"}, "Rangka kerja terpadu pada fail log perlombongan untuk pengurusan sistem pengkomputeran Pendekatan tradisional untuk pengurusan sistem sebahagian besarnya didasarkan pada pakar domain melalui proses pemerolehan pengetahuan yang menerjemahkan pengetahuan domain menjadi peraturan dan polisi operasi. Ini telah terkenal dan dialami sebagai proses yang rumit, intensif buruh, dan ralat. Di samping itu, proses ini sukar dikendalikan dengan persekitaran yang cepat berubah. Dalam makalah ini, kami akan menerangkan usaha penyelidikan kami untuk mewujudkan kerangka bersepadu untuk fail log sistem perlombongan untuk pengurusan automatik. Khususnya, kami menerapkan teknik perlombongan teks untuk mengkategorikan mesej dalam fail log ke dalam situasi umum, meningkatkan ketepatan pengkategorian dengan mempertimbangkan ciri temporal mesej log, mengembangkan teknik perlombongan temporal untuk mengetahui hubungan antara peristiwa yang berbeza, dan menggunakan alat visualisasi untuk menilai dan mengesahkan corak temporal yang menarik untuk pengurusan sistem. [[EENNDD]] aplikasi; hubungan peristiwa; belajar; corak temporal; pengkategorian log; pengurusan sistem"], [{"string": "Mining distance-based outliers from large databases in any metric space Let R be a set of objects . An object o \u2208 R is an outlier , if there exist less than k objects in R whose distances to o are at most r. The values of k , r , and the distance metric are provided by a user at the run time . The objective is to return all outliers with the smallest I\\/O cost . This paper considers a generic version of the problem , where no information is available for outlier computation , except for objects ' mutual distances . We prove an upper bound for the memory consumption which permits the discovery of all outliers by scanning the dataset 3 times . The upper bound turns out to be extremely low in practice , e.g. , less than 1 % of R. Since the actual memory capacity of a realistic DBMS is typically larger , we develop a novel algorithm , which integrates our theoretical findings with carefully-designed heuristics that leverage the additional memory to improve I\\/O efficiency . Our technique reports all outliers by scanning the dataset at most twice in some cases , even once , and significantly outperforms the existing solutions by a factor up to an order of magnitude .", "keywords": ["metric data", "information search and retrieval", "mining", "outlier"], "combined": "Mining distance-based outliers from large databases in any metric space Let R be a set of objects . An object o \u2208 R is an outlier , if there exist less than k objects in R whose distances to o are at most r. The values of k , r , and the distance metric are provided by a user at the run time . The objective is to return all outliers with the smallest I\\/O cost . This paper considers a generic version of the problem , where no information is available for outlier computation , except for objects ' mutual distances . We prove an upper bound for the memory consumption which permits the discovery of all outliers by scanning the dataset 3 times . The upper bound turns out to be extremely low in practice , e.g. , less than 1 % of R. Since the actual memory capacity of a realistic DBMS is typically larger , we develop a novel algorithm , which integrates our theoretical findings with carefully-designed heuristics that leverage the additional memory to improve I\\/O efficiency . Our technique reports all outliers by scanning the dataset at most twice in some cases , even once , and significantly outperforms the existing solutions by a factor up to an order of magnitude . [[EENNDD]] metric data; information search and retrieval; mining; outlier"}, "Perlombongan berdasarkan jarak jauh dari pangkalan data besar di mana-mana ruang metrik Biarkan R menjadi sekumpulan objek. Objek o \u2208 R adalah garis luar, jika terdapat kurang dari k objek di R yang jaraknya ke o paling banyak r. Nilai k, r, dan metrik jarak disediakan oleh pengguna pada masa berjalan. Objektifnya adalah untuk mengembalikan semua outliers dengan kos I \\ / O terkecil. Makalah ini mempertimbangkan versi umum masalah, di mana tidak ada maklumat yang tersedia untuk pengiraan luar, kecuali jarak bersama objek. Kami membuktikan batas atas penggunaan memori yang membolehkan penemuan semua outliers dengan mengimbas set data sebanyak 3 kali. Batas atas ternyata sangat rendah dalam praktik, mis. , kurang dari 1% R. Oleh kerana kapasiti memori sebenar DBMS yang realistik biasanya lebih besar, kami mengembangkan algoritma baru, yang menggabungkan penemuan teori kami dengan heuristik yang dirancang dengan teliti yang memanfaatkan memori tambahan untuk meningkatkan kecekapan I \\ / O. Teknik kami melaporkan semua outliers dengan mengimbas set data paling banyak dua kali dalam beberapa kes, bahkan sekali, dan secara signifikan mengatasi penyelesaian yang ada dengan faktor hingga tahap yang besar. [[EENNDD]] data metrik; carian dan pengambilan maklumat; perlombongan; orang luar"], [{"string": "Structural and temporal analysis of the blogosphere through community factorization The blogosphere has unique structural and temporal properties since blogs are typically used as communication media among human individuals . In this paper , we propose a novel technique that captures the structure and temporal dynamics of blog communities . In our framework , a community is a set of blogs that communicate with each other triggered by some events such as a news article . The community is represented by its structure and temporal dynamics : a community graph indicates how often one blog communicates with another , and a community intensity indicates the activity level of the community that varies over time . Our method , community factorization , extracts such communities from the blogosphere , where the communication among blogs is observed as a set of subgraphs i.e. , threads of discussion . This community extraction is formulated as a factorization problem in the framework of constrained optimization , in which the objective is to best explain the observed interactions in the blogosphere over time . We further provide a scalable algorithm for computing solutions to the constrained optimization problems . Extensive experimental studies on both synthetic and real blog data demonstrate that our technique is able to discover meaningful communities that are not detectable by traditional methods .", "keywords": ["regularization", "iterative search", "non-negative matrix factorization", "community factorization", "blogosphere", "blog"], "combined": "Structural and temporal analysis of the blogosphere through community factorization The blogosphere has unique structural and temporal properties since blogs are typically used as communication media among human individuals . In this paper , we propose a novel technique that captures the structure and temporal dynamics of blog communities . In our framework , a community is a set of blogs that communicate with each other triggered by some events such as a news article . The community is represented by its structure and temporal dynamics : a community graph indicates how often one blog communicates with another , and a community intensity indicates the activity level of the community that varies over time . Our method , community factorization , extracts such communities from the blogosphere , where the communication among blogs is observed as a set of subgraphs i.e. , threads of discussion . This community extraction is formulated as a factorization problem in the framework of constrained optimization , in which the objective is to best explain the observed interactions in the blogosphere over time . We further provide a scalable algorithm for computing solutions to the constrained optimization problems . Extensive experimental studies on both synthetic and real blog data demonstrate that our technique is able to discover meaningful communities that are not detectable by traditional methods . [[EENNDD]] regularization; iterative search; non-negative matrix factorization; community factorization; blogosphere; blog"}, "Analisis struktur dan temporal blogosfera melalui faktorisasi masyarakat Blogosphere mempunyai sifat struktur dan temporal yang unik kerana blog biasanya digunakan sebagai media komunikasi di kalangan individu manusia. Dalam makalah ini, kami mencadangkan teknik novel yang menangkap struktur dan dinamika temporal komuniti blog. Dalam kerangka kerja kami, komuniti adalah sekumpulan blog yang saling berkomunikasi yang dipicu oleh beberapa peristiwa seperti artikel berita. Komuniti diwakili oleh struktur dan dinamika temporalnya: grafik komuniti menunjukkan seberapa kerap satu blog berkomunikasi dengan yang lain, dan intensiti komuniti menunjukkan tahap aktiviti komuniti yang berbeza-beza dari masa ke masa. Kaedah kami, faktorisasi komuniti, mengekstrak komuniti semacam itu dari blogosfera, di mana komunikasi antara blog diperhatikan sebagai sekumpulan subgraf iaitu, rangkaian perbincangan. Pengekstrakan komuniti ini dirumuskan sebagai masalah faktorisasi dalam kerangka pengoptimuman yang dibatasi, di mana objektifnya adalah untuk menerangkan dengan lebih baik interaksi yang diperhatikan di blogosphere dari masa ke masa. Kami seterusnya menyediakan algoritma berskala untuk penyelesaian pengkomputeran untuk masalah pengoptimuman yang terkendali. Kajian eksperimen yang meluas pada data blog sintetik dan nyata menunjukkan bahawa teknik kami dapat menemui komuniti yang bermakna yang tidak dapat dikesan dengan kaedah tradisional. [[EENNDD]] regularisasi; carian berulang; pemfaktoran matriks bukan negatif; pemfaktoran masyarakat; blogosfera; blog"], [{"string": "Augmenting the generalized hough transform to enable the mining of petroglyphs Rock art is an archaeological term for human-made markings on stone . It is believed that there are millions of petroglyphs in North America alone , and the study of this valued cultural resource has implications even beyond anthropology and history . Surprisingly , although image processing , information retrieval and data mining have had large impacts on many human endeavors , they have had essentially zero impact on the study of rock art . In this work we identify the reasons for this , and introduce a novel distance measure and algorithms which allow efficient and effective data mining of large collections of rock art .", "keywords": ["image processing", "similarity search", "cultural artifacts"], "combined": "Augmenting the generalized hough transform to enable the mining of petroglyphs Rock art is an archaeological term for human-made markings on stone . It is believed that there are millions of petroglyphs in North America alone , and the study of this valued cultural resource has implications even beyond anthropology and history . Surprisingly , although image processing , information retrieval and data mining have had large impacts on many human endeavors , they have had essentially zero impact on the study of rock art . In this work we identify the reasons for this , and introduce a novel distance measure and algorithms which allow efficient and effective data mining of large collections of rock art . [[EENNDD]] image processing; similarity search; cultural artifacts"}, "Menambah transformasi hough umum untuk membolehkan penambangan seni petroglyphs Rock adalah istilah arkeologi untuk tanda buatan manusia di atas batu. Dipercayai terdapat berjuta-juta petroglif di Amerika Utara sahaja, dan kajian sumber budaya yang bernilai ini mempunyai implikasi bahkan di luar antropologi dan sejarah. Anehnya, walaupun pemrosesan gambar, pengambilan maklumat dan perlombongan data telah memberi impak besar pada banyak usaha manusia, mereka pada dasarnya tidak memberi kesan pada kajian seni rock. Dalam karya ini, kami mengenal pasti sebab-sebabnya, dan memperkenalkan ukuran dan algoritma jarak jauh yang membolehkan perlombongan data yang cekap dan berkesan bagi koleksi besar seni rock. [[EENNDD]] pemprosesan gambar; carian kesamaan; artifak budaya"], [{"string": "BBM : bayesian browsing model from petabyte-scale data Given a quarter of petabyte click log data , how can we estimate the relevance of each URL for a given query ? In this paper , we propose the Bayesian Browsing Model BBM , a new modeling technique with following advantages : a it does exact inference ; b it is single-pass and parallelizable ; c it is effective . We present two sets of experiments to test model effectiveness and efficiency . On the first set of over 50 million search instances of 1.1 million distinct queries , BBM out-performs the state-of-the-art competitor by 29.2 % in log-likelihood while being 57 times faster . On the second click-log set , spanning a quarter of petabyte data , we showcase the scalability of BBM : we implemented it on a commercial MapReduce cluster , and it took only 3 hours to compute the relevance for 1.15 billion distinct query-URL pairs .", "keywords": ["web search", "click log analysis", "bayesian models"], "combined": "BBM : bayesian browsing model from petabyte-scale data Given a quarter of petabyte click log data , how can we estimate the relevance of each URL for a given query ? In this paper , we propose the Bayesian Browsing Model BBM , a new modeling technique with following advantages : a it does exact inference ; b it is single-pass and parallelizable ; c it is effective . We present two sets of experiments to test model effectiveness and efficiency . On the first set of over 50 million search instances of 1.1 million distinct queries , BBM out-performs the state-of-the-art competitor by 29.2 % in log-likelihood while being 57 times faster . On the second click-log set , spanning a quarter of petabyte data , we showcase the scalability of BBM : we implemented it on a commercial MapReduce cluster , and it took only 3 hours to compute the relevance for 1.15 billion distinct query-URL pairs . [[EENNDD]] web search; click log analysis; bayesian models"}, "BBM: model penjelajahan bayesian dari data skala petabyte Memandangkan seperempat data log klik petabyte, bagaimana kita dapat menganggarkan perkaitan setiap URL untuk pertanyaan tertentu? Dalam makalah ini, kami mengusulkan Bayesian Browsing Model BBM, teknik pemodelan baru dengan kelebihan berikut: ia benar-benar membuat kesimpulan; b adalah hantaran tunggal dan selari; c ia berkesan. Kami membentangkan dua set eksperimen untuk menguji keberkesanan dan kecekapan model. Pada set pertama dari lebih 50 juta contoh carian dari 1.1 juta pertanyaan yang berbeza, BBM mengungguli pesaing canggih sebanyak 29.2% dalam kemungkinan log sementara 57 kali lebih pantas. Pada set log klik kedua, merangkumi seperempat data petabyte, kami menunjukkan skalabilitas BBM: kami menerapkannya pada kluster MapReduce komersial, dan hanya memerlukan 3 jam untuk menghitung relevansi untuk 1,15 bilion pasangan pertanyaan-URL yang berbeza. [[EENNDD]] carian web; analisis log klik; model bayesian"], [{"string": "Dynamic syslog mining for network failure monitoring Syslog monitoring technologies have recently received vast attentions in the areas of network management and network monitoring . They are used to address a wide range of important issues including network failure symptom detection and event correlation discovery . Syslogs are intrinsically dynamic in the sense that they form a time series and that their behavior may change over time . This paper proposes a new methodology of dynamic syslog mining in order to detect failure symptoms with higher confidence and to discover sequential alarm patterns among computer devices . The key ideas of dynamic syslog mining are 1 to represent syslog behavior using a mixture of Hidden Markov Models , 2 to adaptively learn the model using an on-line discounting learning algorithm in combination with dynamic selection of the optimal number of mixture components , and 3 to give anomaly scores using universal test statistics with a dynamically optimized threshold . Using real syslog data we demonstrate the validity of our methodology in the scenarios of failure symptom detection , emerging pattern identification , and correlation discovery .", "keywords": ["model selection", "failure detection", "syslog mining", "correlation analysis", "probabilistic modeling", "artificial intelligence"], "combined": "Dynamic syslog mining for network failure monitoring Syslog monitoring technologies have recently received vast attentions in the areas of network management and network monitoring . They are used to address a wide range of important issues including network failure symptom detection and event correlation discovery . Syslogs are intrinsically dynamic in the sense that they form a time series and that their behavior may change over time . This paper proposes a new methodology of dynamic syslog mining in order to detect failure symptoms with higher confidence and to discover sequential alarm patterns among computer devices . The key ideas of dynamic syslog mining are 1 to represent syslog behavior using a mixture of Hidden Markov Models , 2 to adaptively learn the model using an on-line discounting learning algorithm in combination with dynamic selection of the optimal number of mixture components , and 3 to give anomaly scores using universal test statistics with a dynamically optimized threshold . Using real syslog data we demonstrate the validity of our methodology in the scenarios of failure symptom detection , emerging pattern identification , and correlation discovery . [[EENNDD]] model selection; failure detection; syslog mining; correlation analysis; probabilistic modeling; artificial intelligence"}, "Perlombongan syslog dinamik untuk pemantauan kegagalan rangkaian Teknologi pemantauan Syslog baru-baru ini mendapat perhatian besar dalam bidang pengurusan rangkaian dan pemantauan rangkaian. Mereka digunakan untuk mengatasi berbagai masalah penting termasuk pengesanan gejala kegagalan rangkaian dan penemuan korelasi peristiwa. Syslog secara intrinsik dinamik dalam arti bahawa mereka membentuk siri masa dan tingkah laku mereka mungkin berubah dari masa ke masa. Makalah ini mencadangkan metodologi baru perlombongan syslog dinamik untuk mengesan gejala kegagalan dengan keyakinan yang lebih tinggi dan untuk mengetahui corak penggera berurutan di antara peranti komputer. Idea utama perlombongan syslog dinamik adalah 1 untuk mewakili tingkah laku syslog menggunakan campuran Model Hidden Markov, 2 untuk mempelajari model secara adaptif menggunakan algoritma pembelajaran diskaun on-line dalam kombinasi dengan pemilihan dinamik bilangan komponen campuran yang optimum, dan 3 untuk memberi skor anomali menggunakan statistik ujian universal dengan ambang dioptimumkan secara dinamik. Dengan menggunakan data syslog sebenar, kami menunjukkan kesahan metodologi kami dalam senario pengesanan gejala kegagalan, pengenalan corak yang muncul, dan penemuan korelasi. [[EENNDD]] pemilihan model; pengesanan kegagalan; perlombongan syslog; analisis korelasi; pemodelan probabilistik; kecerdasan buatan"], [{"string": "Anomaly pattern detection in categorical datasets We propose a new method for detecting patterns of anomalies in categorical datasets . We assume that anomalies are generated by some underlying process which affects only a particular subset of the data . Our method consists of two steps : we first use a `` local anomaly detector '' to identify individual records with anomalous attribute values , and then detect patterns where the number of anomalous records is higher than expected . Given the set of anomalies flagged by the local anomaly detector , we search over all subsets of the data defined by any set of fixed values of a subset of the attributes , in order to detect self-similar patterns of anomalies . We wish to detect any such subset of the test data which displays a significant increase in anomalous activity as compared to the normal behavior of the system as indicated by the training data . We perform significance testing to determine if the number of anomalies in any subset of the test data is significantly higher than expected , and propose an efficient algorithm to perform this test over all such subsets of the data . We show that this algorithm is able to accurately detect anomalous patterns in real-world hospital , container shipping and network intrusion data .", "keywords": ["anomaly detection", "pattern detection", "machine learning"], "combined": "Anomaly pattern detection in categorical datasets We propose a new method for detecting patterns of anomalies in categorical datasets . We assume that anomalies are generated by some underlying process which affects only a particular subset of the data . Our method consists of two steps : we first use a `` local anomaly detector '' to identify individual records with anomalous attribute values , and then detect patterns where the number of anomalous records is higher than expected . Given the set of anomalies flagged by the local anomaly detector , we search over all subsets of the data defined by any set of fixed values of a subset of the attributes , in order to detect self-similar patterns of anomalies . We wish to detect any such subset of the test data which displays a significant increase in anomalous activity as compared to the normal behavior of the system as indicated by the training data . We perform significance testing to determine if the number of anomalies in any subset of the test data is significantly higher than expected , and propose an efficient algorithm to perform this test over all such subsets of the data . We show that this algorithm is able to accurately detect anomalous patterns in real-world hospital , container shipping and network intrusion data . [[EENNDD]] anomaly detection; pattern detection; machine learning"}, "Pengesanan corak anomali dalam kumpulan data kategorik Kami mencadangkan kaedah baru untuk mengesan corak anomali dalam set data kategori. Kami menganggap bahawa anomali dihasilkan oleh beberapa proses yang mendasari yang hanya mempengaruhi subset data tertentu. Kaedah kami terdiri daripada dua langkah: pertama kami menggunakan \"pengesan anomali tempatan\" untuk mengenal pasti rekod individu dengan nilai atribut anomali, dan kemudian mengesan corak di mana jumlah catatan anomali lebih tinggi daripada yang dijangkakan. Memandangkan kumpulan anomali yang ditandai oleh pengesan anomali tempatan, kami mencari di semua subset data yang ditentukan oleh sebarang set nilai tetap dari subset atribut, untuk mengesan corak anomali yang serupa dengan diri sendiri. Kami ingin mengesan subset data ujian yang menunjukkan peningkatan yang signifikan dalam aktiviti anomali berbanding dengan tingkah laku normal sistem seperti yang ditunjukkan oleh data latihan. Kami melakukan ujian kepentingan untuk menentukan sama ada bilangan anomali dalam subset data ujian jauh lebih tinggi daripada yang dijangkakan, dan mencadangkan algoritma yang cekap untuk melakukan ujian ini ke atas semua subkumpulan data tersebut. Kami menunjukkan bahawa algoritma ini dapat mengesan corak anomali secara tepat di hospital dunia nyata, penghantaran kontena dan data pencerobohan rangkaian. [[EENNDD]] pengesanan anomali; pengesanan corak; pembelajaran mesin"], [{"string": "Unsupervised feature selection for principal components analysis Principal Components Analysis PCA is the predominant linear dimensionality reduction technique , and has been widely applied on datasets in all scientific domains . We consider , both theoretically and empirically , the topic of unsupervised feature selection for PCA , by leveraging algorithms for the so-called Column Subset Selection Problem CSSP . In words , the CSSP seeks the `` best '' subset of exactly k columns from an m x n data matrix A , and has been extensively studied in the Numerical Linear Algebra community . We present a novel two-stage algorithm for the CSSP . From a theoretical perspective , for small to moderate values of k , this algorithm significantly improves upon the best previously-existing results 24 , 12 for the CSSP . From an empirical perspective , we evaluate this algorithm as an unsupervised feature selection strategy in three application domains of modern statistical data analysis : finance , document-term data , and genetics . We pay particular attention to how this algorithm may be used to select representative or landmark features from an object-feature matrix in an unsupervised manner . In all three application domains , we are able to identify k landmark features , i.e. , columns of the data matrix , that capture nearly the same amount of information as does the subspace that is spanned by the top k `` eigenfeatures . ''", "keywords": ["pca", "random sampling", "numerical linear algebra", "subset selection"], "combined": "Unsupervised feature selection for principal components analysis Principal Components Analysis PCA is the predominant linear dimensionality reduction technique , and has been widely applied on datasets in all scientific domains . We consider , both theoretically and empirically , the topic of unsupervised feature selection for PCA , by leveraging algorithms for the so-called Column Subset Selection Problem CSSP . In words , the CSSP seeks the `` best '' subset of exactly k columns from an m x n data matrix A , and has been extensively studied in the Numerical Linear Algebra community . We present a novel two-stage algorithm for the CSSP . From a theoretical perspective , for small to moderate values of k , this algorithm significantly improves upon the best previously-existing results 24 , 12 for the CSSP . From an empirical perspective , we evaluate this algorithm as an unsupervised feature selection strategy in three application domains of modern statistical data analysis : finance , document-term data , and genetics . We pay particular attention to how this algorithm may be used to select representative or landmark features from an object-feature matrix in an unsupervised manner . In all three application domains , we are able to identify k landmark features , i.e. , columns of the data matrix , that capture nearly the same amount of information as does the subspace that is spanned by the top k `` eigenfeatures . '' [[EENNDD]] pca; random sampling; numerical linear algebra; subset selection"}, "Pemilihan fitur yang tidak diawasi untuk analisis komponen utama Analisis Komponen Utama PCA adalah teknik pengurangan dimensi linear yang dominan, dan telah banyak diterapkan pada set data di semua domain saintifik. Kami mempertimbangkan, baik secara teori dan empirik, topik pemilihan fitur yang tidak diawasi untuk PCA, dengan memanfaatkan algoritma untuk apa yang disebut CSSP Problem Selection Subset Selection Problem. Dengan kata lain, CSSP mencari subkumpulan \"terbaik\" kolom k tepat dari matriks data m x n, dan telah dipelajari secara meluas dalam komuniti Numerical Linear Algebra. Kami membentangkan algoritma dua peringkat novel untuk CSSP. Dari perspektif teoritis, untuk nilai k hingga kecil hingga sederhana, algoritma ini dengan ketara meningkatkan hasil terbaik yang ada sebelumnya 24, 12 untuk CSSP. Dari perspektif empirikal, kami menilai algoritma ini sebagai strategi pemilihan ciri tanpa pengawasan dalam tiga domain aplikasi analisis data statistik moden: kewangan, data istilah dokumen, dan genetik. Kami memberi perhatian khusus kepada bagaimana algoritma ini dapat digunakan untuk memilih ciri perwakilan atau mercu tanda dari matriks ciri-objek dengan cara yang tidak diawasi. Dalam ketiga domain aplikasi, kami dapat mengenal pasti ciri-ciri mercu tanda k, iaitu lajur matriks data, yang menangkap jumlah maklumat yang hampir sama dengan ruang bawah yang dijangkau oleh ciri-ciri eigen teratas. '' [[EENNDD]] pca; persampelan rawak; aljabar linear berangka; pemilihan subset"], [{"string": "Automatic identification of quasi-experimental designs for discovering causal knowledge Researchers in the social and behavioral sciences routinely rely on quasi-experimental designs to discover knowledge from large data-bases . Quasi-experimental designs QEDs exploit fortuitous circumstances in non-experimental data to identify situations sometimes called `` natural experiments '' that provide the equivalent of experimental control and randomization . QEDs allow researchers in domains as diverse as sociology , medicine , and marketing to draw reliable inferences about causal dependencies from non-experimental data . Unfortunately , identifying and exploiting QEDs has remained a painstaking manual activity , requiring researchers to scour available databases and apply substantial knowledge of statistics . However , recent advances in the expressiveness of databases , and increases in their size and complexity , provide the necessary conditions to automatically identify QEDs . In this paper , we describe the first system to discover knowledge by applying quasi-experimental designs that were identified automatically . We demonstrate that QEDs can be identified in a traditional database schema and that such identification requires only a small number of extensions to that schema , knowledge about quasi-experimental design encoded in first-order logic , and a theorem-proving engine . We describe several key innovations necessary to enable this system , including methods for automatically constructing appropriate experimental units and for creating aggregate variables on those units . We show that applying the resulting designs can identify important causal dependencies in real domains , and we provide examples from academic publishing , movie making and marketing , and peer-production systems . Finally , we discuss the integration of QEDs with other approaches to causal discovery , including joint modeling and directed experimentation .", "keywords": ["causal discovery", "quasi-experimental design"], "combined": "Automatic identification of quasi-experimental designs for discovering causal knowledge Researchers in the social and behavioral sciences routinely rely on quasi-experimental designs to discover knowledge from large data-bases . Quasi-experimental designs QEDs exploit fortuitous circumstances in non-experimental data to identify situations sometimes called `` natural experiments '' that provide the equivalent of experimental control and randomization . QEDs allow researchers in domains as diverse as sociology , medicine , and marketing to draw reliable inferences about causal dependencies from non-experimental data . Unfortunately , identifying and exploiting QEDs has remained a painstaking manual activity , requiring researchers to scour available databases and apply substantial knowledge of statistics . However , recent advances in the expressiveness of databases , and increases in their size and complexity , provide the necessary conditions to automatically identify QEDs . In this paper , we describe the first system to discover knowledge by applying quasi-experimental designs that were identified automatically . We demonstrate that QEDs can be identified in a traditional database schema and that such identification requires only a small number of extensions to that schema , knowledge about quasi-experimental design encoded in first-order logic , and a theorem-proving engine . We describe several key innovations necessary to enable this system , including methods for automatically constructing appropriate experimental units and for creating aggregate variables on those units . We show that applying the resulting designs can identify important causal dependencies in real domains , and we provide examples from academic publishing , movie making and marketing , and peer-production systems . Finally , we discuss the integration of QEDs with other approaches to causal discovery , including joint modeling and directed experimentation . [[EENNDD]] causal discovery; quasi-experimental design"}, "Pengenalpastian automatik reka bentuk eksperimental untuk menemui pengetahuan sebab akibat Penyelidik dalam sains sosial dan tingkah laku secara rutin bergantung pada reka bentuk kuasi eksperimen untuk menemui pengetahuan dari pangkalan data yang besar. Reka bentuk separa eksperimen QED memanfaatkan keadaan yang tidak disengajakan dalam data bukan eksperimen untuk mengenal pasti situasi yang kadang-kadang disebut \"eksperimen semula jadi\" yang memberikan setara dengan kawalan eksperimen dan pengacakan. QED membolehkan para penyelidik dalam domain yang beragam seperti sosiologi, perubatan, dan pemasaran untuk membuat kesimpulan yang dapat dipercayai mengenai pergantungan sebab akibat dari data bukan eksperimen. Sayangnya, mengenal pasti dan mengeksploitasi QED tetap menjadi aktiviti manual yang sukar, yang memerlukan para penyelidik untuk mencari pangkalan data yang ada dan menerapkan pengetahuan statistik yang besar. Namun, kemajuan terkini dalam ekspresif pangkalan data, dan peningkatan ukuran dan kerumitannya, memberikan syarat yang diperlukan untuk mengenal pasti QED secara automatik. Dalam makalah ini, kami menerangkan sistem pertama yang menemui pengetahuan dengan menerapkan reka bentuk eksperimen semu yang dikenal pasti secara automatik. Kami menunjukkan bahawa QED dapat dikenal pasti dalam skema pangkalan data tradisional dan bahawa pengenalpastian tersebut hanya memerlukan sebilangan kecil peluasan untuk skema itu, pengetahuan tentang reka bentuk kuasi-eksperimen yang dikodkan dalam logik pesanan pertama, dan mesin yang membuktikan teorema. Kami menerangkan beberapa inovasi utama yang diperlukan untuk membolehkan sistem ini, termasuk kaedah untuk membina unit eksperimen yang sesuai secara automatik dan untuk membuat pemboleh ubah agregat pada unit tersebut. Kami menunjukkan bahawa menerapkan reka bentuk yang dihasilkan dapat mengenal pasti pergantungan kausal yang penting dalam domain nyata, dan kami memberikan contoh dari penerbitan akademik, pembuatan dan pemasaran filem, dan sistem produksi sebaya. Akhirnya, kami membincangkan integrasi QED dengan pendekatan lain untuk penemuan kausal, termasuk pemodelan bersama dan eksperimen terarah. [[EENNDD]] penemuan sebab; reka bentuk separa eksperimen"], [{"string": "A data mining approach to modeling relationships among categories in image collection This paper proposes a data mining approach to modeling relationships among categories in image collection . In our approach , with image feature grouping , a visual dictionary is created for color , texture , and shape feature attributes respectively . Labeling each training image with the keywords in the visual dictionary , a classification tree is built . Based on the statistical properties of the feature space we define a structure , called \u03b1-Semantics Graph , to discover the hidden semantic relationships among the semantic categories embodied in the image collection . With the \u03b1-Semantics Graph , each semantic category is modeled as a unique fuzzy set to explicitly address the semantic uncertainty and semantic overlap among the categories in the feature space . The model is utilized in the semantics-intensive image retrieval application . An algorithm using the classification accuracy measures is developed to combine the built classification tree with the fuzzy set modeling method to deliver semantically relevant image retrieval for a given query image . The experimental evaluations have demonstrated that the proposed approach models the semantic relationships effectively and the image retrieval prototype system utilizing the derived model is promising both in effectiveness and efficiency .", "keywords": ["semantic category", "image collection", "fuzzy model", "relationships"], "combined": "A data mining approach to modeling relationships among categories in image collection This paper proposes a data mining approach to modeling relationships among categories in image collection . In our approach , with image feature grouping , a visual dictionary is created for color , texture , and shape feature attributes respectively . Labeling each training image with the keywords in the visual dictionary , a classification tree is built . Based on the statistical properties of the feature space we define a structure , called \u03b1-Semantics Graph , to discover the hidden semantic relationships among the semantic categories embodied in the image collection . With the \u03b1-Semantics Graph , each semantic category is modeled as a unique fuzzy set to explicitly address the semantic uncertainty and semantic overlap among the categories in the feature space . The model is utilized in the semantics-intensive image retrieval application . An algorithm using the classification accuracy measures is developed to combine the built classification tree with the fuzzy set modeling method to deliver semantically relevant image retrieval for a given query image . The experimental evaluations have demonstrated that the proposed approach models the semantic relationships effectively and the image retrieval prototype system utilizing the derived model is promising both in effectiveness and efficiency . [[EENNDD]] semantic category; image collection; fuzzy model; relationships"}, "Pendekatan perlombongan data untuk memodelkan hubungan antara kategori dalam pengumpulan gambar Makalah ini mencadangkan pendekatan perlombongan data untuk memodelkan hubungan antara kategori dalam pengumpulan gambar. Dalam pendekatan kami, dengan pengelompokan ciri gambar, kamus visual dibuat untuk atribut ciri warna, tekstur, dan bentuk. Melabel setiap gambar latihan dengan kata kunci dalam kamus visual, pokok klasifikasi dibina. Berdasarkan sifat statistik ruang ciri kami menentukan struktur, yang disebut Graf \u03b1-Semantika, untuk mengetahui hubungan semantik tersembunyi di antara kategori semantik yang terkandung dalam koleksi gambar. Dengan Graf \u03b1-Semantik, setiap kategori semantik dimodelkan sebagai satu set kabur yang unik untuk secara eksplisit menangani ketidakpastian semantik dan pertindihan semantik antara kategori di ruang ciri. Model ini digunakan dalam aplikasi pengambilan gambar intensif semantik. Algoritma yang menggunakan ukuran ketepatan klasifikasi dikembangkan untuk menggabungkan pokok klasifikasi yang dibina dengan kaedah pemodelan set kabur untuk menyampaikan pengambilan gambar yang relevan secara semantik untuk gambar pertanyaan yang diberikan. Penilaian eksperimental telah menunjukkan bahawa pendekatan yang dicadangkan memodelkan hubungan semantik dengan berkesan dan sistem prototaip pengambilan gambar yang menggunakan model yang dihasilkan menjanjikan keberkesanan dan kecekapan. [[EENNDD]] kategori semantik; koleksi gambar; model kabur; hubungan"], [{"string": "Discovering additive structure in black box functions Many automated learning procedures lack interpretability , operating effectively as a black box : providing a prediction tool but no explanation of the underlying dynamics that drive it . A common approach to interpretation is to plot the dependence of a learned function on one or two predictors . We present a method that seeks not to display the behavior of a function , but to evaluate the importance of non-additive interactions within any set of variables . Should the function be close to a sum of low dimensional components , these components can be viewed and even modeled parametrically . Alternatively , the work here provides an indication of where intrinsically high-dimensional behavior takes place . The calculations used in this paper correspond closely with the functional ANOVA decomposition ; a well-developed construction in Statistics . In particular , the proposed score of interaction importance measures the loss associated with the projection of the prediction function onto a space of additive models . The algorithm runs in linear time and we present displays of the output as a graphical model of the function for interpretation purposes .", "keywords": ["additive models", "diagnostics", "learning", "functional anova", "draphical models", "feature selection", "visualization", "interpretation"], "combined": "Discovering additive structure in black box functions Many automated learning procedures lack interpretability , operating effectively as a black box : providing a prediction tool but no explanation of the underlying dynamics that drive it . A common approach to interpretation is to plot the dependence of a learned function on one or two predictors . We present a method that seeks not to display the behavior of a function , but to evaluate the importance of non-additive interactions within any set of variables . Should the function be close to a sum of low dimensional components , these components can be viewed and even modeled parametrically . Alternatively , the work here provides an indication of where intrinsically high-dimensional behavior takes place . The calculations used in this paper correspond closely with the functional ANOVA decomposition ; a well-developed construction in Statistics . In particular , the proposed score of interaction importance measures the loss associated with the projection of the prediction function onto a space of additive models . The algorithm runs in linear time and we present displays of the output as a graphical model of the function for interpretation purposes . [[EENNDD]] additive models; diagnostics; learning; functional anova; draphical models; feature selection; visualization; interpretation"}, "Menemui struktur aditif dalam fungsi kotak hitam Banyak prosedur pembelajaran automatik tidak dapat ditafsirkan, beroperasi dengan berkesan sebagai kotak hitam: menyediakan alat ramalan tetapi tidak ada penjelasan mengenai dinamika asas yang mendorongnya. Pendekatan umum untuk pentafsiran adalah merancang pergantungan fungsi yang dipelajari pada satu atau dua peramal. Kami menyajikan kaedah yang bertujuan untuk tidak memperlihatkan tingkah laku fungsi, tetapi untuk menilai kepentingan interaksi bukan aditif dalam sekumpulan pemboleh ubah. Sekiranya fungsinya hampir dengan sejumlah komponen dimensi rendah, komponen ini dapat dilihat dan bahkan dimodelkan secara parametrik. Sebagai alternatif, karya di sini memberikan petunjuk di mana tingkah laku dimensi tinggi secara intrinsik berlaku. Pengiraan yang digunakan dalam makalah ini berkait rapat dengan penguraian ANOVA yang berfungsi; pembinaan yang maju dalam Perangkaan. Khususnya, skor kepentingan interaksi yang dicadangkan mengukur kerugian yang berkaitan dengan unjuran fungsi ramalan ke ruang model tambahan. Algoritma berjalan dalam masa linier dan kami menyajikan paparan output sebagai model grafik fungsi untuk tujuan interpretasi. [[EENNDD]] model aditif; diagnostik; belajar; anova berfungsi; model drafikal; pemilihan ciri; visualisasi; tafsiran"], [{"string": "SNARE : a link analytic system for graph labeling and risk detection Classifying nodes in networks is a task with a wide range of applications . It can be particularly useful in anomaly and fraud detection . Many resources are invested in the task of fraud detection due to the high cost of fraud , and being able to automatically detect potential fraud quickly and precisely allows human investigators to work more efficiently . Many data analytic schemes have been put into use ; however , schemes that bolster link analysis prove promising . This work builds upon the belief propagation algorithm for use in detecting collusion and other fraud schemes . We propose an algorithm called SNARE Social Network Analysis for Risk Evaluation . By allowing one to use domain knowledge as well as link knowledge , the method was very successful for pinpointing misstated accounts in our sample of general ledger data , with a significant improvement over the default heuristic in true positive rates , and a lift factor of up to 6.5 more than twice that of the default heuristic . We also apply SNARE to the task of graph labeling in general on publicly-available datasets . We show that with only some information about the nodes themselves in a network , we get surprisingly high accuracy of labels . Not only is SNARE applicable in a wide variety of domains , but it is also robust to the choice of parameters and highly scalable-linearly with the number of edges in a graph .", "keywords": ["anomaly detection", "belief propagation", "social networks"], "combined": "SNARE : a link analytic system for graph labeling and risk detection Classifying nodes in networks is a task with a wide range of applications . It can be particularly useful in anomaly and fraud detection . Many resources are invested in the task of fraud detection due to the high cost of fraud , and being able to automatically detect potential fraud quickly and precisely allows human investigators to work more efficiently . Many data analytic schemes have been put into use ; however , schemes that bolster link analysis prove promising . This work builds upon the belief propagation algorithm for use in detecting collusion and other fraud schemes . We propose an algorithm called SNARE Social Network Analysis for Risk Evaluation . By allowing one to use domain knowledge as well as link knowledge , the method was very successful for pinpointing misstated accounts in our sample of general ledger data , with a significant improvement over the default heuristic in true positive rates , and a lift factor of up to 6.5 more than twice that of the default heuristic . We also apply SNARE to the task of graph labeling in general on publicly-available datasets . We show that with only some information about the nodes themselves in a network , we get surprisingly high accuracy of labels . Not only is SNARE applicable in a wide variety of domains , but it is also robust to the choice of parameters and highly scalable-linearly with the number of edges in a graph . [[EENNDD]] anomaly detection; belief propagation; social networks"}, "SNARE: sistem analitik pautan untuk pelabelan grafik dan pengesanan risiko Mengklasifikasikan nod dalam rangkaian adalah tugas dengan pelbagai aplikasi. Ia sangat berguna dalam pengesanan anomali dan penipuan. Banyak sumber daya dilaburkan dalam tugas pengesanan penipuan kerana kos penipuan yang tinggi, dan dapat secara automatik mengesan potensi penipuan dengan cepat dan tepat membolehkan penyiasat manusia bekerja dengan lebih efisien. Banyak skema analitik data telah digunakan; namun, skema yang menyokong analisis pautan terbukti menjanjikan. Karya ini dibina berdasarkan algoritma penyebaran kepercayaan untuk digunakan dalam mengesan kolusi dan skema penipuan lain. Kami mencadangkan algoritma yang disebut SNARE Social Network Analysis for Risk Evaluation. Dengan membenarkan seseorang menggunakan pengetahuan domain dan pengetahuan pautan, kaedah ini sangat berjaya untuk menentukan akaun yang salah dalam contoh data lejar umum kami, dengan peningkatan yang signifikan terhadap heuristik lalai dalam kadar positif benar, dan faktor peningkatan hingga 6.5 lebih daripada dua kali ganda daripada heuristik lalai. Kami juga menerapkan SNARE untuk tugas pelabelan grafik secara umum pada set data yang tersedia untuk umum. Kami menunjukkan bahawa dengan hanya beberapa maklumat mengenai node itu sendiri dalam rangkaian, kami mendapat ketepatan label yang sangat tinggi. SNARE tidak hanya berlaku di berbagai domain, tetapi juga kuat untuk pilihan parameter dan sangat skalabel-linier dengan jumlah tepi dalam grafik. [[EENNDD]] pengesanan anomali; penyebaran kepercayaan; rangkaian sosial"], [{"string": "A rank sum test method for informative gene discovery Finding informative genes from microarray data is an important research problem in bioinformatics research and applications . Most of the existing methods rank features according to their discriminative capability and then find a subset of discriminative genes usually top k genes . In particular , t-statistic criterion and its variants have been adopted extensively . This kind of methods rely on the statistics principle of t-test , which requires that the data follows a normal distribution . However , according to our investigation , the normality condition often can not be met in real data sets . To avoid the assumption of the normality condition , in this paper , we propose a rank sum test method for informative gene discovery . The method uses a rank-sum statistic as the ranking criterion . Moreover , we propose using the significance level threshold , instead of the number of informative genes , as the parameter . The significance level threshold as a parameter carries the quality specification in statistics . We follow the Pitman efficiency theory to show that the rank sum method is more accurate and more robust than the t-statistic method in theory . To verify the effectiveness of the rank sum method , we use support vector machine SVM to construct classifiers based on the identified informative genes on two well known data sets , namely colon data and leukemia data . The prediction accuracy reaches 96.2 % on the colon data and 100 % on the leukemia data . The results are clearly better than those from the previous feature ranking methods . By experiments , we also verify that using significance level threshold is more effective than directly specifying an arbitrary k.", "keywords": ["support vector machine", "classification", "rank sum test", "ranking criterion", "informative genes"], "combined": "A rank sum test method for informative gene discovery Finding informative genes from microarray data is an important research problem in bioinformatics research and applications . Most of the existing methods rank features according to their discriminative capability and then find a subset of discriminative genes usually top k genes . In particular , t-statistic criterion and its variants have been adopted extensively . This kind of methods rely on the statistics principle of t-test , which requires that the data follows a normal distribution . However , according to our investigation , the normality condition often can not be met in real data sets . To avoid the assumption of the normality condition , in this paper , we propose a rank sum test method for informative gene discovery . The method uses a rank-sum statistic as the ranking criterion . Moreover , we propose using the significance level threshold , instead of the number of informative genes , as the parameter . The significance level threshold as a parameter carries the quality specification in statistics . We follow the Pitman efficiency theory to show that the rank sum method is more accurate and more robust than the t-statistic method in theory . To verify the effectiveness of the rank sum method , we use support vector machine SVM to construct classifiers based on the identified informative genes on two well known data sets , namely colon data and leukemia data . The prediction accuracy reaches 96.2 % on the colon data and 100 % on the leukemia data . The results are clearly better than those from the previous feature ranking methods . By experiments , we also verify that using significance level threshold is more effective than directly specifying an arbitrary k. [[EENNDD]] support vector machine; classification; rank sum test; ranking criterion; informative genes"}, "Kaedah ujian peringkat tambah untuk penemuan gen informatif Mencari gen bermaklumat dari data microarray adalah masalah penyelidikan penting dalam penyelidikan dan aplikasi bioinformatik. Sebilangan besar kaedah yang ada memberi peringkat ciri mengikut kemampuan diskriminatif mereka dan kemudian mencari subset gen diskriminatif biasanya gen k teratas. Khususnya, kriteria t-statistik dan variannya telah diterima pakai secara meluas. Kaedah seperti ini bergantung pada prinsip statistik ujian-t, yang mengharuskan data mengikuti taburan normal. Namun, menurut penyelidikan kami, keadaan normal biasanya tidak dapat dipenuhi dalam kumpulan data sebenar. Untuk mengelakkan anggapan keadaan normal, dalam makalah ini, kami mencadangkan kaedah ujian peringkat jumlah untuk penemuan gen informatif. Kaedah ini menggunakan statistik peringkat-jumlah sebagai kriteria peringkat. Lebih-lebih lagi, kami mencadangkan penggunaan ambang aras keertian, bukan bilangan gen yang bermaklumat, sebagai parameter. Ambang aras keertian sebagai parameter membawa spesifikasi kualiti dalam statistik. Kami mengikuti teori kecekapan Pitman untuk menunjukkan bahawa kaedah peringkat jumlah lebih tepat dan lebih mantap daripada kaedah t-statistik dalam teori. Untuk mengesahkan keberkesanan kaedah pemeringkatan, kami menggunakan mesin vektor sokongan SVM untuk membina pengkelasan berdasarkan gen maklumat yang dikenal pasti pada dua set data terkenal, iaitu data kolon dan data leukemia. Ketepatan ramalan mencapai 96.2% pada data usus besar dan 100% pada data leukemia. Hasilnya jelas lebih baik daripada kaedah pemeringkatan ciri sebelumnya. Dengan eksperimen, kami juga mengesahkan bahawa menggunakan ambang tahap kepentingan lebih berkesan daripada secara langsung menentukan k sewenang-wenangnya. [[EENNDD]] mesin vektor sokongan; pengelasan; ujian jumlah pangkat; kriteria peringkat; gen bermaklumat"], [{"string": "Social influence analysis in large-scale networks In large social networks , nodes users , entities are influenced by others for various reasons . For example , the colleagues have strong influence on one 's work , while the friends have strong influence on one 's daily life . How to differentiate the social influences from different angles topics ? How to quantify the strength of those social influences ? How to estimate the model on real large networks ? To address these fundamental questions , we propose Topical Affinity Propagation TAP to model the topic-level social influence on large networks . In particular , TAP can take results of any topic modeling and the existing network structure to perform topic-level influence propagation . With the help of the influence analysis , we present several important applications on real data sets such as 1 what are the representative nodes on a given topic ? 2 how to identify the social influences of neighboring nodes on a particular node ? To scale to real large networks , TAP is designed with efficient distributed learning algorithms that is implemented and tested under the Map-Reduce framework . We further present the common characteristics of distributed learning algorithms for Map-Reduce . Finally , we demonstrate the effectiveness and efficiency of TAP on real large data sets .", "keywords": ["large-scale network", "topical analysis propagation", "information search and retrieval", "database applications", "social networks", "social influence analysis"], "combined": "Social influence analysis in large-scale networks In large social networks , nodes users , entities are influenced by others for various reasons . For example , the colleagues have strong influence on one 's work , while the friends have strong influence on one 's daily life . How to differentiate the social influences from different angles topics ? How to quantify the strength of those social influences ? How to estimate the model on real large networks ? To address these fundamental questions , we propose Topical Affinity Propagation TAP to model the topic-level social influence on large networks . In particular , TAP can take results of any topic modeling and the existing network structure to perform topic-level influence propagation . With the help of the influence analysis , we present several important applications on real data sets such as 1 what are the representative nodes on a given topic ? 2 how to identify the social influences of neighboring nodes on a particular node ? To scale to real large networks , TAP is designed with efficient distributed learning algorithms that is implemented and tested under the Map-Reduce framework . We further present the common characteristics of distributed learning algorithms for Map-Reduce . Finally , we demonstrate the effectiveness and efficiency of TAP on real large data sets . [[EENNDD]] large-scale network; topical analysis propagation; information search and retrieval; database applications; social networks; social influence analysis"}, "Analisis pengaruh sosial dalam rangkaian berskala besar Dalam rangkaian sosial yang besar, pengguna nod, entiti dipengaruhi oleh orang lain kerana pelbagai sebab. Sebagai contoh, rakan-rakan mempunyai pengaruh yang kuat terhadap pekerjaan seseorang, sementara rakan-rakan mempunyai pengaruh yang kuat terhadap kehidupan seharian seseorang. Bagaimana membezakan pengaruh sosial dari topik sudut yang berbeza? Bagaimana untuk mengukur kekuatan pengaruh sosial tersebut? Bagaimana mengira model pada rangkaian besar yang sebenar? Untuk mengatasi persoalan asas ini, kami mencadangkan TAP Propagation Affinity Topical untuk memodelkan pengaruh sosial peringkat topik pada rangkaian besar. Khususnya, TAP dapat mengambil hasil pemodelan topik apa pun dan struktur jaringan yang ada untuk melakukan penyebaran pengaruh tingkat topik. Dengan bantuan analisis pengaruh, kami menyajikan beberapa aplikasi penting pada set data sebenar seperti 1 apa node wakil pada topik tertentu? 2 bagaimana mengenal pasti pengaruh sosial jiran tetangga pada nod tertentu? Untuk skala ke rangkaian besar yang nyata, TAP dirancang dengan algoritma pembelajaran terdistribusi yang efisien yang dilaksanakan dan diuji di bawah kerangka Peta-Kurangkan. Kami seterusnya memaparkan ciri umum algoritma pembelajaran yang diedarkan untuk Map-Reduce. Akhirnya, kami menunjukkan keberkesanan dan kecekapan TAP pada set data yang besar. [[EENNDD]] rangkaian berskala besar; penyebaran analisis topikal; carian dan pengambilan maklumat; aplikasi pangkalan data; rangkaian sosial; analisis pengaruh sosial"], [{"string": "Connections between the lines : augmenting social networks with text Network data is ubiquitous , encoding collections of relationships between entities such as people , places , genes , or corporations . While many resources for networks of interesting entities are emerging , most of these can only annotate connections in a limited fashion . Although relationships between entities are rich , it is impractical to manually devise complete characterizations of these relationships for every pair of entities on large , real-world corpora . In this paper we present a novel probabilistic topic model to analyze text corpora and infer descriptions of its entities and of relationships between those entities . We develop variational methods for performing approximate inference on our model and demonstrate that our model can be practically deployed on large corpora such as Wikipedia . We show qualitatively and quantitatively that our model can construct and annotate graphs of relationships and make useful predictions .", "keywords": ["graphical models", "social network learning", "statistical topic models"], "combined": "Connections between the lines : augmenting social networks with text Network data is ubiquitous , encoding collections of relationships between entities such as people , places , genes , or corporations . While many resources for networks of interesting entities are emerging , most of these can only annotate connections in a limited fashion . Although relationships between entities are rich , it is impractical to manually devise complete characterizations of these relationships for every pair of entities on large , real-world corpora . In this paper we present a novel probabilistic topic model to analyze text corpora and infer descriptions of its entities and of relationships between those entities . We develop variational methods for performing approximate inference on our model and demonstrate that our model can be practically deployed on large corpora such as Wikipedia . We show qualitatively and quantitatively that our model can construct and annotate graphs of relationships and make useful predictions . [[EENNDD]] graphical models; social network learning; statistical topic models"}, "Sambungan antara garis: menambah rangkaian sosial dengan teks Data rangkaian ada di mana-mana, merangkumi koleksi hubungan antara entiti seperti orang, tempat, gen, atau syarikat. Walaupun banyak sumber untuk rangkaian entiti menarik muncul, kebanyakan dari ini hanya dapat memberi keterangan sambungan dengan cara yang terhad. Walaupun hubungan antara entiti kaya, tidak praktikal untuk merancang ciri-ciri lengkap hubungan ini secara manual untuk setiap pasangan entiti pada korporat dunia nyata yang besar. Dalam makalah ini kami menyajikan model topik probabilistik novel untuk menganalisis korporat teks dan menyimpulkan perihalan entiti dan hubungan antara entiti tersebut. Kami mengembangkan kaedah variasi untuk melakukan inferensi anggaran pada model kami dan menunjukkan bahawa model kami dapat digunakan secara praktikal pada korporat besar seperti Wikipedia. Kami menunjukkan secara kualitatif dan kuantitatif bahawa model kami dapat membina dan memberi penjelasan grafik hubungan dan membuat ramalan yang berguna. [[EENNDD]] model grafik; pembelajaran rangkaian sosial; model topik statistik"], [{"string": "Ordering patterns by combining opinions from multiple sources Pattern ordering is an important task in data mining because the number of patterns extracted by standard data mining algorithms often exceeds our capacity to manually analyze them . In this paper , we present an effective approach to address the pattern ordering problem by combining the rank information gathered from disparate sources . Although rank aggregation techniques have been developed for applications such as meta-search engines , they are not directly applicable to pattern ordering for two reasons . First , the techniques are mostly supervised , i.e. , they require a sufficient amount of labeled data . Second , the objects to be ranked are assumed to be independent and identically distributed i.i. d , an assumption that seldom holds in pattern ordering . The method proposed in this paper is an adaptation of the original Hedge algorithm , modified to work in an unsupervised learning setting . Techniques for addressing the i.i.d. violation in pattern ordering are also presented . Experimental results demonstrate that our unsupervised Hedge algorithm outperforms many alternative techniques such as those based on weighted average ranking and singular value decomposition .", "keywords": ["pattern ordering", "rank aggregation", "ensemble learning"], "combined": "Ordering patterns by combining opinions from multiple sources Pattern ordering is an important task in data mining because the number of patterns extracted by standard data mining algorithms often exceeds our capacity to manually analyze them . In this paper , we present an effective approach to address the pattern ordering problem by combining the rank information gathered from disparate sources . Although rank aggregation techniques have been developed for applications such as meta-search engines , they are not directly applicable to pattern ordering for two reasons . First , the techniques are mostly supervised , i.e. , they require a sufficient amount of labeled data . Second , the objects to be ranked are assumed to be independent and identically distributed i.i. d , an assumption that seldom holds in pattern ordering . The method proposed in this paper is an adaptation of the original Hedge algorithm , modified to work in an unsupervised learning setting . Techniques for addressing the i.i.d. violation in pattern ordering are also presented . Experimental results demonstrate that our unsupervised Hedge algorithm outperforms many alternative techniques such as those based on weighted average ranking and singular value decomposition . [[EENNDD]] pattern ordering; rank aggregation; ensemble learning"}, "Menyusun corak dengan menggabungkan pendapat dari pelbagai sumber Susunan pola adalah tugas penting dalam perlombongan data kerana bilangan corak yang diekstrak oleh algoritma perlombongan data standard sering kali melebihi kemampuan kita untuk menganalisisnya secara manual. Dalam makalah ini, kami menyajikan pendekatan yang efektif untuk mengatasi masalah susunan pola dengan menggabungkan informasi peringkat yang dikumpulkan dari sumber yang berbeza. Walaupun teknik pengagregatan peringkat telah dikembangkan untuk aplikasi seperti mesin pencari meta, teknik tersebut tidak dapat diterapkan secara langsung pada susunan pola untuk dua sebab. Pertama, teknik ini kebanyakannya diawasi, iaitu, mereka memerlukan jumlah data berlabel yang mencukupi. Kedua, objek yang akan diperingkat dianggap bebas dan diedarkan secara serupa i.i. d, anggapan yang jarang berlaku dalam susunan corak. Kaedah yang dicadangkan dalam makalah ini adalah penyesuaian dari algoritma Hedge yang asli, yang diubahsuai untuk berfungsi dalam suasana pembelajaran yang tidak diawasi. Teknik untuk menangani i.i.d. pelanggaran dalam susunan corak juga dikemukakan. Hasil eksperimen menunjukkan bahawa algoritma Hedge kami yang tidak diawasi mengungguli banyak teknik alternatif seperti yang berdasarkan pada peringkat purata berwajaran dan penguraian nilai tunggal. [[EENNDD]] susunan corak; pengagregatan peringkat; pembelajaran ensemble"], [{"string": "Exploiting place features in link prediction on location-based social networks Link prediction systems have been largely adopted to recommend new friends in online social networks using data about social interactions . With the soaring adoption of location-based social services it becomes possible to take advantage of an additional source of information : the places people visit . In this paper we study the problem of designing a link prediction system for online location-based social networks . We have gathered extensive data about one of these services , Gowalla , with periodic snapshots to capture its temporal evolution . We study the link prediction space , finding that about 30 % of new links are added among `` place-friends '' , i.e. , among users who visit the same places . We show how this prediction space can be made 15 times smaller , while still 66 % of future connections can be discovered . Thus , we define new prediction features based on the properties of the places visited by users which are able to discriminate potential future links among them . Building on these findings , we describe a supervised learning framework which exploits these prediction features to predict new links among friends-of-friends and place-friends . Our evaluation shows how the inclusion of information about places and related user activity offers high link prediction performance . These results open new directions for real-world link recommendation systems on location-based social networks .", "keywords": ["social networks", "location-based services", "database applications", "link prediction"], "combined": "Exploiting place features in link prediction on location-based social networks Link prediction systems have been largely adopted to recommend new friends in online social networks using data about social interactions . With the soaring adoption of location-based social services it becomes possible to take advantage of an additional source of information : the places people visit . In this paper we study the problem of designing a link prediction system for online location-based social networks . We have gathered extensive data about one of these services , Gowalla , with periodic snapshots to capture its temporal evolution . We study the link prediction space , finding that about 30 % of new links are added among `` place-friends '' , i.e. , among users who visit the same places . We show how this prediction space can be made 15 times smaller , while still 66 % of future connections can be discovered . Thus , we define new prediction features based on the properties of the places visited by users which are able to discriminate potential future links among them . Building on these findings , we describe a supervised learning framework which exploits these prediction features to predict new links among friends-of-friends and place-friends . Our evaluation shows how the inclusion of information about places and related user activity offers high link prediction performance . These results open new directions for real-world link recommendation systems on location-based social networks . [[EENNDD]] social networks; location-based services; database applications; link prediction"}, "Mengeksploitasi ciri tempat dalam ramalan pautan di rangkaian sosial berasaskan lokasi Sistem ramalan pautan telah banyak digunakan untuk mengesyorkan rakan baru dalam rangkaian sosial dalam talian menggunakan data mengenai interaksi sosial. Dengan penggunaan perkhidmatan sosial berasaskan lokasi yang meluas, menjadi mungkin untuk memanfaatkan sumber maklumat tambahan: tempat-tempat yang dikunjungi orang. Dalam makalah ini kita mengkaji masalah merancang sistem ramalan pautan untuk rangkaian sosial berasaskan lokasi dalam talian. Kami telah mengumpulkan data yang luas mengenai salah satu perkhidmatan ini, Gowalla, dengan tangkapan gambar berkala untuk menangkap evolusi temporalnya. Kami mengkaji ruang ramalan pautan, mendapati bahawa kira-kira 30% pautan baru ditambahkan di antara \"rakan tempat\", iaitu, di antara pengguna yang mengunjungi tempat yang sama. Kami menunjukkan bagaimana ruang ramalan ini dapat dibuat 15 kali lebih kecil, sementara 66% sambungan masa depan dapat ditemui. Oleh itu, kami menentukan ciri ramalan baru berdasarkan sifat tempat yang dikunjungi oleh pengguna yang dapat membezakan kemungkinan hubungan masa depan di antara mereka. Berdasarkan penemuan ini, kami menerangkan kerangka pembelajaran yang diawasi yang memanfaatkan ciri ramalan ini untuk meramalkan hubungan baru antara rakan-rakan-rakan dan rakan-tempat. Penilaian kami menunjukkan bagaimana kemasukan maklumat mengenai tempat dan aktiviti pengguna yang berkaitan menawarkan prestasi ramalan pautan yang tinggi. Hasil ini membuka petunjuk baru untuk sistem cadangan pautan dunia nyata di rangkaian sosial berasaskan lokasi. [[EENNDD]] rangkaian sosial; perkhidmatan berasaskan lokasi; aplikasi pangkalan data; ramalan pautan"], [{"string": "Wavelet synopsis for data streams : minimizing non-euclidean error We consider the wavelet synopsis construction problem for data streams where given n numbers we wish to estimate the data by constructing a synopsis , whose size , say B is much smaller than n. The B numbers are chosen to minimize a suitable error between the original data and the estimate derived from the synopsis . Several good one-pass wavelet construction streaming algorithms minimizing the l2 error exist . For other error measures , the problem is less understood . We provide the first one-pass small space streaming algorithms with provable error guarantees additive approximation for minimizing a variety of non-Euclidean error measures including all weighted lp including l \u221e and relative error lp metrics . In several previous works solutions for weighted l2 , l \u221e and maximum relative error where the B synopsis coefficients are restricted to be wavelet coefficients of the data were proposed . This restriction yields suboptimal solutions on even fairly simple examples . Other lines of research , such as probabilistic synopsis , imposed restrictions on how the synopsis was arrived at . To the best of our knowledge this paper is the first paper to address the general problem , without any restriction on how the synopsis is arrived at , as well as provide the first streaming algorithms with guaranteed performance for these classes of error measures .", "keywords": ["streaming algorithm", "wavelet synopses", "miscellaneous"], "combined": "Wavelet synopsis for data streams : minimizing non-euclidean error We consider the wavelet synopsis construction problem for data streams where given n numbers we wish to estimate the data by constructing a synopsis , whose size , say B is much smaller than n. The B numbers are chosen to minimize a suitable error between the original data and the estimate derived from the synopsis . Several good one-pass wavelet construction streaming algorithms minimizing the l2 error exist . For other error measures , the problem is less understood . We provide the first one-pass small space streaming algorithms with provable error guarantees additive approximation for minimizing a variety of non-Euclidean error measures including all weighted lp including l \u221e and relative error lp metrics . In several previous works solutions for weighted l2 , l \u221e and maximum relative error where the B synopsis coefficients are restricted to be wavelet coefficients of the data were proposed . This restriction yields suboptimal solutions on even fairly simple examples . Other lines of research , such as probabilistic synopsis , imposed restrictions on how the synopsis was arrived at . To the best of our knowledge this paper is the first paper to address the general problem , without any restriction on how the synopsis is arrived at , as well as provide the first streaming algorithms with guaranteed performance for these classes of error measures . [[EENNDD]] streaming algorithm; wavelet synopses; miscellaneous"}, "Sinopsis Wavelet untuk aliran data: meminimumkan ralat bukan euclidean Kami menganggap masalah pembinaan sinopsis wavelet untuk aliran data di mana diberi nombor n, kami ingin menganggarkan data dengan membina sinopsis, yang ukurannya, mengatakan B jauh lebih kecil daripada n. Nombor B dipilih untuk meminimumkan ralat yang sesuai antara data asal dan anggaran yang diperoleh dari sinopsis. Terdapat beberapa algoritma streaming pembinaan gelombang tunggal yang baik untuk meminimumkan ralat l2. Untuk langkah ralat yang lain, masalahnya kurang difahami. Kami menyediakan algoritma penstriman ruang kecil satu pas pertama dengan ralat yang dapat dibuktikan menjamin penghampiran tambahan untuk meminimumkan pelbagai langkah kesalahan bukan Euclidean termasuk semua lp berwajaran termasuk l \u221e dan metrik ralat relatif lp. Dalam beberapa kerja sebelumnya penyelesaian untuk pemberat l2, l weight dan ralat relatif maksimum di mana pekali sinopsis B dibatasi menjadi pekali gelombang data yang dicadangkan. Sekatan ini menghasilkan penyelesaian yang tidak optimum pada contoh yang cukup mudah. Garis penyelidikan lain, seperti sinopsis probabilistik, mengenakan sekatan bagaimana sinopsis itu sampai. Sepengetahuan kami, makalah ini adalah makalah pertama yang menangani masalah umum, tanpa sekatan bagaimana sinopsis dicapai, dan juga memberikan algoritma streaming pertama dengan prestasi terjamin untuk kelas langkah-langkah kesalahan ini. [[EENNDD]] algoritma penstriman; sinopsis gelombang; pelbagai"], [{"string": "Using association rules for product assortment decisions : a case study", "keywords": ["frequent itemset", "decision support", "product assortment decisions", "association rules"], "combined": "Using association rules for product assortment decisions : a case study [[EENNDD]] frequent itemset; decision support; product assortment decisions; association rules"}, "Menggunakan peraturan persatuan untuk keputusan pelbagai produk: kajian kes [[EENNDD]] item yang kerap; sokongan keputusan; keputusan pelbagai produk; peraturan persatuan"], [{"string": "Mining e-commerce data : the good , the bad , and the ugly Organizations conducting Electronic Commerce e-commerce can greatly benefit from the insight that data mining of transactional and clickstream data provides . Such insight helps not only to improve the electronic channel e.g. , a web site , but it is also a learning vehicle for the bigger organization conducting business at brick-and-mortar stores . The e-commerce site serves as an early alert system for emerging patterns and a laboratory for experimentation . For successful data mining , several ingredients are needed and e-commerce provides all the right ones the Good . Web server logs , which are commonly used as the source of data for mining e-commerce data , were designed to debug web servers , and the data they provide is insufficient , requiring the use of heuristics to reconstruct events . Moreover , many events are never logged in web server logs , limiting the source of data for mining the Bad . Many of the problems of dealing with web server log data can be resolved by properly architecting the e-commerce sites to generate data needed for mining . Even with a good architecture , however , there are challenging problems that remain hard to solve the Ugly . Lessons and metrics based on mining real e-commerce data are presented .", "keywords": ["web site architecture", "application server", "learning", "web server", "e-commerce"], "combined": "Mining e-commerce data : the good , the bad , and the ugly Organizations conducting Electronic Commerce e-commerce can greatly benefit from the insight that data mining of transactional and clickstream data provides . Such insight helps not only to improve the electronic channel e.g. , a web site , but it is also a learning vehicle for the bigger organization conducting business at brick-and-mortar stores . The e-commerce site serves as an early alert system for emerging patterns and a laboratory for experimentation . For successful data mining , several ingredients are needed and e-commerce provides all the right ones the Good . Web server logs , which are commonly used as the source of data for mining e-commerce data , were designed to debug web servers , and the data they provide is insufficient , requiring the use of heuristics to reconstruct events . Moreover , many events are never logged in web server logs , limiting the source of data for mining the Bad . Many of the problems of dealing with web server log data can be resolved by properly architecting the e-commerce sites to generate data needed for mining . Even with a good architecture , however , there are challenging problems that remain hard to solve the Ugly . Lessons and metrics based on mining real e-commerce data are presented . [[EENNDD]] web site architecture; application server; learning; web server; e-commerce"}, "Perlombongan data e-dagang: Organisasi yang baik, buruk, dan jelek yang menjalankan e-dagang Elektronik boleh mendapat banyak manfaat daripada pandangan yang diberikan oleh perlombongan data data transaksi dan aliran klik. Wawasan seperti itu membantu bukan sahaja meningkatkan saluran elektronik, mis. , laman web, tetapi juga merupakan wahana pembelajaran bagi organisasi yang lebih besar yang menjalankan perniagaan di kedai batu bata. Laman web e-dagang berfungsi sebagai sistem amaran awal untuk pola yang muncul dan makmal untuk eksperimen. Untuk perlombongan data yang berjaya, diperlukan beberapa bahan dan e-commerce menyediakan yang baik untuk semua. Log pelayan web, yang biasanya digunakan sebagai sumber data untuk data e-dagang perlombongan, dirancang untuk men-debug pelayan web, dan data yang mereka berikan tidak mencukupi, sehingga memerlukan penggunaan heuristik untuk merekonstruksi peristiwa. Lebih-lebih lagi, banyak peristiwa tidak pernah masuk dalam log pelayan web, sehingga membatasi sumber data untuk melombong Bad. Banyak masalah menangani data log pelayan web dapat diselesaikan dengan membina laman web e-dagang dengan betul untuk menghasilkan data yang diperlukan untuk perlombongan. Walaupun dengan seni bina yang baik, ada masalah yang mencabar yang masih sukar untuk diselesaikan yang Jelek. Pelajaran dan metrik berdasarkan data e-dagang sebenar perlombongan dibentangkan. [[EENNDD]] seni bina laman web; pelayan aplikasi; belajar; pelayan web; e-dagang"], [{"string": "Probabilistic modeling of transaction data with applications to profiling , visualization , and prediction Transaction data is ubiquitous in data mining applications . Examples include market basket data in retail commerce , telephone call records in telecommunications , and Web logs of individual page-requests at Web sites . Profiling consists of using historical transaction data on individuals to construct a model of each individual 's behavior . Simple profiling techniques such as histograms do not generalize well from sparse transaction data . In this paper we investigate the application of probabilistic mixture models to automatically generate profiles from large volumes of transaction data . In effect , the mixture model represents each individual 's behavior as a linear combination of `` basis transactions . '' We evaluate several variations of the model on a large retail transaction data set and show that the proposed model provides improved predictive power over simpler histogram-based techniques , as well as being relatively scalable , interpretable , and flexible . In addition we point to applications in outlier detection , customer ranking , interactive visualization , and so forth . The paper concludes by comparing and relating the proposed framework to other transaction-data modeling techniques such as association rules .", "keywords": ["em algorithm", "mixture models", "types of simulation", "profiles", "transaction data", "electronic commerce"], "combined": "Probabilistic modeling of transaction data with applications to profiling , visualization , and prediction Transaction data is ubiquitous in data mining applications . Examples include market basket data in retail commerce , telephone call records in telecommunications , and Web logs of individual page-requests at Web sites . Profiling consists of using historical transaction data on individuals to construct a model of each individual 's behavior . Simple profiling techniques such as histograms do not generalize well from sparse transaction data . In this paper we investigate the application of probabilistic mixture models to automatically generate profiles from large volumes of transaction data . In effect , the mixture model represents each individual 's behavior as a linear combination of `` basis transactions . '' We evaluate several variations of the model on a large retail transaction data set and show that the proposed model provides improved predictive power over simpler histogram-based techniques , as well as being relatively scalable , interpretable , and flexible . In addition we point to applications in outlier detection , customer ranking , interactive visualization , and so forth . The paper concludes by comparing and relating the proposed framework to other transaction-data modeling techniques such as association rules . [[EENNDD]] em algorithm; mixture models; types of simulation; profiles; transaction data; electronic commerce"}, "Pemodelan probabilistik data transaksi dengan aplikasi ke profil, visualisasi, dan ramalan Data transaksi ada di mana-mana aplikasi perlombongan data. Contohnya termasuk data keranjang pasar dalam perdagangan runcit, catatan panggilan telefon di telekomunikasi, dan log Web setiap permintaan halaman di laman web. Profil terdiri daripada menggunakan data transaksi sejarah pada individu untuk membina model tingkah laku setiap individu. Teknik pembuatan profil ringkas seperti histogram tidak dapat digeneralisasikan dengan baik dari data transaksi yang jarang. Dalam makalah ini kami menyelidiki penerapan model campuran probabilistik untuk menghasilkan profil secara automatik dari sejumlah besar data transaksi. Akibatnya, model campuran mewakili tingkah laku setiap individu sebagai gabungan linear transaksi asas. Kami menilai beberapa variasi model pada set data transaksi runcit yang besar dan menunjukkan bahawa model yang dicadangkan memberikan kekuatan ramalan yang lebih baik daripada teknik berdasarkan histogram yang lebih sederhana, dan juga relatif dapat ditingkatkan, dapat ditafsirkan, dan fleksibel. Sebagai tambahan, kami menunjukkan aplikasi dalam pengesanan outlier, peringkat pelanggan, visualisasi interaktif, dan sebagainya. Makalah ini disimpulkan dengan membandingkan dan menghubungkan kerangka kerja yang dicadangkan dengan teknik pemodelan data transaksi lain seperti peraturan persatuan. [[EENNDD]] algoritma em; model campuran; jenis simulasi; profil; data transaksi; perdagangan elektronik"], [{"string": "Summarizing itemset patterns : a profile-based approach Frequent-pattern mining has been studied extensively on scalable methods for mining various kinds of patterns including itemsets , sequences , and graphs . However , the bottleneck of frequent-pattern mining is not at the efficiency but at the interpretability , due to the huge number of patterns generated by the mining process . In this paper , we examine how to summarize a collection of itemset patterns using only K representatives , a small number of patterns that a user can handle easily . The K representatives should not only cover most of the frequent patterns but also approximate their supports . A generative model is built to extract and profile these representatives , under which the supports of the patterns can be easily recovered without consulting the original dataset . Based on the restoration error , we propose a quality measure function to determine the optimal value of parameter K. Polynomial time algorithms are developed together with several optimization heuristics for efficiency improvement . Empirical studies indicate that we can obtain compact summarization in real datasets .", "keywords": ["summarization", "probabilistic model", "frequent pattern"], "combined": "Summarizing itemset patterns : a profile-based approach Frequent-pattern mining has been studied extensively on scalable methods for mining various kinds of patterns including itemsets , sequences , and graphs . However , the bottleneck of frequent-pattern mining is not at the efficiency but at the interpretability , due to the huge number of patterns generated by the mining process . In this paper , we examine how to summarize a collection of itemset patterns using only K representatives , a small number of patterns that a user can handle easily . The K representatives should not only cover most of the frequent patterns but also approximate their supports . A generative model is built to extract and profile these representatives , under which the supports of the patterns can be easily recovered without consulting the original dataset . Based on the restoration error , we propose a quality measure function to determine the optimal value of parameter K. Polynomial time algorithms are developed together with several optimization heuristics for efficiency improvement . Empirical studies indicate that we can obtain compact summarization in real datasets . [[EENNDD]] summarization; probabilistic model; frequent pattern"}, "Meringkaskan corak itemet: pendekatan berdasarkan profil Perlombongan corak kerap telah dikaji secara meluas mengenai kaedah berskala untuk melombong pelbagai jenis corak termasuk set barang, urutan, dan grafik. Walau bagaimanapun, hambatan penambangan corak yang kerap bukan pada kecekapan tetapi pada kebolehtafsiran, kerana sebilangan besar corak yang dihasilkan oleh proses perlombongan. Dalam makalah ini, kami meneliti cara meringkaskan koleksi pola itemet dengan hanya menggunakan perwakilan K, sebilangan kecil corak yang dapat ditangani oleh pengguna dengan mudah. Wakil K tidak hanya harus merangkumi sebahagian besar corak yang kerap tetapi juga menghampiri sokongan mereka. Model generatif dibina untuk mengekstrak dan mempromosikan wakil-wakil ini, di mana sokongan corak dapat diperoleh dengan mudah tanpa merujuk pada set data asal. Berdasarkan ralat pemulihan, kami mencadangkan fungsi ukuran kualiti untuk menentukan nilai optimum parameter K. Algoritma masa polinomial dikembangkan bersama dengan beberapa heuristik pengoptimuman untuk peningkatan kecekapan. Kajian empirikal menunjukkan bahawa kita dapat memperoleh ringkasan ringkas dalam set data sebenar. [[EENNDD]] ringkasan; model kebarangkalian; corak yang kerap"], [{"string": "Efficient handling of high-dimensional feature spaces by randomized classifier ensembles Handling massive datasets is a difficult problem not only due to prohibitively large numbers of entries but in some cases also due to the very high dimensionality of the data . Often , severe feature selection is performed to limit the number of attributes to a manageable size , which unfortunately can lead to a loss of useful information . Feature space reduction may well be necessary for many stand-alone classifiers , but recent advances in the area of ensemble classifier techniques indicate that overall accurate classifier aggregates can be learned even if each individual classifier operates on incomplete `` feature view '' training data , i.e. , such where certain input attributes are excluded . In fact , by using only small random subsets of features to build individual component classifiers , surprisingly accurate and robust models can be created . In this work we demonstrate how these types of architectures effectively reduce the feature space for submodels and groups of sub-models , which lends itself to efficient sequential and\\/or parallel implementations . Experiments with a randomized version of Adaboost are used to support our arguments , using the text classification task as an example .", "keywords": ["bounded-action devices"], "combined": "Efficient handling of high-dimensional feature spaces by randomized classifier ensembles Handling massive datasets is a difficult problem not only due to prohibitively large numbers of entries but in some cases also due to the very high dimensionality of the data . Often , severe feature selection is performed to limit the number of attributes to a manageable size , which unfortunately can lead to a loss of useful information . Feature space reduction may well be necessary for many stand-alone classifiers , but recent advances in the area of ensemble classifier techniques indicate that overall accurate classifier aggregates can be learned even if each individual classifier operates on incomplete `` feature view '' training data , i.e. , such where certain input attributes are excluded . In fact , by using only small random subsets of features to build individual component classifiers , surprisingly accurate and robust models can be created . In this work we demonstrate how these types of architectures effectively reduce the feature space for submodels and groups of sub-models , which lends itself to efficient sequential and\\/or parallel implementations . Experiments with a randomized version of Adaboost are used to support our arguments , using the text classification task as an example . [[EENNDD]] bounded-action devices"}, "Pengendalian ruang ciri dimensi tinggi yang cekap oleh ensembel pengkelasan rawak Mengendalikan set data yang besar adalah masalah yang sukar bukan sahaja disebabkan oleh sejumlah besar entri yang melarang tetapi dalam beberapa kes juga disebabkan oleh dimensi data yang sangat tinggi. Selalunya, pemilihan ciri yang teruk dilakukan untuk mengehadkan bilangan atribut ke ukuran yang dapat dikendalikan, yang sayangnya dapat menyebabkan kehilangan maklumat yang berguna. Pengurangan ruang ciri mungkin diperlukan untuk banyak pengklasifikasi yang berdiri sendiri, tetapi kemajuan terkini dalam bidang teknik pengkelasan ensemble menunjukkan bahawa agregat pengkelasan keseluruhan yang tepat dapat dipelajari walaupun setiap pengelasan individu beroperasi pada data latihan \"paparan ciri\" yang tidak lengkap, iaitu, di mana atribut input tertentu dikecualikan. Sebenarnya, dengan hanya menggunakan sekumpulan kecil ciri-ciri rawak untuk membina pengelasan komponen individu, model yang tepat dan kukuh dapat dibuat. Dalam karya ini kami menunjukkan bagaimana jenis seni bina ini secara berkesan mengurangkan ruang ciri untuk subodel dan kumpulan sub-model, yang sesuai dengan pelaksanaan berurutan dan \\ / atau pelaksanaan yang selari. Eksperimen dengan versi Adaboost secara rawak digunakan untuk menyokong argumen kami, menggunakan tugas klasifikasi teks sebagai contoh. [[EENNDD]] peranti tindakan terikat"], [{"string": "Shrinkage estimator generalizations of Proximal Support Vector Machines We give a statistical interpretation of Proximal Support Vector Machines PSVM proposed at KDD2001 as linear approximaters to nonlinear Support Vector Machines SVM . We prove that PSVM using a linear kernel is identical to ridge regression , a biased-regression method known in the statistical community for more than thirty years . Techniques from the statistical literature to estimate the tuning constant that appears in the SVM and PSVM framework are discussed . Better shrinkage strategies that incorporate more than one tuning constant are suggested . For nonlinear kernels , the minimization problem posed in the PSVM framework is equivalent to finding the posterior mode of a Bayesian model defined through a Gaussian process on the predictor space . Apart from providing new insights , these interpretations help us attach an estimate of uncertainty to our predictions and enable us to build richer classes of models . In particular , we propose a new algorithm called PSVMMIX which is a combination of ridge regression and a Gaussian process model . Extension to the case of continuous response is straightforward and illustrated with example datasets .", "keywords": ["bayesian models", "classification", "kernel", "regression", "correlation", "probability and statistics", "bias-variance tradeoff"], "combined": "Shrinkage estimator generalizations of Proximal Support Vector Machines We give a statistical interpretation of Proximal Support Vector Machines PSVM proposed at KDD2001 as linear approximaters to nonlinear Support Vector Machines SVM . We prove that PSVM using a linear kernel is identical to ridge regression , a biased-regression method known in the statistical community for more than thirty years . Techniques from the statistical literature to estimate the tuning constant that appears in the SVM and PSVM framework are discussed . Better shrinkage strategies that incorporate more than one tuning constant are suggested . For nonlinear kernels , the minimization problem posed in the PSVM framework is equivalent to finding the posterior mode of a Bayesian model defined through a Gaussian process on the predictor space . Apart from providing new insights , these interpretations help us attach an estimate of uncertainty to our predictions and enable us to build richer classes of models . In particular , we propose a new algorithm called PSVMMIX which is a combination of ridge regression and a Gaussian process model . Extension to the case of continuous response is straightforward and illustrated with example datasets . [[EENNDD]] bayesian models; classification; kernel; regression; correlation; probability and statistics; bias-variance tradeoff"}, "Generalisasi estimator pengecutan Mesin Vektor Sokongan Proksimal Kami memberikan tafsiran statistik Mesin Vektor Sokongan Proksimal PSVM yang dicadangkan pada KDD2001 sebagai penghitung linear kepada Mesin Vektor Sokongan bukan linear SVM. Kami membuktikan bahawa PSVM menggunakan kernel linier sama dengan regresi rabung, kaedah regresi bias yang diketahui dalam komuniti statistik selama lebih dari tiga puluh tahun. Teknik dari literatur statistik untuk mengira pemalar penalaan yang muncul dalam kerangka SVM dan PSVM dibincangkan. Strategi pengecutan yang lebih baik yang menggabungkan lebih daripada satu pemalar penalaan dicadangkan. Untuk kernel nonlinier, masalah pengurangan yang ditimbulkan dalam kerangka PSVM setara dengan mencari mod posterior model Bayesian yang ditentukan melalui proses Gaussian di ruang ramalan. Selain memberikan pandangan baru, tafsiran ini membantu kami melampirkan anggaran ketidakpastian pada ramalan kami dan membolehkan kami membina kelas model yang lebih kaya. Secara khusus, kami mencadangkan algoritma baru yang disebut PSVMMIX yang merupakan gabungan regresi rabung dan model proses Gaussian. Peluasan untuk tindak balas berterusan adalah mudah dan digambarkan dengan contoh set data. [[EENNDD]] model bayesian; pengelasan; kernel; regresi; korelasi; kebarangkalian dan statistik; tradeoff bias-varians"], [{"string": "Aggregating time partitions Partitions of sequential data exist either per se or as a result of sequence segmentation algorithms . It is often the case that the same timeline is partitioned in many different ways . For example , different segmentation algorithms produce different partitions of the same underlying data points . In such cases , we are interested in producing an aggregate partition , i.e. , a segmentation that agrees as much as possible with the input segmentations . Each partition is defined as a set of continuous non-overlapping segments of the timeline . We show that this problem can be solved optimally in polynomial time using dynamic programming . We also propose faster greedy heuristics that work well in practice . We experiment with our algorithms and we demonstrate their utility in clustering the behavior of mobile-phone users and combining the results of different segmentation algorithms on genomic sequences .", "keywords": ["nonnumerical algorithms and problems"], "combined": "Aggregating time partitions Partitions of sequential data exist either per se or as a result of sequence segmentation algorithms . It is often the case that the same timeline is partitioned in many different ways . For example , different segmentation algorithms produce different partitions of the same underlying data points . In such cases , we are interested in producing an aggregate partition , i.e. , a segmentation that agrees as much as possible with the input segmentations . Each partition is defined as a set of continuous non-overlapping segments of the timeline . We show that this problem can be solved optimally in polynomial time using dynamic programming . We also propose faster greedy heuristics that work well in practice . We experiment with our algorithms and we demonstrate their utility in clustering the behavior of mobile-phone users and combining the results of different segmentation algorithms on genomic sequences . [[EENNDD]] nonnumerical algorithms and problems"}, "Agregasi pembahagian masa Sebahagian daripada data berurutan wujud sama ada atau hasil daripada algoritma segmentasi urutan. Sering kali garis masa yang sama dibahagikan dengan pelbagai cara. Sebagai contoh, algoritma segmentasi yang berbeza menghasilkan partisi yang berbeza dari titik data asas yang sama. Dalam kes seperti ini, kami berminat untuk menghasilkan partisi agregat, iaitu, segmentasi yang sepakat sebanyak mungkin dengan segmentasi input. Setiap partisi ditakrifkan sebagai satu set segmen garis masa yang tidak bertindih berterusan. Kami menunjukkan bahawa masalah ini dapat diselesaikan secara optimum dalam masa polinomial menggunakan pengaturcaraan dinamik. Kami juga mencadangkan heuristik tamak yang lebih pantas yang berfungsi dengan baik dalam praktik. Kami bereksperimen dengan algoritma kami dan kami menunjukkan kegunaannya dalam mengumpulkan tingkah laku pengguna telefon bimbit dan menggabungkan hasil algoritma segmentasi yang berbeza pada urutan genom. [[EENNDD]] algoritma dan masalah bukan angka"], [{"string": "Nomograms for visualizing support vector machines We propose a simple yet potentially very effective way of visualizing trained support vector machines . Nomograms are an established model visualization technique that can graphically encode the complete model on a single page . The dimensionality of the visualization does not depend on the number of attributes , but merely on the properties of the kernel . To represent the effect of each predictive feature on the log odds ratio scale as required for the nomograms , we employ logistic regression to convert the distance from the separating hyperplane into a probability . Case studies on selected data sets show that for a technique thought to be a black-box , nomograms can clearly expose its internal structure . By providing an easy-to-interpret visualization the analysts can gain insight and study the effects of predictive factors .", "keywords": ["visualization", "support vector machines", "nomogram", "machine learning"], "combined": "Nomograms for visualizing support vector machines We propose a simple yet potentially very effective way of visualizing trained support vector machines . Nomograms are an established model visualization technique that can graphically encode the complete model on a single page . The dimensionality of the visualization does not depend on the number of attributes , but merely on the properties of the kernel . To represent the effect of each predictive feature on the log odds ratio scale as required for the nomograms , we employ logistic regression to convert the distance from the separating hyperplane into a probability . Case studies on selected data sets show that for a technique thought to be a black-box , nomograms can clearly expose its internal structure . By providing an easy-to-interpret visualization the analysts can gain insight and study the effects of predictive factors . [[EENNDD]] visualization; support vector machines; nomogram; machine learning"}, "Nomogram untuk memvisualisasikan mesin vektor sokongan Kami mencadangkan kaedah mudah tetapi berpotensi sangat berkesan untuk menggambarkan mesin vektor sokongan terlatih. Nomogram adalah teknik visualisasi model yang mapan yang secara grafik dapat menyandikan model lengkap pada satu halaman. Dimensi visualisasi tidak bergantung pada jumlah atribut, tetapi hanya pada sifat kernel. Untuk mewakili kesan setiap ciri ramalan pada skala nisbah log odds seperti yang diperlukan untuk nomogram, kami menggunakan regresi logistik untuk mengubah jarak dari hiperplan pemisah menjadi kebarangkalian. Kajian kes pada set data yang dipilih menunjukkan bahawa untuk teknik yang dianggap kotak hitam, nomogram dapat dengan jelas memperlihatkan struktur dalamannya. Dengan memberikan visualisasi yang mudah ditafsirkan, penganalisis dapat memperoleh pandangan dan mengkaji kesan faktor ramalan. [[EENNDD]] visualisasi; mesin vektor sokongan; nomogram; pembelajaran mesin"], [{"string": "Mining images on semantics via statistical learning In this paper , we have proposed a novel framework to enable hierarchical image classification via statistical learning . By integrating the concept hierarchy for semantic image concept organization , a hierarchical mixture model is proposed to enable multi-level modeling of semantic image concepts and hierarchical classifier combination . Thus , learning the classifiers for the semantic image concepts at the high level of the concept hierarchy can be effectively achieved by detecting the presences of the relevant base-level atomic image concepts . To effectively learn the base-level classifiers for the atomic image concepts at the first level of the concept hierarchy , we have proposed a novel adaptive EM algorithm to achieve more effective model selection and parameter estimation . In addition , a novel penalty term is proposed to effectively eliminate the misleading effects of the outlying unlabeled images on semi-supervised classifier training . Our experimental results in a specific image domain of outdoor photos are very attractive .", "keywords": ["hierarchical mixture model", "adaptive em algorithm", "image classification"], "combined": "Mining images on semantics via statistical learning In this paper , we have proposed a novel framework to enable hierarchical image classification via statistical learning . By integrating the concept hierarchy for semantic image concept organization , a hierarchical mixture model is proposed to enable multi-level modeling of semantic image concepts and hierarchical classifier combination . Thus , learning the classifiers for the semantic image concepts at the high level of the concept hierarchy can be effectively achieved by detecting the presences of the relevant base-level atomic image concepts . To effectively learn the base-level classifiers for the atomic image concepts at the first level of the concept hierarchy , we have proposed a novel adaptive EM algorithm to achieve more effective model selection and parameter estimation . In addition , a novel penalty term is proposed to effectively eliminate the misleading effects of the outlying unlabeled images on semi-supervised classifier training . Our experimental results in a specific image domain of outdoor photos are very attractive . [[EENNDD]] hierarchical mixture model; adaptive em algorithm; image classification"}, "Menambang gambar pada semantik melalui pembelajaran statistik Dalam makalah ini, kami telah mengusulkan kerangka baru untuk memungkinkan klasifikasi gambar hierarki melalui pembelajaran statistik. Dengan mengintegrasikan hierarki konsep untuk organisasi konsep imej semantik, model campuran hierarki dicadangkan untuk memungkinkan pemodelan pelbagai peringkat konsep imej semantik dan gabungan pengklasifikasian hierarki. Oleh itu, mempelajari pengkelasan untuk konsep imej semantik pada tahap tinggi hierarki konsep dapat dicapai dengan berkesan dengan mengesan kehadiran konsep imej atom peringkat asas yang relevan. Untuk mempelajari pengelasan tahap asas untuk konsep imej atom pada tahap pertama hierarki konsep, kami telah mencadangkan algoritma EM adaptif baru untuk mencapai pemilihan model dan anggaran parameter yang lebih berkesan. Di samping itu, istilah penalti baru dicadangkan untuk secara berkesan menghilangkan kesan mengelirukan dari gambar tanpa label terpencil pada latihan pengkelasan separa diselia. Hasil percubaan kami dalam domain gambar khusus gambar luar sangat menarik. [[EENNDD]] model campuran hierarki; algoritma em adaptif; pengelasan gambar"], [{"string": "Selection , combination , and evaluation of effective software sensors for detecting abnormal computer usage We present and empirically analyze a machine-learning approach for detecting intrusions on individual computers . Our Winnow-based algorithm continually monitors user and system behavior , recording such properties as the number of bytes transferred over the last 10 seconds , the programs that currently are running , and the load on the CPU . In all , hundreds of measurements are made and analyzed each second . Using this data , our algorithm creates a model that represents each particular computer 's range of normal behavior . Parameters that determine when an alarm should be raised , due to abnormal activity , are set on a per-computer basis , based on an analysis of training data . A major issue in intrusion-detection systems is the need for very low false-alarm rates . Our empirical results suggest that it is possible to obtain high intrusion-detection rates 95 % and low false-alarm rates less than one per day per computer , without `` stealing '' too many CPU cycles less than 1 % . We also report which system measurements are the most valuable in terms of detecting intrusions . A surprisingly large number of different measurements prove significantly useful .", "keywords": ["user modeling", "anomaly detection", "intrusion detection", "learning", "security and protection", "winnow algorithm", "feature selection", "windows 2000", "machine learning"], "combined": "Selection , combination , and evaluation of effective software sensors for detecting abnormal computer usage We present and empirically analyze a machine-learning approach for detecting intrusions on individual computers . Our Winnow-based algorithm continually monitors user and system behavior , recording such properties as the number of bytes transferred over the last 10 seconds , the programs that currently are running , and the load on the CPU . In all , hundreds of measurements are made and analyzed each second . Using this data , our algorithm creates a model that represents each particular computer 's range of normal behavior . Parameters that determine when an alarm should be raised , due to abnormal activity , are set on a per-computer basis , based on an analysis of training data . A major issue in intrusion-detection systems is the need for very low false-alarm rates . Our empirical results suggest that it is possible to obtain high intrusion-detection rates 95 % and low false-alarm rates less than one per day per computer , without `` stealing '' too many CPU cycles less than 1 % . We also report which system measurements are the most valuable in terms of detecting intrusions . A surprisingly large number of different measurements prove significantly useful . [[EENNDD]] user modeling; anomaly detection; intrusion detection; learning; security and protection; winnow algorithm; feature selection; windows 2000; machine learning"}, "Pemilihan, kombinasi, dan penilaian sensor perisian yang berkesan untuk mengesan penggunaan komputer yang tidak normal. Kami menyajikan dan menganalisis secara empirik pendekatan pembelajaran mesin untuk mengesan gangguan pada komputer individu. Algoritma berasaskan Winnow kami terus memantau tingkah laku pengguna dan sistem, mencatat sifat seperti jumlah bait yang dipindahkan selama 10 saat terakhir, program yang sedang berjalan, dan beban pada CPU. Secara keseluruhan, beratus-ratus pengukuran dibuat dan dianalisis setiap saat. Dengan menggunakan data ini, algoritma kami membuat model yang mewakili setiap tingkah laku normal komputer tertentu. Parameter yang menentukan kapan penggera harus dinaikkan, karena aktivitas yang tidak normal, ditetapkan berdasarkan komputer, berdasarkan analisis data latihan. Isu utama dalam sistem pengesanan pencerobohan adalah keperluan untuk kadar penggera palsu yang sangat rendah. Hasil empirik kami menunjukkan bahawa mungkin untuk mendapatkan kadar pengesanan pencerobohan tinggi 95% dan kadar penggera palsu rendah kurang dari satu per hari setiap komputer, tanpa \"mencuri\" terlalu banyak kitaran CPU kurang dari 1%. Kami juga melaporkan pengukuran sistem mana yang paling berharga dari segi mengesan gangguan. Sebilangan besar ukuran yang berbeza sangat berguna. [[EENNDD]] pemodelan pengguna; pengesanan anomali; pengesanan pencerobohan; belajar; keselamatan dan perlindungan; algoritma winnow; pemilihan ciri; tingkap 2000; pembelajaran mesin"], [{"string": "Apolo : interactive large graph sensemaking by combining machine learning and visualization We present APOLO , a system that uses a mixed-initiative approach to help people interactively explore and make sense of large network datasets . It combines visualization , rich user interaction and machine learning to engage the user in bottom-up sensemaking to gradually build up an understanding over time by starting small , rather than starting big and drilling down . APOLO helps users find relevant information by specifying exemplars , and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest . We demonstrate APOLO 's usage and benefits using a Google Scholar citation graph , consisting of 83,000 articles nodes and 150,000 citations relationships . A demo video of APOLO is available at http:\\/\\/www.cs.cmu.edu\\/~dchau\\/apolo\\/apolo.mp4 .", "keywords": ["belief propagation", "user interfaces", "large network", "sensemaking"], "combined": "Apolo : interactive large graph sensemaking by combining machine learning and visualization We present APOLO , a system that uses a mixed-initiative approach to help people interactively explore and make sense of large network datasets . It combines visualization , rich user interaction and machine learning to engage the user in bottom-up sensemaking to gradually build up an understanding over time by starting small , rather than starting big and drilling down . APOLO helps users find relevant information by specifying exemplars , and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest . We demonstrate APOLO 's usage and benefits using a Google Scholar citation graph , consisting of 83,000 articles nodes and 150,000 citations relationships . A demo video of APOLO is available at http:\\/\\/www.cs.cmu.edu\\/~dchau\\/apolo\\/apolo.mp4 . [[EENNDD]] belief propagation; user interfaces; large network; sensemaking"}, "Apolo: pembuatan sensasi grafik besar interaktif dengan menggabungkan pembelajaran mesin dan visualisasi Kami mempersembahkan APOLO, sistem yang menggunakan pendekatan inisiatif campuran untuk membantu orang meneroka secara interaktif dan memahami kumpulan data rangkaian yang besar. Ini menggabungkan visualisasi, interaksi pengguna yang kaya dan pembelajaran mesin untuk melibatkan pengguna dalam pembuatan akal bawah-atas untuk secara beransur-ansur membangun pemahaman dari masa ke masa dengan memulai kecil, dan bukannya mulai besar dan meneliti. APOLO membantu pengguna mencari maklumat yang relevan dengan menentukan contoh, dan kemudian menggunakan kaedah pembelajaran mesin yang disebut Belief Propagation untuk menyimpulkan node lain yang mungkin menarik. Kami menunjukkan penggunaan dan faedah APOLO menggunakan grafik petikan Google Cendekia, yang terdiri daripada 83,000 simpul artikel dan 150,000 hubungan petikan. Video demo APOLO boleh didapati di http: \\ / \\ / www.cs.cmu.edu \\ / ~ dchau \\ / apolo \\ /apolo.mp4. [[EENNDD]] penyebaran kepercayaan; antara muka pengguna; rangkaian besar; membuat pertimbangan"], [{"string": "Relational learning via latent social dimensions Social media such as blogs , Facebook , Flickr , etc. , presents data in a network format rather than classical IID distribution . To address the interdependency among data instances , relational learning has been proposed , and collective inference based on network connectivity is adopted for prediction . However , connections in social media are often multi-dimensional . An actor can connect to another actor for different reasons , e.g. , alumni , colleagues , living in the same city , sharing similar interests , etc. . Collective inference normally does not differentiate these connections . In this work , we propose to extract latent social dimensions based on network information , and then utilize them as features for discriminative learning . These social dimensions describe diverse affiliations of actors hidden in the network , and the discriminative learning can automatically determine which affiliations are better aligned with the class labels . Such a scheme is preferred when multiple diverse relations are associated with the same network . We conduct extensive experiments on social media data one from a real-world blog site and the other from a popular content sharing site . Our model outperforms representative relational learning methods based on collective inference , especially when few labeled data are available . The sensitivity of this model and its connection to existing methods are also examined .", "keywords": ["behavior prediction", "relational learning", "social dimensions", "modularity", "social media"], "combined": "Relational learning via latent social dimensions Social media such as blogs , Facebook , Flickr , etc. , presents data in a network format rather than classical IID distribution . To address the interdependency among data instances , relational learning has been proposed , and collective inference based on network connectivity is adopted for prediction . However , connections in social media are often multi-dimensional . An actor can connect to another actor for different reasons , e.g. , alumni , colleagues , living in the same city , sharing similar interests , etc. . Collective inference normally does not differentiate these connections . In this work , we propose to extract latent social dimensions based on network information , and then utilize them as features for discriminative learning . These social dimensions describe diverse affiliations of actors hidden in the network , and the discriminative learning can automatically determine which affiliations are better aligned with the class labels . Such a scheme is preferred when multiple diverse relations are associated with the same network . We conduct extensive experiments on social media data one from a real-world blog site and the other from a popular content sharing site . Our model outperforms representative relational learning methods based on collective inference , especially when few labeled data are available . The sensitivity of this model and its connection to existing methods are also examined . [[EENNDD]] behavior prediction; relational learning; social dimensions; modularity; social media"}, "Pembelajaran hubungan melalui dimensi sosial laten Media sosial seperti blog, Facebook, Flickr, dan lain-lain, menyajikan data dalam format rangkaian dan bukannya pengedaran IID klasik. Untuk mengatasi saling ketergantungan antara contoh data, pembelajaran relasional telah diusulkan, dan kesimpulan kolektif berdasarkan kesambungan rangkaian diadopsi untuk ramalan. Walau bagaimanapun, hubungan di media sosial sering bersifat multi-dimensi. Pelakon boleh berhubung dengan pelakon lain kerana alasan yang berbeza, mis , alumni, rakan sekerja, tinggal di bandar yang sama, berkongsi minat yang serupa, dll. Inferens kolektif biasanya tidak membezakan hubungan ini. Dalam karya ini, kami mencadangkan untuk mengekstrak dimensi sosial terpendam berdasarkan maklumat rangkaian, dan kemudian menggunakannya sebagai ciri untuk pembelajaran diskriminatif. Dimensi sosial ini menggambarkan pelbagai afiliasi pelaku yang tersembunyi di dalam rangkaian, dan pembelajaran yang diskriminatif secara automatik dapat menentukan afiliasi mana yang lebih sesuai dengan label kelas. Skema seperti ini lebih disukai apabila hubungan pelbagai ragam dihubungkan dengan rangkaian yang sama. Kami melakukan eksperimen yang meluas pada data media sosial satu dari laman blog dunia nyata dan yang lain dari laman perkongsian kandungan yang popular. Model kami mengungguli kaedah pembelajaran relasional yang mewakili berdasarkan kesimpulan kolektif, terutamanya apabila terdapat sedikit data berlabel. Kepekaan model ini dan kaitannya dengan kaedah yang ada juga dikaji. [[EENNDD]] ramalan tingkah laku; pembelajaran hubungan; dimensi sosial; modulariti; media sosial"], [{"string": "Towards an effective cooperation of the user and the computer for classification", "keywords": ["decision support"], "combined": "Towards an effective cooperation of the user and the computer for classification [[EENNDD]] decision support"}, "Ke arah kerjasama pengguna dan komputer yang berkesan untuk sokongan keputusan klasifikasi [[EENNDD]]"], [{"string": "Acclimatizing Taxonomic Semantics for Hierarchical Content Classification Hierarchical models have been shown to be effective in content classification . However , we observe through empirical study that the performance of a hierarchical model varies with given taxonomies ; even a semantically sound taxonomy has potential to change its structure for better classification . By scrutinizing typical cases , we elucidate why a given semantics-based hierarchy does not work well in content classification , and how it could be improved for accurate hierarchical classification . With these understandings , we propose effective localized solutions that modify the given taxonomy for accurate hierarchical classification . We conduct extensive experiments on both toy and real-world data sets , report improved performance and interesting findings , and provide further analysis of algorithmic issues such as time complexity , robustness , and sensitivity to the number of features .", "keywords": ["hierarchical modeling", "taxonomy adjustment", "text classification", "hierarchical classification"], "combined": "Acclimatizing Taxonomic Semantics for Hierarchical Content Classification Hierarchical models have been shown to be effective in content classification . However , we observe through empirical study that the performance of a hierarchical model varies with given taxonomies ; even a semantically sound taxonomy has potential to change its structure for better classification . By scrutinizing typical cases , we elucidate why a given semantics-based hierarchy does not work well in content classification , and how it could be improved for accurate hierarchical classification . With these understandings , we propose effective localized solutions that modify the given taxonomy for accurate hierarchical classification . We conduct extensive experiments on both toy and real-world data sets , report improved performance and interesting findings , and provide further analysis of algorithmic issues such as time complexity , robustness , and sensitivity to the number of features . [[EENNDD]] hierarchical modeling; taxonomy adjustment; text classification; hierarchical classification"}, "Memperakui Semantik Taksonomi untuk Klasifikasi Kandungan Hierarki Model hierarki telah terbukti berkesan dalam klasifikasi kandungan. Walau bagaimanapun, kami melihat melalui kajian empirikal bahawa prestasi model hierarki berbeza dengan taksonomi yang diberikan; malah taksonomi semantiknya berpotensi untuk mengubah strukturnya untuk klasifikasi yang lebih baik. Dengan meneliti kes-kes biasa, kami menjelaskan mengapa hierarki berdasarkan semantik tertentu tidak berfungsi dengan baik dalam klasifikasi kandungan, dan bagaimana ia dapat diperbaiki untuk klasifikasi hierarki yang tepat. Dengan pemahaman ini, kami mencadangkan penyelesaian tempatan yang berkesan yang mengubah taksonomi yang diberikan untuk klasifikasi hierarki yang tepat. Kami menjalankan eksperimen yang luas pada kedua-dua set data mainan dan dunia nyata, melaporkan peningkatan prestasi dan penemuan yang menarik, dan memberikan analisis lebih lanjut mengenai masalah algoritma seperti kerumitan masa, ketahanan, dan kepekaan terhadap jumlah ciri. [[EENNDD]] pemodelan hierarki; pelarasan taksonomi; pengelasan teks; klasifikasi hierarki"], [{"string": "Hardening soft information sources", "keywords": ["miscellaneous", "data integration"], "combined": "Hardening soft information sources [[EENNDD]] miscellaneous; data integration"}, "Mengeras sumber maklumat yang lembut [[EENNDD]] pelbagai; penyatuan data"], [{"string": "Discovery of climate indices using clustering To analyze the effect of the oceans and atmosphere on land climate , Earth Scientists have developed climate indices , which are time series that summarize the behavior of selected regions of the Earth 's oceans and atmosphere . In the past , Earth scientists have used observation and , more recently , eigenvalue analysis techniques , such as principal components analysis PCA and singular value decomposition SVD , to discover climate indices . However , eigenvalue techniques are only useful for finding a few of the strongest signals . Furthermore , they impose a condition that all discovered signals must be orthogonal to each other , making it difficult to attach a physical interpretation to them . This paper presents an alternative clustering-based methodology for the discovery of climate indices that overcomes these limitiations and is based on clusters that represent regions with relatively homogeneous behavior . The centroids of these clusters are time series that summarize the behavior of the ocean or atmosphere in those regions . Some of these centroids correspond to known climate indices and provide a validation of our methodology ; other centroids are variants of known indices that may provide better predictive power for some land areas ; and still other indices may represent potentially new Earth science phenomena . Finally , we show that cluster based indices generally outperform SVD derived indices , both in terms of area weighted correlation and direct correlation with the known indices .", "keywords": ["time series", "singular value decomposition", "mining scientific data", "earth science data", "clustering"], "combined": "Discovery of climate indices using clustering To analyze the effect of the oceans and atmosphere on land climate , Earth Scientists have developed climate indices , which are time series that summarize the behavior of selected regions of the Earth 's oceans and atmosphere . In the past , Earth scientists have used observation and , more recently , eigenvalue analysis techniques , such as principal components analysis PCA and singular value decomposition SVD , to discover climate indices . However , eigenvalue techniques are only useful for finding a few of the strongest signals . Furthermore , they impose a condition that all discovered signals must be orthogonal to each other , making it difficult to attach a physical interpretation to them . This paper presents an alternative clustering-based methodology for the discovery of climate indices that overcomes these limitiations and is based on clusters that represent regions with relatively homogeneous behavior . The centroids of these clusters are time series that summarize the behavior of the ocean or atmosphere in those regions . Some of these centroids correspond to known climate indices and provide a validation of our methodology ; other centroids are variants of known indices that may provide better predictive power for some land areas ; and still other indices may represent potentially new Earth science phenomena . Finally , we show that cluster based indices generally outperform SVD derived indices , both in terms of area weighted correlation and direct correlation with the known indices . [[EENNDD]] time series; singular value decomposition; mining scientific data; earth science data; clustering"}, "Penemuan indeks iklim menggunakan pengelompokan Untuk menganalisis pengaruh lautan dan atmosfer pada iklim darat, Saintis Bumi telah mengembangkan indeks iklim, yang merupakan rangkaian waktu yang merangkum perilaku wilayah terpilih di lautan dan atmosfer Bumi. Pada masa lalu, saintis Bumi telah menggunakan pemerhatian dan, baru-baru ini, teknik analisis nilai eigen, seperti PCA analisis komponen utama dan penguraian nilai tunggal SVD, untuk menemui indeks iklim. Walau bagaimanapun, teknik nilai eigen hanya berguna untuk mencari beberapa isyarat terkuat. Selanjutnya, mereka mengenakan syarat bahawa semua isyarat yang ditemui mestilah ortogonal satu sama lain, sehingga sukar untuk melampirkan interpretasi fizikal kepada mereka. Makalah ini menyajikan metodologi penggabungan alternatif untuk penemuan indeks iklim yang mengatasi keterbatasan ini dan didasarkan pada kelompok yang mewakili wilayah dengan tingkah laku yang relatif homogen. Pusat dari kelompok ini adalah siri masa yang merangkum tingkah laku lautan atau atmosfer di kawasan-kawasan tersebut. Sebilangan centroid ini sesuai dengan indeks iklim yang diketahui dan memberikan pengesahan metodologi kami; centroid lain adalah varian indeks yang diketahui yang dapat memberikan daya ramalan yang lebih baik untuk beberapa kawasan darat; dan indeks lain mungkin mewakili fenomena sains Bumi yang berpotensi baru. Akhirnya, kami menunjukkan bahawa indeks berdasarkan kluster umumnya mengungguli indeks turunan SVD, baik dari segi korelasi wajaran kawasan dan korelasi langsung dengan indeks yang diketahui. [[EENNDD]] siri masa; penguraian nilai tunggal; data saintifik perlombongan; data sains bumi; pengelompokan"], [{"string": "Exploring social tagging graph for web object classification This paper studies web object classification problem with the novel exploration of social tags . Automatically classifying web objects into manageable semantic categories has long been a fundamental preprocess for indexing , browsing , searching , and mining these objects . The explosive growth of heterogeneous web objects , especially non-textual objects such as products , pictures , and videos , has made the problem of web classification increasingly challenging . Such objects often suffer from a lack of easy-extractable features with semantic information , interconnections between each other , as well as training examples with category labels . In this paper , we explore the social tagging data to bridge this gap . We cast web object classification problem as an optimization problem on a graph of objects and tags . We then propose an efficient algorithm which not only utilizes social tags as enriched semantic features for the objects , but also infers the categories of unlabeled objects from both homogeneous and heterogeneous labeled objects , through the implicit connection of social tags . Experiment results show that the exploration of social tags effectively boosts web object classification . Our algorithm significantly outperforms the state-of-the-art of general classification methods .", "keywords": ["optimization", "general", "web classification", "social tagging"], "combined": "Exploring social tagging graph for web object classification This paper studies web object classification problem with the novel exploration of social tags . Automatically classifying web objects into manageable semantic categories has long been a fundamental preprocess for indexing , browsing , searching , and mining these objects . The explosive growth of heterogeneous web objects , especially non-textual objects such as products , pictures , and videos , has made the problem of web classification increasingly challenging . Such objects often suffer from a lack of easy-extractable features with semantic information , interconnections between each other , as well as training examples with category labels . In this paper , we explore the social tagging data to bridge this gap . We cast web object classification problem as an optimization problem on a graph of objects and tags . We then propose an efficient algorithm which not only utilizes social tags as enriched semantic features for the objects , but also infers the categories of unlabeled objects from both homogeneous and heterogeneous labeled objects , through the implicit connection of social tags . Experiment results show that the exploration of social tags effectively boosts web object classification . Our algorithm significantly outperforms the state-of-the-art of general classification methods . [[EENNDD]] optimization; general; web classification; social tagging"}, "Meneroka grafik penandaan sosial untuk klasifikasi objek web Makalah ini mengkaji masalah pengelasan objek web dengan penerokaan tag sosial. Mengklasifikasikan objek web secara automatik ke dalam kategori semantik yang dapat dikendalikan telah lama menjadi proses asas untuk mengindeks, melayari, mencari, dan melombong objek-objek ini. Pertumbuhan eksplosif objek web yang heterogen, terutamanya objek bukan teks seperti produk, gambar, dan video, menjadikan masalah klasifikasi web semakin mencabar. Objek seperti itu sering mengalami kekurangan ciri yang mudah diekstrak dengan maklumat semantik, saling hubungan antara satu sama lain, dan juga contoh latihan dengan label kategori. Dalam makalah ini, kami meneroka data penandaan sosial untuk merapatkan jurang ini. Kami membuang masalah pengelasan objek web sebagai masalah pengoptimuman pada grafik objek dan tag. Kami kemudian mencadangkan algoritma yang cekap yang tidak hanya menggunakan tag sosial sebagai ciri semantik yang diperkaya untuk objek tersebut, tetapi juga memasukkan kategori objek yang tidak berlabel dari objek berlabel homogen dan heterogen, melalui penyambungan tag sosial yang tersirat. Hasil eksperimen menunjukkan bahawa penerokaan tag sosial secara berkesan meningkatkan klasifikasi objek web. Algoritma kami mengatasi kaedah klasifikasi umum yang terkini. [[EENNDD]] pengoptimuman; umum; klasifikasi web; penandaan sosial"], [{"string": "SEWeP : using site semantics and a taxonomy to enhance the Web personalization process Web personalization is the process of customizing a Web site to the needs of each specific user or set of users , taking advantage of the knowledge acquired through the analysis of the user 's navigational behavior . Integrating usage data with content , structure or user profile data enhances the results of the personalization process . In this paper , we present SEWeP , a system that makes use of both the usage logs and the semantics of a Web site 's content in order to personalize it . Web content is semantically annotated using a conceptual hierarchy taxonomy . We introduce C-logs , an extended form of Web usage logs that encapsulates knowledge derived from the link semantics . C-logs are used as input to the Web usage mining process , resulting in a broader yet semantically focused set of recommendations .", "keywords": ["semantic annotation of web content", "web personalization", "on-line information services", "database applications", "concept hierarchies", "web mining"], "combined": "SEWeP : using site semantics and a taxonomy to enhance the Web personalization process Web personalization is the process of customizing a Web site to the needs of each specific user or set of users , taking advantage of the knowledge acquired through the analysis of the user 's navigational behavior . Integrating usage data with content , structure or user profile data enhances the results of the personalization process . In this paper , we present SEWeP , a system that makes use of both the usage logs and the semantics of a Web site 's content in order to personalize it . Web content is semantically annotated using a conceptual hierarchy taxonomy . We introduce C-logs , an extended form of Web usage logs that encapsulates knowledge derived from the link semantics . C-logs are used as input to the Web usage mining process , resulting in a broader yet semantically focused set of recommendations . [[EENNDD]] semantic annotation of web content; web personalization; on-line information services; database applications; concept hierarchies; web mining"}, "SEWeP: menggunakan semantik laman dan taksonomi untuk meningkatkan proses pemperibadian Web Pemperibadian web adalah proses menyesuaikan laman web dengan keperluan setiap pengguna atau sekumpulan pengguna tertentu, memanfaatkan pengetahuan yang diperoleh melalui analisis pengguna tingkah laku pelayaran. Mengintegrasikan data penggunaan dengan kandungan, struktur atau data profil pengguna meningkatkan hasil proses pemperibadian. Dalam makalah ini, kami menyajikan SEWeP, sebuah sistem yang menggunakan log penggunaan dan semantik kandungan laman web untuk memperibadikannya. Kandungan web dianotasi secara semantik menggunakan taksonomi hierarki konseptual. Kami memperkenalkan C-log, bentuk log penggunaan Web yang diperluas yang merangkumi pengetahuan yang berasal dari semantik pautan. C-log digunakan sebagai masukan untuk proses penambangan penggunaan Web, menghasilkan serangkaian cadangan yang lebih luas dan fokus secara semantik. [[EENNDD]] anotasi semantik kandungan web; pemperibadian web; perkhidmatan maklumat dalam talian; aplikasi pangkalan data; hierarki konsep; perlombongan web"], [{"string": "Discovering the set of fundamental rule changes The world around us changes constantly . Knowing what has changed is an important part of our lives . For businesses , recognizing changes is also crucial . It allows businesses to adapt themselves to the changing market needs . In this paper , we study changes of association rules from one time period to another . One approach is to compare the supports and\\/or confidences of each rule in the two time periods and report the differences . This technique , however , is too simplistic as it tends to report a huge number of rule changes , and many of them are , in fact , simply the snowball effect of a small subset of fundamental changes . Here , we present a technique to highlight the small subset of fundamental changes . A change is fundamental if it can not be explained by some other changes . The proposed technique has been applied to a number of real-life datasets . Experiments results show that the number of rules whose changes are unexplainable is quite small about 20 % of the total number of changes discovered , and many of these unexplainable changes reflect some fundamental shifts in the application domain .", "keywords": ["change mining"], "combined": "Discovering the set of fundamental rule changes The world around us changes constantly . Knowing what has changed is an important part of our lives . For businesses , recognizing changes is also crucial . It allows businesses to adapt themselves to the changing market needs . In this paper , we study changes of association rules from one time period to another . One approach is to compare the supports and\\/or confidences of each rule in the two time periods and report the differences . This technique , however , is too simplistic as it tends to report a huge number of rule changes , and many of them are , in fact , simply the snowball effect of a small subset of fundamental changes . Here , we present a technique to highlight the small subset of fundamental changes . A change is fundamental if it can not be explained by some other changes . The proposed technique has been applied to a number of real-life datasets . Experiments results show that the number of rules whose changes are unexplainable is quite small about 20 % of the total number of changes discovered , and many of these unexplainable changes reflect some fundamental shifts in the application domain . [[EENNDD]] change mining"}, "Menemui sekumpulan perubahan peraturan asas Dunia di sekeliling kita sentiasa berubah. Mengetahui apa yang telah berubah adalah bahagian penting dalam kehidupan kita. Bagi perniagaan, menyedari perubahan juga penting. Ini membolehkan perniagaan menyesuaikan diri dengan perubahan keperluan pasaran. Dalam makalah ini, kami mengkaji perubahan peraturan persatuan dari satu jangka masa ke masa yang lain. Salah satu pendekatan adalah membandingkan sokongan dan \\ / atau kepercayaan setiap peraturan dalam dua jangka masa dan melaporkan perbezaannya. Teknik ini, bagaimanapun, terlalu sederhana kerana cenderung melaporkan sejumlah besar perubahan peraturan, dan banyak dari mereka, sebenarnya, hanyalah kesan bola salji dari sekumpulan kecil perubahan mendasar. Di sini, kami memaparkan teknik untuk menyoroti subset kecil perubahan mendasar. Perubahan adalah asas jika tidak dapat dijelaskan oleh beberapa perubahan lain. Teknik yang dicadangkan telah diterapkan pada sejumlah kumpulan data kehidupan nyata. Hasil eksperimen menunjukkan bahawa jumlah peraturan yang perubahannya tidak dapat dijelaskan cukup kecil sekitar 20% dari jumlah perubahan yang ditemukan, dan banyak perubahan yang tidak dapat dijelaskan ini mencerminkan beberapa perubahan mendasar dalam domain aplikasi. [[EENNDD]] menukar perlombongan"], [{"string": "Model order selection for boolean matrix factorization Matrix factorizations -- where a given data matrix is approximated by a product of two or more factor matrices -- are powerful data mining tools . Among other tasks , matrix factorizations are often used to separate global structure from noise . This , however , requires solving the ` model order selection problem ' of determining where fine-grained structure stops , and noise starts , i.e. , what is the proper size of the factor matrices . Boolean matrix factorization BMF -- where data , factors , and matrix product are Boolean -- has received increased attention from the data mining community in recent years . The technique has desirable properties , such as high interpretability and natural sparsity . But so far no method for selecting the correct model order for BMF has been available . In this paper we propose to use the Minimum Description Length MDL principle for this task . Besides solving the problem , this well-founded approach has numerous benefits , e.g. , it is automatic , does not require a likelihood function , is fast , and , as experiments show , is highly accurate . We formulate the description length function for BMF in general -- making it applicable for any BMF algorithm . We extend an existing algorithm for BMF to use MDL to identify the best Boolean matrix factorization , analyze the complexity of the problem , and perform an extensive experimental evaluation to study its behavior .", "keywords": ["matrix decompositions", "model order selection", "model selection", "minimum description length principle", "matrix factorizations", "boolean matrix factorizations"], "combined": "Model order selection for boolean matrix factorization Matrix factorizations -- where a given data matrix is approximated by a product of two or more factor matrices -- are powerful data mining tools . Among other tasks , matrix factorizations are often used to separate global structure from noise . This , however , requires solving the ` model order selection problem ' of determining where fine-grained structure stops , and noise starts , i.e. , what is the proper size of the factor matrices . Boolean matrix factorization BMF -- where data , factors , and matrix product are Boolean -- has received increased attention from the data mining community in recent years . The technique has desirable properties , such as high interpretability and natural sparsity . But so far no method for selecting the correct model order for BMF has been available . In this paper we propose to use the Minimum Description Length MDL principle for this task . Besides solving the problem , this well-founded approach has numerous benefits , e.g. , it is automatic , does not require a likelihood function , is fast , and , as experiments show , is highly accurate . We formulate the description length function for BMF in general -- making it applicable for any BMF algorithm . We extend an existing algorithm for BMF to use MDL to identify the best Boolean matrix factorization , analyze the complexity of the problem , and perform an extensive experimental evaluation to study its behavior . [[EENNDD]] matrix decompositions; model order selection; model selection; minimum description length principle; matrix factorizations; boolean matrix factorizations"}, "Pemilihan pesanan model untuk pemfaktoran matriks boolean Pemfaktoran matriks - di mana matriks data tertentu dihampirkan oleh produk dengan dua atau lebih matriks faktor - adalah alat perlombongan data yang kuat. Antara tugas lain, pemfaktoran matriks sering digunakan untuk memisahkan struktur global dari kebisingan. Walau bagaimanapun, ini memerlukan penyelesaian \"masalah pemilihan pesanan model\" untuk menentukan di mana struktur berbutir halus berhenti, dan kebisingan bermula, iaitu berapakah ukuran matriks faktor yang tepat. Pemfaktoran matriks Boolean BMF - di mana data, faktor, dan produk matriks adalah Boolean - telah mendapat perhatian dari komuniti perlombongan data dalam beberapa tahun terakhir. Teknik ini mempunyai sifat yang diinginkan, seperti kebolehtafsiran yang tinggi dan kelangkaan semula jadi. Tetapi setakat ini tidak ada kaedah untuk memilih susunan model yang betul untuk BMF. Dalam makalah ini kami mencadangkan untuk menggunakan prinsip MDL Panjang Huraian Minimum untuk tugas ini. Selain menyelesaikan masalah, pendekatan yang baik ini mempunyai banyak faedah, mis. , automatik, tidak memerlukan fungsi kemungkinan, cepat, dan, seperti yang ditunjukkan oleh eksperimen, sangat tepat. Kami merumuskan fungsi panjang keterangan untuk BMF secara umum - menjadikannya dapat digunakan untuk algoritma BMF mana pun. Kami memperluas algoritma yang ada untuk BMF menggunakan MDL untuk mengenal pasti pemfaktoran matriks Boolean terbaik, menganalisis kerumitan masalah, dan melakukan penilaian eksperimen yang luas untuk mengkaji tingkah lakunya. [[EENNDD]] penguraian matriks; pemilihan pesanan model; pemilihan model; prinsip panjang keterangan minimum; pemfaktoran matriks; pemfaktoran matriks boolean"], [{"string": "Improving classification accuracy using automatically extracted training data Classification is a core task in knowledge discovery and data mining , and there has been substantial research effort in developing sophisticated classification models . In a parallel thread , recent work from the NLP community suggests that for tasks such as natural language disambiguation even a simple algorithm can outperform a sophisticated one , if it is provided with large quantities of high quality training data . In those applications , training data occurs naturally in text corpora , and high quality training data sets running into billions of words have been reportedly used . We explore how we can apply the lessons from the NLP community to KDD tasks . Specifically , we investigate how to identify data sources that can yield training data at low cost and study whether the quantity of the automatically extracted training data can compensate for its lower quality . We carry out this investigation for the specific task of inferring whether a search query has commercial intent . We mine toolbar and click logs to extract queries from sites that are predominantly commercial e.g. , Amazon and non-commercial e.g. , Wikipedia . We compare the accuracy obtained using such training data against manually labeled training data . Our results show that we can have large accuracy gains using automatically extracted training data at much lower cost .", "keywords": ["query intent", "classification", "automatically labeled data"], "combined": "Improving classification accuracy using automatically extracted training data Classification is a core task in knowledge discovery and data mining , and there has been substantial research effort in developing sophisticated classification models . In a parallel thread , recent work from the NLP community suggests that for tasks such as natural language disambiguation even a simple algorithm can outperform a sophisticated one , if it is provided with large quantities of high quality training data . In those applications , training data occurs naturally in text corpora , and high quality training data sets running into billions of words have been reportedly used . We explore how we can apply the lessons from the NLP community to KDD tasks . Specifically , we investigate how to identify data sources that can yield training data at low cost and study whether the quantity of the automatically extracted training data can compensate for its lower quality . We carry out this investigation for the specific task of inferring whether a search query has commercial intent . We mine toolbar and click logs to extract queries from sites that are predominantly commercial e.g. , Amazon and non-commercial e.g. , Wikipedia . We compare the accuracy obtained using such training data against manually labeled training data . Our results show that we can have large accuracy gains using automatically extracted training data at much lower cost . [[EENNDD]] query intent; classification; automatically labeled data"}, "Meningkatkan ketepatan klasifikasi menggunakan data latihan yang diekstraksi secara automatik Klasifikasi adalah tugas utama dalam penemuan pengetahuan dan perlombongan data, dan telah ada upaya penelitian yang besar dalam mengembangkan model klasifikasi yang canggih. Dalam urutan yang selari, karya terbaru dari komuniti NLP menunjukkan bahawa untuk tugas-tugas seperti disambiguasi bahasa semula jadi, bahkan algoritma sederhana dapat mengatasi yang canggih, jika disertakan dengan sejumlah besar data latihan berkualiti tinggi. Dalam aplikasi tersebut, data latihan berlaku secara semula jadi dalam korporat teks, dan set data latihan berkualiti tinggi yang menggunakan miliaran kata dilaporkan telah digunakan. Kami meneroka bagaimana kami dapat menerapkan pelajaran dari komuniti NLP untuk tugas-tugas KDD. Secara khusus, kami menyiasat bagaimana mengenal pasti sumber data yang dapat menghasilkan data latihan dengan kos rendah dan mengkaji apakah kuantiti data latihan yang diekstrak secara automatik dapat mengimbangi kualitinya yang lebih rendah. Kami menjalankan penyiasatan ini untuk tugas tertentu untuk menyimpulkan sama ada pertanyaan carian mempunyai maksud komersial. Kami melombong bar alat dan mengklik log untuk mengekstrak pertanyaan dari laman web yang terutama komersial, mis. , Amazon dan bukan komersial mis. , Wikipedia. Kami membandingkan ketepatan yang diperoleh menggunakan data latihan tersebut dengan data latihan yang dilabel secara manual. Hasil kajian kami menunjukkan bahawa kami dapat memperoleh ketepatan yang besar menggunakan data latihan yang diekstrak secara automatik dengan kos yang jauh lebih rendah. [[EENNDD]] maksud pertanyaan; pengelasan; data berlabel secara automatik"], [{"string": "Recovering latent time-series from their observed sums : network tomography with particle filters . Hidden variables , evolving over time , appear in multiple settings , where it is valuable to recover them , typically from observed sums . Our driving application is ` network tomography ' , where we need to estimate the origin-destination OD traffic flows to determine , e.g. , who is communicating with whom in a local area network . This information allows network engineers and managers to solve problems in design , routing , configuration debugging , monitoring and pricing . Unfortunately the direct measurement of the OD traffic is usually difficult , or even impossible ; instead , we can easily measure the loads on every link , that is , sums of desirable OD flows . In this paper we propose i-FILTER , a method to solve this problem , which improves the state-of-the-art by a introducing explicit time dependence , and by b using realistic , non-Gaussian marginals in the statistical models for the traffic flows , as never attempted before . We give experiments on real data , where i-FILTER scales linearly with new observations and out-performs the best existing solutions , in a wide variety of settings . Specifically , on real network traffic measured at CMU , and at AT&T , i-FILTER reduced the estimation errors between 15 % and 46 % in all cases .", "keywords": ["learning", "self-organizing bayesian dynamical system", "particle filter", "link loads", "origin-destination traffic flows", "mcmc", "empirical bayes", "informative priors"], "combined": "Recovering latent time-series from their observed sums : network tomography with particle filters . Hidden variables , evolving over time , appear in multiple settings , where it is valuable to recover them , typically from observed sums . Our driving application is ` network tomography ' , where we need to estimate the origin-destination OD traffic flows to determine , e.g. , who is communicating with whom in a local area network . This information allows network engineers and managers to solve problems in design , routing , configuration debugging , monitoring and pricing . Unfortunately the direct measurement of the OD traffic is usually difficult , or even impossible ; instead , we can easily measure the loads on every link , that is , sums of desirable OD flows . In this paper we propose i-FILTER , a method to solve this problem , which improves the state-of-the-art by a introducing explicit time dependence , and by b using realistic , non-Gaussian marginals in the statistical models for the traffic flows , as never attempted before . We give experiments on real data , where i-FILTER scales linearly with new observations and out-performs the best existing solutions , in a wide variety of settings . Specifically , on real network traffic measured at CMU , and at AT&T , i-FILTER reduced the estimation errors between 15 % and 46 % in all cases . [[EENNDD]] learning; self-organizing bayesian dynamical system; particle filter; link loads; origin-destination traffic flows; mcmc; empirical bayes; informative priors"}, "Memulihkan siri masa laten dari jumlah yang diperhatikan: tomografi rangkaian dengan penapis zarah. Pemboleh ubah tersembunyi, berkembang dari masa ke masa, muncul dalam pelbagai tetapan, di mana sangat berharga untuk memulihkannya, biasanya dari jumlah yang diperhatikan. Aplikasi pemanduan kami adalah \"tomografi rangkaian\", di mana kita perlu menganggarkan aliran lalu lintas OD-tujuan asal untuk menentukan, mis. , yang berkomunikasi dengan siapa dalam rangkaian kawasan setempat. Maklumat ini membolehkan jurutera dan pengurus rangkaian menyelesaikan masalah dalam reka bentuk, perutean, penyahpepijatan konfigurasi, pemantauan dan harga. Malangnya pengukuran langsung lalu lintas OD biasanya sukar, atau bahkan mustahil; sebaliknya, kita dapat dengan mudah mengukur beban pada setiap pautan, iaitu jumlah aliran OD yang diinginkan. Dalam makalah ini kami mengusulkan i-FILTER, sebuah kaedah untuk menyelesaikan masalah ini, yang meningkatkan keadaan canggih dengan memperkenalkan ketergantungan waktu yang eksplisit, dan dengan menggunakan marginal non-Gaussian yang realistik dalam model statistik untuk lalu lintas mengalir, seperti yang tidak pernah dicuba sebelumnya. Kami memberikan eksperimen pada data sebenar, di mana i-FILTER skala secara linear dengan pemerhatian baru dan melakukan penyelesaian terbaik yang ada, dalam pelbagai tetapan. Khususnya, pada trafik rangkaian nyata yang diukur pada CMU, dan di AT&T, i-FILTER mengurangkan kesilapan anggaran antara 15% dan 46% dalam semua kes. [[EENNDD]] pembelajaran; sistem dinamik bayesian yang mengatur sendiri; penapis zarah; beban pautan; aliran trafik asal-destinasi; mcmc; teluk empirikal; maklumat awal"], [{"string": "Relational Markov models and their application to adaptive web navigation Relational Markov models RMMs are a generalization of Markov models where states can be of different types , with each type described by a different set of variables . The domain of each variable can be hierarchically structured , and shrinkage is carried out over the cross product of these hierarchies . RMMs make effective learning possible in domains with very large and heterogeneous state spaces , given only sparse data . We apply them to modeling the behavior of web site users , improving prediction in our PROTEUS architecture for personalizing web sites . We present experiments on an e-commerce and an academic web site showing that RMMs are substantially more accurate than alternative methods , and make good predictions even when applied to previously-unvisited parts of the site .", "keywords": ["personalization", "markov models", "relational probabilistic models", "web mining", "shrinkage"], "combined": "Relational Markov models and their application to adaptive web navigation Relational Markov models RMMs are a generalization of Markov models where states can be of different types , with each type described by a different set of variables . The domain of each variable can be hierarchically structured , and shrinkage is carried out over the cross product of these hierarchies . RMMs make effective learning possible in domains with very large and heterogeneous state spaces , given only sparse data . We apply them to modeling the behavior of web site users , improving prediction in our PROTEUS architecture for personalizing web sites . We present experiments on an e-commerce and an academic web site showing that RMMs are substantially more accurate than alternative methods , and make good predictions even when applied to previously-unvisited parts of the site . [[EENNDD]] personalization; markov models; relational probabilistic models; web mining; shrinkage"}, "Model Relational Markov dan aplikasinya untuk navigasi web adaptif Model Relational Markov RMM adalah generalisasi model Markov di mana keadaan boleh terdiri dari pelbagai jenis, dengan setiap jenis dijelaskan oleh sekumpulan pemboleh ubah yang berbeza. Domain setiap pemboleh ubah dapat disusun secara hierarki, dan penyusutan dilakukan atas produk silang dari hierarki ini. RMM menjadikan pembelajaran berkesan dapat dilakukan dalam domain dengan ruang keadaan yang sangat besar dan heterogen, hanya dengan data yang jarang. Kami menerapkannya untuk memodelkan tingkah laku pengguna laman web, meningkatkan ramalan dalam arsitektur PROTEUS kami untuk memperibadikan laman web. Kami membentangkan eksperimen di laman web e-dagang dan laman web akademik yang menunjukkan bahawa RMM jauh lebih tepat daripada kaedah alternatif, dan membuat ramalan yang baik walaupun diterapkan pada bahagian laman web yang sebelumnya tidak dikunjungi. [[EENNDD]] pemperibadian; model markov; model probabilistik hubungan; perlombongan web; pengecutan"], [{"string": "Large human communication networks : patterns and a utility-driven generator Given a real , and weighted person-to-person network which changes over time , what can we say about the cliques that it contains ? Do the incidents of communication , or weights on the edges of a clique follow any pattern ? Real , and in-person social networks have many more triangles than chance would dictate . As it turns out , there are many more cliques than one would expect , in surprising patterns . In this paper , we study massive real-world social networks formed by direct contacts among people through various personal communication services , such as Phone-Call , SMS , IM etc. . The contributions are the following : a we discover surprising patterns with the cliques , b we report power-laws of the weights on the edges of cliques , c our real networks follow these patterns such that we can trust them to spot outliers and finally , d we propose the first utility-driven graph generator for weighted time-evolving networks , which match the observed patterns . Our study focused on three large datasets , each of which is a different type of communication service , with over one million records , and spans several months of activity .", "keywords": ["social networks", "graph generators", "cliques"], "combined": "Large human communication networks : patterns and a utility-driven generator Given a real , and weighted person-to-person network which changes over time , what can we say about the cliques that it contains ? Do the incidents of communication , or weights on the edges of a clique follow any pattern ? Real , and in-person social networks have many more triangles than chance would dictate . As it turns out , there are many more cliques than one would expect , in surprising patterns . In this paper , we study massive real-world social networks formed by direct contacts among people through various personal communication services , such as Phone-Call , SMS , IM etc. . The contributions are the following : a we discover surprising patterns with the cliques , b we report power-laws of the weights on the edges of cliques , c our real networks follow these patterns such that we can trust them to spot outliers and finally , d we propose the first utility-driven graph generator for weighted time-evolving networks , which match the observed patterns . Our study focused on three large datasets , each of which is a different type of communication service , with over one million records , and spans several months of activity . [[EENNDD]] social networks; graph generators; cliques"}, "Rangkaian komunikasi manusia yang besar: corak dan penjana yang didorong oleh utiliti Memandangkan rangkaian orang-ke-orang yang nyata dan berwajaran yang berubah dari masa ke masa, apa yang dapat kita katakan mengenai klek yang terdapat di dalamnya? Adakah kejadian komunikasi, atau bobot di pinggir klik mengikut corak? Jaringan sosial yang nyata dan peribadi mempunyai lebih banyak segitiga daripada yang ditentukan oleh kebetulan. Ternyata, ada banyak lagi klik daripada yang diharapkan, dalam corak yang mengejutkan. Dalam makalah ini, kami mengkaji rangkaian sosial dunia nyata yang besar yang dibentuk oleh kenalan langsung di antara orang-orang melalui pelbagai perkhidmatan komunikasi peribadi, seperti Telefon-Panggilan, SMS, IM dll. Sumbangannya adalah seperti berikut: a kami menemui corak yang mengejutkan dengan klek, dan kami melaporkan undang-undang kekuatan bobot di pinggir klik, c jaringan sebenar kami mengikuti corak ini sehingga kami dapat mempercayainya untuk melihat outliers dan akhirnya, d kami mencadangkan penjana grafik yang didorong oleh utiliti pertama untuk rangkaian masa yang berwajaran, yang sesuai dengan corak yang diperhatikan. Kajian kami memfokuskan pada tiga kumpulan data besar, masing-masing adalah jenis perkhidmatan komunikasi yang berbeza, dengan lebih dari satu juta rekod, dan merangkumi beberapa bulan aktiviti. [[EENNDD]] rangkaian sosial; penjana grafik; klik"], [{"string": "Turning CARTwheels : an alternating algorithm for mining redescriptions We present an unusual algorithm involving classification trees -- CARTwheels -- where two trees are grown in opposite directions so that they are joined at their leaves . This approach finds application in a new data mining task we formulate , called redescription mining . A redescription is a shift-of-vocabulary , or a different way of communicating information about a given subset of data ; the goal of redescription mining is to find subsets of data that afford multiple descriptions . We highlight the importance of this problem in domains such as bioinformatics , which exhibit an underlying richness and diversity of data descriptors e.g. , genes can be studied in a variety of ways . CARTwheels exploits the duality between class partitions and path partitions in an induced classification tree to model and mine redescriptions . It helps integrate multiple forms of characterizing datasets , situates the knowledge gained from one dataset in the context of others , and harnesses high-level abstractions for uncovering cryptic and subtle features of data . Algorithm design decisions , implementation details , and experimental results are presented .", "keywords": ["data mining in biological domains", "classification trees", "learning", "redescriptions"], "combined": "Turning CARTwheels : an alternating algorithm for mining redescriptions We present an unusual algorithm involving classification trees -- CARTwheels -- where two trees are grown in opposite directions so that they are joined at their leaves . This approach finds application in a new data mining task we formulate , called redescription mining . A redescription is a shift-of-vocabulary , or a different way of communicating information about a given subset of data ; the goal of redescription mining is to find subsets of data that afford multiple descriptions . We highlight the importance of this problem in domains such as bioinformatics , which exhibit an underlying richness and diversity of data descriptors e.g. , genes can be studied in a variety of ways . CARTwheels exploits the duality between class partitions and path partitions in an induced classification tree to model and mine redescriptions . It helps integrate multiple forms of characterizing datasets , situates the knowledge gained from one dataset in the context of others , and harnesses high-level abstractions for uncovering cryptic and subtle features of data . Algorithm design decisions , implementation details , and experimental results are presented . [[EENNDD]] data mining in biological domains; classification trees; learning; redescriptions"}, "Memusing CARTwheels: algoritma gantian untuk penafsiran semula perlombongan Kami menyajikan algoritma yang tidak biasa yang melibatkan pokok klasifikasi - CARTwheels - di mana dua pokok ditanam dalam arah yang bertentangan sehingga mereka bergabung di daunnya. Pendekatan ini menemukan aplikasi dalam tugas perlombongan data baru yang kami rumuskan, yang disebut penambangan semula. Penyusunan semula adalah pergeseran perbendaharaan kata, atau cara yang berbeza untuk menyampaikan maklumat mengenai subset data yang diberikan; tujuan perlombongan reka bentuk semula adalah untuk mencari subkumpulan data yang memberikan banyak penerangan. Kami mengetengahkan kepentingan masalah ini dalam domain seperti bioinformatik, yang menunjukkan kekayaan dan kepelbagaian penerangan data yang mendasari, mis. , gen dapat dikaji dengan pelbagai cara. CARTwheels mengeksploitasi dualitas antara partisi kelas dan partisi jalan dalam pokok klasifikasi yang diinduksi untuk membuat model dan meniru reka bentuk semula. Ia membantu mengintegrasikan pelbagai bentuk set data, meletakkan pengetahuan yang diperoleh dari satu set data dalam konteks yang lain, dan memanfaatkan abstraksi tahap tinggi untuk mengungkap ciri-ciri data samar dan halus. Keputusan reka bentuk algoritma, perincian pelaksanaan, dan hasil eksperimen disajikan. [[EENNDD]] perlombongan data dalam domain biologi; pokok pengelasan; belajar; penerangan semula"], [{"string": "Mining rank-correlated sets of numerical attributes We study the mining of interesting patterns in the presence of numerical attributes . Instead of the usual discretization methods , we propose the use of rank based measures to score the similarity of sets of numerical attributes . New support measures for numerical data are introduced , based on extensions of Kendall 's tau , and Spearman 's Footrule and rho . We show how these support measures are related . Furthermore , we introduce a novel type of pattern combining numerical and categorical attributes . We give efficient algorithms to find all frequent patterns for the proposed support measures , and evaluate their performance on real-life datasets .", "keywords": ["numerical", "data mining", "rank correlation", "systems"], "combined": "Mining rank-correlated sets of numerical attributes We study the mining of interesting patterns in the presence of numerical attributes . Instead of the usual discretization methods , we propose the use of rank based measures to score the similarity of sets of numerical attributes . New support measures for numerical data are introduced , based on extensions of Kendall 's tau , and Spearman 's Footrule and rho . We show how these support measures are related . Furthermore , we introduce a novel type of pattern combining numerical and categorical attributes . We give efficient algorithms to find all frequent patterns for the proposed support measures , and evaluate their performance on real-life datasets . [[EENNDD]] numerical; data mining; rank correlation; systems"}, "Kumpulan atribut penomboran berkorelasi peringkat penambangan Kami mengkaji perlombongan corak menarik dengan adanya atribut berangka. Daripada kaedah diskretisasi yang biasa, kami mencadangkan penggunaan ukuran berdasarkan peringkat untuk menjaring kesamaan set atribut berangka. Langkah-langkah sokongan baru untuk data berangka diperkenalkan, berdasarkan ekstensi tau Kendall, dan Spearman's Footrule dan rho. Kami menunjukkan bagaimana langkah sokongan ini berkaitan. Selanjutnya, kami memperkenalkan jenis corak novel yang menggabungkan atribut numerik dan kategori. Kami memberikan algoritma yang cekap untuk mencari semua corak yang kerap untuk langkah sokongan yang dicadangkan, dan menilai prestasinya pada set data kehidupan sebenar. [[EENNDD]] berangka; perlombongan data; korelasi peringkat; sistem"], [{"string": "Application of kernels to link analysis The application of kernel methods to link analysis is explored . In particular , Kandola et al. 's Neumann kernels are shown to subsume not only the co-citation and bibliographic coupling relatedness but also Kleinberg 's HITS importance . These popular measures of relatedness and importance correspond to the Neumann kernels at the extremes of their parameter range , and hence these kernels can be interpreted as defining a spectrum of link analysis measures intermediate between co-citation\\/bibliographic coupling and HITS . We also show that the kernels based on the graph Laplacian , including the regularized Laplacian and diffusion kernels , provide relatedness measures that overcome some limitations of co-citation relatedness . The property of these kernel-based link analysis measures is examined with a network of bibliographic citations . Practical issues in applying these methods to real data are discussed , and possible solutions are proposed .", "keywords": ["co-citation coupling", "graph kernel", "information search and retrieval", "link analysis", "hits"], "combined": "Application of kernels to link analysis The application of kernel methods to link analysis is explored . In particular , Kandola et al. 's Neumann kernels are shown to subsume not only the co-citation and bibliographic coupling relatedness but also Kleinberg 's HITS importance . These popular measures of relatedness and importance correspond to the Neumann kernels at the extremes of their parameter range , and hence these kernels can be interpreted as defining a spectrum of link analysis measures intermediate between co-citation\\/bibliographic coupling and HITS . We also show that the kernels based on the graph Laplacian , including the regularized Laplacian and diffusion kernels , provide relatedness measures that overcome some limitations of co-citation relatedness . The property of these kernel-based link analysis measures is examined with a network of bibliographic citations . Practical issues in applying these methods to real data are discussed , and possible solutions are proposed . [[EENNDD]] co-citation coupling; graph kernel; information search and retrieval; link analysis; hits"}, "Aplikasi kernel untuk analisis pautan Penerapan kaedah kernel untuk menghubungkan analisis diterokai. Khususnya, Kandola et al. Kernel Neumann diperlihatkan untuk memanfaatkan bukan hanya hubungan penggabungan dan penggabungan bibliografi tetapi juga kepentingan HITS Kleinberg. Ukuran hubungan dan kepentingan yang popular ini sesuai dengan kernel Neumann di hujung julat parameter mereka, dan oleh itu kernel ini dapat ditafsirkan sebagai menentukan spektrum ukuran analisis pautan antara antara penggabungan \\ / bibliografi gabungan dan HITS. Kami juga menunjukkan bahawa kernel berdasarkan grafik Laplacian, termasuk kernel Laplacian dan difusi yang teratur, memberikan langkah-langkah keterkaitan yang mengatasi beberapa batasan keterkaitan kutipan. Sifat ukuran analisis pautan berasaskan kernel ini diperiksa dengan rangkaian kutipan bibliografi. Masalah praktikal dalam menerapkan kaedah ini ke data sebenar dibincangkan, dan kemungkinan penyelesaian dicadangkan. [[EENNDD]] gandingan petikan; kernel grafik; carian dan pengambilan maklumat; analisis pautan; hits"], [{"string": "Next frontier This talk is about the next frontier in knowledge discovery and data mining .", "keywords": ["knowledge representation formalisms and methods", "information search and retrieval"], "combined": "Next frontier This talk is about the next frontier in knowledge discovery and data mining . [[EENNDD]] knowledge representation formalisms and methods; information search and retrieval"}, "Seterusnya perbincangan ini adalah mengenai perbatasan seterusnya dalam penemuan pengetahuan dan perlombongan data. [[EENNDD]] formalisme dan kaedah perwakilan pengetahuan; pencarian dan pencarian maklumat"], [{"string": "Finding tribes : identifying close-knit individuals from employment patterns We present a family of algorithms to uncover tribes-groups of individuals who share unusual sequences of affiliations . While much work inferring community structure describes large-scale trends , we instead search for small groups of tightly linked individuals who behave anomalously with respect to those trends . We apply the algorithms to a large temporal and relational data set consisting of millions of employment records from the National Association of Securities Dealers . The resulting tribes contain individuals at higher risk for fraud , are homogenous with respect to risk scores , and are geographically mobile , all at significant levels compared to random or to other sets of individuals who share affiliations .", "keywords": ["anomaly detection", "dynamic networks", "social networks"], "combined": "Finding tribes : identifying close-knit individuals from employment patterns We present a family of algorithms to uncover tribes-groups of individuals who share unusual sequences of affiliations . While much work inferring community structure describes large-scale trends , we instead search for small groups of tightly linked individuals who behave anomalously with respect to those trends . We apply the algorithms to a large temporal and relational data set consisting of millions of employment records from the National Association of Securities Dealers . The resulting tribes contain individuals at higher risk for fraud , are homogenous with respect to risk scores , and are geographically mobile , all at significant levels compared to random or to other sets of individuals who share affiliations . [[EENNDD]] anomaly detection; dynamic networks; social networks"}, "Mencari suku: mengenal pasti individu yang rapat dari corak pekerjaan Kami menyajikan sekumpulan algoritma untuk membongkar suku-kumpulan individu yang mempunyai urutan gabungan yang tidak biasa. Walaupun banyak kerja yang menyimpulkan struktur masyarakat menggambarkan tren berskala besar, kita sebaliknya mencari kumpulan kecil individu yang berkait rapat yang bersikap tidak normal terhadap tren tersebut. Kami menerapkan algoritma pada set data temporal dan relasional yang besar yang terdiri daripada berjuta-juta rekod pekerjaan dari Persatuan Nasional Penjual Sekuriti. Suku yang dihasilkan mengandungi individu yang berisiko lebih tinggi untuk penipuan, homogen sehubungan dengan skor risiko, dan secara geografi mudah alih, semuanya pada tahap yang signifikan berbanding secara rawak atau kumpulan individu lain yang mempunyai gabungan. [[EENNDD]] pengesanan anomali; rangkaian dinamik; rangkaian sosial"], [{"string": "Algorithms for discovering bucket orders from data Ordering and ranking items of different types are important tasks in various applications , such as query processing and scientific data mining . A total order for the items can be misleading , since there are groups of items that have practically equal ranks . We consider bucket orders , i.e. , total orders with ties . They can be used to capture the essential order information without overfitting the data : they form a useful concept class between total orders and arbitrary partial orders . We address the question of finding a bucket order for a set of items , given pairwise precedence information between the items . We also discuss methods for computing the pairwise precedence data . We describe simple and efficient algorithms for finding good bucket orders . Several of the algorithms have a provable approximation guarantee , and they scale well to large datasets . We provide experimental results on artificial and a real data that show the usefulness of bucket orders and demonstrate the accuracy and efficiency of the algorithms .", "keywords": ["partial order", "ranking", "ordering", "bucket order"], "combined": "Algorithms for discovering bucket orders from data Ordering and ranking items of different types are important tasks in various applications , such as query processing and scientific data mining . A total order for the items can be misleading , since there are groups of items that have practically equal ranks . We consider bucket orders , i.e. , total orders with ties . They can be used to capture the essential order information without overfitting the data : they form a useful concept class between total orders and arbitrary partial orders . We address the question of finding a bucket order for a set of items , given pairwise precedence information between the items . We also discuss methods for computing the pairwise precedence data . We describe simple and efficient algorithms for finding good bucket orders . Several of the algorithms have a provable approximation guarantee , and they scale well to large datasets . We provide experimental results on artificial and a real data that show the usefulness of bucket orders and demonstrate the accuracy and efficiency of the algorithms . [[EENNDD]] partial order; ranking; ordering; bucket order"}, "Algoritma untuk mencari pesanan baldi dari data Menyusun dan menentukan item dari pelbagai jenis adalah tugas penting dalam pelbagai aplikasi, seperti pemprosesan pertanyaan dan perlombongan data saintifik. Jumlah pesanan untuk item boleh mengelirukan, kerana terdapat sekumpulan item yang mempunyai kedudukan yang hampir sama. Kami mempertimbangkan pesanan baldi, iaitu jumlah pesanan dengan ikatan. Mereka boleh digunakan untuk menangkap maklumat pesanan penting tanpa terlalu banyak data: mereka membentuk kelas konsep yang berguna antara jumlah pesanan dan pesanan separa sewenang-wenangnya. Kami menangani persoalan mencari pesanan baldi untuk satu set item, diberikan maklumat keutamaan berpasangan antara item tersebut. Kami juga membincangkan kaedah untuk mengira data keutamaan berpasangan. Kami menerangkan algoritma mudah dan cekap untuk mencari pesanan baldi yang baik. Beberapa algoritma mempunyai jaminan perkiraan yang dapat dibuktikan, dan skala mereka baik ke set data yang besar. Kami memberikan hasil eksperimen pada data buatan dan nyata yang menunjukkan kegunaan pesanan baldi dan menunjukkan ketepatan dan kecekapan algoritma. [[EENNDD]] pesanan separa; peringkat; membuat pesanan; pesanan baldi"], [{"string": "Large-scale matrix factorization with distributed stochastic gradient descent We provide a novel algorithm to approximately factor large matrices with millions of rows , millions of columns , and billions of nonzero elements . Our approach rests on stochastic gradient descent SGD , an iterative stochastic optimization algorithm . We first develop a novel `` stratified '' SGD variant SSGD that applies to general loss-minimization problems in which the loss function can be expressed as a weighted sum of `` stratum losses . '' We establish sufficient conditions for convergence of SSGD using results from stochastic approximation theory and regenerative process theory . We then specialize SSGD to obtain a new matrix-factorization algorithm , called DSGD , that can be fully distributed and run on web-scale datasets using , e.g. , MapReduce . DSGD can handle a wide variety of matrix factorizations . We describe the practical techniques used to optimize performance in our DSGD implementation . Experiments suggest that DSGD converges significantly faster and has better scalability properties than alternative algorithms .", "keywords": ["stochastic gradient descent", "mapreduce", "recommendation system", "distributed matrix factorization"], "combined": "Large-scale matrix factorization with distributed stochastic gradient descent We provide a novel algorithm to approximately factor large matrices with millions of rows , millions of columns , and billions of nonzero elements . Our approach rests on stochastic gradient descent SGD , an iterative stochastic optimization algorithm . We first develop a novel `` stratified '' SGD variant SSGD that applies to general loss-minimization problems in which the loss function can be expressed as a weighted sum of `` stratum losses . '' We establish sufficient conditions for convergence of SSGD using results from stochastic approximation theory and regenerative process theory . We then specialize SSGD to obtain a new matrix-factorization algorithm , called DSGD , that can be fully distributed and run on web-scale datasets using , e.g. , MapReduce . DSGD can handle a wide variety of matrix factorizations . We describe the practical techniques used to optimize performance in our DSGD implementation . Experiments suggest that DSGD converges significantly faster and has better scalability properties than alternative algorithms . [[EENNDD]] stochastic gradient descent; mapreduce; recommendation system; distributed matrix factorization"}, "Faktorisasi matriks berskala besar dengan keturunan kecerunan stokastik terdistribusi Kami menyediakan algoritma baru untuk kira-kira faktor matriks besar dengan berjuta-juta baris, berjuta lajur, dan berbilion elemen bukan sifar. Pendekatan kami bergantung pada kecenderungan stokastik SGD, algoritma pengoptimuman stokastik berulang. Kami mula-mula mengembangkan novel SSGD varian SGD \"stratified\" yang berlaku untuk masalah pengurangan kerugian umum di mana fungsi kerugian dapat dinyatakan sebagai jumlah tertimbang \"kerugian stratum. '' Kami menetapkan syarat yang mencukupi untuk penumpuan SSGD menggunakan hasil dari teori pendekatan stokastik dan teori proses regeneratif. Kami kemudian mengkhususkan SSGD untuk mendapatkan algoritma pemfaktoran matriks baru, yang disebut DSGD, yang dapat diedarkan sepenuhnya dan dijalankan pada set data skala web menggunakan, mis. , MapReduce. DSGD dapat menangani pelbagai faktor faktorisasi matriks. Kami menerangkan teknik praktikal yang digunakan untuk mengoptimumkan prestasi dalam pelaksanaan DSGD kami. Eksperimen menunjukkan bahawa DSGD berkumpul jauh lebih cepat dan mempunyai sifat skalabiliti yang lebih baik daripada algoritma alternatif. [[EENNDD]] keturunan kecerunan stokastik; pengurangan peta; sistem cadangan; pemfaktoran matriks teragih"], [{"string": "Constrained optimization for validation-guided conditional random field learning Conditional random fields CRFs are a class of undirected graphical models which have been widely used for classifying and labeling sequence data . The training of CRFs is typically formulated as an unconstrained optimization problem that maximizes the conditional likelihood . However , maximum likelihood training is prone to overfitting . To address this issue , we propose a novel constrained nonlinear optimization formulation in which the prediction accuracy of cross-validation sets are included as constraints . Instead of requiring multiple passes of training , the constrained formulation allows the cross-validation be handled in one pass of constrained optimization . The new formulation is discontinuous , and classical Lagrangian based constraint handling methods are not applicable . A new constrained optimization algorithm based on the recently proposed extended saddle point theory is developed to learn the constrained CRF model . Experimental results on gene and stock-price prediction tasks show that the constrained formulation is able to significantly improve the generalization ability of CRF training .", "keywords": ["constrained optimization", "extended saddle points", "conditional random fields", "financial", "cross validation"], "combined": "Constrained optimization for validation-guided conditional random field learning Conditional random fields CRFs are a class of undirected graphical models which have been widely used for classifying and labeling sequence data . The training of CRFs is typically formulated as an unconstrained optimization problem that maximizes the conditional likelihood . However , maximum likelihood training is prone to overfitting . To address this issue , we propose a novel constrained nonlinear optimization formulation in which the prediction accuracy of cross-validation sets are included as constraints . Instead of requiring multiple passes of training , the constrained formulation allows the cross-validation be handled in one pass of constrained optimization . The new formulation is discontinuous , and classical Lagrangian based constraint handling methods are not applicable . A new constrained optimization algorithm based on the recently proposed extended saddle point theory is developed to learn the constrained CRF model . Experimental results on gene and stock-price prediction tasks show that the constrained formulation is able to significantly improve the generalization ability of CRF training . [[EENNDD]] constrained optimization; extended saddle points; conditional random fields; financial; cross validation"}, "Pengoptimuman terkawal untuk pembelajaran bidang rawak bersyarat bersyarat yang dipandu pengesahan Medan rawak bersyarat CRF adalah kelas model grafik yang tidak diarahkan yang telah banyak digunakan untuk mengklasifikasikan dan melabel data urutan. Latihan CRF biasanya dirumuskan sebagai masalah pengoptimuman yang tidak dapat disekat yang memaksimumkan kemungkinan bersyarat. Walau bagaimanapun, latihan kemungkinan maksimum cenderung berlebihan. Untuk mengatasi masalah ini, kami mencadangkan rumusan pengoptimuman nonlinier terkendali novel di mana ketepatan ramalan set pengesahan silang dimasukkan sebagai batasan. Daripada memerlukan banyak latihan, formulasi yang dibatasi memungkinkan pengesahan silang ditangani dalam satu lulus pengoptimuman yang dibatasi. Rumusan baru tidak berterusan, dan kaedah pengendalian kekangan berdasarkan Lagrangian klasik tidak berlaku. Algoritma pengoptimuman kendala baru berdasarkan teori titik pelana diperpanjang yang baru-baru ini dikembangkan untuk mempelajari model CRF yang dibatasi. Hasil eksperimen pada tugas ramalan gen dan harga saham menunjukkan bahawa rumusan yang dibatasi dapat meningkatkan kemampuan generalisasi latihan CRF secara signifikan. [[EENNDD]] pengoptimuman terhad; mata pelana yang dilanjutkan; medan rawak bersyarat; kewangan; pengesahan bersilang"], [{"string": "Spectral domain-transfer learning Traditional spectral classification has been proved to be effective in dealing with both labeled and unlabeled data when these data are from the same domain . In many real world applications , however , we wish to make use of the labeled data from one domain called in-domain to classify the unlabeled data in a different domain out-of-domain . This problem often happens when obtaining labeled data in one domain is difficult while there are plenty of labeled data from a related but different domain . In general , this is a transfer learning problem where we wish to classify the unlabeled data through the labeled data even though these data are not from the same domain . In this paper , we formulate this domain-transfer learning problem under a novel spectral classification framework , where the objective function is introduced to seek consistency between the in-domain supervision and the out-of-domain intrinsic structure . Through optimization of the cost function , the label information from the in-domain data is effectively transferred to help classify the unlabeled data from the out-of-domain . We conduct extensive experiments to evaluate our method and show that our algorithm achieves significant improvements on classification performance over many state-of-the-art algorithms .", "keywords": ["transfer learning", "spectral learning", "learning"], "combined": "Spectral domain-transfer learning Traditional spectral classification has been proved to be effective in dealing with both labeled and unlabeled data when these data are from the same domain . In many real world applications , however , we wish to make use of the labeled data from one domain called in-domain to classify the unlabeled data in a different domain out-of-domain . This problem often happens when obtaining labeled data in one domain is difficult while there are plenty of labeled data from a related but different domain . In general , this is a transfer learning problem where we wish to classify the unlabeled data through the labeled data even though these data are not from the same domain . In this paper , we formulate this domain-transfer learning problem under a novel spectral classification framework , where the objective function is introduced to seek consistency between the in-domain supervision and the out-of-domain intrinsic structure . Through optimization of the cost function , the label information from the in-domain data is effectively transferred to help classify the unlabeled data from the out-of-domain . We conduct extensive experiments to evaluate our method and show that our algorithm achieves significant improvements on classification performance over many state-of-the-art algorithms . [[EENNDD]] transfer learning; spectral learning; learning"}, "Pembelajaran pemindahan domain spektral Klasifikasi spektrum tradisional telah terbukti berkesan dalam menangani data berlabel dan tidak berlabel ketika data ini berasal dari domain yang sama. Namun, di banyak aplikasi dunia nyata, kami ingin memanfaatkan data berlabel dari satu domain yang disebut dalam domain untuk mengklasifikasikan data yang tidak berlabel dalam domain yang berbeda di luar domain. Masalah ini sering berlaku apabila memperoleh data berlabel dalam satu domain adalah sukar sedangkan terdapat banyak data berlabel dari domain yang berkaitan tetapi berbeza. Secara umum, ini adalah masalah pembelajaran pemindahan di mana kami ingin mengklasifikasikan data yang tidak berlabel melalui data berlabel walaupun data ini bukan dari domain yang sama. Dalam makalah ini, kami merumuskan masalah pembelajaran pemindahan domain ini di bawah kerangka klasifikasi spektrum baru, di mana fungsi objektif diperkenalkan untuk mencari konsistensi antara pengawasan dalam domain dan struktur intrinsik di luar domain. Melalui pengoptimuman fungsi kos, maklumat label dari data dalam domain dipindahkan secara efektif untuk membantu mengklasifikasikan data yang tidak berlabel dari luar domain. Kami melakukan eksperimen yang meluas untuk menilai kaedah kami dan menunjukkan bahawa algoritma kami mencapai peningkatan yang signifikan terhadap prestasi klasifikasi berbanding banyak algoritma canggih. [[EENNDD]] memindahkan pembelajaran; pembelajaran spektrum; belajar"], [{"string": "Mining complex models from arbitrarily large databases in constant time In this paper we propose a scaling-up method that is applicable to essentially any induction algorithm based on discrete search . The result of applying the method to an algorithm is that its running time becomes independent of the size of the database , while the decisions made are essentially identical to those that would be made given infinite data . The method works within pre-specified memory limits and , as long as the data is iid , only requires accessing it sequentially . It gives anytime results , and can be used to produce batch , stream , time-changing and active-learning versions of an algorithm . We apply the method to learning Bayesian networks , developing an algorithm that is faster than previous ones by orders of magnitude , while achieving essentially the same predictive performance . We observe these gains on a series of large databases `` generated from benchmark networks , on the KDD Cup 2000 e-commerce data , and on a Web log containing 100 million requests .", "keywords": ["scalable learning algorithms", "subsampling", "bayesian networks", "discrete search", "hoeffding bounds"], "combined": "Mining complex models from arbitrarily large databases in constant time In this paper we propose a scaling-up method that is applicable to essentially any induction algorithm based on discrete search . The result of applying the method to an algorithm is that its running time becomes independent of the size of the database , while the decisions made are essentially identical to those that would be made given infinite data . The method works within pre-specified memory limits and , as long as the data is iid , only requires accessing it sequentially . It gives anytime results , and can be used to produce batch , stream , time-changing and active-learning versions of an algorithm . We apply the method to learning Bayesian networks , developing an algorithm that is faster than previous ones by orders of magnitude , while achieving essentially the same predictive performance . We observe these gains on a series of large databases `` generated from benchmark networks , on the KDD Cup 2000 e-commerce data , and on a Web log containing 100 million requests . [[EENNDD]] scalable learning algorithms; subsampling; bayesian networks; discrete search; hoeffding bounds"}, "Perlombongan model kompleks dari pangkalan data besar sewenang-wenangnya dalam masa yang tetap. Dalam makalah ini, kami mencadangkan kaedah penambahbaikan yang dapat diterapkan pada dasarnya algoritma induksi berdasarkan carian diskrit. Hasil penerapan metode untuk algoritma adalah waktu berjalannya tidak bergantung pada ukuran pangkalan data, sementara keputusan yang dibuat pada dasarnya sama dengan yang akan dibuat dengan data yang tidak terbatas. Kaedah ini berfungsi dalam had memori yang telah ditentukan dan, selagi data itu iid, hanya memerlukan mengaksesnya secara berurutan. Ia memberikan hasil bila-bila masa, dan dapat digunakan untuk menghasilkan versi algoritma kumpulan, aliran, perubahan waktu dan pembelajaran aktif. Kami menerapkan kaedah untuk mempelajari rangkaian Bayesian, mengembangkan algoritma yang lebih cepat daripada yang sebelumnya berdasarkan pesanan besarnya, dan pada dasarnya mencapai prestasi ramalan yang sama. Kami melihat keuntungan ini pada rangkaian pangkalan data besar yang dihasilkan dari rangkaian penanda aras, data e-dagang KDD Cup 2000, dan pada log Web yang mengandungi 100 juta permintaan. [[EENNDD]] algoritma pembelajaran berskala; pengambilan sampel; rangkaian bayesian; carian diskret; had cangkul"], [{"string": "Fast best-effort pattern matching in large attributed graphs We focus on large graphs where nodes have attributes , such as a social network where the nodes are labeled with each person 's job title . In such a setting , we want to find subgraphs that match a user query pattern . For example , a `` star '' query would be , `` find a CEO who has strong interactions with a Manager , a Lawyer , and an Accountant , or another structure as close to that as possible '' . Similarly , a `` loop '' query could help spot a money laundering ring . Traditional SQL-based methods , as well as more recent graph indexing methods , will return no answer when an exact match does not exist . This is the first main feature of our method . It can find exact - , as well as near-matches , and it will present them to the user in our proposed `` goodness '' order . For example , our method tolerates indirect paths between , say , the `` CEO '' and the `` Accountant '' of the above sample query , when direct paths do n't exist . Its second feature is scalability . In general , if the query has nq nodes and the data graph has n nodes , the problem needs polynomial time complexity O n n q , which is prohibitive . Our G-Ray `` Graph X-Ray '' method finds high-quality subgraphs in time linear on the size of the data graph . Experimental results on the DLBP author-publication graph with 356K nodes and 1.9 M edges illustrate both the effectiveness and scalability of our approach . The results agree with our intuition , and the speed is excellent . It takes 4 seconds on average fora 4-node query on the DBLP graph .", "keywords": ["attributed graph", "pattern match", "miscellaneous", "random walk"], "combined": "Fast best-effort pattern matching in large attributed graphs We focus on large graphs where nodes have attributes , such as a social network where the nodes are labeled with each person 's job title . In such a setting , we want to find subgraphs that match a user query pattern . For example , a `` star '' query would be , `` find a CEO who has strong interactions with a Manager , a Lawyer , and an Accountant , or another structure as close to that as possible '' . Similarly , a `` loop '' query could help spot a money laundering ring . Traditional SQL-based methods , as well as more recent graph indexing methods , will return no answer when an exact match does not exist . This is the first main feature of our method . It can find exact - , as well as near-matches , and it will present them to the user in our proposed `` goodness '' order . For example , our method tolerates indirect paths between , say , the `` CEO '' and the `` Accountant '' of the above sample query , when direct paths do n't exist . Its second feature is scalability . In general , if the query has nq nodes and the data graph has n nodes , the problem needs polynomial time complexity O n n q , which is prohibitive . Our G-Ray `` Graph X-Ray '' method finds high-quality subgraphs in time linear on the size of the data graph . Experimental results on the DLBP author-publication graph with 356K nodes and 1.9 M edges illustrate both the effectiveness and scalability of our approach . The results agree with our intuition , and the speed is excellent . It takes 4 seconds on average fora 4-node query on the DBLP graph . [[EENNDD]] attributed graph; pattern match; miscellaneous; random walk"}, "Pencocokan corak usaha terbaik yang pantas dalam grafik yang dikaitkan besar Kami memfokuskan pada grafik besar di mana nod mempunyai atribut, seperti rangkaian sosial di mana node dilabel dengan tajuk pekerjaan setiap orang. Dalam suasana seperti itu, kami ingin mencari subgraf yang sesuai dengan corak pertanyaan pengguna. Sebagai contoh, pertanyaan \"bintang\" adalah, \"cari CEO yang mempunyai interaksi yang kuat dengan Pengurus, Peguam, dan Akauntan, atau struktur lain yang sedekat mungkin\". Begitu juga, pertanyaan \"gelung\" dapat membantu mencari cincin pencucian wang. Kaedah berasaskan SQL tradisional, serta kaedah pengindeksan grafik yang lebih baru, tidak akan memberikan jawapan apabila padanan tepat tidak ada. Ini adalah ciri utama pertama kaedah kami. Ia dapat menemukan tepat -, serta hampir-hampir, dan ia akan memberikannya kepada pengguna dalam urutan \"kebaikan\" yang kami cadangkan. Sebagai contoh, kaedah kami bertolak ansur dengan jalan tidak langsung antara, katakanlah, \"CEO\" dan \"Akauntan\" pertanyaan sampel di atas, apabila jalan langsung tidak ada. Ciri kedua adalah skalabiliti. Secara amnya, jika pertanyaan mempunyai nq node dan grafik data mempunyai n node, masalahnya memerlukan kerumitan masa polinomial O n n q, yang sangat melarang. Kaedah \"Grafik X-Ray\" G-Ray kami menemui subgraf berkualiti tinggi dalam jangka masa mengikut ukuran grafik data. Hasil eksperimen pada grafik penerbitan pengarang DLBP dengan nod 356K dan tepi 1.9 M menggambarkan kedua-dua keberkesanan dan skalabiliti pendekatan kami. Hasilnya sesuai dengan intuisi kami, dan kelajuannya sangat baik. Rata-rata memerlukan 4 saat untuk 4-node pertanyaan pada grafik DBLP. [[EENNDD]] graf yang dikaitkan; padanan corak; pelbagai; jalan rawak"], [{"string": "A new multi-view regression approach with an application to customer wallet estimation Motivated by the problem of customer wallet estimation , we propose a new setting for multi-view regression , where we learn a completely unobserved target in our case , customer wallet by modeling it as a `` central link '' in a directed graphical model , connecting multiple sets of observed variables . The resulting conditional independence allows us to reduce the maximum discriminative likelihood estimation problem to a convex optimization problem for exponential linear models . We show that under certain modeling assumptions , in particular , when there exist two conditionally independent views and the noise is Gaussian , this problem can be reduced to a single least squares regression . Thus , for this specific , but widely applicable setting , the `` unsupervised '' multi-view problem can be solved via a simple supervised learning approach . This reduction also allows us to test the statistical independence assumptions underlying the graphical model and perform variable selection . We demonstrate the effectiveness of our approach on our motivating problem of customer wallet estimation and on simulation data .", "keywords": ["regression", "multi-view learning", "learning", "bayesian networks"], "combined": "A new multi-view regression approach with an application to customer wallet estimation Motivated by the problem of customer wallet estimation , we propose a new setting for multi-view regression , where we learn a completely unobserved target in our case , customer wallet by modeling it as a `` central link '' in a directed graphical model , connecting multiple sets of observed variables . The resulting conditional independence allows us to reduce the maximum discriminative likelihood estimation problem to a convex optimization problem for exponential linear models . We show that under certain modeling assumptions , in particular , when there exist two conditionally independent views and the noise is Gaussian , this problem can be reduced to a single least squares regression . Thus , for this specific , but widely applicable setting , the `` unsupervised '' multi-view problem can be solved via a simple supervised learning approach . This reduction also allows us to test the statistical independence assumptions underlying the graphical model and perform variable selection . We demonstrate the effectiveness of our approach on our motivating problem of customer wallet estimation and on simulation data . [[EENNDD]] regression; multi-view learning; learning; bayesian networks"}, "Pendekatan regresi multi-pandangan baru dengan aplikasi untuk anggaran dompet pelanggan Dimotivasi oleh masalah anggaran dompet pelanggan, kami mencadangkan pengaturan baru untuk regresi multi-pandangan, di mana kami mempelajari sasaran yang sama sekali tidak diperhatikan dalam kes kami, dompet pelanggan dengan memodelkannya sebagai \"pautan pusat\" dalam model grafik yang diarahkan, menghubungkan beberapa set pemboleh ubah yang diperhatikan. Kebebasan bersyarat yang dihasilkan membolehkan kita mengurangkan masalah anggaran kemungkinan diskriminasi maksimum kepada masalah pengoptimuman cembung untuk model linear eksponensial. Kami menunjukkan bahawa di bawah andaian pemodelan tertentu, khususnya, apabila ada dua pandangan bersyarat yang bebas dan kebisingan adalah Gaussian, masalah ini dapat dikurangkan menjadi regresi kuadrat tunggal. Oleh itu, untuk pengaturan khusus ini, tetapi banyak berlaku, masalah multi-pandangan \"tanpa pengawasan\" dapat diselesaikan melalui pendekatan pembelajaran yang diawasi sederhana. Pengurangan ini juga membolehkan kita menguji andaian kebebasan statistik yang mendasari model grafik dan melakukan pemilihan berubah-ubah. Kami menunjukkan keberkesanan pendekatan kami terhadap masalah motivasi dugaan pelanggan kami dan data simulasi. [[EENNDD]] regresi; pembelajaran pelbagai pandangan; belajar; rangkaian bayesian"], [{"string": "Topical query decomposition We introduce the problem of query decomposition , where we are given a query and a document retrieval system , and we want to produce a small set of queries whose union of resulting documents corresponds approximately to that of the original query . Ideally , these queries should represent coherent , conceptually well-separated topics . We provide an abstract formulation of the query decomposition problem , and we tackle it from two different perspectives . We first show how the problem can be instantiated as a specific variant of a set cover problem , for which we provide an efficient greedy algorithm . Next , we show how the same problem can be seen as a constrained clustering problem , with a very particular kind of constraint , i.e. , clustering with predefined clusters . We develop a two-phase algorithm based on hierarchical agglomerative clustering followed by dynamic programming . Our experiments , conducted on a set of actual queries in a Web scale search engine , confirm the effectiveness of the proposed solutions .", "keywords": ["set cover", "query recommendation", "communications applications", "clustering"], "combined": "Topical query decomposition We introduce the problem of query decomposition , where we are given a query and a document retrieval system , and we want to produce a small set of queries whose union of resulting documents corresponds approximately to that of the original query . Ideally , these queries should represent coherent , conceptually well-separated topics . We provide an abstract formulation of the query decomposition problem , and we tackle it from two different perspectives . We first show how the problem can be instantiated as a specific variant of a set cover problem , for which we provide an efficient greedy algorithm . Next , we show how the same problem can be seen as a constrained clustering problem , with a very particular kind of constraint , i.e. , clustering with predefined clusters . We develop a two-phase algorithm based on hierarchical agglomerative clustering followed by dynamic programming . Our experiments , conducted on a set of actual queries in a Web scale search engine , confirm the effectiveness of the proposed solutions . [[EENNDD]] set cover; query recommendation; communications applications; clustering"}, "Penguraian pertanyaan topikal Kami memperkenalkan masalah penguraian pertanyaan, di mana kami diberi pertanyaan dan sistem pengambilan dokumen, dan kami ingin menghasilkan sekumpulan kecil pertanyaan yang penyatuan dokumen yang dihasilkan hampir sama dengan pertanyaan asal. Sebaik-baiknya, pertanyaan ini harus mewakili topik yang dipisahkan secara konseptual dan konseptual. Kami memberikan rumusan abstrak mengenai masalah penguraian pertanyaan, dan kami mengatasinya dari dua perspektif yang berbeza. Mula-mula kami menunjukkan bagaimana masalah dapat ditunjukkan sebagai varian tertentu dari masalah penutup set, yang mana kami menyediakan algoritma tamak yang cekap. Seterusnya, kami menunjukkan bagaimana masalah yang sama dapat dilihat sebagai masalah pengelompokan yang dibatasi, dengan jenis kekangan yang sangat khusus, iaitu pengelompokan dengan kelompok yang telah ditentukan. Kami mengembangkan algoritma dua fasa berdasarkan pengelompokan aglomeratif hierarki diikuti dengan pengaturcaraan dinamik. Eksperimen kami, yang dilakukan pada satu set pertanyaan sebenar dalam mesin carian skala Web, mengesahkan keberkesanan penyelesaian yang dicadangkan. [[EENNDD]] set penutup; cadangan pertanyaan; aplikasi komunikasi; pengelompokan"], [{"string": "Algorithms for estimating relative importance in networks Large and complex graphs representing relationships among sets of entities are an increasingly common focus of interest in data analysis -- examples include social networks , Web graphs , telecommunication networks , and biological networks . In interactive analysis of such data a natural query is `` which entities are most important in the network relative to a particular individual or set of individuals ? '' We investigate the problem of answering such queries in this paper , focusing in particular on defining and computing the importance of nodes in a graph relative to one or more root nodes . We define a general framework and a number of different algorithms , building on ideas from social networks , graph theory , Markov models , and Web graph analysis . We experimentally evaluate the different properties of these algorithms on toy graphs and demonstrate how our approach can be used to study relative importance in real-world networks including a network of interactions among September 11th terrorists , a network of collaborative research in biotechnology among companies and universities , and a network of co-authorship relationships among computer science researchers .", "keywords": ["relative importance", "pagerank", "markov chains", "information search and retrieval", "graphs", "social networks"], "combined": "Algorithms for estimating relative importance in networks Large and complex graphs representing relationships among sets of entities are an increasingly common focus of interest in data analysis -- examples include social networks , Web graphs , telecommunication networks , and biological networks . In interactive analysis of such data a natural query is `` which entities are most important in the network relative to a particular individual or set of individuals ? '' We investigate the problem of answering such queries in this paper , focusing in particular on defining and computing the importance of nodes in a graph relative to one or more root nodes . We define a general framework and a number of different algorithms , building on ideas from social networks , graph theory , Markov models , and Web graph analysis . We experimentally evaluate the different properties of these algorithms on toy graphs and demonstrate how our approach can be used to study relative importance in real-world networks including a network of interactions among September 11th terrorists , a network of collaborative research in biotechnology among companies and universities , and a network of co-authorship relationships among computer science researchers . [[EENNDD]] relative importance; pagerank; markov chains; information search and retrieval; graphs; social networks"}, "Algoritma untuk mengira kepentingan relatif dalam rangkaian Grafik besar dan kompleks yang mewakili hubungan antara kumpulan entiti menjadi tumpuan minat yang semakin umum dalam analisis data - contohnya termasuk rangkaian sosial, grafik Web, rangkaian telekomunikasi, dan rangkaian biologi. Dalam analisis interaktif data sedemikian, pertanyaan semula jadi adalah \"entiti mana yang paling penting dalam rangkaian berbanding dengan individu atau kumpulan individu tertentu? '' Kami menyiasat masalah menjawab pertanyaan sedemikian dalam makalah ini, dengan fokus khusus pada menentukan dan mengira pentingnya node dalam grafik berbanding dengan satu atau lebih nod akar. Kami menentukan kerangka umum dan sejumlah algoritma yang berbeza, membangun idea dari rangkaian sosial, teori grafik, model Markov, dan analisis grafik Web. Kami secara eksperimen menilai sifat-sifat yang berbeza dari algoritma ini pada grafik mainan dan menunjukkan bagaimana pendekatan kami dapat digunakan untuk mengkaji kepentingan relatif dalam rangkaian dunia nyata termasuk rangkaian interaksi antara pengganas 11 September, rangkaian penyelidikan kolaboratif dalam bioteknologi di antara syarikat dan universiti , dan rangkaian hubungan pengarang bersama di kalangan penyelidik sains komputer. [[EENNDD]] kepentingan relatif; pagerank; rantai markov; carian dan pengambilan maklumat; grafik; rangkaian sosial"], [{"string": "Online allocation of display ads with smooth delivery Display ads on the Internet are often sold in bundles of thousands or millions of impressions over a particular time period , typically weeks or months . Ad serving systems that assign ads to pages on behalf of publishers must satisfy these contracts , but at the same time try to maximize overall quality of placement . This is usually modeled in the literature as an online allocation problem , where contracts are represented by overall delivery constraints over a finite time horizon . However this model misses an important aspect of ad delivery : time homogeneity . Advertisers who buy these packages expect their ad to be shown smoothly throughout the purchased time period , in order to reach a wider audience , to have a sustained impact , and to support the ads they are running on other media e.g. , television . In this paper we formalize this problem using several nested packing constraints , and develop a tight 1-1 \\/ e - competitive online algorithm for this problem . Our algorithms and analysis require novel techniques as they involve online computation of multiple dual variables per ad . We then show the effectiveness of our algorithms through exhaustive simulation studies on real data sets .", "keywords": ["general", "online matching", "smooth delivery", "ad allocation", "nonnumerical algorithms and problems", "display ads"], "combined": "Online allocation of display ads with smooth delivery Display ads on the Internet are often sold in bundles of thousands or millions of impressions over a particular time period , typically weeks or months . Ad serving systems that assign ads to pages on behalf of publishers must satisfy these contracts , but at the same time try to maximize overall quality of placement . This is usually modeled in the literature as an online allocation problem , where contracts are represented by overall delivery constraints over a finite time horizon . However this model misses an important aspect of ad delivery : time homogeneity . Advertisers who buy these packages expect their ad to be shown smoothly throughout the purchased time period , in order to reach a wider audience , to have a sustained impact , and to support the ads they are running on other media e.g. , television . In this paper we formalize this problem using several nested packing constraints , and develop a tight 1-1 \\/ e - competitive online algorithm for this problem . Our algorithms and analysis require novel techniques as they involve online computation of multiple dual variables per ad . We then show the effectiveness of our algorithms through exhaustive simulation studies on real data sets . [[EENNDD]] general; online matching; smooth delivery; ad allocation; nonnumerical algorithms and problems; display ads"}, "Peruntukan iklan paparan dalam talian dengan penyampaian yang lancar Iklan paparan di Internet sering dijual dalam jumlah ribuan atau berjuta-juta tayangan dalam jangka masa tertentu, biasanya minggu atau bulan. Sistem penayangan iklan yang menetapkan iklan ke halaman atas nama penerbit mesti memenuhi kontrak ini, tetapi pada masa yang sama cuba memaksimumkan kualiti penempatan secara keseluruhan. Ini biasanya dimodelkan dalam literatur sebagai masalah peruntukan dalam talian, di mana kontrak diwakili oleh kekangan penyampaian keseluruhan dalam jangka masa yang terbatas. Walau bagaimanapun, model ini kehilangan aspek penting dalam penyampaian iklan: homogenitas masa. Pengiklan yang membeli pakej ini mengharapkan iklan mereka ditampilkan dengan lancar sepanjang jangka masa yang dibeli, untuk menjangkau khalayak yang lebih luas, memberi kesan yang berterusan, dan untuk menyokong iklan yang mereka jalankan di media lain, mis. , televisyen. Dalam makalah ini kami memformalkan masalah ini dengan menggunakan beberapa kekangan pembungkusan bersarang, dan mengembangkan algoritma dalam talian 1-1 \\ / e - persaingan yang ketat untuk masalah ini. Algoritma dan analisis kami memerlukan teknik baru kerana ia melibatkan pengiraan dalam talian pelbagai pemboleh ubah dua per iklan. Kami kemudian menunjukkan keberkesanan algoritma kami melalui kajian simulasi lengkap pada set data sebenar. [[EENNDD]] umum; pemadanan dalam talian; penghantaran lancar; peruntukan iklan; algoritma dan masalah bukan berangka; iklan paparan"], [{"string": "Hierarchical model-based clustering of large datasets through fractionation and refractionation The goal of clustering is to identify distinct groups in a dataset . Compared to non-parametric clustering methods like complete linkage , hierarchical model-based clustering has the advantage of offering a way to estimate the number of groups present in the data . However , its computational cost is quadratic in the number of items to be clustered , and it is therefore not applicable to large problems . We review an idea called Fractionation , originally conceived by Cutting , Karger , Pedersen and Tukey for non-parametric hierarchical clustering of large datasets , and describe an adaptation of Fractionation to model-based clustering . A further extension , called Refractionation , leads to a procedure that can be successful even in the difficult situation where there are large numbers of small groups .", "keywords": ["refractionation", "fractionation", "clustering", "model-based clustering"], "combined": "Hierarchical model-based clustering of large datasets through fractionation and refractionation The goal of clustering is to identify distinct groups in a dataset . Compared to non-parametric clustering methods like complete linkage , hierarchical model-based clustering has the advantage of offering a way to estimate the number of groups present in the data . However , its computational cost is quadratic in the number of items to be clustered , and it is therefore not applicable to large problems . We review an idea called Fractionation , originally conceived by Cutting , Karger , Pedersen and Tukey for non-parametric hierarchical clustering of large datasets , and describe an adaptation of Fractionation to model-based clustering . A further extension , called Refractionation , leads to a procedure that can be successful even in the difficult situation where there are large numbers of small groups . [[EENNDD]] refractionation; fractionation; clustering; model-based clustering"}, "Pengelompokan berdasarkan data hierarki dari kumpulan data besar melalui pemecahan dan pembiasan Matlamat pengelompokan adalah untuk mengenal pasti kumpulan yang berbeza dalam set data. Berbanding dengan kaedah pengelompokan bukan parametrik seperti hubungan lengkap, pengelompokan berdasarkan model hierarki mempunyai kelebihan menawarkan cara untuk menganggarkan jumlah kumpulan yang ada dalam data. Namun, biaya pengiraannya adalah kuadratik dalam jumlah item yang akan dikelompokkan, dan oleh itu tidak berlaku untuk masalah besar. Kami mengkaji idea yang disebut Fractionation, yang awalnya disusun oleh Cutting, Karger, Pedersen dan Tukey untuk pengelompokan hierarki non-parametrik kumpulan data yang besar, dan menerangkan penyesuaian Fractionation ke pengelompokan berdasarkan model. Perpanjangan selanjutnya, yang disebut Refractionation, membawa kepada prosedur yang dapat berjaya walaupun dalam keadaan sukar di mana terdapat sejumlah besar kelompok kecil. [[EENNDD]] pembiasan; pecahan; pengelompokan; pengelompokan berasaskan model"], [{"string": "Heterogeneous source consensus learning via decision propagation and negotiation Nowadays , enormous amounts of data are continuously generated not only in massive scale , but also from different , sometimes conflicting , views . Therefore , it is important to consolidate different concepts for intelligent decision making . For example , to predict the research areas of some people , the best results are usually achieved by combining and consolidating predictions obtained from the publication network , co-authorship network and the textual content of their publications . Multiple supervised and unsupervised hypotheses can be drawn from these information sources , and negotiating their differences and consolidating decisions usually yields a much more accurate model due to the diversity and heterogeneity of these models . In this paper , we address the problem of `` consensus learning '' among competing hypotheses , which either rely on outside knowledge supervised learning or internal structure unsupervised clustering . We argue that consensus learning is an NP-hard problem and thus propose to solve it by an efficient heuristic method . We construct a belief graph to first propagate predictions from supervised models to the unsupervised , and then negotiate and reach consensus among them . Their final decision is further consolidated by calculating each model 's weight based on its degree of consistency with other models . Experiments are conducted on 20 Newsgroups data , Cora research papers , DBLP author-conference network , and Yahoo ! Movies datasets , and the results show that the proposed method improves the classification accuracy and the clustering quality measure NMI over the best base model by up to 10 % . Furthermore , it runs in time proportional to the number of instances , which is very efficient for large scale data sets .", "keywords": ["consensus", "ensemble", "classification", "heterogeneous sources"], "combined": "Heterogeneous source consensus learning via decision propagation and negotiation Nowadays , enormous amounts of data are continuously generated not only in massive scale , but also from different , sometimes conflicting , views . Therefore , it is important to consolidate different concepts for intelligent decision making . For example , to predict the research areas of some people , the best results are usually achieved by combining and consolidating predictions obtained from the publication network , co-authorship network and the textual content of their publications . Multiple supervised and unsupervised hypotheses can be drawn from these information sources , and negotiating their differences and consolidating decisions usually yields a much more accurate model due to the diversity and heterogeneity of these models . In this paper , we address the problem of `` consensus learning '' among competing hypotheses , which either rely on outside knowledge supervised learning or internal structure unsupervised clustering . We argue that consensus learning is an NP-hard problem and thus propose to solve it by an efficient heuristic method . We construct a belief graph to first propagate predictions from supervised models to the unsupervised , and then negotiate and reach consensus among them . Their final decision is further consolidated by calculating each model 's weight based on its degree of consistency with other models . Experiments are conducted on 20 Newsgroups data , Cora research papers , DBLP author-conference network , and Yahoo ! Movies datasets , and the results show that the proposed method improves the classification accuracy and the clustering quality measure NMI over the best base model by up to 10 % . Furthermore , it runs in time proportional to the number of instances , which is very efficient for large scale data sets . [[EENNDD]] consensus; ensemble; classification; heterogeneous sources"}, "Pembelajaran konsensus sumber yang heterogen melalui penyebaran dan perundingan keputusan Pada masa ini, sejumlah besar data dihasilkan secara berterusan bukan hanya dalam skala besar, tetapi juga dari pandangan yang berbeza, kadang-kadang bertentangan. Oleh itu, adalah penting untuk menggabungkan konsep yang berbeza untuk membuat keputusan yang bijak. Sebagai contoh, untuk meramalkan bidang penyelidikan beberapa orang, hasil terbaik biasanya dicapai dengan menggabungkan dan menggabungkan ramalan yang diperoleh dari rangkaian penerbitan, rangkaian pengarang bersama dan kandungan teks penerbitan mereka. Banyak hipotesis yang diselia dan tidak diawasi dapat diambil dari sumber maklumat ini, dan merundingkan perbezaan mereka dan menyatukan keputusan biasanya menghasilkan model yang jauh lebih tepat kerana kepelbagaian dan heterogenitas model-model ini. Dalam makalah ini, kami menangani masalah \"pembelajaran konsensus\" di antara hipotesis yang bersaing, yang bergantung pada pembelajaran di bawah pengetahuan di luar pengetahuan atau struktur pengelompokan internal yang tidak diawasi. Kami berpendapat bahawa pembelajaran konsensus adalah masalah yang sukar dilakukan oleh NP dan dengan demikian mencadangkan untuk menyelesaikannya dengan kaedah heuristik yang cekap. Kami membina graf kepercayaan untuk terlebih dahulu menyebarkan ramalan dari model yang diselia ke yang tidak diawasi, dan kemudian berunding dan mencapai kata sepakat di antara mereka. Keputusan akhir mereka disatukan dengan mengira berat setiap model berdasarkan tahap konsistensinya dengan model lain. Eksperimen dijalankan pada 20 data Kumpulan Berita, makalah penyelidikan Cora, rangkaian persidangan pengarang DBLP, dan Yahoo! Set data filem, dan hasilnya menunjukkan bahawa kaedah yang dicadangkan meningkatkan ketepatan klasifikasi dan kualiti pengelompokan mengukur NMI berbanding model asas terbaik hingga 10%. Selanjutnya, ia berjalan dalam masa yang sebanding dengan jumlah kejadian, yang sangat efisien untuk set data berskala besar. [[EENNDD]] kata sepakat; ensembel; pengelasan; sumber heterogen"], [{"string": "Visualization of navigation patterns on a Web site using model-based clustering", "keywords": ["sequence clustering", "data visualization", "internet", "web", "model-based clustering"], "combined": "Visualization of navigation patterns on a Web site using model-based clustering [[EENNDD]] sequence clustering; data visualization; internet; web; model-based clustering"}, "Visualisasi corak navigasi di laman web menggunakan pengelompokan urutan berasaskan [[EENNDD]] berdasarkan model; visualisasi data; internet; laman web; pengelompokan berasaskan model"], [{"string": "Unsupervised transfer classification : application to text categorization We study the problem of building the classification model for a target class in the absence of any labeled training example for that class . To address this difficult learning problem , we extend the idea of transfer learning by assuming that the following side information is available : i a collection of labeled examples belonging to other classes in the problem domain , called the auxiliary classes ; ii the class information including the prior of the target class and the correlation between the target class and the auxiliary classes . Our goal is to construct the classification model for the target class by leveraging the above data and information . We refer to this learning problem as unsupervised transfer classification . Our framework is based on the generalized maximum entropy model that is effective in transferring the label information of the auxiliary classes to the target class . A theoretical analysis shows that under certain assumption , the classification model obtained by the proposed approach converges to the optimal model when it is learned from the labeled examples for the target class . Empirical study on text categorization over four different data sets verifies the effectiveness of the proposed approach .", "keywords": ["unsupervised transfer classification", "generalized maximum entropy model", "text categorization", "miscellaneous"], "combined": "Unsupervised transfer classification : application to text categorization We study the problem of building the classification model for a target class in the absence of any labeled training example for that class . To address this difficult learning problem , we extend the idea of transfer learning by assuming that the following side information is available : i a collection of labeled examples belonging to other classes in the problem domain , called the auxiliary classes ; ii the class information including the prior of the target class and the correlation between the target class and the auxiliary classes . Our goal is to construct the classification model for the target class by leveraging the above data and information . We refer to this learning problem as unsupervised transfer classification . Our framework is based on the generalized maximum entropy model that is effective in transferring the label information of the auxiliary classes to the target class . A theoretical analysis shows that under certain assumption , the classification model obtained by the proposed approach converges to the optimal model when it is learned from the labeled examples for the target class . Empirical study on text categorization over four different data sets verifies the effectiveness of the proposed approach . [[EENNDD]] unsupervised transfer classification; generalized maximum entropy model; text categorization; miscellaneous"}, "Klasifikasi pemindahan yang tidak diselia: aplikasi untuk pengkategorian teks Kami mengkaji masalah membina model klasifikasi untuk kelas sasaran sekiranya tidak ada contoh latihan berlabel untuk kelas tersebut. Untuk mengatasi masalah pembelajaran yang sukar ini, kami memperluas idea pemindahan pembelajaran dengan menganggap bahawa maklumat sampingan berikut tersedia: i koleksi contoh berlabel milik kelas lain dalam domain masalah, yang disebut kelas bantu; ii maklumat kelas termasuk yang terdahulu dari kelas sasaran dan korelasi antara kelas sasaran dan kelas tambahan. Matlamat kami adalah untuk membina model klasifikasi untuk kelas sasaran dengan memanfaatkan data dan maklumat di atas. Kami merujuk kepada masalah pembelajaran ini sebagai klasifikasi pemindahan tanpa pengawasan. Rangka kerja kami didasarkan pada model entropi maksimum umum yang berkesan dalam memindahkan maklumat label kelas tambahan ke kelas sasaran. Analisis teori menunjukkan bahawa dengan anggapan tertentu, model klasifikasi yang diperoleh dengan pendekatan yang dicadangkan akan berubah menjadi model yang optimum apabila dipelajari dari contoh berlabel untuk kelas sasaran. Kajian empirikal mengenai pengkategorian teks terhadap empat set data yang berbeza mengesahkan keberkesanan pendekatan yang dicadangkan. [[EENNDD]] klasifikasi pemindahan tanpa pengawasan; model entropi maksimum umum; pengkategorian teks; pelbagai"], [{"string": "Extending na\u00efve Bayes classifiers using long itemsets", "keywords": ["lazy learning", "association mining", "bayesian learning", "classification"], "combined": "Extending na\u00efve Bayes classifiers using long itemsets [[EENNDD]] lazy learning; association mining; bayesian learning; classification"}, "Memperluas pengelasan Bayes naif menggunakan set item panjang [[EENNDD]] pembelajaran malas; perlombongan persatuan; pembelajaran bayesian; pengelasan"], [{"string": "Natural communities in large linked networks We are interested in finding natural communities in large-scale linked networks . Our ultimate goal is to track changes over time in such communities . For such temporal tracking , we require a clustering algorithm that is relatively stable under small perturbations of the input data . We have developed an efficient , scalable agglomerative strategy and applied it to the citation graph of the NEC CiteSeer database 250,000 papers ; 4.5 million citations . Agglomerative clustering techniques are known to be unstable on data in which the community structure is not strong . We find that some communities are essentially random and thus unstable while others are natural and will appear in most clusterings . These natural communities will enable us to track the evolution of communities over time .", "keywords": ["large linked networks", "information search and retrieval", "hierarchical agglomerative clustering", "stability", "natural communities"], "combined": "Natural communities in large linked networks We are interested in finding natural communities in large-scale linked networks . Our ultimate goal is to track changes over time in such communities . For such temporal tracking , we require a clustering algorithm that is relatively stable under small perturbations of the input data . We have developed an efficient , scalable agglomerative strategy and applied it to the citation graph of the NEC CiteSeer database 250,000 papers ; 4.5 million citations . Agglomerative clustering techniques are known to be unstable on data in which the community structure is not strong . We find that some communities are essentially random and thus unstable while others are natural and will appear in most clusterings . These natural communities will enable us to track the evolution of communities over time . [[EENNDD]] large linked networks; information search and retrieval; hierarchical agglomerative clustering; stability; natural communities"}, "Komuniti semula jadi dalam rangkaian bersambung besar Kami berminat untuk mencari komuniti semula jadi dalam rangkaian berskala besar. Matlamat utama kami adalah untuk mengesan perubahan dari masa ke masa dalam komuniti tersebut. Untuk penjejakan temporal seperti itu, kami memerlukan algoritma kluster yang relatif stabil di bawah gangguan input data. Kami telah mengembangkan strategi aglomeratif yang cekap dan terukur dan menerapkannya pada grafik petikan pangkalan data NEC CiteSeer 250,000 makalah; 4.5 juta petikan. Teknik pengelompokan agregatif diketahui tidak stabil pada data di mana struktur komuniti tidak kuat. Kami mendapati bahawa sesetengah komuniti pada dasarnya adalah rawak dan dengan itu tidak stabil sementara yang lain bersifat semula jadi dan akan muncul dalam kebanyakan kelompok. Komuniti semula jadi ini akan membolehkan kita mengesan evolusi masyarakat dari masa ke masa. [[EENNDD]] rangkaian berangkai besar; pencarian dan pengambilan maklumat; pengelompokan aglomeratif hierarki; kestabilan; komuniti semula jadi"], [{"string": "Customer targeting models using actively-selected web content We consider the problem of predicting the likelihood that a company will purchase a new product from a seller . The statistical models we have developed at IBM for this purpose rely on historical transaction data coupled with structured firmographic information like the company revenue , number of employees and so on . In this paper , we extend this methodology to include additional text-based features based on analysis of the content on each company 's website . Empirical results demonstrate that incorporating such web content can significantly improve customer targeting . Furthermore , we present methods to actively select only the web content that is likely to improve our models , while reducing the costs of acquisition and processing .", "keywords": ["active feature-value acquisition", "active learning", "text categorization", "learning", "web mining", "models"], "combined": "Customer targeting models using actively-selected web content We consider the problem of predicting the likelihood that a company will purchase a new product from a seller . The statistical models we have developed at IBM for this purpose rely on historical transaction data coupled with structured firmographic information like the company revenue , number of employees and so on . In this paper , we extend this methodology to include additional text-based features based on analysis of the content on each company 's website . Empirical results demonstrate that incorporating such web content can significantly improve customer targeting . Furthermore , we present methods to actively select only the web content that is likely to improve our models , while reducing the costs of acquisition and processing . [[EENNDD]] active feature-value acquisition; active learning; text categorization; learning; web mining; models"}, "Model penargetan pelanggan menggunakan kandungan web yang dipilih secara aktif Kami menganggap masalah untuk meramalkan kemungkinan syarikat akan membeli produk baru dari penjual. Model statistik yang telah kami kembangkan di IBM untuk tujuan ini bergantung pada data transaksi sejarah yang digabungkan dengan maklumat firma berstruktur seperti pendapatan syarikat, jumlah pekerja dan sebagainya. Dalam makalah ini, kami memperluas metodologi ini untuk memasukkan ciri-ciri tambahan berdasarkan teks berdasarkan analisis kandungan di laman web setiap syarikat. Hasil empirikal menunjukkan bahawa memasukkan kandungan web sedemikian dapat meningkatkan penyasaran pelanggan dengan ketara. Selanjutnya, kami menyajikan kaedah untuk memilih secara aktif hanya kandungan web yang cenderung meningkatkan model kami, sambil mengurangkan kos pemerolehan dan pemprosesan. [[EENNDD]] pemerolehan nilai ciri aktif; pembelajaran aktif; pengkategorian teks; belajar; perlombongan web; model"], [{"string": "Building connected neighborhood graphs for isometric data embedding Neighborhood graph construction is usually the first step in algorithms for isometric data embedding and manifold learning that cope with the problem of projecting high dimensional data to a low space . This paper begins by explaining the algorithmic fundamentals of techniques for isometric data embedding and derives a general classification of these techniques . We will see that the nearest neighbor approaches commonly used to construct neighborhood graphs do not guarantee connectedness of the constructed neighborhood graphs and , consequently , may cause an algorithm fail to project data to a single low dimensional coordinate system . In this paper , we review three existing methods to construct k-edge-connected neighborhood graphs and propose a new method to construct k-connected neighborhood graphs . These methods are applicable to a wide range of data including data distributed among clusters . Their features are discussed and compared through experiments .", "keywords": ["data embedding", "manifold learning", "dimensionality reduction", "graph connectivity"], "combined": "Building connected neighborhood graphs for isometric data embedding Neighborhood graph construction is usually the first step in algorithms for isometric data embedding and manifold learning that cope with the problem of projecting high dimensional data to a low space . This paper begins by explaining the algorithmic fundamentals of techniques for isometric data embedding and derives a general classification of these techniques . We will see that the nearest neighbor approaches commonly used to construct neighborhood graphs do not guarantee connectedness of the constructed neighborhood graphs and , consequently , may cause an algorithm fail to project data to a single low dimensional coordinate system . In this paper , we review three existing methods to construct k-edge-connected neighborhood graphs and propose a new method to construct k-connected neighborhood graphs . These methods are applicable to a wide range of data including data distributed among clusters . Their features are discussed and compared through experiments . [[EENNDD]] data embedding; manifold learning; dimensionality reduction; graph connectivity"}, "Membina grafik kejiranan bersambung untuk penyisipan data isometrik Pembinaan grafik kejiranan biasanya merupakan langkah pertama dalam algoritma untuk penyisipan data isometrik dan pembelajaran manifold yang mengatasi masalah memproyeksikan data dimensi tinggi ke ruang rendah. Makalah ini dimulakan dengan menjelaskan asas algoritma teknik untuk penyisipan data isometrik dan memperoleh klasifikasi umum teknik ini. Kami akan melihat bahawa pendekatan jiran terdekat yang biasa digunakan untuk membina grafik kejiranan tidak menjamin keterkaitan grafik kejiranan yang dibina dan, akibatnya, boleh menyebabkan algoritma gagal memproyeksikan data ke sistem koordinat dimensi rendah tunggal. Dalam makalah ini, kami mengkaji tiga kaedah yang ada untuk membina grafik kejiranan yang terhubung dengan k-edge dan mencadangkan kaedah baru untuk membina grafik kejiranan yang dihubungkan dengan k. Kaedah-kaedah ini berlaku untuk pelbagai data termasuk data yang diedarkan di antara kelompok. Ciri-ciri mereka dibincangkan dan dibandingkan melalui eksperimen. [[EENNDD]] penyisipan data; pembelajaran pelbagai; pengurangan dimensi; penyambungan graf"], [{"string": "Growing decision trees on support-less association rules", "keywords": ["decision tree", "decision support", "database applications", "association rules"], "combined": "Growing decision trees on support-less association rules [[EENNDD]] decision tree; decision support; database applications; association rules"}, "Menumbuhkan pokok keputusan berdasarkan peraturan persatuan tanpa sokongan [[EENNDD]] pokok keputusan; sokongan keputusan; aplikasi pangkalan data; peraturan persatuan"], [{"string": "PaintingClass : interactive construction , visualization and exploration of decision trees Decision trees are commonly used for classification . We propose to use decision trees not just for classification but also for the wider purpose of knowledge discovery , because visualizing the decision tree can reveal much valuable information in the data . We introduce PaintingClass , a system for interactive construction , visualization and exploration of decision trees . PaintingClass provides an intuitive layout and convenient navigation of the decision tree . PaintingClass also provides the user the means to interactively construct the decision tree . Each node in the decision tree is displayed as a visual projection of the data . Through actual examples and comparison with other classification methods , we show that the user can effectively use PaintingClass to construct a decision tree and explore the decision tree to gain additional knowledge .", "keywords": ["decision trees", "interactive visualization", "information visualization", "classification", "visual data mining"], "combined": "PaintingClass : interactive construction , visualization and exploration of decision trees Decision trees are commonly used for classification . We propose to use decision trees not just for classification but also for the wider purpose of knowledge discovery , because visualizing the decision tree can reveal much valuable information in the data . We introduce PaintingClass , a system for interactive construction , visualization and exploration of decision trees . PaintingClass provides an intuitive layout and convenient navigation of the decision tree . PaintingClass also provides the user the means to interactively construct the decision tree . Each node in the decision tree is displayed as a visual projection of the data . Through actual examples and comparison with other classification methods , we show that the user can effectively use PaintingClass to construct a decision tree and explore the decision tree to gain additional knowledge . [[EENNDD]] decision trees; interactive visualization; information visualization; classification; visual data mining"}, "PaintingClass: pembinaan interaktif, visualisasi dan penerokaan pokok keputusan Pokok keputusan biasanya digunakan untuk klasifikasi. Kami mencadangkan untuk menggunakan pohon keputusan bukan hanya untuk klasifikasi tetapi juga untuk tujuan penemuan pengetahuan yang lebih luas, kerana memvisualisasikan pohon keputusan dapat mendedahkan banyak maklumat berharga dalam data. Kami memperkenalkan PaintingClass, sistem untuk pembinaan interaktif, visualisasi dan penerokaan keputusan. PaintingClass menyediakan susun atur yang intuitif dan navigasi yang tepat dari pokok keputusan. PaintingClass juga memberi pengguna kaedah untuk membina pokok keputusan secara interaktif. Setiap simpul di pohon keputusan dipaparkan sebagai unjuran visual data. Melalui contoh dan perbandingan sebenar dengan kaedah klasifikasi lain, kami menunjukkan bahawa pengguna dapat menggunakan PaintingClass dengan berkesan untuk membina pohon keputusan dan meneroka pohon keputusan untuk memperoleh pengetahuan tambahan. [[EENNDD]] pokok keputusan; visualisasi interaktif; visualisasi maklumat; pengelasan; perlombongan data visual"], [{"string": "Rotation invariant distance measures for trajectories For the discovery of similar patterns in 1D time-series , it is very typical to perform a normalization of the data for example a transformation so that the data follow a zero mean and unit standard deviation . Such transformations can reveal latent patterns and are very commonly used in datamining applications . However , when dealing with multidimensional time-series , which appear naturally in applications such as video-tracking , motion-capture etc , similar motion patterns can also be expressed at different orientations . It is therefore imperative to provide support for additional transformations , such as rotation . In this work , we transform the positional information of moving data , into a space that is translation , scale and rotation invariant . Our distance measure in the new space is able to detect elastic matches and can be efficiently lower bounded , thus being computationally tractable . The proposed methods are easy to implement , fast to compute and can have many applications for real world problems , in areas such as handwriting recognition and posture estimation in motion-capture data . Finally , we empirically demonstrate the accuracy and the efficiency of the technique , using real and synthetic handwriting data .", "keywords": ["time warping", "trajectories", "rotation invariance"], "combined": "Rotation invariant distance measures for trajectories For the discovery of similar patterns in 1D time-series , it is very typical to perform a normalization of the data for example a transformation so that the data follow a zero mean and unit standard deviation . Such transformations can reveal latent patterns and are very commonly used in datamining applications . However , when dealing with multidimensional time-series , which appear naturally in applications such as video-tracking , motion-capture etc , similar motion patterns can also be expressed at different orientations . It is therefore imperative to provide support for additional transformations , such as rotation . In this work , we transform the positional information of moving data , into a space that is translation , scale and rotation invariant . Our distance measure in the new space is able to detect elastic matches and can be efficiently lower bounded , thus being computationally tractable . The proposed methods are easy to implement , fast to compute and can have many applications for real world problems , in areas such as handwriting recognition and posture estimation in motion-capture data . Finally , we empirically demonstrate the accuracy and the efficiency of the technique , using real and synthetic handwriting data . [[EENNDD]] time warping; trajectories; rotation invariance"}, "Ukuran jarak invarian putaran untuk lintasan Untuk penemuan corak serupa dalam siri masa 1D, sangat biasa melakukan normalisasi data misalnya transformasi sehingga data mengikuti min sifar dan sisihan piawai unit. Transformasi seperti itu dapat memperlihatkan corak laten dan sangat biasa digunakan dalam aplikasi pemeriksaan data. Namun, ketika berhadapan dengan siri waktu multidimensi, yang muncul secara semula jadi dalam aplikasi seperti penjejakan video, tangkapan gerakan dll, corak gerakan serupa juga dapat dinyatakan pada orientasi yang berbeza. Oleh itu, sangat mustahak untuk memberikan sokongan untuk transformasi tambahan, seperti putaran. Dalam karya ini, kami mengubah maklumat kedudukan data bergerak, menjadi ruang yang terjemahan, skala dan putaran tidak berubah. Ukuran jarak kami di ruang baru dapat mengesan padatan elastik dan dapat dibatasi dengan cekap, sehingga dapat dikira secara komputasi. Kaedah yang dicadangkan mudah dilaksanakan, cepat dikira dan dapat memiliki banyak aplikasi untuk masalah dunia nyata, dalam bidang seperti pengenalan tulisan tangan dan perkiraan postur dalam data tangkapan gerakan. Akhirnya, kami secara empirik menunjukkan ketepatan dan kecekapan teknik, menggunakan data tulisan tangan sebenar dan sintetik. [[EENNDD]] masa melengkung; lintasan; invarians putaran"], [{"string": "Single-shot detection of multiple categories of text using parametric mixture models In this paper , we address the problem of detecting multiple topics or categories of text where each text is not assumed to belong to one of a number of mutually exclusive categories . Conventionally , the binary classification approach has been employed , in which whether or not text belongs to a category is judged by the binary classifier for every category . In this paper , we propose a more sophisticated approach to simultaneously detect multiple categories of text using parametric mixture models PMMs , newly presented in this paper . PMMs are probabilistic generative models for text that has multiple categories . Our PMMs are essentially different from the conventional mixture of multinomial distributions in the sense that in the former several basis multinomial parameters are mixed in the parameter space , while in the latter several multinomial components are mixed . We derive efficient learning algorithms for PMMs within the framework of the maximum a posteriori estimate . We also empirically show that our method can outperform the conventional binary approach when applied to multitopic detection of World Wide Web pages , focusing on those from the `` yahoo.com '' domain .", "keywords": ["probabilistic algorithms"], "combined": "Single-shot detection of multiple categories of text using parametric mixture models In this paper , we address the problem of detecting multiple topics or categories of text where each text is not assumed to belong to one of a number of mutually exclusive categories . Conventionally , the binary classification approach has been employed , in which whether or not text belongs to a category is judged by the binary classifier for every category . In this paper , we propose a more sophisticated approach to simultaneously detect multiple categories of text using parametric mixture models PMMs , newly presented in this paper . PMMs are probabilistic generative models for text that has multiple categories . Our PMMs are essentially different from the conventional mixture of multinomial distributions in the sense that in the former several basis multinomial parameters are mixed in the parameter space , while in the latter several multinomial components are mixed . We derive efficient learning algorithms for PMMs within the framework of the maximum a posteriori estimate . We also empirically show that our method can outperform the conventional binary approach when applied to multitopic detection of World Wide Web pages , focusing on those from the `` yahoo.com '' domain . [[EENNDD]] probabilistic algorithms"}, "Pengesanan satu pukulan dari beberapa kategori teks menggunakan model campuran parametrik Dalam makalah ini, kita menangani masalah mengesan beberapa topik atau kategori teks di mana setiap teks tidak dianggap termasuk dalam salah satu daripada beberapa kategori yang saling eksklusif. Secara konvensional, pendekatan klasifikasi binari telah digunakan, di mana teks tergolong dalam kategori atau tidak dinilai oleh pengkelasan binari untuk setiap kategori. Dalam makalah ini, kami mencadangkan pendekatan yang lebih canggih untuk mengesan pelbagai kategori teks secara serentak menggunakan PMM model campuran parametrik, yang baru dibentangkan dalam makalah ini. PMM adalah model generatif probabilistik untuk teks yang mempunyai pelbagai kategori. PMM kami pada dasarnya berbeza dari campuran konvensional pengagihan multinomial dalam arti bahawa sebelumnya beberapa asas parameter multinomial dicampurkan di ruang parameter, sementara yang terakhir beberapa komponen multinomial dicampur. Kami memperoleh algoritma pembelajaran yang berkesan untuk PMM dalam kerangka anggaran a posteriori maksimum. Kami juga menunjukkan secara empirik bahawa kaedah kami dapat mengungguli pendekatan binari konvensional ketika digunakan untuk pengesanan multitopik halaman World Wide Web, dengan fokus pada yang berasal dari domain \"yahoo.com\". [[EENNDD]] algoritma probabilistik"], [{"string": "Data mining solves tough semiconductor manufacturing problems", "keywords": ["rule induction", "semiconductor yield enhancement", "neural networks", "manufacturing optimization", "pattern recognition", "self organizing maps", "machine learning"], "combined": "Data mining solves tough semiconductor manufacturing problems [[EENNDD]] rule induction; semiconductor yield enhancement; neural networks; manufacturing optimization; pattern recognition; self organizing maps; machine learning"}, "Perlombongan data menyelesaikan masalah pembuatan semikonduktor yang sukar [[EENNDD]] aruhan peraturan; peningkatan hasil semikonduktor; rangkaian saraf; pengoptimuman pembuatan; pengecaman corak; menyusun peta sendiri; pembelajaran mesin"], [{"string": "Learning patterns in the dynamics of biological networks Our dynamic graph-based relational mining approach has been developed to learn structural patterns in biological networks as they change over time . The analysis of dynamic networks is important not only to understand life at the system-level , but also to discover novel patterns in other structural data . Most current graph-based data mining approaches overlook dynamic features of biological networks , because they are focused on only static graphs . Our approach analyzes a sequence of graphs and discovers rules that capture the changes that occur between pairs of graphs in the sequence . These rules represent the graph rewrite rules that the first graph must go through to be isomorphic to the second graph . Then , our approach feeds the graph rewrite rules into a machine learning system that learns general transformation rules describing the types of changes that occur for a class of dynamic biological networks . The discovered graph-rewriting rules show how biological networks change over time , and the transformation rules show the repeated patterns in the structural changes . In this paper , we apply our approach to biological networks to evaluate our approach and to understand how the biosystems change over time . We evaluate our results using coverage and prediction metrics , and compare to biological literature .", "keywords": ["learning", "graph mining", "biological network", "dynamic network analysis", "graph rewriting rule"], "combined": "Learning patterns in the dynamics of biological networks Our dynamic graph-based relational mining approach has been developed to learn structural patterns in biological networks as they change over time . The analysis of dynamic networks is important not only to understand life at the system-level , but also to discover novel patterns in other structural data . Most current graph-based data mining approaches overlook dynamic features of biological networks , because they are focused on only static graphs . Our approach analyzes a sequence of graphs and discovers rules that capture the changes that occur between pairs of graphs in the sequence . These rules represent the graph rewrite rules that the first graph must go through to be isomorphic to the second graph . Then , our approach feeds the graph rewrite rules into a machine learning system that learns general transformation rules describing the types of changes that occur for a class of dynamic biological networks . The discovered graph-rewriting rules show how biological networks change over time , and the transformation rules show the repeated patterns in the structural changes . In this paper , we apply our approach to biological networks to evaluate our approach and to understand how the biosystems change over time . We evaluate our results using coverage and prediction metrics , and compare to biological literature . [[EENNDD]] learning; graph mining; biological network; dynamic network analysis; graph rewriting rule"}, "Corak pembelajaran dalam dinamik rangkaian biologi Pendekatan penambangan relasional berasaskan grafik dinamik kami telah dikembangkan untuk mempelajari corak struktur dalam rangkaian biologi apabila mereka berubah dari masa ke masa. Analisis rangkaian dinamik penting bukan hanya untuk memahami kehidupan di peringkat sistem, tetapi juga untuk mencari corak baru dalam data struktur lain. Sebilangan besar pendekatan perlombongan data berasaskan grafik semasa mengabaikan ciri dinamik rangkaian biologi, kerana hanya tertumpu pada grafik statik. Pendekatan kami menganalisis urutan grafik dan mencari peraturan yang menangkap perubahan yang berlaku antara pasangan graf dalam urutan. Peraturan ini mewakili peraturan menulis semula grafik yang mesti dilalui oleh grafik pertama untuk menjadi isomorfik ke grafik kedua. Kemudian, pendekatan kami memasukkan peraturan menulis semula grafik ke dalam sistem pembelajaran mesin yang mempelajari peraturan transformasi umum yang menerangkan jenis perubahan yang berlaku untuk kelas rangkaian biologi dinamik. Peraturan penulisan semula grafik yang ditemui menunjukkan bagaimana rangkaian biologi berubah dari masa ke masa, dan peraturan transformasi menunjukkan corak berulang dalam perubahan struktur. Dalam makalah ini, kami menerapkan pendekatan kami ke rangkaian biologi untuk menilai pendekatan kami dan untuk memahami bagaimana biosistem berubah dari masa ke masa. Kami menilai hasil kami menggunakan metrik liputan dan ramalan, dan membandingkan dengan literatur biologi. [[EENNDD]] pembelajaran; perlombongan grafik; rangkaian biologi; analisis rangkaian dinamik; peraturan penulisan semula grafik"], [{"string": "Predicting customer shopping lists from point-of-sale purchase data This paper describes a prototype that predicts the shopping lists for customers in a retail store . The shopping list prediction is one aspect of a larger system we have developed for retailers to provide individual and personalized interactions with customers as they navigate through the retail store . Instead of using traditional personalization approaches , such as clustering or segmentation , we learn separate classifiers for each customer from historical transactional data . This allows us to make very fine-grained and accurate predictions about what items a particular individual customer will buy on a given shopping trip . We formally frame the shopping list prediction as a classification problem , describe the algorithms and methodology behind our system , its impact on the business case in which we frame it , and explore some of the properties of the data source that make it an interesting testbed for KDD algorithms . Our results show that we can predict a shopper 's shopping list with high levels of accuracy , precision , and recall . We believe that this work impacts both the data mining and the retail business community . The formulation of shopping list prediction as a machine learning problem results in algorithms that should be useful beyond retail shopping list prediction . For retailers , the result is not only a practical system that increases revenues by up to 11 % , but also enhances customer experience and loyalty by giving them the tools to individually interact with customers and anticipate their needs .", "keywords": ["machine learning", "pos data", "applications", "classification"], "combined": "Predicting customer shopping lists from point-of-sale purchase data This paper describes a prototype that predicts the shopping lists for customers in a retail store . The shopping list prediction is one aspect of a larger system we have developed for retailers to provide individual and personalized interactions with customers as they navigate through the retail store . Instead of using traditional personalization approaches , such as clustering or segmentation , we learn separate classifiers for each customer from historical transactional data . This allows us to make very fine-grained and accurate predictions about what items a particular individual customer will buy on a given shopping trip . We formally frame the shopping list prediction as a classification problem , describe the algorithms and methodology behind our system , its impact on the business case in which we frame it , and explore some of the properties of the data source that make it an interesting testbed for KDD algorithms . Our results show that we can predict a shopper 's shopping list with high levels of accuracy , precision , and recall . We believe that this work impacts both the data mining and the retail business community . The formulation of shopping list prediction as a machine learning problem results in algorithms that should be useful beyond retail shopping list prediction . For retailers , the result is not only a practical system that increases revenues by up to 11 % , but also enhances customer experience and loyalty by giving them the tools to individually interact with customers and anticipate their needs . [[EENNDD]] machine learning; pos data; applications; classification"}, "Meramalkan senarai belanja pelanggan dari data pembelian tempat jualan Kertas ini menerangkan prototaip yang meramalkan senarai belanja untuk pelanggan di kedai runcit. Ramalan senarai beli-belah adalah salah satu aspek sistem yang lebih besar yang telah kami kembangkan untuk peruncit untuk menyediakan interaksi individu dan peribadi dengan pelanggan semasa mereka menavigasi di kedai runcit. Daripada menggunakan pendekatan pemperibadian tradisional, seperti pengelompokan atau segmentasi, kami mempelajari pengkelasan yang berasingan untuk setiap pelanggan dari data transaksi sejarah. Ini membolehkan kita membuat ramalan yang sangat tepat dan tepat mengenai item apa yang akan dibeli oleh pelanggan individu dalam perjalanan membeli-belah tertentu. Kami secara rasmi menyusun ramalan senarai beli-belah sebagai masalah klasifikasi, menerangkan algoritma dan metodologi di sebalik sistem kami, kesannya terhadap kes perniagaan di mana kami membuat kerangka tersebut, dan meneroka beberapa sifat sumber data yang menjadikannya ujian yang menarik untuk Algoritma KDD. Hasil kami menunjukkan bahawa kami dapat meramalkan senarai belanja pembeli dengan tahap ketepatan, ketepatan, dan penarikan balik yang tinggi. Kami percaya bahawa kerja ini memberi kesan kepada perlombongan data dan komuniti perniagaan runcit. Perumusan ramalan senarai beli-belah sebagai masalah pembelajaran mesin menghasilkan algoritma yang semestinya berguna di luar ramalan senarai belanja runcit. Bagi peruncit, hasilnya bukan hanya sistem praktis yang meningkatkan pendapatan hingga 11%, tetapi juga meningkatkan pengalaman dan kesetiaan pelanggan dengan memberi mereka alat untuk berinteraksi secara individu dengan pelanggan dan menjangkakan keperluan mereka. [[EENNDD]] pembelajaran mesin; data pos; permohonan; pengelasan"], [{"string": "Learning spatially variant dissimilarity SVaD measures Clustering algorithms typically operate on a feature vector representation of the data and find clusters that are compact with respect to an assumed dis similarity measure between the data points in feature space . This makes the type of clusters identified highly dependent on the assumed similarity measure . Building on recent work in this area , we formally define a class of spatially varying dissimilarity measures and propose algorithms to learn the dissimilarity measure automatically from the data . The idea is to identify clusters that are compact with respect to the unknown spatially varying dissimilarity measure . Our experiments show that the proposed algorithms are more stable and achieve better accuracy on various textual data sets when compared with similar algorithms proposed in the literature .", "keywords": ["learning dissimilarity measures", "clustering"], "combined": "Learning spatially variant dissimilarity SVaD measures Clustering algorithms typically operate on a feature vector representation of the data and find clusters that are compact with respect to an assumed dis similarity measure between the data points in feature space . This makes the type of clusters identified highly dependent on the assumed similarity measure . Building on recent work in this area , we formally define a class of spatially varying dissimilarity measures and propose algorithms to learn the dissimilarity measure automatically from the data . The idea is to identify clusters that are compact with respect to the unknown spatially varying dissimilarity measure . Our experiments show that the proposed algorithms are more stable and achieve better accuracy on various textual data sets when compared with similar algorithms proposed in the literature . [[EENNDD]] learning dissimilarity measures; clustering"}, "Mempelajari ketidaksamaan varian spasial SVaD mengukur Algoritma pengelompokan biasanya beroperasi pada perwakilan vektor ciri data dan mencari kelompok yang padat sehubungan dengan ukuran kesamaan disangka antara titik data di ruang ciri. Ini menjadikan jenis kelompok yang dikenal pasti sangat bergantung pada ukuran kesamaan yang diandaikan. Berdasarkan kerja terbaru di bidang ini, kami secara formal menentukan kelas ukuran ketidaksamaan spasial yang berbeza dan mencadangkan algoritma untuk mempelajari ukuran ketidaksamaan secara automatik dari data. Ideanya adalah untuk mengenal pasti kelompok yang padat dengan ukuran perbezaan yang tidak diketahui secara spasial. Eksperimen kami menunjukkan bahawa algoritma yang dicadangkan lebih stabil dan mencapai ketepatan yang lebih baik pada pelbagai set data teks jika dibandingkan dengan algoritma serupa yang dicadangkan dalam literatur. [[EENNDD]] langkah-langkah ketidaksamaan pembelajaran; pengelompokan"], [{"string": "Cross-language information retrieval using PARAFAC2 A standard approach to cross-language information retrieval CLIR uses Latent Semantic Analysis LSA in conjunction with a multilingual parallel aligned corpus . This approach has been shown to be successful in identifying similar documents across languages - or more precisely , retrieving the most similar document in one language to a query in another language . However , the approach has severe drawbacks when applied to a related task , that of clustering documents `` language-independently '' , so that documents about similar topics end up closest to one another in the semantic space regardless of their language . The problem is that documents are generally more similar to other documents in the same language than they are to documents in a different language , but on the same topic . As a result , when using multilingual LSA , documents will in practice cluster by language , not by topic . We propose a novel application of PARAFAC2 which is a variant of PARAFAC , a multi-way generalization of the singular value decomposition SVD to overcome this problem . Instead of forming a single multilingual term-by-document matrix which , under LSA , is subjected to SVD , we form an irregular three-way array , each slice of which is a separate term-by-document matrix for a single language in the parallel corpus . The goal is to compute an SVD for each language such that V the matrix of right singular vectors is the same across all languages . Effectively , PARAFAC2 imposes the constraint , not present in standard LSA , that the `` concepts '' in all documents in the parallel corpus are the same regardless of language . Intuitively , this constraint makes sense , since the whole purpose of using a parallel corpus is that exactly the same concepts are expressed in the translations . We tested this approach by comparing the performance of PARAFAC2 with standard LSA in solving a particular CLIR problem . From our results , we conclude that PARAFAC2 offers a very promising alternative to LSA not only for multilingual document clustering , but also for solving other problems in cross-language information retrieval .", "keywords": ["information retrieval", "latent semantic analysis", "parafac2", "performance evaluation", "multilingual"], "combined": "Cross-language information retrieval using PARAFAC2 A standard approach to cross-language information retrieval CLIR uses Latent Semantic Analysis LSA in conjunction with a multilingual parallel aligned corpus . This approach has been shown to be successful in identifying similar documents across languages - or more precisely , retrieving the most similar document in one language to a query in another language . However , the approach has severe drawbacks when applied to a related task , that of clustering documents `` language-independently '' , so that documents about similar topics end up closest to one another in the semantic space regardless of their language . The problem is that documents are generally more similar to other documents in the same language than they are to documents in a different language , but on the same topic . As a result , when using multilingual LSA , documents will in practice cluster by language , not by topic . We propose a novel application of PARAFAC2 which is a variant of PARAFAC , a multi-way generalization of the singular value decomposition SVD to overcome this problem . Instead of forming a single multilingual term-by-document matrix which , under LSA , is subjected to SVD , we form an irregular three-way array , each slice of which is a separate term-by-document matrix for a single language in the parallel corpus . The goal is to compute an SVD for each language such that V the matrix of right singular vectors is the same across all languages . Effectively , PARAFAC2 imposes the constraint , not present in standard LSA , that the `` concepts '' in all documents in the parallel corpus are the same regardless of language . Intuitively , this constraint makes sense , since the whole purpose of using a parallel corpus is that exactly the same concepts are expressed in the translations . We tested this approach by comparing the performance of PARAFAC2 with standard LSA in solving a particular CLIR problem . From our results , we conclude that PARAFAC2 offers a very promising alternative to LSA not only for multilingual document clustering , but also for solving other problems in cross-language information retrieval . [[EENNDD]] information retrieval; latent semantic analysis; parafac2; performance evaluation; multilingual"}, "Pengambilan maklumat lintas bahasa menggunakan PARAFAC2 Pendekatan standard untuk pengambilan maklumat silang bahasa CLIR menggunakan Latent Semantic Analysis LSA bersama dengan korpus sejajar berbilang bahasa. Pendekatan ini telah terbukti berhasil dalam mengenal pasti dokumen yang serupa di semua bahasa - atau lebih tepatnya, mengambil dokumen yang paling serupa dalam satu bahasa ke pertanyaan dalam bahasa lain. Walau bagaimanapun, pendekatan ini mempunyai kelemahan yang teruk ketika diterapkan pada tugas yang berkaitan, iaitu pengelompokan dokumen \"bebas bahasa\", sehingga dokumen mengenai topik serupa berakhir berdekatan antara satu sama lain di ruang semantik tanpa mengira bahasa mereka. Masalahnya adalah bahawa dokumen pada umumnya lebih mirip dengan dokumen lain dalam bahasa yang sama daripada dokumen dengan bahasa yang berbeza, tetapi pada topik yang sama. Akibatnya, ketika menggunakan LSA multibahasa, dokumen dalam praktiknya akan dikelompokkan berdasarkan bahasa, bukan berdasarkan topik. Kami mencadangkan aplikasi baru PARAFAC2 yang merupakan varian PARAFAC, generalisasi pelbagai arah penguraian nilai tunggal SVD untuk mengatasi masalah ini. Daripada membentuk matriks term-by-document tunggal berbilang bahasa yang, di bawah LSA, dikenakan SVD, kami membentuk susunan tiga arah yang tidak teratur, masing-masing potongannya adalah matriks istilah demi dokumen yang terpisah untuk satu bahasa di korpus selari. Tujuannya adalah untuk mengira SVD untuk setiap bahasa sehingga V matriks vektor tunggal tepat adalah sama di semua bahasa. Secara berkesan, PARAFAC2 mengenakan kekangan, tidak terdapat dalam LSA standard, bahawa \"konsep\" dalam semua dokumen dalam korpus selari adalah sama tanpa mengira bahasa. Secara intuitif, kekangan ini masuk akal, kerana keseluruhan tujuan menggunakan korpus selari adalah konsep yang sama persis dinyatakan dalam terjemahan. Kami menguji pendekatan ini dengan membandingkan prestasi PARAFAC2 dengan LSA standard dalam menyelesaikan masalah CLIR tertentu. Dari hasil kami, kami menyimpulkan bahawa PARAFAC2 menawarkan alternatif yang sangat menjanjikan untuk LSA bukan hanya untuk pengelompokan dokumen multibahasa, tetapi juga untuk menyelesaikan masalah lain dalam pencarian maklumat lintas bahasa. [[EENNDD]] pengambilan maklumat; analisis semantik laten; parafac2; penilaian prestasi; berbilang bahasa"], [{"string": "Scalable robust covariance and correlation estimates for data mining Covariance and correlation estimates have important applications in data mining . In the presence of outliers , classical estimates of covariance and correlation matrices are not reliable . A small fraction of outliers , in some cases even a single outlier , can distort the classical covariance and correlation estimates making them virtually useless . That is , correlations for the vast majority of the data can be very erroneously reported ; principal components transformations can be misleading ; and multidimensional outlier detection via Mahalanobis distances can fail to detect outliers . There is plenty of statistical literature on robust covariance and correlation matrix estimates with an emphasis on affine-equivariant estimators that possess high breakdown points and small worst case biases . All such estimators have unacceptable exponential complexity in the number of variables and quadratic complexity in the number of observations . In this paper we focus on several variants of robust covariance and correlation matrix estimates with quadratic complexity in the number of variables and linear complexity in the number of observations . These estimators are based on several forms of pairwise robust covariance and correlation estimates . The estimators studied include two fast estimators based on coordinate-wise robust transformations embedded in an overall procedure recently proposed by 14 . We show that the estimators have attractive robustness properties , and give an example that uses one of the estimators in the new Insightful Miner data mining product .", "keywords": ["outliers", "robust estimators", "scalable algorithm", "robust statistics"], "combined": "Scalable robust covariance and correlation estimates for data mining Covariance and correlation estimates have important applications in data mining . In the presence of outliers , classical estimates of covariance and correlation matrices are not reliable . A small fraction of outliers , in some cases even a single outlier , can distort the classical covariance and correlation estimates making them virtually useless . That is , correlations for the vast majority of the data can be very erroneously reported ; principal components transformations can be misleading ; and multidimensional outlier detection via Mahalanobis distances can fail to detect outliers . There is plenty of statistical literature on robust covariance and correlation matrix estimates with an emphasis on affine-equivariant estimators that possess high breakdown points and small worst case biases . All such estimators have unacceptable exponential complexity in the number of variables and quadratic complexity in the number of observations . In this paper we focus on several variants of robust covariance and correlation matrix estimates with quadratic complexity in the number of variables and linear complexity in the number of observations . These estimators are based on several forms of pairwise robust covariance and correlation estimates . The estimators studied include two fast estimators based on coordinate-wise robust transformations embedded in an overall procedure recently proposed by 14 . We show that the estimators have attractive robustness properties , and give an example that uses one of the estimators in the new Insightful Miner data mining product . [[EENNDD]] outliers; robust estimators; scalable algorithm; robust statistics"}, "Anggaran kovarians dan korelasi yang kuat untuk penambangan data Anggaran kovarians dan korelasi mempunyai aplikasi penting dalam perlombongan data. Dengan kehadiran outlier, anggaran klasik matriks kovarians dan korelasi tidak boleh dipercayai. Sebilangan kecil outlier, dalam beberapa kes bahkan satu outlier, dapat memutarbelitkan kovarians klasik dan perkiraan korelasi menjadikannya hampir tidak berguna. Iaitu, korelasi untuk sebahagian besar data dapat dilaporkan dengan keliru; transformasi komponen utama boleh mengelirukan; dan pengesanan outlier multidimensi melalui jarak Mahalanobis gagal mengesan outliers. Terdapat banyak literatur statistik mengenai anggaran matriks kovarians dan korelasi yang kuat dengan penekanan pada estimator persamaan-persamaan yang mempunyai titik pecahan tinggi dan bias kes terburuk yang kecil. Semua penganggar tersebut mempunyai kerumitan eksponensial yang tidak dapat diterima dalam jumlah pemboleh ubah dan kerumitan kuadratik dalam jumlah pemerhatian. Dalam makalah ini kita menumpukan pada beberapa varian anggaran matriks kovarians dan korelasi yang kuat dengan kerumitan kuadratik dalam jumlah pemboleh ubah dan kerumitan linear dalam jumlah pemerhatian. Penganggar ini berdasarkan beberapa bentuk anggaran kovarians dan korelasi kuat berpasangan. Estimator yang dikaji merangkumi dua estimator pantas berdasarkan transformasi tegas koordinat yang tertanam dalam prosedur keseluruhan yang baru-baru ini dicadangkan oleh 14. Kami menunjukkan bahawa penganggar mempunyai sifat kekuatan yang menarik, dan memberikan contoh yang menggunakan salah satu penganggar dalam produk perlombongan data Insightful Miner yang baru. [[EENNDD]] penyekat; penganggar yang kukuh; algoritma berskala; statistik yang mantap"], [{"string": "Topic-conditioned novelty detection Automated detection of the first document reporting each new event in temporally-sequenced streams of documents is an open challenge . In this paper we propose a new approach which addresses this problem in two stages : 1 using a supervised learning algorithm to classify the on-line document stream into pre-defined broad topic categories , and 2 performing topic-conditioned novelty detection for documents in each topic . We also focus on exploiting named-entities for event-level novelty detection and using feature-based heuristics derived from the topic histories . Evaluating these methods using a set of broadcast news stories , our results show substantial performance gains over the traditional one-level approach to the novelty detection problem .", "keywords": ["named entity", "text classification", "information search and retrieval", "design methodology", "novelty detection", "feature selection"], "combined": "Topic-conditioned novelty detection Automated detection of the first document reporting each new event in temporally-sequenced streams of documents is an open challenge . In this paper we propose a new approach which addresses this problem in two stages : 1 using a supervised learning algorithm to classify the on-line document stream into pre-defined broad topic categories , and 2 performing topic-conditioned novelty detection for documents in each topic . We also focus on exploiting named-entities for event-level novelty detection and using feature-based heuristics derived from the topic histories . Evaluating these methods using a set of broadcast news stories , our results show substantial performance gains over the traditional one-level approach to the novelty detection problem . [[EENNDD]] named entity; text classification; information search and retrieval; design methodology; novelty detection; feature selection"}, "Pengesanan kebaruan berdasarkan topik Pengesanan automatik dokumen pertama yang melaporkan setiap peristiwa baru dalam aliran dokumen yang disusun secara sementara adalah cabaran terbuka. Dalam makalah ini kami mengusulkan pendekatan baru yang menangani masalah ini dalam dua tahap: 1 menggunakan algoritma pembelajaran yang diawasi untuk mengklasifikasikan aliran dokumen on-line ke dalam kategori topik luas yang telah ditentukan, dan 2 melakukan pengesanan kebaruan topik untuk setiap dokumen topik. Kami juga memberi tumpuan untuk mengeksploitasi entiti bernama untuk pengesanan kebaruan peringkat peristiwa dan menggunakan heuristik berasaskan ciri yang berasal dari sejarah topik. Dengan menilai kaedah ini menggunakan satu set berita berita yang disiarkan, hasil kami menunjukkan peningkatan prestasi yang besar berbanding pendekatan satu tahap tradisional untuk masalah pengesanan kebaruan. [[EENNDD]] bernama entiti; pengelasan teks; pencarian dan pengambilan maklumat; metodologi reka bentuk; pengesanan kebaruan; pemilihan ciri"], [{"string": "Cluster-based concept invention for statistical relational learning We use clustering to derive new relations which augment database schema used in automatic generation of predictive features in statistical relational learning . Entities derived from clusters increase the expressivity of feature spaces by creating new first-class concepts which contribute to the creation of new features . For example , in CiteSeer , papers can be clustered based on words or citations giving `` topics '' , and authors can be clustered based on documents they co-author giving `` communities '' . Such cluster-derived concepts become part of more complex feature expressions . Out of the large number of generated features , those which improve predictive accuracy are kept in the model , as decided by statistical feature selection criteria . We present results demonstrating improved accuracy on two tasks , venue prediction and link prediction , using CiteSeer data .", "keywords": ["feature generation", "relational learning", "learning", "clustering"], "combined": "Cluster-based concept invention for statistical relational learning We use clustering to derive new relations which augment database schema used in automatic generation of predictive features in statistical relational learning . Entities derived from clusters increase the expressivity of feature spaces by creating new first-class concepts which contribute to the creation of new features . For example , in CiteSeer , papers can be clustered based on words or citations giving `` topics '' , and authors can be clustered based on documents they co-author giving `` communities '' . Such cluster-derived concepts become part of more complex feature expressions . Out of the large number of generated features , those which improve predictive accuracy are kept in the model , as decided by statistical feature selection criteria . We present results demonstrating improved accuracy on two tasks , venue prediction and link prediction , using CiteSeer data . [[EENNDD]] feature generation; relational learning; learning; clustering"}, "Penemuan konsep berasaskan kluster untuk pembelajaran relasional statistik Kami menggunakan pengelompokan untuk memperoleh hubungan baru yang menambah skema pangkalan data yang digunakan dalam penjanaan ciri ramalan automatik dalam pembelajaran hubungan statistik. Entiti yang berasal dari kelompok meningkatkan ekspresi ruang ciri dengan mencipta konsep kelas pertama yang menyumbang kepada penciptaan ciri baru. Sebagai contoh, di CiteSeer, makalah dapat dikelompokkan berdasarkan kata-kata atau petikan yang memberikan \"topik\", dan pengarang dapat dikelompokkan berdasarkan dokumen yang mereka bersama-sama memberikan \"komuniti\". Konsep kluster yang dihasilkan menjadi sebahagian daripada ungkapan ciri yang lebih kompleks. Dari sejumlah besar fitur yang dihasilkan, yang meningkatkan ketepatan ramalan disimpan dalam model, seperti yang ditentukan oleh kriteria pemilihan ciri statistik. Kami membentangkan hasil yang menunjukkan peningkatan ketepatan pada dua tugas, ramalan tempat dan ramalan pautan, menggunakan data CiteSeer. [[EENNDD]] penjanaan ciri; pembelajaran hubungan; belajar; pengelompokan"], [{"string": "A DEA approach for model combination This paper proposes a novel Data Envelopment Analysis DEA based approach for model combination . We first prove that for the 2-class classification problems DEA models identify the same convex hull as the popular ROC analysis used for model combination . For general k-class classifiers , we then develop a DEA-based method to combine multiple classifiers . Experiments show that the method outperforms other benchmark methods and suggest that DEA can be a promising tool for model combination .", "keywords": ["data envelopment analysis", "roc", "learning", "model combination"], "combined": "A DEA approach for model combination This paper proposes a novel Data Envelopment Analysis DEA based approach for model combination . We first prove that for the 2-class classification problems DEA models identify the same convex hull as the popular ROC analysis used for model combination . For general k-class classifiers , we then develop a DEA-based method to combine multiple classifiers . Experiments show that the method outperforms other benchmark methods and suggest that DEA can be a promising tool for model combination . [[EENNDD]] data envelopment analysis; roc; learning; model combination"}, "Pendekatan DEA untuk kombinasi model Makalah ini mengusulkan novel Analisis Pembaharuan Data Pendekatan berdasarkan DEA untuk kombinasi model. Kami mula-mula membuktikan bahawa untuk masalah klasifikasi 2 kelas model DEA mengenal pasti lambung cembung yang sama dengan analisis ROC yang popular digunakan untuk kombinasi model. Untuk pengklasifikasi k-class umum, kami kemudian mengembangkan kaedah berasaskan DEA untuk menggabungkan beberapa pengklasifikasi. Eksperimen menunjukkan bahawa kaedah ini mengatasi kaedah penanda aras yang lain dan menunjukkan bahawa DEA dapat menjadi alat yang menjanjikan untuk kombinasi model. [[EENNDD]] analisis envelopment data; roc; belajar; gabungan model"], [{"string": "Unsupervised feature selection for multi-cluster data In many data analysis tasks , one is often confronted with very high dimensional data . Feature selection techniques are designed to find the relevant feature subset of the original features which can facilitate clustering , classification and retrieval . In this paper , we consider the feature selection problem in unsupervised learning scenario , which is particularly difficult due to the absence of class labels that would guide the search for relevant information . The feature selection problem is essentially a combinatorial optimization problem which is computationally expensive . Traditional unsupervised feature selection methods address this issue by selecting the top ranked features based on certain scores computed independently for each feature . These approaches neglect the possible correlation between different features and thus can not produce an optimal feature subset . Inspired from the recent developments on manifold learning and L1-regularized models for subset selection , we propose in this paper a new approach , called Multi-Cluster Feature Selection MCFS , for unsupervised feature selection . Specifically , we select those features such that the multi-cluster structure of the data can be best preserved . The corresponding optimization problem can be efficiently solved since it only involves a sparse eigen-problem and a L1-regularized least squares problem . Extensive experimental results over various real-life data sets have demonstrated the superiority of the proposed algorithm .", "keywords": ["unsupervised", "feature selection", "clustering"], "combined": "Unsupervised feature selection for multi-cluster data In many data analysis tasks , one is often confronted with very high dimensional data . Feature selection techniques are designed to find the relevant feature subset of the original features which can facilitate clustering , classification and retrieval . In this paper , we consider the feature selection problem in unsupervised learning scenario , which is particularly difficult due to the absence of class labels that would guide the search for relevant information . The feature selection problem is essentially a combinatorial optimization problem which is computationally expensive . Traditional unsupervised feature selection methods address this issue by selecting the top ranked features based on certain scores computed independently for each feature . These approaches neglect the possible correlation between different features and thus can not produce an optimal feature subset . Inspired from the recent developments on manifold learning and L1-regularized models for subset selection , we propose in this paper a new approach , called Multi-Cluster Feature Selection MCFS , for unsupervised feature selection . Specifically , we select those features such that the multi-cluster structure of the data can be best preserved . The corresponding optimization problem can be efficiently solved since it only involves a sparse eigen-problem and a L1-regularized least squares problem . Extensive experimental results over various real-life data sets have demonstrated the superiority of the proposed algorithm . [[EENNDD]] unsupervised; feature selection; clustering"}, "Pemilihan fitur yang tidak diawasi untuk data multi-kluster Dalam banyak tugas analisis data, seseorang sering berhadapan dengan data dimensi yang sangat tinggi. Teknik pemilihan ciri dirancang untuk mencari subset ciri yang relevan dari ciri-ciri asli yang dapat memudahkan pengelompokan, klasifikasi dan pengambilan. Dalam makalah ini, kami mempertimbangkan masalah pemilihan ciri dalam senario pembelajaran yang tidak diawasi, yang sangat sukar kerana ketiadaan label kelas yang akan memandu pencarian maklumat yang relevan. Masalah pemilihan ciri pada dasarnya adalah masalah pengoptimuman gabungan yang harganya mahal. Kaedah pemilihan ciri tradisional yang tidak diawasi menangani masalah ini dengan memilih ciri peringkat teratas berdasarkan skor tertentu yang dihitung secara bebas untuk setiap ciri. Pendekatan ini mengabaikan kemungkinan hubungan antara ciri yang berbeza dan dengan itu tidak dapat menghasilkan subset ciri yang optimum. Terinspirasi dari perkembangan baru-baru ini tentang pembelajaran manifold dan model reguler L1 untuk pemilihan subset, kami mencadangkan dalam makalah ini pendekatan baru, yang disebut Multi-Cluster Feature Selection MCFS, untuk pemilihan fitur yang tidak diawasi. Secara khusus, kami memilih ciri-ciri tersebut sehingga struktur data multi-kluster dapat dijaga dengan baik. Masalah pengoptimuman yang sesuai dapat diselesaikan dengan cekap kerana hanya melibatkan masalah eigen yang jarang dan masalah kuadrat terkecil L1. Hasil eksperimen yang meluas terhadap pelbagai kumpulan data kehidupan nyata telah menunjukkan keunggulan algoritma yang dicadangkan. [[EENNDD]] tidak diselia; pemilihan ciri; pengelompokan"], [{"string": "1-dimensional splines as building blocks for improving accuracy of risk outcomes models Transformation of both the response variable and the predictors is commonly used in fitting regression models . However , these transformation methods do not always provide the maximum linear correlation between the response variable and the predictors , especially when there are non-linear relationships between predictors and the response such as the medical data set used in this study . A spline based transformation method is proposed that is second order smooth , continuous , and minimizes the mean squared error between the response and each predictor . Since the computation time for generating this spline is O n , the processing time is reasonable with massive data sets . In contrast to cubic smoothing splines , the resulting transformation equations also display a high level of efficiency for scoring . Data used for predicting health outcomes contains an abundance of non-linear relationships between predictors and the outcomes requiring an algorithm for modeling them accurately . Thus , a transformation that fits an adaptive cubic spline to each of a set of variables is proposed . These curves are used as a set of transformation functions on the predictors . A case study of how the transformed variables can be fed into a simple linear regression model to predict risk outcomes is presented . The results show significant improvement over the performance of the original variables in both linear and non-linear models .", "keywords": ["variable transformation", "data mining", "adaptive", "spline", "learning", "prediction", "outcomes", "risk", "linear model"], "combined": "1-dimensional splines as building blocks for improving accuracy of risk outcomes models Transformation of both the response variable and the predictors is commonly used in fitting regression models . However , these transformation methods do not always provide the maximum linear correlation between the response variable and the predictors , especially when there are non-linear relationships between predictors and the response such as the medical data set used in this study . A spline based transformation method is proposed that is second order smooth , continuous , and minimizes the mean squared error between the response and each predictor . Since the computation time for generating this spline is O n , the processing time is reasonable with massive data sets . In contrast to cubic smoothing splines , the resulting transformation equations also display a high level of efficiency for scoring . Data used for predicting health outcomes contains an abundance of non-linear relationships between predictors and the outcomes requiring an algorithm for modeling them accurately . Thus , a transformation that fits an adaptive cubic spline to each of a set of variables is proposed . These curves are used as a set of transformation functions on the predictors . A case study of how the transformed variables can be fed into a simple linear regression model to predict risk outcomes is presented . The results show significant improvement over the performance of the original variables in both linear and non-linear models . [[EENNDD]] variable transformation; data mining; adaptive; spline; learning; prediction; outcomes; risk; linear model"}, "1-dimensi splines sebagai blok bangunan untuk meningkatkan ketepatan model hasil risiko Transformasi kedua-dua pemboleh ubah tindak balas dan peramal biasanya digunakan dalam pemasangan model regresi. Walau bagaimanapun, kaedah transformasi ini tidak selalu memberikan korelasi linier maksimum antara pemboleh ubah tindak balas dan peramal, terutamanya apabila terdapat hubungan non-linear antara peramal dan tindak balas seperti kumpulan data perubatan yang digunakan dalam kajian ini. Kaedah transformasi berdasarkan spline diusulkan iaitu urutan kedua lancar, berterusan, dan meminimumkan ralat kuasa dua rata antara tindak balas dan setiap peramal. Oleh kerana masa pengiraan untuk menghasilkan spline ini adalah O n, masa pemprosesan adalah wajar dengan set data yang besar. Berbeza dengan spline smoothing cubic, persamaan transformasi yang dihasilkan juga menunjukkan tahap kecekapan yang tinggi untuk mendapatkan markah. Data yang digunakan untuk meramalkan hasil kesihatan mengandungi banyak hubungan tidak linear antara peramal dan hasil yang memerlukan algoritma untuk memodelkannya dengan tepat. Oleh itu, dicadangkan transformasi yang sesuai dengan spline cubic adaptif untuk setiap set pemboleh ubah. Lengkung ini digunakan sebagai sekumpulan fungsi transformasi pada peramal. Satu kajian kes mengenai bagaimana pemboleh ubah yang diubah dapat dimasukkan ke dalam model regresi linear sederhana untuk meramalkan hasil risiko disajikan. Hasilnya menunjukkan peningkatan yang signifikan terhadap prestasi pemboleh ubah asal dalam model linear dan bukan linear. [[EENNDD]] transformasi pemboleh ubah; perlombongan data; adaptif; spline; belajar; ramalan; hasil; risiko; model linear"], [{"string": "Large-scale sparse logistic regression Logistic Regression is a well-known classification method that has been used widely in many applications of data mining , machine learning , computer vision , and bioinformatics . Sparse logistic regression embeds feature selection in the classification framework using the l1-norm regularization , and is attractive in many applications involving high-dimensional data . In this paper , we propose Lassplore for solving large-scale sparse logistic regression . Specifically , we formulate the problem as the l1-ball constrained smooth convex optimization , and propose to solve the problem using the Nesterov 's method , an optimal first-order black-box method for smooth convex optimization . One of the critical issues in the use of the Nesterov 's method is the estimation of the step size at each of the optimization iterations . Previous approaches either applies the constant step size which assumes that the Lipschitz gradient is known in advance , or requires a sequence of decreasing step size which leads to slow convergence in practice . In this paper , we propose an adaptive line search scheme which allows to tune the step size adaptively and meanwhile guarantees the optimal convergence rate . Empirical comparisons with several state-of-the-art algorithms demonstrate the efficiency of the proposed Lassplore algorithm for large-scale problems .", "keywords": ["l1-ball constraint", "logistic regression", "nesterov's method", "adaptive line search", "sparse learning"], "combined": "Large-scale sparse logistic regression Logistic Regression is a well-known classification method that has been used widely in many applications of data mining , machine learning , computer vision , and bioinformatics . Sparse logistic regression embeds feature selection in the classification framework using the l1-norm regularization , and is attractive in many applications involving high-dimensional data . In this paper , we propose Lassplore for solving large-scale sparse logistic regression . Specifically , we formulate the problem as the l1-ball constrained smooth convex optimization , and propose to solve the problem using the Nesterov 's method , an optimal first-order black-box method for smooth convex optimization . One of the critical issues in the use of the Nesterov 's method is the estimation of the step size at each of the optimization iterations . Previous approaches either applies the constant step size which assumes that the Lipschitz gradient is known in advance , or requires a sequence of decreasing step size which leads to slow convergence in practice . In this paper , we propose an adaptive line search scheme which allows to tune the step size adaptively and meanwhile guarantees the optimal convergence rate . Empirical comparisons with several state-of-the-art algorithms demonstrate the efficiency of the proposed Lassplore algorithm for large-scale problems . [[EENNDD]] l1-ball constraint; logistic regression; nesterov's method; adaptive line search; sparse learning"}, "Regresi logistik jarang skala besar Logistik Regresi adalah kaedah klasifikasi terkenal yang telah digunakan secara meluas dalam banyak aplikasi perlombongan data, pembelajaran mesin, penglihatan komputer, dan bioinformatik. Regresi logistik yang jarang menyisipkan pemilihan fitur dalam kerangka klasifikasi menggunakan regularisasi l1-norma, dan menarik dalam banyak aplikasi yang melibatkan data dimensi tinggi. Dalam makalah ini, kami mencadangkan Lassplore untuk menyelesaikan regresi logistik jarang skala besar. Secara khusus, kami merumuskan masalah sebagai pengoptimuman cembung halus terkendali bola l1, dan mengusulkan untuk menyelesaikan masalah dengan menggunakan metode Nesterov, metode kotak hitam orde pertama yang optimum untuk pengoptimuman cembung halus. Salah satu masalah penting dalam penggunaan kaedah Nesterov adalah perkiraan ukuran langkah pada setiap lelaran pengoptimuman. Pendekatan sebelumnya sama ada menggunakan ukuran langkah tetap yang mengandaikan bahawa kecerunan Lipschitz diketahui sebelumnya, atau memerlukan urutan penurunan ukuran langkah yang menyebabkan penumpuan perlahan dalam praktik. Dalam makalah ini, kami mencadangkan skema carian garis adaptif yang memungkinkan untuk menyesuaikan ukuran langkah secara adaptif dan sementara itu menjamin kadar penumpuan yang optimum. Perbandingan empirikal dengan beberapa algoritma canggih menunjukkan kecekapan algoritma Lassplore yang dicadangkan untuk masalah skala besar. [[EENNDD]] Kekangan l1-bola; regresi logistik; kaedah nesterov; carian garis adaptif; pembelajaran yang jarang"], [{"string": "Making every bit count : fast nonlinear axis scaling Existing axis scaling and dimensionality methods focus on preserving structure , usually determined via the Euclidean distance . In other words , they inherently assume that the Euclidean distance is already correct . We instead propose a novel nonlinear approach driven by an information-theoretic viewpoint , which we show is also strongly linked to intrinsic dimensionality , or degrees of freedom ; and uniformity . Nonlinear transformations based on common probability distributions , combined with information-driven selection , simultaneously reduce the number of dimensions required and increase the value of those we retain . Experiments on real data confirm that this approach reveals correlations , finds novel attributes , and scales well .", "keywords": ["computation of transforms"], "combined": "Making every bit count : fast nonlinear axis scaling Existing axis scaling and dimensionality methods focus on preserving structure , usually determined via the Euclidean distance . In other words , they inherently assume that the Euclidean distance is already correct . We instead propose a novel nonlinear approach driven by an information-theoretic viewpoint , which we show is also strongly linked to intrinsic dimensionality , or degrees of freedom ; and uniformity . Nonlinear transformations based on common probability distributions , combined with information-driven selection , simultaneously reduce the number of dimensions required and increase the value of those we retain . Experiments on real data confirm that this approach reveals correlations , finds novel attributes , and scales well . [[EENNDD]] computation of transforms"}, "Membuat setiap kiraan bit: penskalaan paksi bukan linier cepat Kaedah penskalaan dan dimensi paksi yang ada menumpukan pada pemeliharaan struktur, biasanya ditentukan melalui jarak Euclidean. Dengan kata lain, mereka secara semula jadi menganggap bahawa jarak Euclidean sudah betul. Kami sebaliknya mencadangkan pendekatan nonlinear novel yang didorong oleh sudut pandang teori-maklumat, yang kami tunjukkan juga sangat berkaitan dengan dimensi intrinsik, atau tahap kebebasan; dan keseragaman. Transformasi tidak linier berdasarkan pengagihan kebarangkalian umum, digabungkan dengan pemilihan berdasarkan maklumat, sekaligus mengurangkan bilangan dimensi yang diperlukan dan meningkatkan nilai yang kita simpan. Eksperimen pada data sebenar mengesahkan bahawa pendekatan ini menunjukkan korelasi, menemukan atribut novel, dan skala dengan baik. [[EENNDD]] pengiraan transformasi"], [{"string": "Understanding captions in biomedical publications From the standpoint of the automated extraction of scientific knowledge , an important but little-studied part of scientific publications are the figures and accompanying captions . Captions are dense in information , but also contain many extra-grammatical constructs , making them awkward to process with standard information extraction methods . We propose a scheme for `` understanding '' captions in biomedical publications by extracting and classifying `` image pointers '' references to the accompanying image . We evaluate a number of automated methods for this task , including hand-coded methods , methods based on existing learning techniques , and methods based on novel learning techniques . The best of these methods leads to a usefully accurate tool for caption-understanding , with both recall and precision in excess of 94 % on the most important single class in a combined extraction\\/classification task .", "keywords": ["information extraction", "bioinformatics", "information search and retrieval", "learning", "boosting"], "combined": "Understanding captions in biomedical publications From the standpoint of the automated extraction of scientific knowledge , an important but little-studied part of scientific publications are the figures and accompanying captions . Captions are dense in information , but also contain many extra-grammatical constructs , making them awkward to process with standard information extraction methods . We propose a scheme for `` understanding '' captions in biomedical publications by extracting and classifying `` image pointers '' references to the accompanying image . We evaluate a number of automated methods for this task , including hand-coded methods , methods based on existing learning techniques , and methods based on novel learning techniques . The best of these methods leads to a usefully accurate tool for caption-understanding , with both recall and precision in excess of 94 % on the most important single class in a combined extraction\\/classification task . [[EENNDD]] information extraction; bioinformatics; information search and retrieval; learning; boosting"}, "Memahami kapsyen dalam penerbitan bioperubatan Dari sudut pengekstrakan pengetahuan saintifik secara automatik, bahagian penerbitan ilmiah yang penting tetapi tidak banyak dikaji adalah angka dan kapsyen yang menyertainya. Kapsyen padat dalam maklumat, tetapi juga mengandung banyak konstruksi ekstra-tatabahasa, menjadikannya canggung untuk diproses dengan kaedah pengekstrakan maklumat standard. Kami mencadangkan skema untuk \"memahami\" kapsyen dalam penerbitan bioperubatan dengan mengekstrak dan mengklasifikasikan rujukan \"penunjuk gambar\" pada gambar yang disertakan. Kami menilai sebilangan kaedah automatik untuk tugas ini, termasuk kaedah kod tangan, kaedah berdasarkan teknik pembelajaran yang ada, dan kaedah berdasarkan teknik pembelajaran novel. Kaedah terbaik ini membawa kepada alat yang tepat untuk memahami kapsyen, dengan penarikan dan ketepatan melebihi 94% pada kelas tunggal yang paling penting dalam tugas pengekstrakan gabungan / / klasifikasi. [[EENNDD]] pengekstrakan maklumat; bioinformatik; carian dan pengambilan maklumat; belajar; meningkatkan"], [{"string": "Combining linguistic and statistical analysis to extract relations from web documents The World Wide Web provides a nearly endless source of knowledge , which is mostly given in natural language . A first step towards exploiting this data automatically could be to extract pairs of a given semantic relation from text documents - for example all pairs of a person and her birthdate . One strategy for this task is to find text patterns that express the semantic relation , to generalize these patterns , and to apply them to a corpus to find new pairs . In this paper , we show that this approach profits significantly when deep linguistic structures are used instead of surface text patterns . We demonstrate how linguistic structures can be represented for machine learning , and we provide a theoretical analysis of the pattern matching approach . We show the benefits of our approach by extensive experiments with our prototype system LEILA .", "keywords": ["relation extraction", "learning", "machine learning", "pattern matching"], "combined": "Combining linguistic and statistical analysis to extract relations from web documents The World Wide Web provides a nearly endless source of knowledge , which is mostly given in natural language . A first step towards exploiting this data automatically could be to extract pairs of a given semantic relation from text documents - for example all pairs of a person and her birthdate . One strategy for this task is to find text patterns that express the semantic relation , to generalize these patterns , and to apply them to a corpus to find new pairs . In this paper , we show that this approach profits significantly when deep linguistic structures are used instead of surface text patterns . We demonstrate how linguistic structures can be represented for machine learning , and we provide a theoretical analysis of the pattern matching approach . We show the benefits of our approach by extensive experiments with our prototype system LEILA . [[EENNDD]] relation extraction; learning; machine learning; pattern matching"}, "Menggabungkan analisis linguistik dan statistik untuk mengekstrak hubungan dari dokumen web World Wide Web menyediakan sumber pengetahuan yang hampir tidak berkesudahan, yang kebanyakannya diberikan dalam bahasa semula jadi. Langkah pertama untuk mengeksploitasi data ini secara automatik adalah mengekstrak pasangan hubungan semantik yang diberikan dari dokumen teks - misalnya semua pasangan seseorang dan tarikh lahirnya. Salah satu strategi untuk tugas ini adalah mencari corak teks yang mengekspresikan hubungan semantik, menggeneralisasikan pola-pola ini, dan menerapkannya ke korpus untuk mencari pasangan baru. Dalam makalah ini, kami menunjukkan bahawa pendekatan ini mendapat keuntungan yang besar apabila struktur linguistik yang mendalam digunakan dan bukannya corak teks permukaan. Kami menunjukkan bagaimana struktur linguistik dapat direpresentasikan untuk pembelajaran mesin, dan kami memberikan analisis teori mengenai pendekatan pencocokan pola. Kami menunjukkan faedah pendekatan kami dengan eksperimen yang luas dengan sistem prototaip kami LEILA. [[EENNDD]] pengekstrakan hubungan; belajar; pembelajaran mesin; padanan corak"], [{"string": "Depth first generation of long patterns", "keywords": ["association rules"], "combined": "Depth first generation of long patterns [[EENNDD]] association rules"}, "Kedalaman generasi pertama corak panjang [[EENNDD]] peraturan persatuan"], [{"string": "Efficient mining of weighted association rules WAR", "keywords": ["ordered shrinkage", "weighted association rules", "database applications"], "combined": "Efficient mining of weighted association rules WAR [[EENNDD]] ordered shrinkage; weighted association rules; database applications"}, "Perlombongan peraturan persatuan berwajaran yang cekap WAR [[EENNDD]] memerintahkan pengecutan; peraturan persatuan berwajaran; aplikasi pangkalan data"], [{"string": "Hierarchical topic segmentation of websites In this paper , we consider the problem of identifying and segmenting topically cohesive regions in the URL tree of a large website . Each page of the website is assumed to have a topic label or a distribution on topic labels generated using a standard classifier . We develop a set of cost measures characterizing the benefit accrued by introducing a segmentation of the site based on the topic labels . We propose a general framework to use these measures for describing the quality of a segmentation ; we also provide an efficient algorithm to find the best segmentation in this framework . Extensive experiments on human-labeled data confirm the soundness of our framework and suggest that a judicious choice of cost measures allows the algorithm to perform surprisingly accurate topical segmentations .", "keywords": ["classification", "information search and retrieval", "gain ratio", "kl-distance", "website segmentation", "tree partitioning", "facility location", "website hierarchy"], "combined": "Hierarchical topic segmentation of websites In this paper , we consider the problem of identifying and segmenting topically cohesive regions in the URL tree of a large website . Each page of the website is assumed to have a topic label or a distribution on topic labels generated using a standard classifier . We develop a set of cost measures characterizing the benefit accrued by introducing a segmentation of the site based on the topic labels . We propose a general framework to use these measures for describing the quality of a segmentation ; we also provide an efficient algorithm to find the best segmentation in this framework . Extensive experiments on human-labeled data confirm the soundness of our framework and suggest that a judicious choice of cost measures allows the algorithm to perform surprisingly accurate topical segmentations . [[EENNDD]] classification; information search and retrieval; gain ratio; kl-distance; website segmentation; tree partitioning; facility location; website hierarchy"}, "Segmentasi topik hierarki laman web Dalam makalah ini, kami mempertimbangkan masalah mengenal pasti dan menyegmentasikan kawasan-kawasan yang kohesif secara topikal di pohon URL laman web besar. Setiap halaman laman web diasumsikan mempunyai label topik atau sebaran pada label topik yang dihasilkan menggunakan pengkelasan standard. Kami mengembangkan satu set ukuran kos yang mencirikan keuntungan yang diperoleh dengan memperkenalkan segmentasi laman web berdasarkan label topik. Kami mencadangkan kerangka umum untuk menggunakan langkah-langkah ini untuk menggambarkan kualiti segmentasi; kami juga menyediakan algoritma yang cekap untuk mencari segmentasi terbaik dalam kerangka ini. Eksperimen yang meluas pada data berlabel manusia mengesahkan kewajaran kerangka kami dan menunjukkan bahawa pilihan ukuran kos yang bijaksana membolehkan algoritma melakukan segmentasi topikal yang sangat tepat. [[EENNDD]] klasifikasi; carian dan pengambilan maklumat; nisbah keuntungan; jarak kl; segmentasi laman web; pembahagian pokok; lokasi kemudahan; hierarki laman web"], [{"string": "Prominent streak discovery in sequence data This paper studies the problem of prominent streak discovery in sequence data . Given a sequence of values , a prominent streak is a long consecutive subsequence consisting of only large small values . For finding prominent streaks , we make the observation that prominent streaks are skyline points in two dimensions - streak interval length and minimum value in the interval . Our solution thus hinges upon the idea to separate the two steps in prominent streak discovery ' candidate streak generation and skyline operation over candidate streaks . For candidate generation , we propose the concept of local prominent streak LPS . We prove that prominent streaks are a subset of LPSs and the number of LPSs is less than the length of a data sequence , in comparison with the quadratic number of candidates produced by a brute-force baseline method . We develop efficient algorithms based on the concept of LPS . The non-linear LPS-based method NLPS considers a superset of LPSs as candidates , and the linear LPS-based method LLPS further guarantees to consider only LPSs . The results of experiments using multiple real datasets verified the effectiveness of the proposed methods and showed orders of magnitude performance improvement against the baseline method .", "keywords": ["time-series database", "sequence database", "skyline query"], "combined": "Prominent streak discovery in sequence data This paper studies the problem of prominent streak discovery in sequence data . Given a sequence of values , a prominent streak is a long consecutive subsequence consisting of only large small values . For finding prominent streaks , we make the observation that prominent streaks are skyline points in two dimensions - streak interval length and minimum value in the interval . Our solution thus hinges upon the idea to separate the two steps in prominent streak discovery ' candidate streak generation and skyline operation over candidate streaks . For candidate generation , we propose the concept of local prominent streak LPS . We prove that prominent streaks are a subset of LPSs and the number of LPSs is less than the length of a data sequence , in comparison with the quadratic number of candidates produced by a brute-force baseline method . We develop efficient algorithms based on the concept of LPS . The non-linear LPS-based method NLPS considers a superset of LPSs as candidates , and the linear LPS-based method LLPS further guarantees to consider only LPSs . The results of experiments using multiple real datasets verified the effectiveness of the proposed methods and showed orders of magnitude performance improvement against the baseline method . [[EENNDD]] time-series database; sequence database; skyline query"}, "Penemuan coretan yang menonjol dalam data urutan Makalah ini mengkaji masalah penemuan rentetan yang menonjol dalam data urutan. Diberi urutan nilai, rentetan yang menonjol adalah susunan yang berturut-turut yang terdiri daripada hanya nilai-nilai kecil yang besar. Untuk mencari garis-garis yang menonjol, kami membuat pemerhatian bahawa garis-garis yang menonjol adalah titik langit dalam dua dimensi - panjang selang garis dan nilai minimum dalam selang. Oleh itu, penyelesaian kami bergantung pada idea untuk memisahkan dua langkah dalam penjanaan corak penemuan calon dan operasi cakrawala berbanding garis calon. Untuk generasi calon, kami mencadangkan konsep LPS bertaraf tempatan. Kami membuktikan bahawa garis-garis yang menonjol adalah subset dari LPS dan bilangan LPS kurang dari panjang urutan data, dibandingkan dengan bilangan kuadratik calon yang dihasilkan dengan kaedah garis dasar kekuatan kasar. Kami mengembangkan algoritma yang cekap berdasarkan konsep LPS. Kaedah berasaskan LPS bukan linear NLPS menganggap superset LPS sebagai calon, dan kaedah berasaskan LPS linear LLPS lebih menjamin untuk mempertimbangkan hanya LPS. Hasil eksperimen menggunakan banyak set data nyata mengesahkan keberkesanan kaedah yang dicadangkan dan menunjukkan peningkatan prestasi prestasi terhadap kaedah asas. [[EENNDD]] pangkalan data siri masa; pangkalan data urutan; pertanyaan latar langit"], [{"string": "Combining email models for false positive reduction Machine learning and data mining can be effectively used to model , classify and discover interesting information for a wide variety of data including email . The Email Mining Toolkit , EMT , has been designed to provide a wide range of analyses for arbitrary email sources . Depending upon the task , one can usually achieve very high accuracy , but with some amount of false positive tradeoff . Generally false positives are prohibitively expensive in the real world . In the case of spam detection , for example , even if one email is misclassified , this may be unacceptable if it is a very important email . Much work has been done to improve specific algorithms for the task of detecting unwanted messages , but less work has been report on leveraging multiple algorithms and correlating models in this particular domain of email analysis . EMT has been updated with new correlation functions allowing the analyst to integrate a number of EMT 's user behavior models available in the core technology . We present results of combining classifier outputs for improving both accuracy and reducing false positives for the problem of spam detection . We apply these methods to a very large email data set and show results of different combination methods on these corpora . We introduce a new method to compare multiple and combined classifiers , and show how it differs from past work . The method analyzes the relative gain and maximum possible accuracy that can be achieved for certain combinations of classifiers to automatically choose the best combination .", "keywords": ["data mining", "multiple classifiers", "email mining", "spam", "aggregators", "false positive reduction", "model combination"], "combined": "Combining email models for false positive reduction Machine learning and data mining can be effectively used to model , classify and discover interesting information for a wide variety of data including email . The Email Mining Toolkit , EMT , has been designed to provide a wide range of analyses for arbitrary email sources . Depending upon the task , one can usually achieve very high accuracy , but with some amount of false positive tradeoff . Generally false positives are prohibitively expensive in the real world . In the case of spam detection , for example , even if one email is misclassified , this may be unacceptable if it is a very important email . Much work has been done to improve specific algorithms for the task of detecting unwanted messages , but less work has been report on leveraging multiple algorithms and correlating models in this particular domain of email analysis . EMT has been updated with new correlation functions allowing the analyst to integrate a number of EMT 's user behavior models available in the core technology . We present results of combining classifier outputs for improving both accuracy and reducing false positives for the problem of spam detection . We apply these methods to a very large email data set and show results of different combination methods on these corpora . We introduce a new method to compare multiple and combined classifiers , and show how it differs from past work . The method analyzes the relative gain and maximum possible accuracy that can be achieved for certain combinations of classifiers to automatically choose the best combination . [[EENNDD]] data mining; multiple classifiers; email mining; spam; aggregators; false positive reduction; model combination"}, "Menggabungkan model e-mel untuk pengurangan positif palsu Pembelajaran mesin dan perlombongan data dapat digunakan dengan berkesan untuk memodelkan, mengklasifikasikan dan menemui maklumat menarik untuk pelbagai data termasuk e-mel. Toolkit Perlombongan E-mel, EMT, telah dirancang untuk menyediakan pelbagai analisis untuk sumber e-mel sewenang-wenangnya. Bergantung pada tugas, seseorang biasanya dapat mencapai ketepatan yang sangat tinggi, tetapi dengan sejumlah pertukaran positif palsu. Umumnya, positif palsu sangat mahal di dunia nyata. Dalam kes pengesanan spam, misalnya, walaupun satu e-mel salah diklasifikasikan, ini mungkin tidak dapat diterima jika e-mel itu sangat penting. Banyak pekerjaan telah dilakukan untuk meningkatkan algoritma khusus untuk tugas mengesan pesan yang tidak diingini, tetapi lebih sedikit pekerjaan yang dilaporkan mengenai memanfaatkan beberapa algoritma dan model korelasi dalam domain analisis e-mel ini. EMT telah diperbaharui dengan fungsi korelasi baru yang memungkinkan penganalisis untuk mengintegrasikan sejumlah model tingkah laku pengguna EMT yang tersedia dalam teknologi teras. Kami menyajikan hasil menggabungkan output pengkelasan untuk meningkatkan ketepatan dan mengurangkan positif palsu untuk masalah pengesanan spam. Kami menerapkan kaedah ini pada kumpulan data e-mel yang sangat besar dan menunjukkan hasil kaedah kombinasi yang berbeza pada korporat ini. Kami memperkenalkan kaedah baru untuk membandingkan pengkelasan berganda dan gabungan, dan menunjukkan bagaimana ia berbeza dengan kerja masa lalu. Kaedah menganalisis keuntungan relatif dan ketepatan maksimum yang dapat dicapai untuk kombinasi pengklasifikasi tertentu untuk memilih kombinasi terbaik secara automatik. [[EENNDD]] perlombongan data; pelbagai pengelasan; perlombongan e-mel; spam; pengagregat; pengurangan positif palsu; gabungan model"], [{"string": "CLOSET + : searching for the best strategies for mining frequent closed itemsets Mining frequent closed itemsets provides complete and non-redundant results for frequent pattern analysis . Extensive studies have proposed various strategies for efficient frequent closed itemset mining , such as depth-first search vs. breadthfirst search , vertical formats vs. horizontal formats , tree-structure vs. other data structures , top-down vs. bottom-up traversal , pseudo projection vs. physical projection of conditional database , etc. . It is the right time to ask `` what are the pros and cons of the strategies ? '' and `` what and how can we pick and integrate the best strategies to achieve higher performance in general cases ? `` In this study , we answer the above questions by a systematic study of the search strategies and develop a winning algorithm CLOSET + . CLOSET + integrates the advantages of the previously proposed effective strategies as well as some ones newly developed here . A thorough performance study on synthetic and real data sets has shown the advantages of the strategies and the improvement of CLOSET + over existing mining algorithms , including CLOSET , CHARM and OP , in terms of runtime , memory usage and scalability .", "keywords": ["mining methods and algorithms", "frequent closed itemsets", "association rules"], "combined": "CLOSET + : searching for the best strategies for mining frequent closed itemsets Mining frequent closed itemsets provides complete and non-redundant results for frequent pattern analysis . Extensive studies have proposed various strategies for efficient frequent closed itemset mining , such as depth-first search vs. breadthfirst search , vertical formats vs. horizontal formats , tree-structure vs. other data structures , top-down vs. bottom-up traversal , pseudo projection vs. physical projection of conditional database , etc. . It is the right time to ask `` what are the pros and cons of the strategies ? '' and `` what and how can we pick and integrate the best strategies to achieve higher performance in general cases ? `` In this study , we answer the above questions by a systematic study of the search strategies and develop a winning algorithm CLOSET + . CLOSET + integrates the advantages of the previously proposed effective strategies as well as some ones newly developed here . A thorough performance study on synthetic and real data sets has shown the advantages of the strategies and the improvement of CLOSET + over existing mining algorithms , including CLOSET , CHARM and OP , in terms of runtime , memory usage and scalability . [[EENNDD]] mining methods and algorithms; frequent closed itemsets; association rules"}, "CLOSET +: mencari strategi terbaik untuk melombong set barang yang sering ditutup Melombong set barang yang kerap ditutup memberikan hasil yang lengkap dan tidak berlebihan untuk analisis corak yang kerap. Kajian yang meluas telah mencadangkan pelbagai strategi untuk perlombongan itemet yang sering ditutup dengan cekap, seperti carian pertama-mendalam berbanding carian pertama lebar, format menegak vs format mendatar, struktur pokok berbanding struktur data lain, traversal atas-bawah vs bawah-atas, unjuran palsu vs unjuran fizikal pangkalan data bersyarat, dll. Ini adalah waktu yang tepat untuk bertanya \"apa kelebihan dan kekurangan strategi? \"dan\" apa dan bagaimana kita dapat memilih dan menggabungkan strategi terbaik untuk mencapai prestasi yang lebih tinggi dalam kes umum? \"Dalam kajian ini, kami menjawab soalan di atas dengan kajian sistematik strategi pencarian dan mengembangkan algoritma pemenang CLOSET +. CLOSET + menggabungkan kelebihan strategi berkesan yang dicadangkan sebelumnya dan juga beberapa strategi yang baru dibangunkan di sini. Kajian prestasi menyeluruh pada set data sintetik dan nyata telah menunjukkan kelebihan strategi dan peningkatan CLOSET + berbanding algoritma perlombongan yang ada, termasuk CLOSET, CHARM dan OP, dari segi jangka masa, penggunaan memori dan skalabiliti. [[EENNDD]] kaedah dan algoritma perlombongan; set barang yang kerap ditutup; peraturan persatuan"], [{"string": "Evolutionary spectral clustering by incorporating temporal smoothness Evolutionary clustering is an emerging research area essential to important applications such as clustering dynamic Web and blog contents and clustering data streams . In evolutionary clustering , a good clustering result should fit the current data well , while simultaneously not deviate too dramatically from the recent history . To fulfill this dual purpose , a measure of temporal smoothness is integrated in the overall measure of clustering quality . In this paper , we propose two frameworks that incorporate temporal smoothness in evolutionary spectral clustering . For both frameworks , we start with intuitions gained from the well-known k-means clustering problem , and then propose and solve corresponding cost functions for the evolutionary spectral clustering problems . Our solutions to the evolutionary spectral clustering problems provide more stable and consistent clustering results that are less sensitive to short-term noises while at the same time are adaptive to long-term cluster drifts . Furthermore , we demonstrate that our methods provide the optimal solutions to the relaxed versions of the corresponding evolutionary k-means clustering problems . Performance experiments over a number of real and synthetic data sets illustrate our evolutionary spectral clustering methods provide more robust clustering results that are not sensitive to noise and can adapt to data drifts .", "keywords": ["temporal smoothness", "evolutionary spectral clustering", "mining data streams", "preserving cluster membership", "treserving cluster quality"], "combined": "Evolutionary spectral clustering by incorporating temporal smoothness Evolutionary clustering is an emerging research area essential to important applications such as clustering dynamic Web and blog contents and clustering data streams . In evolutionary clustering , a good clustering result should fit the current data well , while simultaneously not deviate too dramatically from the recent history . To fulfill this dual purpose , a measure of temporal smoothness is integrated in the overall measure of clustering quality . In this paper , we propose two frameworks that incorporate temporal smoothness in evolutionary spectral clustering . For both frameworks , we start with intuitions gained from the well-known k-means clustering problem , and then propose and solve corresponding cost functions for the evolutionary spectral clustering problems . Our solutions to the evolutionary spectral clustering problems provide more stable and consistent clustering results that are less sensitive to short-term noises while at the same time are adaptive to long-term cluster drifts . Furthermore , we demonstrate that our methods provide the optimal solutions to the relaxed versions of the corresponding evolutionary k-means clustering problems . Performance experiments over a number of real and synthetic data sets illustrate our evolutionary spectral clustering methods provide more robust clustering results that are not sensitive to noise and can adapt to data drifts . [[EENNDD]] temporal smoothness; evolutionary spectral clustering; mining data streams; preserving cluster membership; treserving cluster quality"}, "Pengelompokan spektrum evolusi dengan menggabungkan kelancaran temporal Pengelompokan evolusi adalah bidang penyelidikan yang muncul yang penting untuk aplikasi penting seperti pengelompokan isi Web dan blog yang dinamis dan pengelompokan aliran data. Dalam pengelompokan evolusi, hasil pengelompokan yang baik harus sesuai dengan data semasa dengan baik, dan pada masa yang sama tidak menyimpang terlalu dramatik dari sejarah baru-baru ini. Untuk memenuhi tujuan ganda ini, ukuran kelancaran temporal disatukan dalam ukuran kualiti pengelompokan secara keseluruhan. Dalam makalah ini, kami mencadangkan dua kerangka kerja yang menggabungkan kelancaran temporal dalam pengelompokan spektrum evolusi. Untuk kedua kerangka kerja, kita mulai dengan intuisi yang diperoleh dari masalah pengelompokan k-berarti yang terkenal, dan kemudian mencadangkan dan menyelesaikan fungsi kos yang sesuai untuk masalah pengelompokan spektrum evolusi. Penyelesaian kami untuk masalah pengelompokan spektrum evolusi memberikan hasil pengelompokan yang lebih stabil dan konsisten yang kurang sensitif terhadap bunyi jangka pendek dan pada masa yang sama dapat menyesuaikan diri dengan pergeseran kelompok jangka panjang. Selanjutnya, kami menunjukkan bahawa kaedah kami memberikan penyelesaian yang optimum untuk versi santai dari masalah pengelompokan k-berarti evolusi yang sesuai. Eksperimen prestasi ke atas sejumlah set data nyata dan sintetik menggambarkan kaedah pengelompokan spektrum evolusi kami memberikan hasil pengelompokan yang lebih kuat yang tidak sensitif terhadap bunyi bising dan dapat menyesuaikan diri dengan peralihan data. [[EENNDD]] kelancaran temporal; pengelompokan spektrum evolusi; aliran data perlombongan; mengekalkan keanggotaan kluster; kualiti kluster yang memuaskan"], [{"string": "A framework for ontology-driven subspace clustering Traditional clustering is a descriptive task that seeks to identify homogeneous groups of objects based on the values of their attributes . While domain knowledge is always the best way to justify clustering , few clustering algorithms have ever take domain knowledge into consideration . In this paper , the domain knowledge is represented by hierarchical ontology . We develop a framework by directly incorporating domain knowledge into clustering process , yielding a set of clusters with strong ontology implication . During the clustering process , ontology information is utilized to efficiently prune the exponential search space of the subspace clustering algorithms . Meanwhile , the algorithm generates automatical interpretation of the clustering result by mapping the natural hierarchical organized subspace clusters with significant categorical enrichment onto the ontology hierarchy . Our experiments on a set of gene expression data using gene ontology demonstrate that our pruning technique driven by ontology significantly improve the clustering performance with minimal degradation of the cluster quality . Meanwhile , many hierarchical organizations of gene clusters corresponding to a sub-hierarchies in gene ontology were also successfully captured .", "keywords": ["ontology", "subspace clustering", "tendency preserving"], "combined": "A framework for ontology-driven subspace clustering Traditional clustering is a descriptive task that seeks to identify homogeneous groups of objects based on the values of their attributes . While domain knowledge is always the best way to justify clustering , few clustering algorithms have ever take domain knowledge into consideration . In this paper , the domain knowledge is represented by hierarchical ontology . We develop a framework by directly incorporating domain knowledge into clustering process , yielding a set of clusters with strong ontology implication . During the clustering process , ontology information is utilized to efficiently prune the exponential search space of the subspace clustering algorithms . Meanwhile , the algorithm generates automatical interpretation of the clustering result by mapping the natural hierarchical organized subspace clusters with significant categorical enrichment onto the ontology hierarchy . Our experiments on a set of gene expression data using gene ontology demonstrate that our pruning technique driven by ontology significantly improve the clustering performance with minimal degradation of the cluster quality . Meanwhile , many hierarchical organizations of gene clusters corresponding to a sub-hierarchies in gene ontology were also successfully captured . [[EENNDD]] ontology; subspace clustering; tendency preserving"}, "Kerangka kerja untuk pengelompokan ruang bawah yang didorong oleh ontologi Pengelompokan tradisional adalah tugas deskriptif yang bertujuan untuk mengenal pasti kumpulan objek yang homogen berdasarkan nilai atributnya. Walaupun pengetahuan domain selalu merupakan kaedah terbaik untuk membenarkan pengelompokan, beberapa algoritma pengelompokan pernah mempertimbangkan pengetahuan domain. Dalam makalah ini, pengetahuan domain diwakili oleh ontologi hierarki. Kami mengembangkan kerangka kerja dengan memasukkan pengetahuan domain secara langsung ke dalam proses pengelompokan, menghasilkan sekumpulan kelompok dengan implikasi ontologi yang kuat. Selama proses pengelompokan, informasi ontologi digunakan untuk memangkas ruang pencarian eksponensial dari algoritma pengelompokan ruang bawah secara efisien. Sementara itu, algoritma menghasilkan interpretasi automatik hasil pengelompokan dengan memetakan kelompok ruang bawah tanah yang disusun secara hierarki semula jadi dengan pengayaan kategori yang signifikan ke dalam hierarki ontologi. Eksperimen kami pada sekumpulan data ekspresi gen menggunakan ontologi gen menunjukkan bahawa teknik pemangkasan kami yang didorong oleh ontologi meningkatkan prestasi pengelompokan secara signifikan dengan penurunan kualiti kluster yang minimum. Sementara itu, banyak organisasi hierarki kelompok gen yang sesuai dengan sub-hierarki dalam ontologi gen juga berjaya ditangkap. [[EENNDD]] ontologi; pengelompokan ruang bawah tanah; kecenderungan memelihara"], [{"string": "Inferring networks of diffusion and influence Information diffusion and virus propagation are fundamental processes talking place in networks . While it is often possible to directly observe when nodes become infected , observing individual transmissions i.e. , who infects whom or who influences whom is typically very difficult . Furthermore , in many applications , the underlying network over which the diffusions and propagations spread is actually unobserved . We tackle these challenges by developing a method for tracing paths of diffusion and influence through networks and inferring the networks over which contagions propagate . Given the times when nodes adopt pieces of information or become infected , we identify the optimal network that best explains the observed infection times . Since the optimization problem is NP-hard to solve exactly , we develop an efficient approximation algorithm that scales to large datasets and in practice gives provably near-optimal performance . We demonstrate the effectiveness of our approach by tracing information cascades in a set of 170 million blogs and news articles over a one year period to infer how information flows through the online media space . We find that the diffusion network of news tends to have a core-periphery structure with a small set of core media sites that diffuse information to the rest of the Web . These sites tend to have stable circles of influence with more general news media sites acting as connectors between them .", "keywords": ["news media", "information cascades", "social networks", "meme-tracking", "blogs", "networks of diffusion"], "combined": "Inferring networks of diffusion and influence Information diffusion and virus propagation are fundamental processes talking place in networks . While it is often possible to directly observe when nodes become infected , observing individual transmissions i.e. , who infects whom or who influences whom is typically very difficult . Furthermore , in many applications , the underlying network over which the diffusions and propagations spread is actually unobserved . We tackle these challenges by developing a method for tracing paths of diffusion and influence through networks and inferring the networks over which contagions propagate . Given the times when nodes adopt pieces of information or become infected , we identify the optimal network that best explains the observed infection times . Since the optimization problem is NP-hard to solve exactly , we develop an efficient approximation algorithm that scales to large datasets and in practice gives provably near-optimal performance . We demonstrate the effectiveness of our approach by tracing information cascades in a set of 170 million blogs and news articles over a one year period to infer how information flows through the online media space . We find that the diffusion network of news tends to have a core-periphery structure with a small set of core media sites that diffuse information to the rest of the Web . These sites tend to have stable circles of influence with more general news media sites acting as connectors between them . [[EENNDD]] news media; information cascades; social networks; meme-tracking; blogs; networks of diffusion"}, "Menyimpulkan rangkaian penyebaran dan pengaruh Penyebaran maklumat dan penyebaran virus adalah proses asas yang menjadi tumpuan dalam rangkaian. Walaupun sering dapat dilihat secara langsung ketika nod dijangkiti, memerhatikan penularan individu iaitu, siapa yang menjangkiti siapa atau yang mempengaruhi siapa yang biasanya sangat sukar. Selanjutnya, dalam banyak aplikasi, rangkaian yang mendasari penyebaran dan penyebaran penyebaran sebenarnya tidak disedari. Kami menangani cabaran ini dengan mengembangkan metode untuk menelusuri jalan penyebaran dan pengaruh melalui jaringan dan menyimpulkan jaringan yang menyebarkan penularan. Memandangkan masa ketika nod menggunakan maklumat atau dijangkiti, kami mengenal pasti rangkaian optimum yang paling baik menerangkan masa jangkitan yang diperhatikan. Oleh kerana masalah pengoptimuman NP-sukar untuk diselesaikan dengan tepat, kami mengembangkan algoritma penghampiran yang efisien yang menimbang ke set data yang besar dan dalam praktiknya memberikan prestasi yang hampir hampir optimum. Kami menunjukkan keberkesanan pendekatan kami dengan menelusuri lata maklumat dalam satu set 170 juta blog dan artikel berita dalam jangka masa satu tahun untuk membuat kesimpulan bagaimana maklumat mengalir melalui ruang media dalam talian. Kami mendapati bahawa rangkaian penyebaran berita cenderung mempunyai struktur inti-pinggir dengan sekumpulan kecil laman media teras yang menyebarkan maklumat ke seluruh Web. Laman web ini cenderung mempunyai lingkaran pengaruh yang stabil dengan laman media berita yang lebih umum bertindak sebagai penghubung di antara mereka. [[EENNDD]] media berita; lata maklumat; rangkaian sosial; meme-tracking; blog; rangkaian penyebaran"], [{"string": "Anonymizing transaction databases for publication This paper considers the problem of publishing `` transaction data '' for research purposes . Each transaction is an arbitrary set of items chosen from a large universe . Detailed transaction data provides an electronic image of one 's life . This has two implications . One , transaction data are excellent candidates for data mining research . Two , use of transaction data would raise serious concerns over individual privacy . Therefore , before transaction data is released for data mining , it must be made anonymous so that data subjects can not be re-identified . The challenge is that transaction data has no structure and can be extremely high dimensional . Traditional anonymization methods lose too much information on such data . To date , there has been no satisfactory privacy notion and solution proposed for anonymizing transaction data . This paper proposes one way to address this issue .", "keywords": ["anonymity", "data publishing", "transaction database"], "combined": "Anonymizing transaction databases for publication This paper considers the problem of publishing `` transaction data '' for research purposes . Each transaction is an arbitrary set of items chosen from a large universe . Detailed transaction data provides an electronic image of one 's life . This has two implications . One , transaction data are excellent candidates for data mining research . Two , use of transaction data would raise serious concerns over individual privacy . Therefore , before transaction data is released for data mining , it must be made anonymous so that data subjects can not be re-identified . The challenge is that transaction data has no structure and can be extremely high dimensional . Traditional anonymization methods lose too much information on such data . To date , there has been no satisfactory privacy notion and solution proposed for anonymizing transaction data . This paper proposes one way to address this issue . [[EENNDD]] anonymity; data publishing; transaction database"}, "Pangkalan data transaksi tanpa nama untuk diterbitkan Makalah ini mempertimbangkan masalah penerbitan \"data transaksi\" untuk tujuan penyelidikan. Setiap transaksi adalah sekumpulan barang sewenang-wenangnya yang dipilih dari alam semesta yang besar. Data transaksi terperinci memberikan gambaran elektronik kehidupan seseorang. Ini mempunyai dua implikasi. Satu, data transaksi adalah calon yang sangat baik untuk penyelidikan perlombongan data. Dua, penggunaan data transaksi akan menimbulkan kebimbangan serius terhadap privasi individu. Oleh itu, sebelum data transaksi dikeluarkan untuk perlombongan data, data tersebut mesti dibuat tanpa nama sehingga subjek data tidak dapat dikenali semula. Tantangannya ialah data transaksi tidak mempunyai struktur dan dimensi yang sangat tinggi. Kaedah anonimisasi tradisional kehilangan terlalu banyak maklumat mengenai data tersebut. Hingga kini, belum ada gagasan privasi yang memuaskan dan penyelesaian yang dicadangkan untuk menganonimkan data transaksi. Makalah ini mencadangkan satu cara untuk mengatasi masalah ini. [[EENNDD]] tanpa nama; penerbitan data; pangkalan data transaksi"], [{"string": "Bias and controversy : beyond the statistical deviation In this paper , we investigate how deviation in evaluation activities may reveal bias on the part of reviewers and controversy on the part of evaluated objects . We focus on a ` data-centric approach ' where the evaluation data is assumed to represent the ` ground truth ' . The standard statistical approaches take evaluation and deviation at face value . We argue that attention should be paid to the subjectivity of evaluation , judging the evaluation score not just on ` what is being said ' deviation , but also on ` who says it ' reviewer as well as on ` whom it is said about ' object . Furthermore , we observe that bias and controversy are mutually dependent , as there is more bias if there is higher deviation on a less controversial object . To address this mutual dependency , we propose a reinforcement model to identify bias and controversy . We test our model on real-life data to verify its applicability .", "keywords": ["controversy", "bias", "social and behavioral sciences", "evaluation", "information systems applications", "social network"], "combined": "Bias and controversy : beyond the statistical deviation In this paper , we investigate how deviation in evaluation activities may reveal bias on the part of reviewers and controversy on the part of evaluated objects . We focus on a ` data-centric approach ' where the evaluation data is assumed to represent the ` ground truth ' . The standard statistical approaches take evaluation and deviation at face value . We argue that attention should be paid to the subjectivity of evaluation , judging the evaluation score not just on ` what is being said ' deviation , but also on ` who says it ' reviewer as well as on ` whom it is said about ' object . Furthermore , we observe that bias and controversy are mutually dependent , as there is more bias if there is higher deviation on a less controversial object . To address this mutual dependency , we propose a reinforcement model to identify bias and controversy . We test our model on real-life data to verify its applicability . [[EENNDD]] controversy; bias; social and behavioral sciences; evaluation; information systems applications; social network"}, "Bias dan kontroversi: melangkaui penyimpangan statistik Dalam makalah ini, kami menyiasat bagaimana penyimpangan dalam aktiviti penilaian dapat menunjukkan bias di pihak pengulas dan kontroversi di pihak objek yang dinilai. Kami fokus pada pendekatan data-centric di mana data penilaian dianggap mewakili 'kebenaran dasar'. Pendekatan statistik standard mengambil penilaian dan penyimpangan pada nilai muka. Kami berpendapat bahawa perhatian harus diberikan kepada subjektiviti penilaian, menilai skor penilaian bukan hanya pada penyimpangan \"apa yang dikatakan\", tetapi juga pada pemeriksa \"siapa yang mengatakannya\" dan juga pada objek \"siapa yang dikatakan tentang\". Selanjutnya, kita melihat bahawa bias dan kontroversi saling bergantung, kerana ada lebih banyak bias jika ada penyimpangan yang lebih tinggi pada objek yang kurang kontroversial. Untuk mengatasi saling bergantung ini, kami mencadangkan model pengukuhan untuk mengenal pasti bias dan kontroversi. Kami menguji model kami pada data kehidupan sebenar untuk mengesahkan kesesuaiannya. [[EENNDD]] kontroversi; berat sebelah; sains sosial dan tingkah laku; penilaian; aplikasi sistem maklumat; rangkaian sosial"], [{"string": "Co-clustering based classification for out-of-domain documents In many real world applications , labeled data are in short supply . It often happens that obtaining labeled data in a new domain is expensive and time consuming , while there may be plenty of labeled data from a related but different domain . Traditional machine learning is not able to cope well with learning across different domains . In this paper , we address this problem for a text-mining task , where the labeled data are under one distribution in one domain known as in-domain data , while the unlabeled data are under a related but different domain known as out-of-domain data . Our general goal is to learn from the in-domain and apply the learned knowledge to out-of-domain . We propose a co-clustering based classification CoCC algorithm to tackle this problem . Co-clustering is used as a bridge to propagate the class structure and knowledge from the in-domain to the out-of-domain . We present theoretical and empirical analysis to show that our algorithm is able to produce high quality classification results , even when the distributions between the two data are different . The experimental results show that our algorithm greatly improves the classification performance over the traditional learning algorithms .", "keywords": ["co-clustering", "out-of-domain", "kullback-leibler divergence", "classification"], "combined": "Co-clustering based classification for out-of-domain documents In many real world applications , labeled data are in short supply . It often happens that obtaining labeled data in a new domain is expensive and time consuming , while there may be plenty of labeled data from a related but different domain . Traditional machine learning is not able to cope well with learning across different domains . In this paper , we address this problem for a text-mining task , where the labeled data are under one distribution in one domain known as in-domain data , while the unlabeled data are under a related but different domain known as out-of-domain data . Our general goal is to learn from the in-domain and apply the learned knowledge to out-of-domain . We propose a co-clustering based classification CoCC algorithm to tackle this problem . Co-clustering is used as a bridge to propagate the class structure and knowledge from the in-domain to the out-of-domain . We present theoretical and empirical analysis to show that our algorithm is able to produce high quality classification results , even when the distributions between the two data are different . The experimental results show that our algorithm greatly improves the classification performance over the traditional learning algorithms . [[EENNDD]] co-clustering; out-of-domain; kullback-leibler divergence; classification"}, "Klasifikasi berasaskan kluster bersama untuk dokumen di luar domain Dalam banyak aplikasi dunia nyata, data berlabel kekurangan. Selalunya berlaku bahawa memperoleh data berlabel di domain baru adalah mahal dan memakan masa, sementara mungkin ada banyak data berlabel dari domain yang berkaitan tetapi berbeza. Pembelajaran mesin tradisional tidak dapat mengatasi pembelajaran di pelbagai domain. Dalam makalah ini, kami mengatasi masalah ini untuk tugas pencarian teks, di mana data berlabel berada di bawah satu pengedaran dalam satu domain yang dikenal sebagai data dalam domain, sedangkan data yang tidak berlabel berada di bawah domain yang berkaitan tetapi berbeda yang dikenal sebagai di luar data domain. Matlamat umum kami adalah belajar dari dalam domain dan menerapkan pengetahuan yang dipelajari ke luar domain. Kami mencadangkan algoritma CoCC klasifikasi berasaskan kluster untuk mengatasi masalah ini. Penggabungan bersama digunakan sebagai jambatan untuk menyebarkan struktur dan pengetahuan kelas dari dalam-domain ke luar-domain. Kami menyajikan analisis teoritis dan empirikal untuk menunjukkan bahawa algoritma kami dapat menghasilkan hasil klasifikasi berkualiti tinggi, walaupun pengedaran antara kedua-dua data tersebut berbeza. Hasil eksperimen menunjukkan bahawa algoritma kami sangat meningkatkan prestasi klasifikasi berbanding algoritma pembelajaran tradisional. [[EENNDD]] penggabungan bersama; di luar domain; perbezaan kullback-leibler; pengelasan"], [{"string": "Fast nonlinear regression via eigenimages applied to galactic morphology Astronomy increasingly faces the issue of massive , unwieldly data sets . The Sloan Digital Sky Survey SDSS 11 has so far generated tens of millions of images of distant galaxies , of which only a tiny fraction have been morphologically classified . Morphological classification in this context is achieved by fitting a parametric model of galaxy shape to a galaxy image . This is a nonlinear regression problem , whose challenges are threefold , 1 blurring of the image caused by atmosphere and mirror imperfections , 2 large numbers of local minima , and 3 massive data sets . Our strategy is to use the eigenimages of the parametric model to form a new feature space , and then to map both target image and the model parameters into this feature space . In this low-dimensional space we search for the best image-to-parameter match . To search the space , we sample it by creating a database of many random parameter vectors prototypes and mapping them into the feature space . The search problem then becomes one of finding the best prototype match , so the fitting process a nearest-neighbor search . In addition to the savings realized by decomposing the original space into an eigenspace , we can use the fact that the model is a linear sum of functions to reduce the prototypes further : the only prototypes stored are the components of the model function . A modified form of nearest neighbor is used to search among them . Additional complications arise in the form of missing data and heteroscedasticity , both of which are addressed with weighted linear regression . Compared to existing techniques , speed-ups ach-ieved are between 2 and 3 orders of magnitude . This should enable the analysis of the entire SDSS dataset .", "keywords": ["morphology", "astronomy", "nearest neighbor", "pincipal component analysis", "regression", "implementation"], "combined": "Fast nonlinear regression via eigenimages applied to galactic morphology Astronomy increasingly faces the issue of massive , unwieldly data sets . The Sloan Digital Sky Survey SDSS 11 has so far generated tens of millions of images of distant galaxies , of which only a tiny fraction have been morphologically classified . Morphological classification in this context is achieved by fitting a parametric model of galaxy shape to a galaxy image . This is a nonlinear regression problem , whose challenges are threefold , 1 blurring of the image caused by atmosphere and mirror imperfections , 2 large numbers of local minima , and 3 massive data sets . Our strategy is to use the eigenimages of the parametric model to form a new feature space , and then to map both target image and the model parameters into this feature space . In this low-dimensional space we search for the best image-to-parameter match . To search the space , we sample it by creating a database of many random parameter vectors prototypes and mapping them into the feature space . The search problem then becomes one of finding the best prototype match , so the fitting process a nearest-neighbor search . In addition to the savings realized by decomposing the original space into an eigenspace , we can use the fact that the model is a linear sum of functions to reduce the prototypes further : the only prototypes stored are the components of the model function . A modified form of nearest neighbor is used to search among them . Additional complications arise in the form of missing data and heteroscedasticity , both of which are addressed with weighted linear regression . Compared to existing techniques , speed-ups ach-ieved are between 2 and 3 orders of magnitude . This should enable the analysis of the entire SDSS dataset . [[EENNDD]] morphology; astronomy; nearest neighbor; pincipal component analysis; regression; implementation"}, "Regresi nonlinear yang cepat melalui gambar eigen yang diterapkan pada morfologi galaksi Astronomi semakin menghadapi masalah set data yang besar dan tidak berat sebelah. Sloan Digital Sky Survey SDSS 11 setakat ini telah menghasilkan puluhan juta gambar galaksi yang jauh, yang mana hanya sebahagian kecil yang diklasifikasikan secara morfologi. Klasifikasi morfologi dalam konteks ini dicapai dengan memasukkan model parametrik bentuk galaksi ke gambar galaksi. Ini adalah masalah regresi nonlinier, yang cabarannya tiga kali ganda, 1 pengaburan gambar yang disebabkan oleh ketidaksempurnaan suasana dan cermin, 2 sebilangan besar minima tempatan, dan 3 set data besar. Strategi kami adalah menggunakan gambar eigen dari model parametrik untuk membentuk ruang fitur baru, dan kemudian memetakan kedua gambar sasaran dan parameter model ke ruang fitur ini. Di ruang dimensi rendah ini, kami mencari padanan gambar-ke-parameter terbaik. Untuk mencari ruang, kami mencontohinya dengan membuat pangkalan data dengan banyak prototaip vektor parameter rawak dan memetakannya ke ruang ciri. Masalah pencarian kemudian menjadi masalah mencari padanan prototaip terbaik, sehingga proses mencari pencarian tetangga terdekat. Sebagai tambahan kepada penjimatan yang direalisasikan dengan menguraikan ruang asal menjadi ruang eigens, kita dapat menggunakan fakta bahawa model adalah jumlah fungsi linear untuk mengurangkan prototaip lebih jauh: satu-satunya prototaip yang disimpan adalah komponen fungsi model. Bentuk jiran terdekat yang diubah suai digunakan untuk mencari di antara mereka. Komplikasi tambahan timbul dalam bentuk data yang hilang dan heteroskedastisitas, yang keduanya ditangani dengan regresi linier berwajaran. Berbanding dengan teknik yang ada, speed-up yang dicapai antara 2 dan 3 orde magnitud. Ini harus memungkinkan analisis keseluruhan set data SDSS. [[EENNDD]] morfologi; astronomi; jiran terdekat; analisis komponen prinsipal; regresi; pelaksanaan"], [{"string": "Grafting-light : fast , incremental feature selection and structure learning of Markov random fields Feature selection is an important task in order to achieve better generalizability in high dimensional learning , and structure learning of Markov random fields MRFs can automatically discover the inherent structures underlying complex data . Both problems can be cast as solving an l1-norm regularized parameter estimation problem . The existing Grafting method can avoid doing inference on dense graphs in structure learning by incrementally selecting new features . However , Grafting performs a greedy step to optimize over free parameters once new features are included . This greedy strategy results in low efficiency when parameter learning is itself non-trivial , such as in MRFs , in which parameter learning depends on an expensive subroutine to calculate gradients . The complexity of calculating gradients in MRFs is typically exponential to the size of maximal cliques . In this paper , we present a fast algorithm called Grafting-Light to solve the l1-norm regularized maximum likelihood estimation of MRFs for efficient feature selection and structure learning . Grafting-Light iteratively performs one-step of orthant-wise gradient descent over free parameters and selects new features . This lazy strategy is guaranteed to converge to the global optimum and can effectively select significant features . On both synthetic and real data sets , we show that Grafting-Light is much more efficient than Grafting for both feature selection and structure learning , and performs comparably with the optimal batch method that directly optimizes over all the features for feature selection but is much more efficient and accurate for structure learning of MRFs .", "keywords": ["feature selection", "structure learning", "markov random fields"], "combined": "Grafting-light : fast , incremental feature selection and structure learning of Markov random fields Feature selection is an important task in order to achieve better generalizability in high dimensional learning , and structure learning of Markov random fields MRFs can automatically discover the inherent structures underlying complex data . Both problems can be cast as solving an l1-norm regularized parameter estimation problem . The existing Grafting method can avoid doing inference on dense graphs in structure learning by incrementally selecting new features . However , Grafting performs a greedy step to optimize over free parameters once new features are included . This greedy strategy results in low efficiency when parameter learning is itself non-trivial , such as in MRFs , in which parameter learning depends on an expensive subroutine to calculate gradients . The complexity of calculating gradients in MRFs is typically exponential to the size of maximal cliques . In this paper , we present a fast algorithm called Grafting-Light to solve the l1-norm regularized maximum likelihood estimation of MRFs for efficient feature selection and structure learning . Grafting-Light iteratively performs one-step of orthant-wise gradient descent over free parameters and selects new features . This lazy strategy is guaranteed to converge to the global optimum and can effectively select significant features . On both synthetic and real data sets , we show that Grafting-Light is much more efficient than Grafting for both feature selection and structure learning , and performs comparably with the optimal batch method that directly optimizes over all the features for feature selection but is much more efficient and accurate for structure learning of MRFs . [[EENNDD]] feature selection; structure learning; markov random fields"}, "Grafting-light: pemilihan ciri cepat dan tambahan dan pembelajaran struktur medan rawak Markov Pemilihan ciri adalah tugas penting untuk mencapai keboleheneralisasian yang lebih baik dalam pembelajaran dimensi tinggi, dan pembelajaran struktur bidang rawak Markov MRF dapat secara automatik menemui struktur yang wujud yang mendasari data kompleks . Kedua-dua masalah tersebut dapat diselesaikan sebagai penyelesaian masalah anggaran parameter teratur l1-norma. Kaedah Grafting yang ada dapat mengelakkan daripada membuat kesimpulan pada grafik padat dalam pembelajaran struktur dengan memilih secara bertahap fitur baru. Walau bagaimanapun, Grafting melakukan langkah tamak untuk mengoptimumkan parameter percuma setelah ciri baru disertakan. Strategi tamak ini menghasilkan kecekapan rendah ketika pembelajaran parameter itu sendiri tidak sepele, seperti di MRF, di mana pembelajaran parameter bergantung pada subrutin yang mahal untuk mengira kecerunan. Kerumitan mengira kecerunan dalam MRF biasanya bersifat eksponensial terhadap ukuran klik maksimum. Dalam makalah ini, kami menyajikan algoritma cepat yang disebut Grafting-Light untuk menyelesaikan perkiraan kemungkinan maksimum MR1 MRF untuk pemilihan ciri dan pembelajaran struktur yang cekap. Grafting-Light secara berulang-ulang melakukan satu langkah penurunan kecerunan orthant berdasarkan parameter percuma dan memilih ciri baru. Strategi malas ini dijamin dapat bergabung ke tahap optimum global dan dapat memilih ciri-ciri penting dengan berkesan. Pada kedua-dua set data sintetik dan nyata, kami menunjukkan bahawa Grafting-Light jauh lebih efisien daripada Grafting untuk pemilihan ciri dan pembelajaran struktur, dan berkinerja setanding dengan kaedah kumpulan optimum yang secara langsung mengoptimumkan semua ciri untuk pemilihan ciri tetapi jauh lebih banyak cekap dan tepat untuk pembelajaran struktur MRF. [[EENNDD]] pemilihan ciri; pembelajaran struktur; medan rawak markov"], [{"string": "SyMP : an efficient clustering approach to identify clusters of arbitrary shapes in large data sets We propose a new clustering algorithm , called SyMP , which is based on synchronization of pulse-coupled oscillators . SyMP represents each data point by an Integrate-and-Fire oscillator and uses the relative similarity between the points to model the interaction between the oscillators . SyMP is robust to noise and outliers , determines the number of clusters in an unsupervised manner , identifies clusters of arbitrary shapes , and can handle very large data sets . The robustness of SyMP is an intrinsic property of the synchronization mechanism . To determine the optimum number of clusters , SyMP uses a dynamic resolution parameter . To identify clusters of various shapes , SyMP models each cluster by multiple Gaussian components . The number of components is automatically determined using a dynamic intra-cluster resolution parameter . Clusters with simple shapes would be modeled by few components while clusters with more complex shapes would require a larger number of components . The scalable version of SyMP uses an efficient incremental approach that requires a simple pass through the data set . The proposed clustering approach is empirically evaluated with several synthetic and real data sets , and its performance is compared with CURE .", "keywords": ["gaussian mixture models", "large datasets"], "combined": "SyMP : an efficient clustering approach to identify clusters of arbitrary shapes in large data sets We propose a new clustering algorithm , called SyMP , which is based on synchronization of pulse-coupled oscillators . SyMP represents each data point by an Integrate-and-Fire oscillator and uses the relative similarity between the points to model the interaction between the oscillators . SyMP is robust to noise and outliers , determines the number of clusters in an unsupervised manner , identifies clusters of arbitrary shapes , and can handle very large data sets . The robustness of SyMP is an intrinsic property of the synchronization mechanism . To determine the optimum number of clusters , SyMP uses a dynamic resolution parameter . To identify clusters of various shapes , SyMP models each cluster by multiple Gaussian components . The number of components is automatically determined using a dynamic intra-cluster resolution parameter . Clusters with simple shapes would be modeled by few components while clusters with more complex shapes would require a larger number of components . The scalable version of SyMP uses an efficient incremental approach that requires a simple pass through the data set . The proposed clustering approach is empirically evaluated with several synthetic and real data sets , and its performance is compared with CURE . [[EENNDD]] gaussian mixture models; large datasets"}, "SyMP: pendekatan pengelompokan yang cekap untuk mengenal pasti kelompok bentuk sewenang-wenangnya dalam set data besar. Kami mencadangkan algoritma pengelompokan baru, yang disebut SyMP, yang berdasarkan pada penyegerakan pengayun berpasangan nadi. SyMP mewakili setiap titik data oleh pengayun Integrasi-dan-Api dan menggunakan persamaan relatif antara titik-titik untuk memodelkan interaksi antara pengayun. SyMP kuat terhadap kebisingan dan outlier, menentukan jumlah kelompok dengan cara yang tidak diawasi, mengenal pasti kelompok bentuk sewenang-wenangnya, dan dapat menangani kumpulan data yang sangat besar. Kekukuhan SyMP adalah sifat intrinsik mekanisme penyegerakan. Untuk menentukan bilangan kelompok yang optimum, SyMP menggunakan parameter resolusi dinamik. Untuk mengenal pasti kelompok pelbagai bentuk, SyMP memodelkan setiap kluster dengan beberapa komponen Gaussian. Bilangan komponen ditentukan secara automatik menggunakan parameter resolusi intra-kluster yang dinamik. Kelompok dengan bentuk sederhana akan dimodelkan oleh beberapa komponen sementara kelompok dengan bentuk yang lebih kompleks akan memerlukan sejumlah komponen yang lebih besar. Versi SyMP yang boleh diskalakan menggunakan pendekatan penambahbaikan yang efisien yang memerlukan kaedah mudah melalui set data. Pendekatan pengelompokan yang dicadangkan dinilai secara empirik dengan beberapa kumpulan data sintetik dan nyata, dan prestasinya dibandingkan dengan CURE. [[EENNDD]] model campuran gaussian; set data besar"], [{"string": "Cross domain distribution adaptation via kernel mapping When labeled examples are limited and difficult to obtain , transfer learning employs knowledge from a source domain to improve learning accuracy in the target domain . However , the assumption made by existing approaches , that the marginal and conditional probabilities are directly related between source and target domains , has limited applicability in either the original space or its linear transformations . To solve this problem , we propose an adaptive kernel approach that maps the marginal distribution of target-domain and source-domain data into a common kernel space , and utilize a sample selection strategy to draw conditional probabilities between the two domains closer . We formally show that under the kernel-mapping space , the difference in distributions between the two domains is bounded ; and the prediction error of the proposed approach can also be bounded . Experimental results demonstrate that the proposed method outperforms both traditional inductive classifiers and the state-of-the-art boosting-based transfer algorithms on most domains , including text categorization and web page ratings . In particular , it can achieve around 10 % higher accuracy than other approaches for the text categorization problem . The source code and datasets are available from the authors .", "keywords": ["domain transfer", "ensemble", "generalization bound", "kernel"], "combined": "Cross domain distribution adaptation via kernel mapping When labeled examples are limited and difficult to obtain , transfer learning employs knowledge from a source domain to improve learning accuracy in the target domain . However , the assumption made by existing approaches , that the marginal and conditional probabilities are directly related between source and target domains , has limited applicability in either the original space or its linear transformations . To solve this problem , we propose an adaptive kernel approach that maps the marginal distribution of target-domain and source-domain data into a common kernel space , and utilize a sample selection strategy to draw conditional probabilities between the two domains closer . We formally show that under the kernel-mapping space , the difference in distributions between the two domains is bounded ; and the prediction error of the proposed approach can also be bounded . Experimental results demonstrate that the proposed method outperforms both traditional inductive classifiers and the state-of-the-art boosting-based transfer algorithms on most domains , including text categorization and web page ratings . In particular , it can achieve around 10 % higher accuracy than other approaches for the text categorization problem . The source code and datasets are available from the authors . [[EENNDD]] domain transfer; ensemble; generalization bound; kernel"}, "Penyesuaian pengedaran domain silang melalui pemetaan kernel Apabila contoh berlabel terhad dan sukar diperoleh, pembelajaran pemindahan menggunakan pengetahuan dari domain sumber untuk meningkatkan ketepatan pembelajaran dalam domain sasaran. Walau bagaimanapun, anggapan yang dibuat oleh pendekatan yang ada, bahawa kebarangkalian marginal dan bersyarat secara langsung berkaitan antara domain sumber dan sasaran, mempunyai penerapan yang terbatas dalam ruang asal atau transformasi liniernya. Untuk menyelesaikan masalah ini, kami mengusulkan pendekatan kernel adaptif yang memetakan distribusi marginal domain sasaran dan data sumber ke ruang kernel umum, dan menggunakan strategi pemilihan sampel untuk mendekatkan kebarangkalian bersyarat antara kedua domain tersebut. Kami secara formal menunjukkan bahawa di bawah ruang pemetaan kernel, perbezaan pembahagian antara dua domain dibatasi; dan kesalahan ramalan pendekatan yang dicadangkan juga dapat dibatasi. Hasil eksperimen menunjukkan bahawa kaedah yang dicadangkan mengungguli pengklasifikasi induktif tradisional dan algoritma pemindahan berasaskan peningkatan terkini di kebanyakan domain, termasuk pengkategorian teks dan penilaian halaman web. Secara khusus, ia dapat mencapai ketepatan sekitar 10% lebih tinggi daripada pendekatan lain untuk masalah pengkategorian teks. Kod sumber dan set data tersedia dari pengarang. [[EENNDD]] pemindahan domain; ensembel; generalisasi terikat; kernel"], [{"string": "Regression error characteristic surfaces This paper presents a generalization of Regression Error Characteristic REC curves . REC curves describe the cumulative distribution function of the prediction error of models and can be seen as a generalization of ROC curves to regression problems . REC curves provide useful information for analyzing the performance of models , particularly when compared to error statistics like for instance the Mean Squared Error . In this paper we present Regression Error Characteristic REC surfaces that introduce a further degree of detail by plotting the cumulative distribution function of the errors across the distribution of the target variable , i.e. the joint cumulative distribution function of the errors and the target variable . This provides a more detailed analysis of the performance of models when compared to REC curves . This extra detail is particularly relevant in applications with non-uniform error costs , where it is important to study the performance of models for specific ranges of the target variable . In this paper we present the notion of REC surfaces , describe how to use them to compare the performance of models , and illustrate their use with an important practical class of applications : the prediction of rare extreme values .", "keywords": ["regression problems", "evaluation metrics", "model comparisons"], "combined": "Regression error characteristic surfaces This paper presents a generalization of Regression Error Characteristic REC curves . REC curves describe the cumulative distribution function of the prediction error of models and can be seen as a generalization of ROC curves to regression problems . REC curves provide useful information for analyzing the performance of models , particularly when compared to error statistics like for instance the Mean Squared Error . In this paper we present Regression Error Characteristic REC surfaces that introduce a further degree of detail by plotting the cumulative distribution function of the errors across the distribution of the target variable , i.e. the joint cumulative distribution function of the errors and the target variable . This provides a more detailed analysis of the performance of models when compared to REC curves . This extra detail is particularly relevant in applications with non-uniform error costs , where it is important to study the performance of models for specific ranges of the target variable . In this paper we present the notion of REC surfaces , describe how to use them to compare the performance of models , and illustrate their use with an important practical class of applications : the prediction of rare extreme values . [[EENNDD]] regression problems; evaluation metrics; model comparisons"}, "Permukaan ciri ralat regresi Makalah ini menyajikan generalisasi keluk REC Karakteristik Kesalahan Regresi. Kurva REC menerangkan fungsi taburan kumulatif ralat ramalan model dan dapat dilihat sebagai generalisasi keluk ROC terhadap masalah regresi. Keluk REC memberikan maklumat yang berguna untuk menganalisis prestasi model, terutama jika dibandingkan dengan statistik ralat seperti misalnya Ralat Kuadrat Rata. Dalam makalah ini kami menyajikan permukaan REC Karakteristik Kesalahan Regresi yang memperkenalkan tahap perincian lebih lanjut dengan memplot fungsi pengagihan kumulatif dari ralat merentasi pemboleh ubah sasaran, iaitu fungsi pembahagian kumulatif bersama kesalahan dan pemboleh ubah sasaran. Ini memberikan analisis yang lebih terperinci mengenai prestasi model jika dibandingkan dengan kurva REC. Perincian tambahan ini sangat relevan dalam aplikasi dengan kos ralat yang tidak seragam, di mana penting untuk mengkaji prestasi model untuk julat tertentu dari pemboleh ubah sasaran. Dalam makalah ini kami memaparkan konsep permukaan REC, menerangkan bagaimana menggunakannya untuk membandingkan prestasi model, dan menggambarkan penggunaannya dengan kelas aplikasi praktikal yang penting: ramalan nilai ekstrem yang jarang berlaku. [[EENNDD]] masalah regresi; sukatan penilaian; perbandingan model"], [{"string": "Extracting discriminative concepts for domain adaptation in text mining One common predictive modeling challenge occurs in text mining problems is that the training data and the operational testing data are drawn from different underlying distributions . This poses a great difficulty for many statistical learning methods . However , when the distribution in the source domain and the target domain are not identical but related , there may exist a shared concept space to preserve the relation . Consequently a good feature representation can encode this concept space and minimize the distribution gap . To formalize this intuition , we propose a domain adaptation method that parameterizes this concept space by linear transformation under which we explicitly minimize the distribution difference between the source domain with sufficient labeled data and target domains with only unlabeled data , while at the same time minimizing the empirical loss on the labeled data in the source domain . Another characteristic of our method is its capability for considering multiple classes and their interactions simultaneously . We have conducted extensive experiments on two common text mining problems , namely , information extraction and document classification to demonstrate the effectiveness of our proposed method .", "keywords": ["text mining", "domain adaptation", "feature extraction", "miscellaneous"], "combined": "Extracting discriminative concepts for domain adaptation in text mining One common predictive modeling challenge occurs in text mining problems is that the training data and the operational testing data are drawn from different underlying distributions . This poses a great difficulty for many statistical learning methods . However , when the distribution in the source domain and the target domain are not identical but related , there may exist a shared concept space to preserve the relation . Consequently a good feature representation can encode this concept space and minimize the distribution gap . To formalize this intuition , we propose a domain adaptation method that parameterizes this concept space by linear transformation under which we explicitly minimize the distribution difference between the source domain with sufficient labeled data and target domains with only unlabeled data , while at the same time minimizing the empirical loss on the labeled data in the source domain . Another characteristic of our method is its capability for considering multiple classes and their interactions simultaneously . We have conducted extensive experiments on two common text mining problems , namely , information extraction and document classification to demonstrate the effectiveness of our proposed method . [[EENNDD]] text mining; domain adaptation; feature extraction; miscellaneous"}, "Mengekstrak konsep diskriminatif untuk penyesuaian domain dalam perlombongan teks Satu cabaran pemodelan ramalan yang biasa berlaku dalam masalah perlombongan teks adalah bahawa data latihan dan data ujian operasi diambil dari sebaran yang berlainan. Ini menimbulkan kesukaran besar bagi banyak kaedah pembelajaran statistik. Namun, apabila pengedaran di domain sumber dan domain sasaran tidak sama tetapi berkaitan, mungkin ada ruang konsep bersama untuk memelihara hubungan. Oleh itu, perwakilan ciri yang baik dapat mengekod ruang konsep ini dan mengurangkan jurang pengedaran. Untuk meresmikan intuisi ini, kami mencadangkan kaedah penyesuaian domain yang memusatkan ruang konsep ini dengan transformasi linear di mana kami secara eksplisit meminimumkan perbezaan taburan antara domain sumber dengan data berlabel yang mencukupi dan domain sasaran dengan hanya data yang tidak berlabel, dan pada masa yang sama meminimumkan kehilangan empirik pada data berlabel di domain sumber. Ciri lain kaedah kami adalah kemampuannya untuk mempertimbangkan beberapa kelas dan interaksi mereka secara serentak. Kami telah melakukan eksperimen yang luas mengenai dua masalah perlombongan teks yang biasa, iaitu, pengekstrakan maklumat dan klasifikasi dokumen untuk menunjukkan keberkesanan kaedah yang dicadangkan kami. [[EENNDD]] perlombongan teks; penyesuaian domain; pengekstrakan ciri; pelbagai"], [{"string": "A multiple tree algorithm for the efficient association of asteroid observations In this paper we examine the problem of efficiently finding sets of observations that conform to a given underlying motion model . While this problem is often phrased as a tracking problem , where it is called track initiation , it is useful in a variety of tasks where we want to find correspondences or patterns in spatial-temporal data . Unfortunately , this problem often suffers from a combinatorial explosion in the number of potential sets that must be evaluated . We consider the problem with respect to large-scale asteroid observation data , where the goal is to find associations among the observations that correspond to the same underlying asteroid . In this domain , it is vital that we can efficiently extract the underlying associations . We introduce a new methodology for track initiation that exhaustively considers all possible linkages . We then introduce an exact tree-based algorithm for tractably finding all compatible sets of points . Further , we extend this approach to use multiple trees , exploiting structure from several time steps at once . We compare this approach to a standard sequential approach and show how the use of multiple trees can provide a significant benefit .", "keywords": ["track initiation", "multiple tree algorithms"], "combined": "A multiple tree algorithm for the efficient association of asteroid observations In this paper we examine the problem of efficiently finding sets of observations that conform to a given underlying motion model . While this problem is often phrased as a tracking problem , where it is called track initiation , it is useful in a variety of tasks where we want to find correspondences or patterns in spatial-temporal data . Unfortunately , this problem often suffers from a combinatorial explosion in the number of potential sets that must be evaluated . We consider the problem with respect to large-scale asteroid observation data , where the goal is to find associations among the observations that correspond to the same underlying asteroid . In this domain , it is vital that we can efficiently extract the underlying associations . We introduce a new methodology for track initiation that exhaustively considers all possible linkages . We then introduce an exact tree-based algorithm for tractably finding all compatible sets of points . Further , we extend this approach to use multiple trees , exploiting structure from several time steps at once . We compare this approach to a standard sequential approach and show how the use of multiple trees can provide a significant benefit . [[EENNDD]] track initiation; multiple tree algorithms"}, "Algoritma berbilang pokok untuk perkaitan pemerhatian asteroid yang cekap Dalam makalah ini kita mengkaji masalah mencari set pemerhatian dengan cekap yang sesuai dengan model gerakan asas yang diberikan. Walaupun masalah ini sering digambarkan sebagai masalah pelacakan, di mana ia disebut inisiasi trek, ini berguna dalam berbagai tugas di mana kita ingin mencari korespondensi atau corak dalam data spasial-temporal. Malangnya, masalah ini sering berlaku akibat ledakan kombinatori dalam jumlah set berpotensi yang mesti dinilai. Kami menganggap masalah berkenaan dengan data pemerhatian asteroid berskala besar, di mana tujuannya adalah untuk mencari hubungan antara pemerhatian yang sesuai dengan asteroid yang sama. Dalam domain ini, sangat penting agar kita dapat mengekstrak perkaitan yang mendasari dengan cekap. Kami memperkenalkan metodologi baru untuk memulakan trek yang secara menyeluruh mempertimbangkan semua kemungkinan hubungan. Kami kemudian memperkenalkan algoritma berasaskan pokok yang tepat untuk mencari semua set titik yang sesuai. Selanjutnya, kami memperluas pendekatan ini untuk menggunakan beberapa pokok, memanfaatkan struktur dari beberapa langkah sekaligus. Kami membandingkan pendekatan ini dengan pendekatan berurutan standard dan menunjukkan bagaimana penggunaan pelbagai pokok dapat memberikan manfaat yang signifikan. [[EENNDD]] permulaan trek; pelbagai algoritma pokok"], [{"string": "Incremental quantile estimation for massive tracking", "keywords": ["sequential estimation", "applications", "stochastic approximation", "model validation and analysis", "equi-depth histograms", "dynamic database", "customer profiles", "ewma", "customer relationship management", "percentiles", "transaction data", "simulation output analysis", "massive data"], "combined": "Incremental quantile estimation for massive tracking [[EENNDD]] sequential estimation; applications; stochastic approximation; model validation and analysis; equi-depth histograms; dynamic database; customer profiles; ewma; customer relationship management; percentiles; transaction data; simulation output analysis; massive data"}, "Anggaran kuantil tambahan untuk pengesanan berurutan [[EENNDD]] berjujukan; permohonan; pendekatan stokastik; pengesahan dan analisis model; histogram kedalaman ekuiti; pangkalan data dinamik; profil pelanggan; ewma; pengurusan hubungan pelanggan; persentil; data transaksi; analisis output simulasi; data besar-besaran"], [{"string": "Deriving quantitative models for correlation clusters Correlation clustering aims at grouping the data set into correlation clusters such that the objects in the same cluster exhibit a certain density and are all associated to a common arbitrarily oriented hyperplane of arbitrary dimensionality . Several algorithms for this task have been proposed recently . However , all algorithms only compute the partitioning of the data into clusters . This is only a first step in the pipeline of advanced data analysis and system modelling . The second post-clustering step of deriving a quantitative model for each correlation cluster has not been addressed so far . In this paper , we describe an original approach to handle this second step . We introduce a general method that can extract quantitative information on the linear dependencies within a correlation clustering . Our concepts are independent of the clustering model and can thus be applied as a post-processing step to any correlation clustering algorithm . Furthermore , we show how these quantitative models can be used to predict the probability distribution that an object is created by these models . Our broad experimental evaluation demonstrates the beneficial impact of our method on several applications of significant practical importance .", "keywords": ["data mining", "cluster description", "correlation clustering", "cluster model", "clustering"], "combined": "Deriving quantitative models for correlation clusters Correlation clustering aims at grouping the data set into correlation clusters such that the objects in the same cluster exhibit a certain density and are all associated to a common arbitrarily oriented hyperplane of arbitrary dimensionality . Several algorithms for this task have been proposed recently . However , all algorithms only compute the partitioning of the data into clusters . This is only a first step in the pipeline of advanced data analysis and system modelling . The second post-clustering step of deriving a quantitative model for each correlation cluster has not been addressed so far . In this paper , we describe an original approach to handle this second step . We introduce a general method that can extract quantitative information on the linear dependencies within a correlation clustering . Our concepts are independent of the clustering model and can thus be applied as a post-processing step to any correlation clustering algorithm . Furthermore , we show how these quantitative models can be used to predict the probability distribution that an object is created by these models . Our broad experimental evaluation demonstrates the beneficial impact of our method on several applications of significant practical importance . [[EENNDD]] data mining; cluster description; correlation clustering; cluster model; clustering"}, "Mendapatkan model kuantitatif untuk kelompok korelasi Pengelompokan korelasi bertujuan mengelompokkan kumpulan data menjadi kelompok korelasi sehingga objek dalam kelompok yang sama menunjukkan kepadatan tertentu dan semuanya berkaitan dengan hiperplan yang sama berorientasi sewenang-wenangnya dimensi sewenang-wenangnya. Beberapa algoritma untuk tugas ini telah dicadangkan baru-baru ini. Walau bagaimanapun, semua algoritma hanya mengira pemisahan data ke dalam kelompok. Ini hanya merupakan langkah pertama dalam proses analisis data lanjutan dan pemodelan sistem. Langkah pasca-kluster kedua untuk menghasilkan model kuantitatif untuk setiap kelompok korelasi belum ditangani sejauh ini. Dalam makalah ini, kami menerangkan pendekatan asal untuk menangani langkah kedua ini. Kami memperkenalkan kaedah umum yang dapat mengekstrak maklumat kuantitatif mengenai ketergantungan linear dalam pengelompokan korelasi. Konsep kami bebas dari model pengelompokan dan dengan itu dapat diterapkan sebagai langkah pasca pemprosesan untuk algoritma pengelompokan korelasi apa pun. Selanjutnya, kami menunjukkan bagaimana model-model kuantitatif ini dapat digunakan untuk memprediksi taburan kebarangkalian suatu objek dibuat oleh model-model ini. Penilaian eksperimental kami yang luas menunjukkan kesan yang baik dari kaedah kami pada beberapa aplikasi yang sangat mustahak. [[EENNDD]] perlombongan data; penerangan kluster; pengelompokan korelasi; model kluster; pengelompokan"], [{"string": "Optimizing time series discretization for knowledge discovery Knowledge Discovery in time series usually requires symbolic time series . Many discretization methods that convert numeric time series to symbolic time series ignore the temporal order of values . This often leads to symbols that do not correspond to states of the process generating the time series and can not be interpreted meaningfully . We propose a new method for meaningful unsupervised discretization of numeric time series called Persist . The algorithm is based on the Kullback-Leibler divergence between the marginal and the self-transition probability distributions of the discretization symbols . Its performance is evaluated on both artificial and real life data in comparison to the most common discretization methods . Persist achieves significantly higher accuracy than existing static methods and is robust against noise . It also outperforms Hidden Markov Models for all but very simple cases .", "keywords": ["time series", "discretization", "pattern recognition", "persistence"], "combined": "Optimizing time series discretization for knowledge discovery Knowledge Discovery in time series usually requires symbolic time series . Many discretization methods that convert numeric time series to symbolic time series ignore the temporal order of values . This often leads to symbols that do not correspond to states of the process generating the time series and can not be interpreted meaningfully . We propose a new method for meaningful unsupervised discretization of numeric time series called Persist . The algorithm is based on the Kullback-Leibler divergence between the marginal and the self-transition probability distributions of the discretization symbols . Its performance is evaluated on both artificial and real life data in comparison to the most common discretization methods . Persist achieves significantly higher accuracy than existing static methods and is robust against noise . It also outperforms Hidden Markov Models for all but very simple cases . [[EENNDD]] time series; discretization; pattern recognition; persistence"}, "Mengoptimumkan diskritisasi siri masa untuk penemuan pengetahuan Penemuan Pengetahuan dalam siri masa biasanya memerlukan siri masa simbolik. Banyak kaedah diskretisasi yang menukar siri masa berangka menjadi siri masa simbolik mengabaikan susunan nilai temporal. Ini sering membawa kepada simbol yang tidak sesuai dengan keadaan proses menghasilkan siri masa dan tidak dapat ditafsirkan secara bermakna. Kami mencadangkan kaedah baru untuk diskritisasi siri masa berangka yang tidak diawasi yang disebut Persist. Algoritma ini didasarkan pada perbezaan Kullback-Leibler antara taburan kebarangkalian marginal dan peralihan diri simbol diskretisasi. Kinerjanya dinilai pada data buatan dan kehidupan sebenar berbanding dengan kaedah diskretisasi yang paling biasa. Persist mencapai ketepatan yang jauh lebih tinggi daripada kaedah statik yang ada dan kuat terhadap bunyi. Ia juga mengatasi Model Markov Tersembunyi untuk semua kes tetapi sangat mudah. [[EENNDD]] siri masa; budi bicara; pengecaman corak; kegigihan"], [{"string": "A framework for specifying explicit bias for revision of approximate information extraction rules", "keywords": ["theory revision", "information extraction", "text mining", "user guided revision"], "combined": "A framework for specifying explicit bias for revision of approximate information extraction rules [[EENNDD]] theory revision; information extraction; text mining; user guided revision"}, "Kerangka kerja untuk menentukan bias eksplisit untuk penyemakan anggaran kaedah pengekstrakan maklumat [[EENNDD]] semakan teori; pengekstrakan maklumat; perlombongan teks; semakan berpandukan pengguna"], [{"string": "MARK : a boosting algorithm for heterogeneous kernel models Support Vector Machines and other kernel methods have proven to be very effective for nonlinear inference . Practical issues are how to select the type of kernel including any parameters and how to deal with the computational issues caused by the fact that the kernel matrix grows quadratically with the data . Inspired by ensemble and boosting methods like MART , we propose the Multiple Additive Regression Kernels MARK algorithm to address these issues . MARK considers a large potentially infinite library of kernel matrices formed by different kernel functions and parameters . Using gradient boosting\\/column generation , MARK constructs columns of the heterogeneous kernel matrix the base hypotheses on the fly and then adds them into the kernel ensemble . Regularization methods such as used in SVM , kernel ridge regression , and MART , are used to prevent overfitting . We investigate how MARK is applied to heterogeneous kernel ridge regression . The resulting algorithm is simple to implement and efficient . Kernel parameter selection is handled within MARK . Sampling and `` weak '' kernels are used to further enhance the computational efficiency of the resulting additive algorithm . The user can incorporate and potentially extract domain knowledge by restricting the kernel library to interpretable kernels . MARK compares very favorably with SVM and kernel ridge regression on several benchmark datasets .", "keywords": ["probabilistic algorithms"], "combined": "MARK : a boosting algorithm for heterogeneous kernel models Support Vector Machines and other kernel methods have proven to be very effective for nonlinear inference . Practical issues are how to select the type of kernel including any parameters and how to deal with the computational issues caused by the fact that the kernel matrix grows quadratically with the data . Inspired by ensemble and boosting methods like MART , we propose the Multiple Additive Regression Kernels MARK algorithm to address these issues . MARK considers a large potentially infinite library of kernel matrices formed by different kernel functions and parameters . Using gradient boosting\\/column generation , MARK constructs columns of the heterogeneous kernel matrix the base hypotheses on the fly and then adds them into the kernel ensemble . Regularization methods such as used in SVM , kernel ridge regression , and MART , are used to prevent overfitting . We investigate how MARK is applied to heterogeneous kernel ridge regression . The resulting algorithm is simple to implement and efficient . Kernel parameter selection is handled within MARK . Sampling and `` weak '' kernels are used to further enhance the computational efficiency of the resulting additive algorithm . The user can incorporate and potentially extract domain knowledge by restricting the kernel library to interpretable kernels . MARK compares very favorably with SVM and kernel ridge regression on several benchmark datasets . [[EENNDD]] probabilistic algorithms"}, "MARK: algoritma penambahbaikan untuk model kernel heterogen Mesin Vektor Sokongan dan kaedah kernel lain terbukti sangat berkesan untuk inferensi nonlinear. Masalah praktikal adalah bagaimana memilih jenis kernel termasuk parameter apa pun dan bagaimana menangani masalah komputasi yang disebabkan oleh fakta bahawa matriks kernel tumbuh secara kuadratik dengan data. Diilhamkan oleh kaedah ensemble dan boost seperti MART, kami mencadangkan algoritma Multiple Additive Regression Kernels MARK untuk mengatasi masalah ini. MARK menganggap perpustakaan besar matriks kernel yang berpotensi besar yang dibentuk oleh fungsi dan parameter kernel yang berbeza. Dengan menggunakan gradient boosting \\ / column column, MARK membina lajur matriks kernel yang heterogen asas hipotesis dengan cepat dan kemudian menambahkannya ke dalam ensemble kernel. Kaedah regularisasi seperti yang digunakan dalam SVM, regresi kernel ridge, dan MART, digunakan untuk mencegah overfitting. Kami menyiasat bagaimana MARK diterapkan pada regresi rabung kernel yang heterogen. Algoritma yang dihasilkan mudah dilaksanakan dan cekap. Pemilihan parameter kernel dikendalikan dalam MARK. Sampel dan kernel \"lemah\" digunakan untuk meningkatkan kecekapan komputasi algoritma aditif yang dihasilkan. Pengguna dapat menggabungkan dan berpotensi mengekstrak pengetahuan domain dengan menghadkan pustaka kernel kepada kernel yang dapat ditafsirkan. MARK dibandingkan dengan regresi permatang SVM dan kernel pada beberapa set data penanda aras. [[EENNDD]] algoritma probabilistik"], [{"string": "Predicting bounce rates in sponsored search advertisements This paper explores an important and relatively unstudied quality measure of a sponsored search advertisement : bounce rate . The bounce rate of an ad can be informally defined as the fraction of users who click on the ad but almost immediately move on to other tasks . A high bounce rate can lead to poor advertiser return on investment , and suggests search engine users may be having a poor experience following the click . In this paper , we first provide quantitative analysis showing that bounce rate is an effective measure of user satisfaction . We then address the question , can we predict bounce rate by analyzing the features of the advertisement ? An affirmative answer would allow advertisers and search engines to predict the effectiveness and quality of advertisements before they are shown . We propose solutions to this problem involving large-scale learning methods that leverage features drawn from ad creatives in addition to their keywords and landing pages .", "keywords": ["bounce rate", "applications", "machine learning", "online advertisement"], "combined": "Predicting bounce rates in sponsored search advertisements This paper explores an important and relatively unstudied quality measure of a sponsored search advertisement : bounce rate . The bounce rate of an ad can be informally defined as the fraction of users who click on the ad but almost immediately move on to other tasks . A high bounce rate can lead to poor advertiser return on investment , and suggests search engine users may be having a poor experience following the click . In this paper , we first provide quantitative analysis showing that bounce rate is an effective measure of user satisfaction . We then address the question , can we predict bounce rate by analyzing the features of the advertisement ? An affirmative answer would allow advertisers and search engines to predict the effectiveness and quality of advertisements before they are shown . We propose solutions to this problem involving large-scale learning methods that leverage features drawn from ad creatives in addition to their keywords and landing pages . [[EENNDD]] bounce rate; applications; machine learning; online advertisement"}, "Meramalkan kadar pentalan dalam iklan carian yang ditaja Kertas ini meneroka ukuran kualiti penting dan agak tidak dipelajari dari iklan carian yang ditaja: kadar pentalan. Kadar pantulan iklan dapat didefinisikan secara tidak formal sebagai pecahan pengguna yang mengklik iklan tetapi hampir segera beralih ke tugas lain. Kadar pantulan yang tinggi boleh menyebabkan pengembalian pelaburan pengiklan yang buruk, dan menunjukkan pengguna enjin carian mungkin mengalami pengalaman yang buruk berikutan klik tersebut. Dalam makalah ini, pertama-tama kami memberikan analisis kuantitatif yang menunjukkan bahawa kadar pentalan adalah ukuran kepuasan pengguna yang berkesan. Kami kemudian menjawab soalan, dapatkah kita meramalkan kadar pentalan dengan menganalisis ciri iklan? Jawapan afirmatif akan membolehkan pengiklan dan mesin pencari meramalkan keberkesanan dan kualiti iklan sebelum iklan tersebut dipaparkan. Kami mencadangkan penyelesaian untuk masalah ini yang melibatkan kaedah pembelajaran berskala besar yang memanfaatkan ciri yang diambil dari kreatif iklan selain kata kunci dan halaman arahan mereka. [[EENNDD]] kadar pentalan; permohonan; pembelajaran mesin; iklan dalam talian"], [{"string": "Fully automatic cross-associations Large , sparse binary matrices arise in numerous data mining applications , such as the analysis of market baskets , web graphs , social networks , co-citations , as well as information retrieval , collaborative filtering , sparse matrix reordering , etc. . Virtually all popular methods for the analysis of such matrices -- e.g. , k-means clustering , METIS graph partitioning , SVD\\/PCA and frequent itemset mining -- require the user to specify various parameters , such as the number of clusters , number of principal components , number of partitions , and `` support . '' Choosing suitable values for such parameters is a challenging problem.Cross-association is a joint decomposition of a binary matrix into disjoint row and column groups such that the rectangular intersections of groups are homogeneous . Starting from first principles , we furnish a clear , information-theoretic criterion to choose a good cross-association as well as its parameters , namely , the number of row and column groups . We provide scalable algorithms to approach the optimal . Our algorithm is parameter-free , and requires no user intervention . In practice it scales linearly with the problem size , and is thus applicable to very large matrices . Finally , we present experiments on multiple synthetic and real-life datasets , where our method gives high-quality , intuitive results .", "keywords": ["cross-association", "mdl", "content analysis and indexing"], "combined": "Fully automatic cross-associations Large , sparse binary matrices arise in numerous data mining applications , such as the analysis of market baskets , web graphs , social networks , co-citations , as well as information retrieval , collaborative filtering , sparse matrix reordering , etc. . Virtually all popular methods for the analysis of such matrices -- e.g. , k-means clustering , METIS graph partitioning , SVD\\/PCA and frequent itemset mining -- require the user to specify various parameters , such as the number of clusters , number of principal components , number of partitions , and `` support . '' Choosing suitable values for such parameters is a challenging problem.Cross-association is a joint decomposition of a binary matrix into disjoint row and column groups such that the rectangular intersections of groups are homogeneous . Starting from first principles , we furnish a clear , information-theoretic criterion to choose a good cross-association as well as its parameters , namely , the number of row and column groups . We provide scalable algorithms to approach the optimal . Our algorithm is parameter-free , and requires no user intervention . In practice it scales linearly with the problem size , and is thus applicable to very large matrices . Finally , we present experiments on multiple synthetic and real-life datasets , where our method gives high-quality , intuitive results . [[EENNDD]] cross-association; mdl; content analysis and indexing"}, "Perhimpunan silang automatik Matriks binari yang jarang dan besar timbul dalam banyak aplikasi perlombongan data, seperti analisis bakul pasaran, grafik web, rangkaian sosial, petikan, serta pengambilan maklumat, penapisan kolaboratif, penyusunan semula matriks jarang, dll. . Hampir semua kaedah popular untuk analisis matriks tersebut - mis. , k-bermaksud pengelompokan, partisi grafik METIS, SVD \\ / PCA dan perlombongan itemet kerap - menghendaki pengguna menentukan pelbagai parameter, seperti jumlah kluster, jumlah komponen utama, jumlah partisi, dan `` sokongan. Memilih nilai yang sesuai untuk parameter tersebut adalah masalah yang mencabar. Perhubungan silang adalah penguraian bersama matriks binari menjadi kumpulan baris dan lajur yang tidak sama sehingga persimpangan segi empat tepat kumpulan adalah homogen. Bermula dari prinsip pertama, kami memberikan kriteria teori maklumat yang jelas untuk memilih pergaulan silang yang baik serta parameternya, iaitu jumlah kumpulan baris dan lajur. Kami menyediakan algoritma berskala untuk mendekati yang optimum. Algoritma kami bebas parameter, dan tidak memerlukan campur tangan pengguna. Dalam praktiknya, skala secara linear sesuai dengan ukuran masalah, dan dengan itu berlaku untuk matriks yang sangat besar. Akhirnya, kami membentangkan eksperimen pada beberapa kumpulan data sintetik dan kehidupan sebenar, di mana kaedah kami memberikan hasil intuitif berkualiti tinggi. [[EENNDD]] persatuan silang; mdl; analisis kandungan dan pengindeksan"], [{"string": "Detecting adversarial advertisements in the wild In a large online advertising system , adversaries may attempt to profit from the creation of low quality or harmful advertisements . In this paper , we present a large scale data mining effort that detects and blocks such adversarial advertisements for the benefit and safety of our users . Because both false positives and false negatives have high cost , our deployed system uses a tiered strategy combining automated and semi-automated methods to ensure reliable classification . We also employ strategies to address the challenges of learning from highly skewed data at scale , allocating the effort of human experts , leveraging domain expert knowledge , and independently assessing the effectiveness of our system .", "keywords": ["data mining", "applications", "adversarial learning", "online advertisement"], "combined": "Detecting adversarial advertisements in the wild In a large online advertising system , adversaries may attempt to profit from the creation of low quality or harmful advertisements . In this paper , we present a large scale data mining effort that detects and blocks such adversarial advertisements for the benefit and safety of our users . Because both false positives and false negatives have high cost , our deployed system uses a tiered strategy combining automated and semi-automated methods to ensure reliable classification . We also employ strategies to address the challenges of learning from highly skewed data at scale , allocating the effort of human experts , leveraging domain expert knowledge , and independently assessing the effectiveness of our system . [[EENNDD]] data mining; applications; adversarial learning; online advertisement"}, "Mengesan iklan bermasalah di alam bebas Dalam sistem pengiklanan dalam talian yang besar, musuh mungkin berusaha memperoleh keuntungan dari penciptaan iklan berkualiti rendah atau berbahaya. Dalam makalah ini, kami menyajikan usaha perlombongan data berskala besar yang mengesan dan menyekat iklan lawan seperti itu untuk kepentingan dan keselamatan pengguna kami. Oleh kerana kedua-dua positif palsu dan negatif palsu mempunyai biaya tinggi, sistem kami menggunakan strategi bertingkat yang menggabungkan kaedah automatik dan separa automatik untuk memastikan klasifikasi yang boleh dipercayai. Kami juga menggunakan strategi untuk mengatasi tantangan belajar dari data yang sangat miring pada skala, mengalokasikan usaha pakar manusia, memanfaatkan pengetahuan pakar domain, dan menilai keberkesanan sistem kami secara bebas. [[EENNDD]] perlombongan data; permohonan; pembelajaran lawan; iklan dalam talian"], [{"string": "SVM selective sampling for ranking with application to data retrieval Learning ranking or preference functions has been a major issue in the machine learning community and has produced many applications in information retrieval . SVMs Support Vector Machines - a classification and regression methodology - have also shown excellent performance in learning ranking functions . They effectively learn ranking functions of high generalization based on the `` large-margin '' principle and also systematically support nonlinear ranking by the `` kernel trick '' . In this paper , we propose an SVM selective sampling technique for learning ranking functions . SVM selective sampling or active learning with SVM has been studied in the context of classification . Such techniques reduce the labeling effort in learning classification functions by selecting only the most informative samples to be labeled . However , they are not extendable to learning ranking functions , as the labeled data in ranking is relative ordering , or partial orders of data . Our proposed sampling technique effectively learns an accurate SVM ranking function with fewer partial orders . We apply our sampling technique to the data retrieval application , which enables fuzzy search on relational databases by interacting with users for learning their preferences . Experimental results show a significant reduction of the labeling effort in inducing accurate ranking functions .", "keywords": ["ranking", "support vector machine", "selective sampling", "active learning", "miscellaneous"], "combined": "SVM selective sampling for ranking with application to data retrieval Learning ranking or preference functions has been a major issue in the machine learning community and has produced many applications in information retrieval . SVMs Support Vector Machines - a classification and regression methodology - have also shown excellent performance in learning ranking functions . They effectively learn ranking functions of high generalization based on the `` large-margin '' principle and also systematically support nonlinear ranking by the `` kernel trick '' . In this paper , we propose an SVM selective sampling technique for learning ranking functions . SVM selective sampling or active learning with SVM has been studied in the context of classification . Such techniques reduce the labeling effort in learning classification functions by selecting only the most informative samples to be labeled . However , they are not extendable to learning ranking functions , as the labeled data in ranking is relative ordering , or partial orders of data . Our proposed sampling technique effectively learns an accurate SVM ranking function with fewer partial orders . We apply our sampling technique to the data retrieval application , which enables fuzzy search on relational databases by interacting with users for learning their preferences . Experimental results show a significant reduction of the labeling effort in inducing accurate ranking functions . [[EENNDD]] ranking; support vector machine; selective sampling; active learning; miscellaneous"}, "Pensampelan selektif SVM untuk pemeringkatan dengan aplikasi ke pengambilan data Ranking pembelajaran atau fungsi pilihan telah menjadi isu utama dalam komuniti pembelajaran mesin dan telah menghasilkan banyak aplikasi dalam pencarian maklumat. Mesin Vektor Sokongan SVM - metodologi klasifikasi dan regresi - juga menunjukkan prestasi yang sangat baik dalam pembelajaran fungsi peringkat. Mereka secara efektif mempelajari fungsi peringkat generalisasi tinggi berdasarkan prinsip \"margin-besar\" dan juga secara sistematik menyokong peringkat nonlinear oleh \"kernel trick\". Dalam makalah ini, kami mencadangkan teknik pensampelan selektif SVM untuk belajar fungsi peringkat. Persampelan selektif SVM atau pembelajaran aktif dengan SVM telah dikaji dalam konteks klasifikasi. Teknik sedemikian mengurangkan usaha pelabelan dalam mempelajari fungsi klasifikasi dengan memilih hanya sampel yang paling bermaklumat untuk dilabel. Namun, mereka tidak dapat diperluas ke fungsi peringkat belajar, karena data berlabel dalam peringkat adalah susunan relatif, atau pesanan sebagian data. Teknik pensampelan yang dicadangkan kami berkesan mempelajari fungsi peringkat SVM yang tepat dengan pesanan separa lebih sedikit. Kami menerapkan teknik pensampelan kami pada aplikasi pengambilan data, yang memungkinkan pencarian kabur pada pangkalan data relasional dengan berinteraksi dengan pengguna untuk mempelajari pilihan mereka. Hasil eksperimen menunjukkan penurunan yang signifikan dari usaha pelabelan dalam mendorong fungsi peringkat yang tepat. [[EENNDD]] kedudukan; mesin vektor sokongan; persampelan selektif; pembelajaran aktif; pelbagai"], [{"string": "A parallel learning algorithm for text classification Text classification is the process of classifying documents into predefined categories based on their content . Existing supervised learning algorithms to automatically classify text need sufficient labeled documents to learn accurately . Applying the Expectation-Maximization EM algorithm to this problem is an alternative approach that utilizes a large pool of unlabeled documents to augment the available labeled documents . Unfortunately , the time needed to learn with these large unlabeled documents is too high . This paper introduces a novel parallel learning algorithm for text classification task . The parallel algorithm is based on the combination of the EM algorithm and the naive Bayes classifier . Our goal is to improve the computational time in learning and classifying process . We studied the performance of our parallel algorithm on a large Linux PC cluster called PIRUN Cluster . We report both timing and accuracy results . These results indicate that the proposed parallel algorithm is capable of handling large document collections .", "keywords": ["text classification", "parallel expectation-maximization algorithm", "naive bayes", "cluster computing"], "combined": "A parallel learning algorithm for text classification Text classification is the process of classifying documents into predefined categories based on their content . Existing supervised learning algorithms to automatically classify text need sufficient labeled documents to learn accurately . Applying the Expectation-Maximization EM algorithm to this problem is an alternative approach that utilizes a large pool of unlabeled documents to augment the available labeled documents . Unfortunately , the time needed to learn with these large unlabeled documents is too high . This paper introduces a novel parallel learning algorithm for text classification task . The parallel algorithm is based on the combination of the EM algorithm and the naive Bayes classifier . Our goal is to improve the computational time in learning and classifying process . We studied the performance of our parallel algorithm on a large Linux PC cluster called PIRUN Cluster . We report both timing and accuracy results . These results indicate that the proposed parallel algorithm is capable of handling large document collections . [[EENNDD]] text classification; parallel expectation-maximization algorithm; naive bayes; cluster computing"}, "Algoritma pembelajaran selari untuk klasifikasi teks Klasifikasi teks adalah proses mengklasifikasikan dokumen ke dalam kategori yang ditentukan berdasarkan kandungannya. Algoritma pembelajaran yang diselia yang ada untuk mengklasifikasikan teks secara automatik memerlukan dokumen berlabel yang mencukupi untuk belajar dengan tepat. Menerapkan algoritma EM Ekspektasi-Maksimum untuk masalah ini adalah pendekatan alternatif yang menggunakan banyak dokumen tanpa label untuk menambah dokumen berlabel yang tersedia. Malangnya, masa yang diperlukan untuk belajar dengan dokumen berlabel besar ini terlalu tinggi. Makalah ini memperkenalkan algoritma pembelajaran selari novel untuk tugas klasifikasi teks. Algoritma selari didasarkan pada gabungan algoritma EM dan pengkelasan Bayes yang naif. Matlamat kami adalah untuk meningkatkan masa pengiraan dalam proses pembelajaran dan pengkelasan. Kami mengkaji prestasi algoritma selari kami pada kluster PC Linux besar yang disebut PIRUN Cluster. Kami melaporkan keputusan masa dan ketepatan. Hasil ini menunjukkan bahawa algoritma selari yang dicadangkan mampu menangani koleksi dokumen yang besar. [[EENNDD]] pengelasan teks; algoritma jangkaan-maksimisasi selari; bayes naif; pengkomputeran kluster"], [{"string": "Fast query execution for retrieval models based on path-constrained random walks Many recommendation and retrieval tasks can be represented as proximity queries on a labeled directed graph , with typed nodes representing documents , terms , and metadata , and labeled edges representing the relationships between them . Recent work has shown that the accuracy of the widely-used random-walk-based proximity measures can be improved by supervised learning - in particular , one especially effective learning technique is based on Path-Constrained Random Walks PCRW , in which similarity is defined by a learned combination of constrained random walkers , each constrained to follow only a particular sequence of edge labels away from the query nodes . The PCRW based method significantly outperformed unsupervised random walk based queries , and models with learned edge weights . Unfortunately , PCRW query systems are expensive to evaluate . In this study we evaluate the use of approximations to the computation of the PCRW distributions , including fingerprinting , particle filtering , and truncation strategies . In experiments on several recommendation and retrieval problems using two large scientific publications corpora we show speedups of factors of 2 to 100 with little loss in accuracy .", "keywords": ["path-constrained random walks", "information search and retrieval", "learning to rank", "filtering and recommending", "relational retrieval"], "combined": "Fast query execution for retrieval models based on path-constrained random walks Many recommendation and retrieval tasks can be represented as proximity queries on a labeled directed graph , with typed nodes representing documents , terms , and metadata , and labeled edges representing the relationships between them . Recent work has shown that the accuracy of the widely-used random-walk-based proximity measures can be improved by supervised learning - in particular , one especially effective learning technique is based on Path-Constrained Random Walks PCRW , in which similarity is defined by a learned combination of constrained random walkers , each constrained to follow only a particular sequence of edge labels away from the query nodes . The PCRW based method significantly outperformed unsupervised random walk based queries , and models with learned edge weights . Unfortunately , PCRW query systems are expensive to evaluate . In this study we evaluate the use of approximations to the computation of the PCRW distributions , including fingerprinting , particle filtering , and truncation strategies . In experiments on several recommendation and retrieval problems using two large scientific publications corpora we show speedups of factors of 2 to 100 with little loss in accuracy . [[EENNDD]] path-constrained random walks; information search and retrieval; learning to rank; filtering and recommending; relational retrieval"}, "Pelaksanaan pertanyaan cepat untuk model pengambilan berdasarkan jalan rawak yang dibatasi oleh jalan. Banyak tugas cadangan dan pengambilan dapat ditunjukkan sebagai pertanyaan jarak dekat pada grafik yang diarahkan berlabel, dengan node yang ditaip mewakili dokumen, istilah, dan metadata, dan tepi berlabel yang mewakili hubungan antara mereka. Hasil kerja terbaru menunjukkan bahawa ketepatan langkah-langkah jarak berjalan secara rawak yang digunakan secara meluas dapat ditingkatkan dengan pembelajaran yang diawasi - khususnya, satu teknik pembelajaran yang sangat berkesan adalah berdasarkan Path-Constrained Random Walks PCRW, di mana persamaan ditentukan oleh kombinasi terpelajar dari pejalan kaki rawak terkekang, masing-masing terpaksa mengikuti urutan label tepi tertentu sahaja dari nod pertanyaan. Kaedah berasaskan PCRW secara signifikan mengatasi pertanyaan berdasarkan jalan rawak tanpa pengawasan, dan model dengan berat kelebihan yang dipelajari. Malangnya, sistem pertanyaan PCRW mahal untuk dinilai. Dalam kajian ini kami menilai penggunaan pendekatan untuk pengiraan pengedaran PCRW, termasuk cap jari, penyaringan partikel, dan strategi pemotongan. Dalam eksperimen pada beberapa cadangan dan masalah pengambilan menggunakan dua syarikat penerbitan ilmiah besar, kami menunjukkan peningkatan faktor 2 hingga 100 dengan kehilangan ketepatan yang sedikit. [[EENNDD]] jalan rawak yang dikendalikan oleh jalan; carian dan pengambilan maklumat; belajar berpangkat; menapis dan mengesyorkan; pengambilan hubungan"], [{"string": "Statistical modeling of large-scale simulation data With the advent of fast computer systems , scientists are now able to generate terabytes of simulation data . Unfortunately , the sheer size of these data sets has made efficient exploration of them impossible . To aid scientists in gleaning insight from their simulation data , we have developed an ad-hoc query infrastructure . Our system , called AQSim short for Ad-hoc Queries for Simulation reduces the data storage requirements and query access times in two stages . First , it creates and stores mathematical and statistical models of the data at multiple resolutions . Second , it evaluates queries on the models of the data instead of on the entire data set . In this paper , we present two simple but effective statistical modeling techniques for simulation data . Our first modeling technique computes the `` true '' unbiased mean of systematic partitions of the data . It makes no assumptions about the distribution of the data and uses a variant of the root mean square error to evaluate a model . Our second statistical modeling technique uses the Andersen-Darling goodness-of-fit method on systematic partitions of the data . This method evaluates a model by how well it passes the normality test on the data . Both of our statistical models effectively answer range queries . At each resolution of the data , we compute the precision of our answer to the user 's query by scaling the one-sided Chebyshev Inequalities with the original mesh 's topology . We combine precisions at different resolutions by calculating their weighted average . Our experimental evaluations on two scientific simulation data sets illustrate the value of using these statistical modeling techniques on multiple resolutions of large simulation data sets .", "keywords": ["statistical modeling", "large-scale scientific data sets", "approximate ad-hoc queries"], "combined": "Statistical modeling of large-scale simulation data With the advent of fast computer systems , scientists are now able to generate terabytes of simulation data . Unfortunately , the sheer size of these data sets has made efficient exploration of them impossible . To aid scientists in gleaning insight from their simulation data , we have developed an ad-hoc query infrastructure . Our system , called AQSim short for Ad-hoc Queries for Simulation reduces the data storage requirements and query access times in two stages . First , it creates and stores mathematical and statistical models of the data at multiple resolutions . Second , it evaluates queries on the models of the data instead of on the entire data set . In this paper , we present two simple but effective statistical modeling techniques for simulation data . Our first modeling technique computes the `` true '' unbiased mean of systematic partitions of the data . It makes no assumptions about the distribution of the data and uses a variant of the root mean square error to evaluate a model . Our second statistical modeling technique uses the Andersen-Darling goodness-of-fit method on systematic partitions of the data . This method evaluates a model by how well it passes the normality test on the data . Both of our statistical models effectively answer range queries . At each resolution of the data , we compute the precision of our answer to the user 's query by scaling the one-sided Chebyshev Inequalities with the original mesh 's topology . We combine precisions at different resolutions by calculating their weighted average . Our experimental evaluations on two scientific simulation data sets illustrate the value of using these statistical modeling techniques on multiple resolutions of large simulation data sets . [[EENNDD]] statistical modeling; large-scale scientific data sets; approximate ad-hoc queries"}, "Pemodelan statistik data simulasi berskala besar Dengan munculnya sistem komputer yang cepat, para saintis kini dapat menghasilkan terabyte data simulasi. Malangnya, ukuran set data ini tidak mustahil membuat penerokaan yang efisien. Untuk membantu para saintis dalam mengumpulkan pandangan dari data simulasi mereka, kami telah mengembangkan infrastruktur pertanyaan ad-hoc. Sistem kami, yang dipanggil AQSim pendek untuk Ad-hoc Queries for Simulation mengurangkan keperluan penyimpanan data dan masa akses pertanyaan dalam dua peringkat. Pertama, ia membuat dan menyimpan model matematik dan statistik data pada pelbagai resolusi. Kedua, ia menilai pertanyaan pada model data dan bukannya pada keseluruhan kumpulan data. Dalam makalah ini, kami menyajikan dua teknik pemodelan statistik yang mudah tetapi berkesan untuk data simulasi. Teknik pemodelan pertama kami mengira rata-rata partisi data yang \"tidak benar\" yang tidak berat sebelah. Ia tidak membuat andaian mengenai penyebaran data dan menggunakan varian ralat punca rata-rata untuk menilai model. Teknik pemodelan statistik kedua kami menggunakan kaedah Andersen-Darling kebaikan-fit pada partisi data yang sistematik. Kaedah ini menilai model dengan seberapa baik ia lulus ujian normalitas pada data. Kedua-dua model statistik kami berkesan menjawab pelbagai pertanyaan. Pada setiap ketetapan data, kami menghitung ketepatan jawapan kami terhadap pertanyaan pengguna dengan menskalakan Ketidakseimbangan Chebyshev satu sisi dengan topologi mesh yang asli. Kami menggabungkan ketepatan pada resolusi yang berbeza dengan mengira purata wajarannya. Penilaian eksperimental kami pada dua set data simulasi saintifik menggambarkan nilai menggunakan teknik pemodelan statistik ini pada pelbagai resolusi set data simulasi besar. [[EENNDD]] pemodelan statistik; set data saintifik berskala besar; anggaran pertanyaan ad-hoc"], [{"string": "Segmentation-based modeling for advanced targeted marketing Fingerhut Business Intelligence BI has a long and successful history of building statistical models to predict consumer behavior . The models constructed are typically segmentation-based models in which the target audience is split into subpopulations i.e. , customer segments and individually tailored statistical models are then developed for each segment . Such models are commonly employed in the direct-mail industry ; however , segmentation is often performed on an ad-hoc basis without directly considering how segmentation affects the accuracy of the resulting segment models . Fingerhut BI approached IBM Research with the problem of how to build segmentation-based models more effectively so as to maximize predictive accuracy . The IBM Advanced Targeted Marketing-Single EventsTM IBM ATM-SETM solution is the result of IBM Research and Fingerhut BI directing their efforts jointly towards solving this problem . This paper presents an evaluation of ATM-SE 's modeling capabilities using data from Fingerhut 's catalog mailings .", "keywords": ["decision trees", "segmentation-based models", "targeted marketing", "linear regression", "logistic regression", "database applications", "feature selection"], "combined": "Segmentation-based modeling for advanced targeted marketing Fingerhut Business Intelligence BI has a long and successful history of building statistical models to predict consumer behavior . The models constructed are typically segmentation-based models in which the target audience is split into subpopulations i.e. , customer segments and individually tailored statistical models are then developed for each segment . Such models are commonly employed in the direct-mail industry ; however , segmentation is often performed on an ad-hoc basis without directly considering how segmentation affects the accuracy of the resulting segment models . Fingerhut BI approached IBM Research with the problem of how to build segmentation-based models more effectively so as to maximize predictive accuracy . The IBM Advanced Targeted Marketing-Single EventsTM IBM ATM-SETM solution is the result of IBM Research and Fingerhut BI directing their efforts jointly towards solving this problem . This paper presents an evaluation of ATM-SE 's modeling capabilities using data from Fingerhut 's catalog mailings . [[EENNDD]] decision trees; segmentation-based models; targeted marketing; linear regression; logistic regression; database applications; feature selection"}, "Pemodelan berdasarkan segmentasi untuk pemasaran yang disasarkan maju Fingerhut Business Intelligence BI mempunyai sejarah panjang dan berjaya dalam membangun model statistik untuk meramalkan tingkah laku pengguna. Model yang dibina biasanya adalah model berdasarkan segmentasi di mana sasaran sasaran dibahagikan kepada subpopulasi iaitu, segmen pelanggan dan model statistik yang disesuaikan secara individu kemudian dikembangkan untuk setiap segmen. Model sedemikian biasanya digunakan dalam industri surat terus; namun, segmentasi sering dilakukan secara ad-hoc tanpa mempertimbangkan secara langsung bagaimana segmentasi mempengaruhi ketepatan model segmen yang dihasilkan. Fingerhut BI mendekati IBM Research dengan masalah bagaimana membina model berdasarkan segmentasi dengan lebih berkesan sehingga dapat memaksimumkan ketepatan ramalan. Penyelesaian IBM Advanced Targeted Marketing-Single EventsTM IBM ATM-SETM adalah hasil IBM Research dan Fingerhut BI mengarahkan usaha mereka bersama untuk menyelesaikan masalah ini. Makalah ini menyajikan penilaian kemampuan pemodelan ATM-SE menggunakan data dari surat katalog Fingerhut. [[EENNDD]] pokok keputusan; model berasaskan segmentasi; pemasaran yang disasarkan; regresi linear; regresi logistik; aplikasi pangkalan data; pemilihan ciri"], [{"string": "Failure detection and localization in component based systems by online tracking The increasing complexity of today 's systems makes fast and accurate failure detection essential for their use in mission-critical applications . Various monitoring methods provide a large amount of data about system 's behavior . Analyzing this data with advanced statistical methods holds the promise of not only detecting the errors faster , but also detecting errors which are difficult to catch with current monitoring tools . Two challenges to building such detection tools are : the high dimensionality of observation data , which makes the models expensive to apply , and frequent system changes , which make the models expensive to update . In this paper , we present algorithms to reduce the dimensionality of data in a way that makes it easy to adapt to system changes . We decompose the observation data into signal and noise subspaces . Two statistics , the Hotelling T2 score and squared prediction error SPE are calculated to represent the data characteristics in signal and noise subspaces respectively . Instead of tracking the original data , we use a sequentially discounting expectation maximization SDEM algorithm to learn the distribution of the two extracted statistics . A failure event can then be detected based on the abnormal change of the distribution . Applying our technique to component interaction data in a simple e-commerce application shows better accuracy than building independent profiles for each component . Additionally , experiments on synthetic data show that the detection accuracy is high even for changing systems .", "keywords": ["internet services", "online tracking", "distributed computing", "learning", "subspace decomposition", "failure detection", "statistics", "system management"], "combined": "Failure detection and localization in component based systems by online tracking The increasing complexity of today 's systems makes fast and accurate failure detection essential for their use in mission-critical applications . Various monitoring methods provide a large amount of data about system 's behavior . Analyzing this data with advanced statistical methods holds the promise of not only detecting the errors faster , but also detecting errors which are difficult to catch with current monitoring tools . Two challenges to building such detection tools are : the high dimensionality of observation data , which makes the models expensive to apply , and frequent system changes , which make the models expensive to update . In this paper , we present algorithms to reduce the dimensionality of data in a way that makes it easy to adapt to system changes . We decompose the observation data into signal and noise subspaces . Two statistics , the Hotelling T2 score and squared prediction error SPE are calculated to represent the data characteristics in signal and noise subspaces respectively . Instead of tracking the original data , we use a sequentially discounting expectation maximization SDEM algorithm to learn the distribution of the two extracted statistics . A failure event can then be detected based on the abnormal change of the distribution . Applying our technique to component interaction data in a simple e-commerce application shows better accuracy than building independent profiles for each component . Additionally , experiments on synthetic data show that the detection accuracy is high even for changing systems . [[EENNDD]] internet services; online tracking; distributed computing; learning; subspace decomposition; failure detection; statistics; system management"}, "Pengesanan kegagalan dan penyetempatan dalam sistem berasaskan komponen dengan penjejakan dalam talian Kerumitan sistem yang semakin meningkat menjadikan pengesanan kegagalan yang cepat dan tepat penting untuk penggunaannya dalam aplikasi yang penting. Pelbagai kaedah pemantauan memberikan sejumlah besar data mengenai tingkah laku sistem. Menganalisis data ini dengan kaedah statistik canggih memegang janji untuk tidak hanya mengesan kesalahan lebih cepat, tetapi juga mengesan kesalahan yang sukar ditangkap dengan alat pemantauan semasa. Dua cabaran untuk membangun alat pengesanan seperti itu: dimensi tinggi data pemerhatian, yang menjadikan model mahal untuk diterapkan, dan perubahan sistem yang kerap, yang menjadikan model mahal untuk diperbarui. Dalam makalah ini, kami menyajikan algoritma untuk mengurangkan dimensi data dengan cara yang memudahkan untuk menyesuaikan diri dengan perubahan sistem. Kami menguraikan data pemerhatian ke ruang isyarat dan bunyi. Dua statistik, skor Hotelling T2 dan SPE ralat ramalan kuasa dua dikira untuk mewakili ciri-ciri data di ruang bawah isyarat dan bunyi. Daripada menjejaki data asal, kami menggunakan algoritma SDEM pemaksimalan jangkaan penguraian secara berurutan untuk mempelajari pengedaran dua statistik yang diekstrak. Kejadian kegagalan kemudian dapat dikesan berdasarkan perubahan taburan yang tidak normal. Menerapkan teknik kami ke data interaksi komponen dalam aplikasi e-dagang yang sederhana menunjukkan ketepatan yang lebih baik daripada membina profil bebas untuk setiap komponen. Selain itu, eksperimen pada data sintetik menunjukkan bahawa ketepatan pengesanan adalah tinggi walaupun untuk mengubah sistem. [[EENNDD]] perkhidmatan internet; penjejakan dalam talian; pengkomputeran diedarkan; belajar; penguraian ruang bawah; pengesanan kegagalan; statistik; pengurusan sistem"], [{"string": "Mining frequent item sets by opportunistic projection In this paper , we present a novel algorithm Opportune Project for mining complete set of frequent item sets by projecting databases to grow a frequent item set tree . Our algorithm is fundamentally different from those proposed in the past in that it opportunistically chooses between two different structures , array-based or tree-based , to represent projected transaction subsets , and heuristically decides to build unfiltered pseudo projection or to make a filtered copy according to features of the subsets . More importantly , we propose novel methods to build tree-based pseudo projections and array-based unfiltered projections for projected transaction subsets , which makes our algorithm both CPU time efficient and memory saving . Basically , the algorithm grows the frequent item set tree by depth first search , whereas breadth first search is used to build the upper portion of the tree if necessary . We test our algorithm versus several other algorithms on real world datasets , such as BMS-POS , and on IBM artificial datasets . The empirical results show that our algorithm is not only the most efficient on both sparse and dense databases at all levels of support threshold , but also highly scalable to very large databases .", "keywords": ["frequent patterns", "association rules"], "combined": "Mining frequent item sets by opportunistic projection In this paper , we present a novel algorithm Opportune Project for mining complete set of frequent item sets by projecting databases to grow a frequent item set tree . Our algorithm is fundamentally different from those proposed in the past in that it opportunistically chooses between two different structures , array-based or tree-based , to represent projected transaction subsets , and heuristically decides to build unfiltered pseudo projection or to make a filtered copy according to features of the subsets . More importantly , we propose novel methods to build tree-based pseudo projections and array-based unfiltered projections for projected transaction subsets , which makes our algorithm both CPU time efficient and memory saving . Basically , the algorithm grows the frequent item set tree by depth first search , whereas breadth first search is used to build the upper portion of the tree if necessary . We test our algorithm versus several other algorithms on real world datasets , such as BMS-POS , and on IBM artificial datasets . The empirical results show that our algorithm is not only the most efficient on both sparse and dense databases at all levels of support threshold , but also highly scalable to very large databases . [[EENNDD]] frequent patterns; association rules"}, "Melombong set item kerap dengan unjuran oportunistik Dalam makalah ini, kami menyajikan algoritma baru Projek Peluang untuk melombong set lengkap item yang kerap dengan memproyeksikan pangkalan data untuk menanam pokok set item yang kerap. Algoritma kami pada asasnya berbeza dari yang dicadangkan pada masa lalu kerana ia secara oportunis memilih antara dua struktur yang berbeza, berdasarkan array atau berdasarkan pokok, untuk mewakili subset transaksi yang diproyeksikan, dan secara heuristis memutuskan untuk membuat unjuran pseudo tanpa filter atau membuat salinan yang disaring mengikut ke ciri subset. Lebih penting lagi, kami mencadangkan kaedah baru untuk membina unjuran pseudo berasaskan pohon dan unjuran tanpa filter berdasarkan larik untuk subset transaksi yang diproyeksikan, yang menjadikan algoritma kami baik masa CPU dan penjimatan memori. Pada asasnya, algoritma menumbuhkan pokok set item yang kerap dengan carian pertama yang mendalam, sedangkan carian luas pertama digunakan untuk membina bahagian atas pokok jika perlu. Kami menguji algoritma kami berbanding beberapa algoritma lain pada set data dunia nyata, seperti BMS-POS, dan pada set data buatan IBM. Hasil empirikal menunjukkan bahawa algoritma kami bukan sahaja paling efisien pada pangkalan data yang jarang dan padat di semua tahap ambang sokongan, tetapi juga sangat sesuai untuk pangkalan data yang sangat besar. [[EENNDD]] corak yang kerap; peraturan persatuan"], [{"string": "Audience selection for on-line brand advertising : privacy-friendly social network targeting This paper describes and evaluates privacy-friendly methods for extracting quasi-social networks from browser behavior on user-generated content sites , for the purpose of finding good audiences for brand advertising as opposed to click maximizing , for example . Targeting social-network neighbors resonates well with advertisers , and on-line browsing behavior data counterintuitively can allow the identification of good audiences anonymously . Besides being one of the first papers to our knowledge on data mining for on-line brand advertising , this paper makes several important contributions . We introduce a framework for evaluating brand audiences , in analogy to predictive-modeling holdout evaluation . We introduce methods for extracting quasi-social networks from data on visitations to social networking pages , without collecting any information on the identities of the browsers or the content of the social-network pages . We introduce measures of brand proximity in the network , and show that audiences with high brand proximity indeed show substantially higher brand affinity . Finally , we provide evidence that the quasi-social network embeds a true social network , which along with results from social theory offers one explanation for the increase in brand affinity of the selected audiences .", "keywords": ["privacy", "on-line advertising", "social networks", "predictive modeling", "user-generated content"], "combined": "Audience selection for on-line brand advertising : privacy-friendly social network targeting This paper describes and evaluates privacy-friendly methods for extracting quasi-social networks from browser behavior on user-generated content sites , for the purpose of finding good audiences for brand advertising as opposed to click maximizing , for example . Targeting social-network neighbors resonates well with advertisers , and on-line browsing behavior data counterintuitively can allow the identification of good audiences anonymously . Besides being one of the first papers to our knowledge on data mining for on-line brand advertising , this paper makes several important contributions . We introduce a framework for evaluating brand audiences , in analogy to predictive-modeling holdout evaluation . We introduce methods for extracting quasi-social networks from data on visitations to social networking pages , without collecting any information on the identities of the browsers or the content of the social-network pages . We introduce measures of brand proximity in the network , and show that audiences with high brand proximity indeed show substantially higher brand affinity . Finally , we provide evidence that the quasi-social network embeds a true social network , which along with results from social theory offers one explanation for the increase in brand affinity of the selected audiences . [[EENNDD]] privacy; on-line advertising; social networks; predictive modeling; user-generated content"}, "Pemilihan khalayak untuk iklan jenama on-line: penargetan rangkaian sosial yang mesra privasi Makalah ini menerangkan dan menilai kaedah mesra privasi untuk mengekstrak rangkaian sosial-sosial dari tingkah laku penyemak imbas di laman web yang dihasilkan pengguna, dengan tujuan mencari khalayak yang baik untuk iklan jenama berbanding memaksimumkan klik, sebagai contoh. Mensasarkan tetangga rangkaian sosial sesuai dengan pengiklan, dan data tingkah laku melayari dalam talian secara bertentangan dapat memungkinkan pengenalan khalayak yang baik tanpa nama. Selain menjadi salah satu makalah pertama untuk pengetahuan kami mengenai perlombongan data untuk iklan jenama dalam talian, makalah ini memberikan beberapa sumbangan penting. Kami memperkenalkan kerangka kerja untuk menilai khalayak jenama, dengan analogi kepada penilaian penahanan model-prediktif. Kami memperkenalkan kaedah untuk mengekstrak rangkaian kuasi-sosial dari data lawatan ke laman rangkaian sosial, tanpa mengumpulkan maklumat mengenai identiti penyemak imbas atau isi dari laman rangkaian sosial. Kami memperkenalkan ukuran kedekatan jenama dalam rangkaian, dan menunjukkan bahawa khalayak dengan kedekatan jenama yang tinggi memang menunjukkan pertalian jenama yang jauh lebih tinggi. Akhirnya, kami memberikan bukti bahawa rangkaian kuasi-sosial merangkumi rangkaian sosial yang benar, yang bersama dengan hasil dari teori sosial menawarkan satu penjelasan untuk peningkatan pertalian jenama khalayak yang dipilih. [[EENNDD]] privasi; pengiklanan dalam talian; rangkaian sosial; pemodelan ramalan; kandungan yang dihasilkan pengguna"], [{"string": "Understandable models Of music collections based on exhaustive feature generation with temporal statistics Data mining in large collections of polyphonic music has recently received increasing interest by companies along with the advent of commercial online distribution of music . Important applications include the categorization of songs into genres and the recommendation of songs according to musical similarity and the customer 's musical preferences . Modeling genre or timbre of polyphonic music is at the core of these tasks and has been recognized as a difficult problem . Many audio features have been proposed , but they do not provide easily understandable descriptions of music . They do not explain why a genre was chosen or in which way one song is similar to another . We present an approach that combines large scale feature generation with meta learning techniques to obtain meaningful features for musical similarity . We perform exhaustive feature generation based on temporal statistics and train regression models to summarize a subset of these features into a single descriptor of a particular notion of music . Using several such models we produce a concise semantic description of each song . Genre classification models based on these semantic features are shown to be better understandable and almost as accurate as traditional methods .", "keywords": ["feature generation", "meta learning", "logistic regression", "miscellaneous", "genre classification", "music mining"], "combined": "Understandable models Of music collections based on exhaustive feature generation with temporal statistics Data mining in large collections of polyphonic music has recently received increasing interest by companies along with the advent of commercial online distribution of music . Important applications include the categorization of songs into genres and the recommendation of songs according to musical similarity and the customer 's musical preferences . Modeling genre or timbre of polyphonic music is at the core of these tasks and has been recognized as a difficult problem . Many audio features have been proposed , but they do not provide easily understandable descriptions of music . They do not explain why a genre was chosen or in which way one song is similar to another . We present an approach that combines large scale feature generation with meta learning techniques to obtain meaningful features for musical similarity . We perform exhaustive feature generation based on temporal statistics and train regression models to summarize a subset of these features into a single descriptor of a particular notion of music . Using several such models we produce a concise semantic description of each song . Genre classification models based on these semantic features are shown to be better understandable and almost as accurate as traditional methods . [[EENNDD]] feature generation; meta learning; logistic regression; miscellaneous; genre classification; music mining"}, "Model koleksi muzik yang dapat difahami berdasarkan penjanaan ciri yang lengkap dengan statistik temporal Perlombongan data dalam koleksi besar muzik polifonik baru-baru ini mendapat minat yang meningkat oleh syarikat seiring dengan munculnya pengedaran muzik dalam talian komersial. Aplikasi penting termasuk pengkategorian lagu menjadi genre dan saranan lagu mengikut kesamaan muzik dan pilihan muzik pelanggan. Pemodelan genre atau timbre muzik polifonik adalah teras tugas ini dan telah diakui sebagai masalah yang sukar. Banyak ciri audio telah diusulkan, tetapi tidak memberikan gambaran muzik yang mudah difahami. Mereka tidak menjelaskan mengapa genre dipilih atau dengan cara mana satu lagu serupa dengan yang lain. Kami menyajikan pendekatan yang menggabungkan penjanaan fitur berskala besar dengan teknik meta pembelajaran untuk mendapatkan ciri yang bermakna untuk kesamaan muzik. Kami melakukan penjanaan fitur yang lengkap berdasarkan statistik temporal dan model regresi kereta api untuk meringkaskan subset dari ciri-ciri ini menjadi deskriptor tunggal mengenai konsep muzik tertentu. Dengan menggunakan beberapa model seperti itu, kami menghasilkan penerangan semantik yang ringkas untuk setiap lagu. Model klasifikasi genre berdasarkan ciri semantik ini terbukti lebih mudah difahami dan hampir setepat kaedah tradisional. [[EENNDD]] penjanaan ciri; pembelajaran meta; regresi logistik; pelbagai; klasifikasi genre; perlombongan muzik"], [{"string": "Local sparsity control for naive Bayes with extreme misclassification costs In applications of data mining characterized by highly skewed misclassification costs certain types of errors become virtually unacceptable . This limits the utility of a classifier to a range in which such constraints can be met . Naive Bayes , which has proven to be very useful in text mining applications due to high scalability , can be particularly affected . Although its 0\\/1 loss tends to be small , its misclassifications are often made with apparently high confidence . Aside from efforts to better calibrate Naive Bayes scores , it has been shown that its accuracy depends on document sparsity and feature selection can lead to marked improvement in classification performance . Traditionally , sparsity is controlled globally , and the result for any particular document may vary . In this work we examine the merits of local sparsity control for Naive Bayes in the context of highly asymmetric misclassification costs . In experiments with three benchmark document collections we demonstrate clear advantages of document-level feature selection . In the extreme cost setting , multinomial Naive Bayes with local sparsity control is able to outperform even some of the recently proposed effective improvements to the Naive Bayes classifier . There are also indications that local feature selection may be preferable in different cost settings .", "keywords": ["high recall classification", "text categorization", "feature selection", "naive bayes"], "combined": "Local sparsity control for naive Bayes with extreme misclassification costs In applications of data mining characterized by highly skewed misclassification costs certain types of errors become virtually unacceptable . This limits the utility of a classifier to a range in which such constraints can be met . Naive Bayes , which has proven to be very useful in text mining applications due to high scalability , can be particularly affected . Although its 0\\/1 loss tends to be small , its misclassifications are often made with apparently high confidence . Aside from efforts to better calibrate Naive Bayes scores , it has been shown that its accuracy depends on document sparsity and feature selection can lead to marked improvement in classification performance . Traditionally , sparsity is controlled globally , and the result for any particular document may vary . In this work we examine the merits of local sparsity control for Naive Bayes in the context of highly asymmetric misclassification costs . In experiments with three benchmark document collections we demonstrate clear advantages of document-level feature selection . In the extreme cost setting , multinomial Naive Bayes with local sparsity control is able to outperform even some of the recently proposed effective improvements to the Naive Bayes classifier . There are also indications that local feature selection may be preferable in different cost settings . [[EENNDD]] high recall classification; text categorization; feature selection; naive bayes"}, "Pengawalan sparsiti tempatan untuk Bayes naif dengan kos salah klasifikasi yang melampau Dalam aplikasi perlombongan data yang dicirikan oleh kos misklasifikasi yang sangat miring, jenis kesalahan tertentu menjadi hampir tidak dapat diterima. Ini mengehadkan kegunaan pengkelasan ke julat di mana kekangan tersebut dapat dipenuhi. Naive Bayes, yang terbukti sangat berguna dalam aplikasi perlombongan teks kerana skalabilitas yang tinggi, dapat sangat terjejas. Walaupun kerugian 0 \\ / 1 cenderung kecil, salah klasifikasinya sering dibuat dengan keyakinan yang tinggi. Selain usaha untuk mengkalibrasi skor Naive Bayes dengan lebih baik, telah ditunjukkan bahawa ketepatannya bergantung pada kelangkaan dokumen dan pemilihan ciri dapat menyebabkan peningkatan prestasi klasifikasi yang ketara. Secara tradisinya, sparsity dikawal secara global, dan hasilnya untuk setiap dokumen tertentu mungkin berbeza. Dalam karya ini, kami mengkaji kelebihan kawalan sparsity tempatan untuk Naive Bayes dalam konteks kos salah klasifikasi yang sangat tidak simetri. Dalam eksperimen dengan tiga koleksi dokumen penanda aras, kami menunjukkan kelebihan yang jelas dari pemilihan ciri peringkat dokumen. Dalam penetapan kos yang melampau, Naive Bayes multinomial dengan kawalan sparsity tempatan mampu mengungguli bahkan beberapa penambahbaikan berkesan yang baru-baru ini dicadangkan kepada pengkelasan Naive Bayes. Terdapat juga petunjuk bahawa pemilihan ciri tempatan mungkin lebih baik dalam pengaturan kos yang berbeza. [[EENNDD]] klasifikasi ingat tinggi; pengkategorian teks; pemilihan ciri; bayes naif"], [{"string": "Integration of profile hidden Markov model output into association rule mining Scientific models typically depend on parameters . Preserving the parameter dependence of models in the pattern mining context opens up several applications . Within association rule mining ARM , the choice of parameters can be studied with more flexibly then in traditional model building . Studying support , confidence , and other rule metrics as a function of model parameters allows conclusions on assumptions underlying the models . We present efficient techniques to handle multiple model output data sets at little more than the cost of one . We integrate output from hidden Markov models into the association rule mining framework , demonstrating the potential for frequent pattern mining in the field of scientific modeling and experimentation .", "keywords": ["model mining", "profile hidden markov model", "association rule mining"], "combined": "Integration of profile hidden Markov model output into association rule mining Scientific models typically depend on parameters . Preserving the parameter dependence of models in the pattern mining context opens up several applications . Within association rule mining ARM , the choice of parameters can be studied with more flexibly then in traditional model building . Studying support , confidence , and other rule metrics as a function of model parameters allows conclusions on assumptions underlying the models . We present efficient techniques to handle multiple model output data sets at little more than the cost of one . We integrate output from hidden Markov models into the association rule mining framework , demonstrating the potential for frequent pattern mining in the field of scientific modeling and experimentation . [[EENNDD]] model mining; profile hidden markov model; association rule mining"}, "Penggabungan profil model Markov yang tersembunyi ke dalam perlombongan peraturan persatuan Model saintifik biasanya bergantung pada parameter. Memelihara pergantungan parameter model dalam konteks perlombongan pola membuka beberapa aplikasi. Dalam ARM perlombongan peraturan persatuan, pilihan parameter dapat dikaji dengan lebih fleksibel daripada pada pembuatan model tradisional. Mempelajari sokongan, keyakinan, dan metrik peraturan lain sebagai fungsi parameter model memungkinkan kesimpulan mengenai andaian yang mendasari model. Kami menyajikan teknik yang cekap untuk menangani beberapa set data output model dengan harga yang lebih tinggi daripada satu. Kami mengintegrasikan output dari model Markov yang tersembunyi ke dalam kerangka perlombongan peraturan persatuan, menunjukkan potensi perlombongan corak yang kerap dalam bidang pemodelan dan eksperimen saintifik. [[EENNDD]] perlombongan model; model markov tersembunyi profil; perlombongan peraturan persatuan"], [{"string": "Correlation search in graph databases Correlation mining has gained great success in many application domains for its ability to capture the underlying dependency between objects . However , the research of correlation mining from graph databases is still lacking despite the fact that graph data , especially in various scientific domains , proliferate in recent years . In this paper , we propose a new problem of correlation mining from graph databases , called Correlated Graph Search CGS . CGS adopts Pearson 's correlation coefficient as a correlation measure to take into consideration the occurrence distributions of graphs . However , the problem poses significant challenges , since every subgraph of a graph in the database is a candidate but the number of subgraphs is exponential . We derive two necessary conditions which set bounds on the occurrence probability of a candidate in the database . With this result , we design an efficient algorithm that operates on a much smaller projected database and thus we are able to obtain a significantly smaller set of candidates . To further improve the efficiency , we develop three heuristic rules and apply them on the candidate set to further reduce the search space . Our extensive experiments demonstrate the effectiveness of our method on candidate reduction . The results also justify the efficiency of our algorithm in mining correlations from large real and synthetic datasets .", "keywords": ["pearson's correlation coefficient", "graph databases", "correlation"], "combined": "Correlation search in graph databases Correlation mining has gained great success in many application domains for its ability to capture the underlying dependency between objects . However , the research of correlation mining from graph databases is still lacking despite the fact that graph data , especially in various scientific domains , proliferate in recent years . In this paper , we propose a new problem of correlation mining from graph databases , called Correlated Graph Search CGS . CGS adopts Pearson 's correlation coefficient as a correlation measure to take into consideration the occurrence distributions of graphs . However , the problem poses significant challenges , since every subgraph of a graph in the database is a candidate but the number of subgraphs is exponential . We derive two necessary conditions which set bounds on the occurrence probability of a candidate in the database . With this result , we design an efficient algorithm that operates on a much smaller projected database and thus we are able to obtain a significantly smaller set of candidates . To further improve the efficiency , we develop three heuristic rules and apply them on the candidate set to further reduce the search space . Our extensive experiments demonstrate the effectiveness of our method on candidate reduction . The results also justify the efficiency of our algorithm in mining correlations from large real and synthetic datasets . [[EENNDD]] pearson's correlation coefficient; graph databases; correlation"}, "Pencarian korelasi dalam pangkalan data grafik Perlombongan korelasi telah memperoleh kejayaan besar di banyak domain aplikasi kerana kemampuannya untuk menangkap ketergantungan yang mendasari antara objek. Walau bagaimanapun, penyelidikan perlombongan korelasi dari pangkalan data grafik masih kurang walaupun data grafik, terutama dalam pelbagai domain saintifik, berkembang pesat dalam beberapa tahun terakhir. Dalam makalah ini, kami mencadangkan masalah baru perlombongan korelasi dari pangkalan data grafik, yang disebut Correlated Graph Search CGS. CGS menggunakan pekali korelasi Pearson sebagai ukuran korelasi untuk mengambil kira taburan grafik yang berlaku. Walau bagaimanapun, masalah ini menimbulkan cabaran yang besar, kerana setiap subgraf grafik dalam pangkalan data adalah calon tetapi jumlah subgraf adalah eksponensial. Kami memperoleh dua syarat yang diperlukan yang menetapkan kemungkinan berlakunya calon dalam pangkalan data. Dengan hasil ini, kami merancang algoritma yang cekap yang beroperasi pada pangkalan data yang diproyeksikan jauh lebih kecil dan dengan itu kami dapat memperoleh satu set calon yang jauh lebih kecil. Untuk meningkatkan lagi kecekapan, kami mengembangkan tiga peraturan heuristik dan menerapkannya pada calon yang akan mengurangi ruang pencarian. Eksperimen kami yang luas menunjukkan keberkesanan kaedah kami terhadap pengurangan calon. Hasilnya juga membenarkan kecekapan algoritma kami dalam korelasi perlombongan dari kumpulan data nyata dan sintetik. [[EENNDD]] pekali korelasi pearson; pangkalan data grafik; korelasi"], [{"string": "The mathematics of causal inference I will review concepts , principles , and mathematical tools that were found useful in applications involving causal and counterfactual relationships . This semantical framework , enriched with a few ideas from logic and graph theory , gives rise to a complete , coherent , and friendly calculus of causation that unifies the graphical and counterfactual approaches to causation and resolves many long-standing problems in several of the sciences . These include questions of causal effect estimation , policy analysis , and the integration of data from diverse studies . Of special interest to KDD researchers would be the following topics : The Mediation Formula , and what it tells us about direct and indirect effects . What mathematics can tell us about `` external validity '' or `` generalizing from experiments '' What can graph theory tell us about recovering from sample-selection bias .", "keywords": ["miscellaneous", "causal inference"], "combined": "The mathematics of causal inference I will review concepts , principles , and mathematical tools that were found useful in applications involving causal and counterfactual relationships . This semantical framework , enriched with a few ideas from logic and graph theory , gives rise to a complete , coherent , and friendly calculus of causation that unifies the graphical and counterfactual approaches to causation and resolves many long-standing problems in several of the sciences . These include questions of causal effect estimation , policy analysis , and the integration of data from diverse studies . Of special interest to KDD researchers would be the following topics : The Mediation Formula , and what it tells us about direct and indirect effects . What mathematics can tell us about `` external validity '' or `` generalizing from experiments '' What can graph theory tell us about recovering from sample-selection bias . [[EENNDD]] miscellaneous; causal inference"}, "Matematik inferens kausal Saya akan mengkaji konsep, prinsip, dan alat matematik yang didapati berguna dalam aplikasi yang melibatkan hubungan sebab dan akibat. Kerangka semantik ini, yang diperkaya dengan beberapa idea dari teori logik dan grafik, menimbulkan kalkulus penyebab yang lengkap, koheren, dan ramah yang menyatukan pendekatan grafik dan kontraktual untuk penyebab dan menyelesaikan banyak masalah lama dalam beberapa sains. Ini merangkumi persoalan estimasi kesan kausal, analisis kebijakan, dan penyatuan data dari pelbagai kajian. Yang menarik bagi para penyelidik KDD adalah topik berikut: Formula Mediasi, dan apa yang diceritakannya mengenai kesan langsung dan tidak langsung. Apa yang dapat diberitahu oleh matematik kepada kita mengenai \"kesahan luaran\" atau \"generalisasi dari eksperimen\" Apa teori grafik dapat memberitahu kita tentang pemulihan dari bias pemilihan sampel. [[EENNDD]] pelbagai; inferens sebab"], [{"string": "Fast direction-aware proximity for graph mining In this paper we study asymmetric proximity measures on directed graphs , which quantify the relationships between two nodes or two groups of nodes . The measures are useful in several graph mining tasks , including clustering , link prediction and connection subgraph discovery . Our proximity measure is based on the conceptof escape probability . This way , we strive to summarize the multiple facets of nodes-proximity , while avoiding some of the pitfalls to which alternative proximity measures are susceptible . A unique feature of the measures is accounting for the underlying directional information . We put a special emphasis on computational efficiency , and develop fast solutions that are applicable in several settings . Our experimental study shows the usefulness of our proposed direction-aware proximity method for several applications , and that our algorithms achieve a significant speedup up to 50,000 x over straight forward implementations .", "keywords": ["proximity", "random walk", "graph mining", "miscellaneous"], "combined": "Fast direction-aware proximity for graph mining In this paper we study asymmetric proximity measures on directed graphs , which quantify the relationships between two nodes or two groups of nodes . The measures are useful in several graph mining tasks , including clustering , link prediction and connection subgraph discovery . Our proximity measure is based on the conceptof escape probability . This way , we strive to summarize the multiple facets of nodes-proximity , while avoiding some of the pitfalls to which alternative proximity measures are susceptible . A unique feature of the measures is accounting for the underlying directional information . We put a special emphasis on computational efficiency , and develop fast solutions that are applicable in several settings . Our experimental study shows the usefulness of our proposed direction-aware proximity method for several applications , and that our algorithms achieve a significant speedup up to 50,000 x over straight forward implementations . [[EENNDD]] proximity; random walk; graph mining; miscellaneous"}, "Kedekatan arah cepat untuk perlombongan grafik Dalam makalah ini kita mengkaji langkah-langkah kedekatan asimetri pada graf yang diarahkan, yang mengukur hubungan antara dua nod atau dua kumpulan nod. Langkah-langkah itu berguna dalam beberapa tugas perlombongan grafik, termasuk pengelompokan, ramalan pautan dan penemuan subgraf sambungan. Ukuran kedekatan kami berdasarkan konsep kebarangkalian melarikan diri. Dengan cara ini, kita berusaha untuk meringkaskan pelbagai aspek jarak-simpul, sambil mengelakkan beberapa perangkap yang mana langkah-langkah jarak dekatnya rentan. Ciri unik langkah-langkah tersebut adalah memperakui maklumat arah yang mendasari. Kami memberi penekanan khusus pada kecekapan komputasi, dan mengembangkan penyelesaian pantas yang dapat diterapkan dalam beberapa tetapan. Kajian eksperimental kami menunjukkan kegunaan kaedah kedekatan arah yang kami cadangkan untuk beberapa aplikasi, dan algoritma kami mencapai peningkatan yang signifikan hingga 50,000 x berbanding pelaksanaan lurus ke depan. [[EENNDD]] berdekatan; jalan rawak; perlombongan grafik; pelbagai"], [{"string": "Similarity analysis on government regulations Government regulations are semi-structured text documents that are often voluminous , heavily cross-referenced between provisions and even ambiguous . Multiple sources of regulations lead to difficulties in both understanding and complying with all applicable codes . In this work , we propose a framework for regulation management and similarity analysis . An online repository for legal documents is created with the help of text mining tool , and users can access regulatory documents either through the natural hierarchy of provisions or from a taxonomy generated by knowledge engineers based on concepts . Our similarity analysis core identifies relevant provisions and brings them to the user 's attention , and this is performed by utilizing both the hierarchical and referential structures of regulations to provide a better comparison between provisions . Preliminary results show that our system reveals hidden similarities that are not apparent between provisions based on node content comparisons .", "keywords": ["legal informatics", "regulations", "similarity analysis", "text mining"], "combined": "Similarity analysis on government regulations Government regulations are semi-structured text documents that are often voluminous , heavily cross-referenced between provisions and even ambiguous . Multiple sources of regulations lead to difficulties in both understanding and complying with all applicable codes . In this work , we propose a framework for regulation management and similarity analysis . An online repository for legal documents is created with the help of text mining tool , and users can access regulatory documents either through the natural hierarchy of provisions or from a taxonomy generated by knowledge engineers based on concepts . Our similarity analysis core identifies relevant provisions and brings them to the user 's attention , and this is performed by utilizing both the hierarchical and referential structures of regulations to provide a better comparison between provisions . Preliminary results show that our system reveals hidden similarities that are not apparent between provisions based on node content comparisons . [[EENNDD]] legal informatics; regulations; similarity analysis; text mining"}, "Analisis kesamaan pada peraturan pemerintah Peraturan pemerintah adalah dokumen teks semi-struktur yang sering kali banyak, banyak rujukan antara peruntukan dan bahkan tidak jelas. Pelbagai sumber peraturan menyebabkan kesukaran dalam memahami dan mematuhi semua kod yang berlaku. Dalam karya ini, kami mencadangkan kerangka kerja untuk pengurusan peraturan dan analisis kesamaan. Sebuah repositori dalam talian untuk dokumen undang-undang dibuat dengan bantuan alat perlombongan teks, dan pengguna dapat mengakses dokumen peraturan baik melalui hierarki peruntukan semula jadi atau dari taksonomi yang dihasilkan oleh jurutera pengetahuan berdasarkan konsep. Inti analisis kesamaan kami mengenal pasti ketentuan yang relevan dan membawanya ke perhatian pengguna, dan ini dilakukan dengan memanfaatkan struktur peraturan hierarki dan rujukan untuk memberikan perbandingan yang lebih baik antara ketentuan. Hasil awal menunjukkan bahawa sistem kami mendedahkan persamaan tersembunyi yang tidak jelas antara peruntukan berdasarkan perbandingan kandungan nod. [[EENNDD]] informatik undang-undang; peraturan; analisis kesamaan; perlombongan teks"], [{"string": "A generalized maximum entropy approach to bregman co-clustering and matrix approximation Co-clustering is a powerful data mining technique with varied applications such as text clustering , microarray analysis and recommender systems . Recently , an information-theoretic co-clustering approach applicable to empirical joint probability distributions was proposed . In many situations , co-clustering of more general matrices is desired . In this paper , we present a substantially generalized co-clustering framework wherein any Bregman divergence can be used in the objective function , and various conditional expectation based constraints can be considered based on the statistics that need to be preserved . Analysis of the co-clustering problem leads to the minimum Bregman information principle , which generalizes the maximum entropy principle , and yields an elegant meta algorithm that is guaranteed to achieve local optimality . Our methodology yields new algorithms and also encompasses several previously known clustering and co-clustering algorithms based on alternate minimization .", "keywords": ["bregman divergences", "co-clustering", "matrix approximation", "learning"], "combined": "A generalized maximum entropy approach to bregman co-clustering and matrix approximation Co-clustering is a powerful data mining technique with varied applications such as text clustering , microarray analysis and recommender systems . Recently , an information-theoretic co-clustering approach applicable to empirical joint probability distributions was proposed . In many situations , co-clustering of more general matrices is desired . In this paper , we present a substantially generalized co-clustering framework wherein any Bregman divergence can be used in the objective function , and various conditional expectation based constraints can be considered based on the statistics that need to be preserved . Analysis of the co-clustering problem leads to the minimum Bregman information principle , which generalizes the maximum entropy principle , and yields an elegant meta algorithm that is guaranteed to achieve local optimality . Our methodology yields new algorithms and also encompasses several previously known clustering and co-clustering algorithms based on alternate minimization . [[EENNDD]] bregman divergences; co-clustering; matrix approximation; learning"}, "Pendekatan entropi maksimum yang umum untuk kluster bersama bregman dan pendekatan matriks Co-clustering adalah teknik perlombongan data yang kuat dengan aplikasi yang bervariasi seperti pengelompokan teks, analisis microarray dan sistem pengesyorkan. Baru-baru ini, pendekatan pengelompokan bersama teori-maklumat yang berlaku untuk pengagihan kebarangkalian bersama empirikal dicadangkan. Dalam banyak keadaan, pengelompokan matriks yang lebih umum diinginkan. Dalam makalah ini, kami menyajikan kerangka kerja pengelompokan bersama secara umum di mana perbezaan Bregman dapat digunakan dalam fungsi objektif, dan berbagai batasan berdasarkan harapan dapat dipertimbangkan berdasarkan statistik yang perlu dipertahankan. Analisis masalah kluster bersama mengarah pada prinsip maklumat Bregman minimum, yang menyamaratakan prinsip entropi maksimum, dan menghasilkan algoritma meta elegan yang dijamin dapat mencapai tahap optimum tempatan. Metodologi kami menghasilkan algoritma baru dan juga merangkumi beberapa algoritma clustering dan co-clustering yang diketahui sebelumnya berdasarkan pengurangan alternatif. [[EENNDD]] perbezaan bregman; penggabungan bersama; penghampiran matriks; belajar"], [{"string": "Land cover change detection : a case study The study of land cover change is an important problem in the Earth Science domain because of its impacts on local climate , radiation balance , biogeochemistry , hydrology , and the diversity and abundance of terrestrial species . Most well-known change detection techniques from statistics , signal processing and control theory are not well-suited for the massive high-dimensional spatio-temporal data sets from Earth Science due to limitations such as high computational complexity and the inability to take advantage of seasonality and spatio-temporal autocorrelation inherent in Earth Science data . In our work , we seek to address these challenges with new change detection techniques that are based on data mining approaches . Specifically , in this paper we have performed a case study for a new change detection technique for the land cover change detection problem . We study land cover change in the state of California , focusing on the San Francisco Bay Area and perform an extended study on the entire state . We also perform a comparative evaluation on forests in the entire state . These results demonstrate the utility of data mining techniques for the land cover change detection problem .", "keywords": ["time series", "change detection", "land cover", "land use"], "combined": "Land cover change detection : a case study The study of land cover change is an important problem in the Earth Science domain because of its impacts on local climate , radiation balance , biogeochemistry , hydrology , and the diversity and abundance of terrestrial species . Most well-known change detection techniques from statistics , signal processing and control theory are not well-suited for the massive high-dimensional spatio-temporal data sets from Earth Science due to limitations such as high computational complexity and the inability to take advantage of seasonality and spatio-temporal autocorrelation inherent in Earth Science data . In our work , we seek to address these challenges with new change detection techniques that are based on data mining approaches . Specifically , in this paper we have performed a case study for a new change detection technique for the land cover change detection problem . We study land cover change in the state of California , focusing on the San Francisco Bay Area and perform an extended study on the entire state . We also perform a comparative evaluation on forests in the entire state . These results demonstrate the utility of data mining techniques for the land cover change detection problem . [[EENNDD]] time series; change detection; land cover; land use"}, "Pengesanan perubahan tutupan tanah: kajian kes Kajian perubahan tutupan tanah adalah masalah penting dalam domain Sains Bumi kerana kesannya terhadap iklim tempatan, keseimbangan radiasi, biogeokimia, hidrologi, dan kepelbagaian dan kelimpahan spesies terestrial. Teknik pengesanan perubahan yang paling terkenal dari statistik, pemprosesan isyarat dan teori kawalan tidak sesuai untuk kumpulan data spatio-temporal dimensi tinggi besar-besaran dari Sains Bumi kerana keterbatasan seperti kerumitan komputasi yang tinggi dan ketidakupayaan untuk memanfaatkan musim dan autokorelasi spatio-temporal yang wujud dalam data Sains Bumi. Dalam kerja kami, kami berusaha untuk menangani cabaran ini dengan teknik pengesanan perubahan baru yang berdasarkan pendekatan perlombongan data. Secara khusus, dalam makalah ini kami telah melakukan studi kasus untuk teknik pengesanan perubahan baru untuk masalah pengesanan perubahan penutup tanah. Kami mengkaji perubahan penutup tanah di negara bagian California, dengan fokus di San Francisco Bay Area dan melakukan kajian lanjutan di seluruh negeri. Kami juga melakukan penilaian perbandingan hutan di seluruh negeri. Hasil ini menunjukkan kegunaan teknik perlombongan data untuk masalah pengesanan perubahan penutup tanah. [[EENNDD]] siri masa; pengesanan perubahan; perlindungan tanah; penggunaan tanah"], [{"string": "Using retrieval measures to assess similarity in mining dynamic web clickstreams While scalable data mining methods are expected to cope with massive Web data , coping with evolving trends in noisy data in a continuous fashion , and without any unnecessary stoppages and reconfigurations is still an open challenge . This dynamic and single pass setting can be cast within the framework of mining evolving data streams . In this paper , we explore the task of mining mass user profiles by discovering evolving Web session clusters in a single pass with a recently proposed scalable immune based clustering approach TECNO-STREAMS , and study the effect of the choice of different similarity measures on the mining process and on the interpretation of the mined patterns . We propose a simple similarity measure that has the advantage of explicitly coupling the precision and coverage criteria to the early learning stages , and furthermore requiring that the affinity of the data to the learned profiles or summaries be defined by the minimum of their coverage or precision , hence requiring that the learned profiles are simultaneously precise and complete , with no compromises . In our experiments , we study the task of mining evolving user profiles from Web clickstream data web usage mining in a single pass , and under different trend sequencing scenarios , showing that compared oto the cosine similarity measure , the proposed similarity measure explicitly based on precision and coverage allows the discovery of more correct profiles at the same precision or recall quality levels .", "keywords": ["personalization", "stream data mining", "mining evolving data", "artificial immune systems", "web mining", "clustering"], "combined": "Using retrieval measures to assess similarity in mining dynamic web clickstreams While scalable data mining methods are expected to cope with massive Web data , coping with evolving trends in noisy data in a continuous fashion , and without any unnecessary stoppages and reconfigurations is still an open challenge . This dynamic and single pass setting can be cast within the framework of mining evolving data streams . In this paper , we explore the task of mining mass user profiles by discovering evolving Web session clusters in a single pass with a recently proposed scalable immune based clustering approach TECNO-STREAMS , and study the effect of the choice of different similarity measures on the mining process and on the interpretation of the mined patterns . We propose a simple similarity measure that has the advantage of explicitly coupling the precision and coverage criteria to the early learning stages , and furthermore requiring that the affinity of the data to the learned profiles or summaries be defined by the minimum of their coverage or precision , hence requiring that the learned profiles are simultaneously precise and complete , with no compromises . In our experiments , we study the task of mining evolving user profiles from Web clickstream data web usage mining in a single pass , and under different trend sequencing scenarios , showing that compared oto the cosine similarity measure , the proposed similarity measure explicitly based on precision and coverage allows the discovery of more correct profiles at the same precision or recall quality levels . [[EENNDD]] personalization; stream data mining; mining evolving data; artificial immune systems; web mining; clustering"}, "Menggunakan langkah-langkah pengambilan untuk menilai kesamaan dalam aliran klik web dinamik perlombongan Walaupun kaedah penambangan data berskala diharapkan dapat mengatasi data Web yang besar, mengatasi tren yang berkembang dalam data yang bising secara berterusan, dan tanpa penghentian dan konfigurasi semula yang tidak perlu masih menjadi cabaran terbuka. Pengaturan lulus dinamik dan lulus tunggal ini dapat dilemparkan dalam kerangka aliran data berkembang perlombongan. Dalam makalah ini, kami meneroka tugas melombong profil pengguna massa dengan menemui kelompok sesi Web yang berkembang dalam satu hantaran dengan pendekatan pengelompokan berdasarkan imun yang baru-baru ini yang dicadangkan, TECNO-STREAMS, dan mengkaji kesan pilihan langkah persamaan yang berbeza terhadap perlombongan proses dan pada tafsiran corak perlombongan. Kami mencadangkan ukuran kesamaan sederhana yang mempunyai kelebihan menggabungkan kriteria ketepatan dan liputan secara eksplisit dengan tahap pembelajaran awal, dan selanjutnya mengharuskan pertalian data dengan profil atau ringkasan yang dipelajari ditentukan oleh minimum liputan atau ketepatannya, oleh itu memerlukan profil yang dipelajari secara serentak tepat dan lengkap, tanpa kompromi. Dalam eksperimen kami, kami mengkaji tugas melombong profil pengguna yang berkembang dari perlombongan penggunaan data aliran data klik dalam satu pas, dan di bawah senario urutan trend yang berbeza, menunjukkan bahawa dibandingkan dengan ukuran kesamaan kosinus, ukuran kesamaan yang dicadangkan secara eksplisit berdasarkan ketepatan dan liputan membolehkan penemuan profil yang lebih tepat pada tahap kualiti ketepatan atau penarikan yang sama. [[EENNDD]] pemperibadian; melombong data aliran; data berkembang perlombongan; sistem imun buatan; perlombongan web; pengelompokan"], [{"string": "Spatially regularized logistic regression for disease mapping on large moving populations Spatial analysis of disease risk , or disease mapping , typically relies on information about the residence and health status of individuals from population under study . However , residence information has its limitations because people are exposed to numerous disease risks as they spend time outside of their residences . Thanks to the wide-spread use of mobile phones and GPS-enabled devices , it is becoming possible to obtain a detailed record about the movement of human populations . Availability of movement information opens up an opportunity to improve the accuracy of disease mapping . Starting with an assumption that an individual 's disease risk is a weighted average of risks at the locations which were visited , we show that disease mapping can be accomplished by spatially regularized logistic regression . Due to the inherent sparsity of movement data , the proposed approach can be applied to large populations and over large spatial grids . In our experiments , we were able to map disease for a simulated population with 1.6 million people and a spatial grid with 65 thousand locations in several minutes . The results indicate that movement information can improve the accuracy of disease mapping as compared to residential data only . We also studied a privacy-preserving scenario in which only the aggregate statistics are available about the movement of the overall population , while detailed movement information is available only for individuals with disease . The results indicate that the accuracy of disease mapping remains satisfactory when learning from movement data sanitized in this way .", "keywords": ["privacy", "spatial-temporal data mining", "regularization", "movement trajectories", "disease mapping", "spatial epidemiology"], "combined": "Spatially regularized logistic regression for disease mapping on large moving populations Spatial analysis of disease risk , or disease mapping , typically relies on information about the residence and health status of individuals from population under study . However , residence information has its limitations because people are exposed to numerous disease risks as they spend time outside of their residences . Thanks to the wide-spread use of mobile phones and GPS-enabled devices , it is becoming possible to obtain a detailed record about the movement of human populations . Availability of movement information opens up an opportunity to improve the accuracy of disease mapping . Starting with an assumption that an individual 's disease risk is a weighted average of risks at the locations which were visited , we show that disease mapping can be accomplished by spatially regularized logistic regression . Due to the inherent sparsity of movement data , the proposed approach can be applied to large populations and over large spatial grids . In our experiments , we were able to map disease for a simulated population with 1.6 million people and a spatial grid with 65 thousand locations in several minutes . The results indicate that movement information can improve the accuracy of disease mapping as compared to residential data only . We also studied a privacy-preserving scenario in which only the aggregate statistics are available about the movement of the overall population , while detailed movement information is available only for individuals with disease . The results indicate that the accuracy of disease mapping remains satisfactory when learning from movement data sanitized in this way . [[EENNDD]] privacy; spatial-temporal data mining; regularization; movement trajectories; disease mapping; spatial epidemiology"}, "Regresi logistik secara spasial untuk pemetaan penyakit pada populasi bergerak besar Analisis spasial risiko penyakit, atau pemetaan penyakit, biasanya bergantung pada maklumat mengenai tempat tinggal dan status kesihatan individu dari populasi yang dikaji. Walau bagaimanapun, maklumat tempat tinggal mempunyai batasan kerana orang terdedah kepada banyak risiko penyakit kerana mereka menghabiskan masa di luar kediaman mereka. Berkat penggunaan telefon bimbit dan peranti berkemampuan GPS yang meluas, menjadi mustahil untuk memperoleh rekod terperinci mengenai pergerakan populasi manusia. Ketersediaan maklumat pergerakan membuka peluang untuk meningkatkan ketepatan pemetaan penyakit. Dimulai dengan anggapan bahawa risiko penyakit seseorang adalah rata-rata risiko tertimbang di lokasi yang dikunjungi, kami menunjukkan bahawa pemetaan penyakit dapat dilakukan dengan regresi logistik yang disusun secara spasial. Oleh kerana kelangkaan data pergerakan yang melekat, pendekatan yang dicadangkan dapat diterapkan pada populasi besar dan lebih banyak grid spasial. Dalam eksperimen kami, kami dapat memetakan penyakit untuk populasi simulasi dengan 1.6 juta orang dan grid spasial dengan 65 ribu lokasi dalam beberapa minit. Hasilnya menunjukkan bahawa maklumat pergerakan dapat meningkatkan ketepatan pemetaan penyakit jika dibandingkan dengan data tempat tinggal sahaja. Kami juga mengkaji senario pemeliharaan privasi di mana hanya statistik agregat yang tersedia mengenai pergerakan penduduk secara keseluruhan, sementara maklumat pergerakan terperinci hanya tersedia untuk individu yang mempunyai penyakit. Hasilnya menunjukkan bahawa ketepatan pemetaan penyakit tetap memuaskan ketika belajar dari data pergerakan yang dibersihkan dengan cara ini. [[EENNDD]] privasi; perlombongan data spatial-temporal; pengaturcaraan; lintasan pergerakan; pemetaan penyakit; epidemiologi spatial"], [{"string": "Learning sparse metrics via linear programming Calculation of object similarity , for example through a distance function , is a common part of data mining and machine learning algorithms . This calculation is crucial for efficiency since distances are usually evaluated a large number of times , the classical example being query-by-example find objects that are similar to a given query object . Moreover , the performance of these algorithms depends critically on choosing a good distance function . However , it is often the case that 1 the correct distance is unknown or chosen by hand , and 2 its calculation is computationally expensive e.g. , such as for large dimensional objects . In this paper , we propose a method for constructing relative-distance preserving low-dimensional mapping sparse mappings . This method allows learning unknown distance functions or approximating known functions with the additional property of reducing distance computation time . We present an algorithm that given examples of proximity comparisons among triples of objects object i is more like object j than object k , learns a distance function , in as few dimensions as possible , that preserves these distance relationships . The formulation is based on solving a linear programming optimization problem that finds an optimal mapping for the given dataset and distance relationships . Unlike other popular embedding algorithms , this method can easily generalize to new points , does not have local minima , and explicitly models computational efficiency by finding a mapping that is sparse , i.e. one that depends on a small subset of features or dimensions . Experimental evaluation shows that the proposed formulation compares favorably with a state-of-the art method in several publicly available datasets .", "keywords": ["metric learning", "linear programming", "information search and retrieval", "dimensionality reduction", "linear projections", "miscellaneous", "convex optimization", "relative distance constraints"], "combined": "Learning sparse metrics via linear programming Calculation of object similarity , for example through a distance function , is a common part of data mining and machine learning algorithms . This calculation is crucial for efficiency since distances are usually evaluated a large number of times , the classical example being query-by-example find objects that are similar to a given query object . Moreover , the performance of these algorithms depends critically on choosing a good distance function . However , it is often the case that 1 the correct distance is unknown or chosen by hand , and 2 its calculation is computationally expensive e.g. , such as for large dimensional objects . In this paper , we propose a method for constructing relative-distance preserving low-dimensional mapping sparse mappings . This method allows learning unknown distance functions or approximating known functions with the additional property of reducing distance computation time . We present an algorithm that given examples of proximity comparisons among triples of objects object i is more like object j than object k , learns a distance function , in as few dimensions as possible , that preserves these distance relationships . The formulation is based on solving a linear programming optimization problem that finds an optimal mapping for the given dataset and distance relationships . Unlike other popular embedding algorithms , this method can easily generalize to new points , does not have local minima , and explicitly models computational efficiency by finding a mapping that is sparse , i.e. one that depends on a small subset of features or dimensions . Experimental evaluation shows that the proposed formulation compares favorably with a state-of-the art method in several publicly available datasets . [[EENNDD]] metric learning; linear programming; information search and retrieval; dimensionality reduction; linear projections; miscellaneous; convex optimization; relative distance constraints"}, "Belajar metrik jarang melalui pengaturcaraan linear Pengiraan kesamaan objek, misalnya melalui fungsi jarak, adalah bahagian umum dari algoritma perlombongan data dan pembelajaran mesin. Pengiraan ini sangat penting untuk kecekapan kerana jarak biasanya dinilai sebilangan besar kali, contoh klasik adalah pertanyaan demi contoh mencari objek yang serupa dengan objek pertanyaan yang diberikan. Lebih-lebih lagi, prestasi algoritma ini sangat bergantung pada pemilihan fungsi jarak yang baik. Walau bagaimanapun, kebiasaannya 1 jarak yang betul tidak diketahui atau dipilih dengan tangan, dan 2 pengiraannya adalah mahal secara komputasi. , seperti untuk objek dimensi besar. Dalam makalah ini, kami mencadangkan kaedah untuk membina jarak relatif yang memelihara pemetaan jarang berdimensi pemetaan jarang. Kaedah ini membolehkan belajar fungsi jarak yang tidak diketahui atau menghampiri fungsi yang diketahui dengan sifat tambahan mengurangkan masa pengiraan jarak. Kami memaparkan algoritma yang memberikan contoh perbandingan jarak antara tiga objek objek i lebih seperti objek j daripada objek k, mempelajari fungsi jarak, dalam beberapa dimensi yang mungkin, yang memelihara hubungan jarak ini. Rumusannya didasarkan pada penyelesaian masalah pengoptimuman pengaturcaraan linier yang menemukan pemetaan optimum untuk hubungan set data dan jarak yang diberikan. Tidak seperti algoritma penyematan popular yang lain, kaedah ini dapat membuat generalisasi dengan mudah ke titik baru, tidak mempunyai minimum tempatan, dan secara eksplisit memodelkan kecekapan komputasi dengan mencari pemetaan yang jarang, iaitu yang bergantung pada sekumpulan kecil fitur atau dimensi. Penilaian eksperimental menunjukkan bahawa rumusan yang dicadangkan dibandingkan dengan kaedah terkini dalam beberapa set data yang tersedia untuk umum. [[EENNDD]] pembelajaran metrik; pengaturcaraan linear; carian dan pengambilan maklumat; pengurangan dimensi; unjuran linear; pelbagai; pengoptimuman cembung; kekangan jarak relatif"], [{"string": "On demand classification of data streams Current models of the classification problem do not effectively handle bursts of particular classes coming in at different times . In fact , the current model of the classification problem simply concentrates on methods for one-pass classification modeling of very large data sets . Our model for data stream classification views the data stream classification problem from the point of view of a dynamic approach in which simultaneous training and testing streams are used for dynamic classification of data sets . This model reflects real life situations effectively , since it is desirable to classify test streams in real time over an evolving training and test stream . The aim here is to create a classification system in which the training model can adapt quickly to the changes of the underlying data stream . In order to achieve this goal , we propose an on-demand classification process which can dynamically select the appropriate window of past training data to build the classifier . The empirical results indicate that the system maintains a high classification accuracy in an evolving data stream , while providing an efficient solution to the classification task .", "keywords": ["data streams", "classification"], "combined": "On demand classification of data streams Current models of the classification problem do not effectively handle bursts of particular classes coming in at different times . In fact , the current model of the classification problem simply concentrates on methods for one-pass classification modeling of very large data sets . Our model for data stream classification views the data stream classification problem from the point of view of a dynamic approach in which simultaneous training and testing streams are used for dynamic classification of data sets . This model reflects real life situations effectively , since it is desirable to classify test streams in real time over an evolving training and test stream . The aim here is to create a classification system in which the training model can adapt quickly to the changes of the underlying data stream . In order to achieve this goal , we propose an on-demand classification process which can dynamically select the appropriate window of past training data to build the classifier . The empirical results indicate that the system maintains a high classification accuracy in an evolving data stream , while providing an efficient solution to the classification task . [[EENNDD]] data streams; classification"}, "Berdasarkan permintaan klasifikasi aliran data Model semasa dari masalah klasifikasi tidak berkesan menangani ledakan kelas tertentu yang masuk pada masa yang berlainan. Sebenarnya, model masalah klasifikasi semasa hanya tertumpu pada kaedah untuk pemodelan klasifikasi satu pas bagi set data yang sangat besar. Model kami untuk klasifikasi aliran data melihat masalah klasifikasi aliran data dari sudut pendekatan dinamik di mana aliran latihan dan ujian serentak digunakan untuk klasifikasi dinamik set data. Model ini menggambarkan situasi kehidupan sebenar dengan berkesan, kerana adalah wajar untuk mengklasifikasikan aliran ujian dalam masa nyata berbanding aliran latihan dan ujian yang terus berkembang. Tujuannya di sini adalah untuk membuat sistem klasifikasi di mana model latihan dapat menyesuaikan diri dengan cepat terhadap perubahan aliran data yang mendasari. Untuk mencapai tujuan ini, kami mencadangkan proses klasifikasi berdasarkan permintaan yang dapat secara dinamis memilih tetingkap data latihan masa lalu yang sesuai untuk membina pengklasifikasi. Hasil empirik menunjukkan bahawa sistem mengekalkan ketepatan klasifikasi yang tinggi dalam aliran data yang berkembang, sambil memberikan penyelesaian yang efisien untuk tugas klasifikasi. [[EENNDD]] aliran data; pengelasan"], [{"string": "Probabilistic discovery of time series motifs Several important time series data mining problems reduce to the core task of finding approximately repeated subsequences in a longer time series . In an earlier work , we formalized the idea of approximately repeated subsequences by introducing the notion of time series motifs . Two limitations of this work were the poor scalability of the motif discovery algorithm , and the inability to discover motifs in the presence of noise . Here we address these limitations by introducing a novel algorithm inspired by recent advances in the problem of pattern discovery in biosequences . Our algorithm is probabilistic in nature , but as we show empirically and theoretically , it can find time series motifs with very high probability even in the presence of noise or `` do n't care '' symbols . Not only is the algorithm fast , but it is an anytime algorithm , producing likely candidate motifs almost immediately , and gradually improving the quality of results over time .", "keywords": ["data mining", "time series", "database applications", "motifs", "randomized algorithms"], "combined": "Probabilistic discovery of time series motifs Several important time series data mining problems reduce to the core task of finding approximately repeated subsequences in a longer time series . In an earlier work , we formalized the idea of approximately repeated subsequences by introducing the notion of time series motifs . Two limitations of this work were the poor scalability of the motif discovery algorithm , and the inability to discover motifs in the presence of noise . Here we address these limitations by introducing a novel algorithm inspired by recent advances in the problem of pattern discovery in biosequences . Our algorithm is probabilistic in nature , but as we show empirically and theoretically , it can find time series motifs with very high probability even in the presence of noise or `` do n't care '' symbols . Not only is the algorithm fast , but it is an anytime algorithm , producing likely candidate motifs almost immediately , and gradually improving the quality of results over time . [[EENNDD]] data mining; time series; database applications; motifs; randomized algorithms"}, "Penemuan probabilistik motif siri masa Beberapa masalah perlombongan data siri masa penting menjadi tugas utama untuk mencari kira-kira kejadian berulang dalam siri masa yang lebih lama. Dalam karya terdahulu, kami memformalkan idea tentang pengulangan berulang dengan memperkenalkan konsep motif siri masa. Dua batasan karya ini adalah skalabilitas buruk algoritma penemuan motif, dan ketidakmampuan untuk menemui motif dengan adanya bunyi. Di sini kita menangani batasan-batasan ini dengan memperkenalkan algoritma novel yang diilhami oleh kemajuan terkini dalam masalah penemuan corak dalam biokejadian. Algoritma kami bersifat probabilistik, tetapi seperti yang kami tunjukkan secara empirikal dan teoritis, ia dapat menemukan motif siri masa dengan kebarangkalian yang sangat tinggi walaupun terdapat bunyi atau simbol \"tidak peduli\". Algoritma tidak hanya cepat, tetapi juga algoritma kapan saja, menghasilkan motif calon yang mungkin segera, dan secara beransur-ansur meningkatkan kualiti hasil dari masa ke masa. [[EENNDD]] perlombongan data; siri masa; aplikasi pangkalan data; motif; algoritma rawak"], [{"string": "Learning subspace kernels for classification Kernel methods have been applied successfully in many data mining tasks . Subspace kernel learning was recently proposed to discover an effective low-dimensional subspace of a kernel feature space for improved classification . In this paper , we propose to construct a subspace kernel using the Hilbert-Schmidt Independence Criterion HSIC . We show that the optimal subspace kernel can be obtained efficiently by solving an eigenvalue problem . One limitation of the existing subspace kernel learning formulations is that the kernel learning and classification are independent and the subspace kernel may not be optimally adapted for classification . To overcome this limitation , we propose a joint optimization framework , in which we learn the subspace kernel and subsequent classifiers simultaneously . In addition , we propose a novel learning formulation that extracts an uncorrelated subspace kernel to reduce the redundant information in a subspace kernel . Following the idea from multiple kernel learning , we extend the proposed formulations to the case when multiple kernels are available and need to be combined . We show that the integration of subspace kernels can be formulated as a semidefinite program SDP which is computationally expensive . To improve the efficiency of the SDP formulation , we propose an equivalent semi-infinite linear program SILP formulation which can be solved efficiently by the column generation technique . Experimental results on a collection of benchmark data sets demonstrate the effectiveness of the proposed algorithms .", "keywords": ["subspace kernel", "hilbert-schmidt independence criterion", "classification", "support vector machines"], "combined": "Learning subspace kernels for classification Kernel methods have been applied successfully in many data mining tasks . Subspace kernel learning was recently proposed to discover an effective low-dimensional subspace of a kernel feature space for improved classification . In this paper , we propose to construct a subspace kernel using the Hilbert-Schmidt Independence Criterion HSIC . We show that the optimal subspace kernel can be obtained efficiently by solving an eigenvalue problem . One limitation of the existing subspace kernel learning formulations is that the kernel learning and classification are independent and the subspace kernel may not be optimally adapted for classification . To overcome this limitation , we propose a joint optimization framework , in which we learn the subspace kernel and subsequent classifiers simultaneously . In addition , we propose a novel learning formulation that extracts an uncorrelated subspace kernel to reduce the redundant information in a subspace kernel . Following the idea from multiple kernel learning , we extend the proposed formulations to the case when multiple kernels are available and need to be combined . We show that the integration of subspace kernels can be formulated as a semidefinite program SDP which is computationally expensive . To improve the efficiency of the SDP formulation , we propose an equivalent semi-infinite linear program SILP formulation which can be solved efficiently by the column generation technique . Experimental results on a collection of benchmark data sets demonstrate the effectiveness of the proposed algorithms . [[EENNDD]] subspace kernel; hilbert-schmidt independence criterion; classification; support vector machines"}, "Pembelajaran kernel subspace untuk klasifikasi Kaedah Kernel telah berjaya diterapkan dalam banyak tugas perlombongan data. Pembelajaran kernel subspace baru-baru ini diusulkan untuk mencari ruang ruang dimensi rendah yang berkesan dari ruang ciri kernel untuk klasifikasi yang lebih baik. Dalam makalah ini, kami mengusulkan untuk membina kernel subruang menggunakan Hilbert-Schmidt Independence Criterion HSIC. Kami menunjukkan bahawa kernel ruang bawah yang optimum dapat diperoleh dengan cekap dengan menyelesaikan masalah nilai eigen. Satu batasan dari formulasi pembelajaran kernel subspace yang ada adalah bahawa pembelajaran dan klasifikasi kernel bebas dan kernel subspace mungkin tidak disesuaikan secara optimum untuk klasifikasi. Untuk mengatasi batasan ini, kami mencadangkan kerangka pengoptimuman bersama, di mana kami mempelajari kernel subspace dan pengklasifikasi berikutnya secara serentak. Sebagai tambahan, kami mengusulkan formulasi pembelajaran novel yang mengekstrak kernel subspace yang tidak berkorelasi untuk mengurangkan maklumat berlebihan dalam kernel subspace. Mengikuti idea dari pembelajaran kernel berganda, kami memperluas rumusan yang diusulkan hingga beberapa kernel tersedia dan perlu digabungkan. Kami menunjukkan bahawa integrasi kernel ruang bawah dapat dirumuskan sebagai SDP program semidefinite yang sangat mahal. Untuk meningkatkan kecekapan formulasi SDP, kami mencadangkan program SILP program linear separa tak terhingga yang dapat diselesaikan dengan berkesan dengan teknik penjanaan lajur. Hasil eksperimen pada kumpulan set data penanda aras menunjukkan keberkesanan algoritma yang dicadangkan. [[EENNDD]] kernel subspace; kriteria kemerdekaan hilbert-schmidt; pengelasan; mesin vektor sokongan"], [{"string": "Discovering frequent patterns in sensitive data Discovering frequent patterns from data is a popular exploratory technique in datamining . However , if the data are sensitive e.g. , patient health records , user behavior records releasing information about significant patterns or trends carries significant risk to privacy . This paper shows how one can accurately discover and release the most significant patterns along with their frequencies in a data set containing sensitive information , while providing rigorous guarantees of privacy for the individuals whose information is stored there . We present two efficient algorithms for discovering the k most frequent patterns in a data set of sensitive records . Our algorithms satisfy differential privacy , a recently introduced definition that provides meaningful privacy guarantees in the presence of arbitrary external information . Differentially private algorithms require a degree of uncertainty in their output to preserve privacy . Our algorithms handle this by returning ` noisy ' lists of patterns that are close to the actual list of k most frequent patterns in the data . We define a new notion of utility that quantifies the output accuracy of private top-k pattern mining algorithms . In typical data sets , our utility criterion implies low false positive and false negative rates in the reported lists . We prove that our methods meet the new utility criterion ; we also demonstrate the performance of our algorithms through extensive experiments on the transaction data sets from the FIMI repository . While the paper focuses on frequent pattern mining , the techniques developed here are relevant whenever the data mining output is a list of elements ordered according to an appropriately ` robust ' measure of interest .", "keywords": ["general", "frequent itemsets", "privacy", "exponential mechanism", "differential privacy", "frequent patterns"], "combined": "Discovering frequent patterns in sensitive data Discovering frequent patterns from data is a popular exploratory technique in datamining . However , if the data are sensitive e.g. , patient health records , user behavior records releasing information about significant patterns or trends carries significant risk to privacy . This paper shows how one can accurately discover and release the most significant patterns along with their frequencies in a data set containing sensitive information , while providing rigorous guarantees of privacy for the individuals whose information is stored there . We present two efficient algorithms for discovering the k most frequent patterns in a data set of sensitive records . Our algorithms satisfy differential privacy , a recently introduced definition that provides meaningful privacy guarantees in the presence of arbitrary external information . Differentially private algorithms require a degree of uncertainty in their output to preserve privacy . Our algorithms handle this by returning ` noisy ' lists of patterns that are close to the actual list of k most frequent patterns in the data . We define a new notion of utility that quantifies the output accuracy of private top-k pattern mining algorithms . In typical data sets , our utility criterion implies low false positive and false negative rates in the reported lists . We prove that our methods meet the new utility criterion ; we also demonstrate the performance of our algorithms through extensive experiments on the transaction data sets from the FIMI repository . While the paper focuses on frequent pattern mining , the techniques developed here are relevant whenever the data mining output is a list of elements ordered according to an appropriately ` robust ' measure of interest . [[EENNDD]] general; frequent itemsets; privacy; exponential mechanism; differential privacy; frequent patterns"}, "Mencari corak kerap dalam data sensitif Mencari corak yang kerap dari data adalah teknik penerokaan yang popular dalam pemeriksaan data. Walau bagaimanapun, jika data sensitif mis. , rekod kesihatan pesakit, rekod tingkah laku pengguna yang melepaskan maklumat mengenai corak atau tren yang signifikan membawa risiko yang besar terhadap privasi. Makalah ini menunjukkan bagaimana seseorang dapat dengan tepat menemui dan melepaskan corak yang paling signifikan bersama dengan frekuensi mereka dalam satu set data yang mengandungi maklumat sensitif, sambil memberikan jaminan privasi yang ketat bagi individu yang maklumatnya disimpan di sana. Kami membentangkan dua algoritma yang cekap untuk menemui corak k paling kerap dalam set data rekod sensitif. Algoritma kami memenuhi privasi berbeza, definisi yang baru diperkenalkan yang memberikan jaminan privasi yang bermakna dengan adanya maklumat luaran yang sewenang-wenangnya. Algoritma peribadi yang berbeza memerlukan tahap ketidakpastian dalam outputnya untuk menjaga privasi. Algoritma kami mengatasinya dengan mengembalikan senarai corak `bising 'yang hampir dengan senarai sebenar corak k paling kerap dalam data. Kami menentukan konsep utiliti baru yang mengukur ketepatan output algoritma perlombongan corak top-k swasta. Dalam set data biasa, kriteria utiliti kami menyiratkan kadar positif positif dan negatif palsu rendah dalam senarai yang dilaporkan. Kami membuktikan bahawa kaedah kami memenuhi kriteria utiliti baru; kami juga menunjukkan prestasi algoritma kami melalui eksperimen yang luas pada set data transaksi dari repositori FIMI. Walaupun makalah ini memfokus pada perlombongan corak yang kerap, teknik yang dikembangkan di sini relevan setiap kali hasil perlombongan data adalah senarai elemen yang disusun mengikut ukuran minat yang \"kuat\". [[EENNDD]] umum; set barang yang kerap; privasi; mekanisme eksponen; privasi berbeza; corak yang kerap"], [{"string": "Spotting out emerging artists using geo-aware analysis of P2P query strings Record label companies would like to identify potential artists as early as possible in their careers , before other companies approach the artists with competing contracts . The vast number of candidates makes the process of identifying the ones with high success potential time consuming and laborious . This paper demonstrates how datamining of P2P query strings can be used in order to mechanize most of this detection process . Using a unique intercepting system over the Gnutella network , we were able to capture an unprecedented amount of geographically identified geo-aware queries , allowing us to investigate the diffusion of music related queries in time and space . Our solution is based on the observation that emerging artists , especially rappers , have a discernible stronghold of fans in their hometown area , where they are able to perform and market their music . In a file sharing network , this is reflected as a delta function spatial distribution of content queries . Using this observation , we devised a detection algorithm for emerging artists , that looks for performers with sharp increase in popularity in a small geographic region though still unnoticable nation wide . The algorithm can suggest a short list of artists with breakthrough potential , from which we showed that about 30 % translate the potential to national success .", "keywords": ["emerging artists", "model development", "p2p queries"], "combined": "Spotting out emerging artists using geo-aware analysis of P2P query strings Record label companies would like to identify potential artists as early as possible in their careers , before other companies approach the artists with competing contracts . The vast number of candidates makes the process of identifying the ones with high success potential time consuming and laborious . This paper demonstrates how datamining of P2P query strings can be used in order to mechanize most of this detection process . Using a unique intercepting system over the Gnutella network , we were able to capture an unprecedented amount of geographically identified geo-aware queries , allowing us to investigate the diffusion of music related queries in time and space . Our solution is based on the observation that emerging artists , especially rappers , have a discernible stronghold of fans in their hometown area , where they are able to perform and market their music . In a file sharing network , this is reflected as a delta function spatial distribution of content queries . Using this observation , we devised a detection algorithm for emerging artists , that looks for performers with sharp increase in popularity in a small geographic region though still unnoticable nation wide . The algorithm can suggest a short list of artists with breakthrough potential , from which we showed that about 30 % translate the potential to national success . [[EENNDD]] emerging artists; model development; p2p queries"}, "Menggali artis yang baru muncul menggunakan analisis geo-sedar mengenai rentetan pertanyaan P2P Syarikat label rakaman ingin mengenal pasti bakal artis seawal mungkin dalam kerjaya mereka, sebelum syarikat lain mendekati artis dengan kontrak yang bersaing. Sebilangan besar calon membuat proses mengenal pasti mereka yang mempunyai kejayaan tinggi berpotensi memakan masa dan sukar. Makalah ini menunjukkan bagaimana pengumpulan data rentetan pertanyaan P2P dapat digunakan untuk mekanisasi sebahagian besar proses pengesanan ini. Dengan menggunakan sistem pemintas yang unik melalui rangkaian Gnutella, kami dapat menangkap sejumlah pertanyaan yang diketahui secara geografi yang belum pernah terjadi sebelumnya, yang membolehkan kami menyiasat penyebaran pertanyaan berkaitan muzik dalam masa dan ruang. Penyelesaian kami adalah berdasarkan pemerhatian bahawa seniman yang baru muncul, terutama rapper, memiliki kubu peminat yang dapat dilihat di kawasan kampung halaman mereka, di mana mereka dapat membuat persembahan dan memasarkan muzik mereka. Dalam rangkaian perkongsian fail, ini tercermin sebagai distribusi spatial fungsi delta pertanyaan kandungan. Dengan menggunakan pemerhatian ini, kami membuat algoritma pengesanan untuk seniman yang baru muncul, yang mencari pelakon dengan peningkatan populariti yang tajam di wilayah geografi kecil walaupun masih tidak dapat digambarkan di seluruh negara. Algoritma dapat mencadangkan senarai pendek artis dengan potensi terobosan, dari mana kami menunjukkan bahawa sekitar 30% menerjemahkan potensi kepada kejayaan nasional. [[EENNDD]] artis yang baru muncul; pembangunan model; pertanyaan p2p"], [{"string": "Information genealogy : uncovering the flow of ideas in non-hyperlinked document databases We now have incrementally-grown databases of text documents ranging back for over a decade in areas ranging from personal email , to news-articles and conference proceedings . While accessing individual documents is easy , methods for overviewing and understanding these collections as a whole are lacking in number and in scope . In this paper , we address one such global analysis task , namely the problem of automatically uncovering how ideas spread through the collection over time . We refer to this problem as Information Genealogy . In contrast to bibliometric methods that are limited to collections with explicit citation structure , we investigate content-based methods requiring only the text and timestamps of the documents . In particular , we propose a language-modeling approach and a likelihood ratio test to detect influence between documents in a statistically well-founded way . Furthermore , we show how this method can be used to infer citation graphs and to identify the most influential documents in the collection . Experiments on the NIPS conference proceedings and the Physics ArXiv show that our method is more effective than methods based on document similarity .", "keywords": ["text mining", "temporal data", "language models", "miscellaneous", "information genealogy", "citation inference", "flow of ideas"], "combined": "Information genealogy : uncovering the flow of ideas in non-hyperlinked document databases We now have incrementally-grown databases of text documents ranging back for over a decade in areas ranging from personal email , to news-articles and conference proceedings . While accessing individual documents is easy , methods for overviewing and understanding these collections as a whole are lacking in number and in scope . In this paper , we address one such global analysis task , namely the problem of automatically uncovering how ideas spread through the collection over time . We refer to this problem as Information Genealogy . In contrast to bibliometric methods that are limited to collections with explicit citation structure , we investigate content-based methods requiring only the text and timestamps of the documents . In particular , we propose a language-modeling approach and a likelihood ratio test to detect influence between documents in a statistically well-founded way . Furthermore , we show how this method can be used to infer citation graphs and to identify the most influential documents in the collection . Experiments on the NIPS conference proceedings and the Physics ArXiv show that our method is more effective than methods based on document similarity . [[EENNDD]] text mining; temporal data; language models; miscellaneous; information genealogy; citation inference; flow of ideas"}, "Salasilah maklumat: mengungkap aliran idea dalam pangkalan data dokumen yang tidak mempunyai tautan Kami sekarang mempunyai pangkalan data dokumen teks yang dikembangkan secara bertahap sejak lebih dari satu dekad di bidang mulai dari e-mel peribadi, hingga artikel berita dan prosiding persidangan. Walaupun mengakses dokumen individu adalah mudah, kaedah untuk melihat dan memahami koleksi ini secara keseluruhan kurang jumlah dan skopnya. Dalam makalah ini, kita membahas satu tugas analisis global seperti itu, yaitu masalah mengungkap secara automatik bagaimana idea tersebar melalui koleksi dari masa ke masa. Kami menyebut masalah ini sebagai Salasilah Maklumat. Berbeza dengan kaedah bibliometrik yang terbatas pada koleksi dengan struktur kutipan eksplisit, kami menyelidiki kaedah berdasarkan kandungan yang hanya memerlukan teks dan cap waktu dokumen. Secara khusus, kami mencadangkan pendekatan pemodelan bahasa dan ujian nisbah kemungkinan untuk mengesan pengaruh antara dokumen dengan cara yang berdasarkan statistik. Selanjutnya, kami menunjukkan bagaimana kaedah ini dapat digunakan untuk menyimpulkan grafik kutipan dan untuk mengenal pasti dokumen yang paling berpengaruh dalam koleksi. Eksperimen pada prosiding persidangan NIPS dan Fizik ArXiv menunjukkan bahawa kaedah kami lebih berkesan daripada kaedah berdasarkan kesamaan dokumen. [[EENNDD]] perlombongan teks; data temporal; model bahasa; pelbagai; salasilah maklumat; inferens petikan; aliran idea"], [{"string": "BGP-lens : patterns and anomalies in internet routing updates The Border Gateway Protocol BGP is one of the fundamental computer communication protocols . Monitoring and mining BGP update messages can directly reveal the health and stability of Internet routing . Here we make two contributions : firstly we find patterns in BGP updates , like self-similarity , power-law and lognormal marginals ; secondly using these patterns , we find anomalies . Specifically , we develop BGP-lens , an automated BGP updates analysis tool , that has three desirable properties : a It is effective , able to identify phenomena that would otherwise go unnoticed , such as a peculiar ` clothesline ' behavior or prolonged ` spikes ' that last as long as 8 hours ; b It is scalable , using algorithms are all linear on the number of time-ticks ; and c It is admin-friendly , giving useful leads for phenomenon of interest . We showcase the capabilities of BGP-lens by identifying surprising phenomena verified by syadmins , over a massive trace of BGP updates spanning 2 years , from the publicly available site datapository.net .", "keywords": ["anomalies", "patterns", "bgp monitoring", "self-similarity"], "combined": "BGP-lens : patterns and anomalies in internet routing updates The Border Gateway Protocol BGP is one of the fundamental computer communication protocols . Monitoring and mining BGP update messages can directly reveal the health and stability of Internet routing . Here we make two contributions : firstly we find patterns in BGP updates , like self-similarity , power-law and lognormal marginals ; secondly using these patterns , we find anomalies . Specifically , we develop BGP-lens , an automated BGP updates analysis tool , that has three desirable properties : a It is effective , able to identify phenomena that would otherwise go unnoticed , such as a peculiar ` clothesline ' behavior or prolonged ` spikes ' that last as long as 8 hours ; b It is scalable , using algorithms are all linear on the number of time-ticks ; and c It is admin-friendly , giving useful leads for phenomenon of interest . We showcase the capabilities of BGP-lens by identifying surprising phenomena verified by syadmins , over a massive trace of BGP updates spanning 2 years , from the publicly available site datapository.net . [[EENNDD]] anomalies; patterns; bgp monitoring; self-similarity"}, "BGP-lensa: corak dan anomali dalam kemas kini penghalaan internet Border Gateway Protocol BGP adalah salah satu protokol komunikasi komputer asas. Pemantauan dan penambangan mesej kemas kini BGP secara langsung dapat memperlihatkan kesihatan dan kestabilan penghalaan Internet. Di sini kita memberikan dua sumbangan: pertama kita dapati corak dalam kemas kini BGP, seperti kesamaan diri, undang-undang kuasa dan marginal yang tidak normal; kedua menggunakan corak ini, kita menemui anomali. Secara khusus, kami mengembangkan lensa BGP, alat analisis kemas kini BGP automatik, yang mempunyai tiga sifat yang diinginkan: a Ia berkesan, dapat mengenal pasti fenomena yang mungkin tidak disedari, seperti tingkah laku \"jemuran\" yang aneh atau \"lonjakan\" yang berpanjangan yang bertahan selama 8 jam; b Ini boleh diskalakan, menggunakan algoritma semuanya linier pada bilangan kiraan masa; dan c Ini mesra admin, memberi petunjuk berguna untuk fenomena minat. Kami mempamerkan kemampuan lensa BGP dengan mengenal pasti fenomena mengejutkan yang disahkan oleh syadmin, melalui jejak kemas kini BGP yang besar sepanjang 2 tahun, dari datapository.net laman web yang tersedia untuk umum. [[EENNDD]] anomali; corak; pemantauan bgp; persamaan diri"], [{"string": "Active learning with direct query construction Active learning may hold the key for solving the data scarcity problem in supervised learning , i.e. , the lack of labeled data . Indeed , labeling data is a costly process , yet an active learner may request labels of only selected instances , thus reducing labeling work dramatically . Most previous works of active learning are , however , pool-based ; that is , a pool of unlabeled examples is given and the learner can only select examples from the pool to query for their labels . This type of active learning has several weaknesses . In this paper we propose novel active learning algorithms that construct examples directly to query for labels . We study both a specific active learner based on the decision tree algorithm , and a general active learner that can work with any base learning algorithm . As there is no restriction on what examples to be queried , our methods are shown to often query fewer examples to reduce the predictive error quickly . This casts doubt on the usefulness of the pool in pool-based active learning . Nevertheless , our methods can be easily adapted to work with a given pool of unlabeled examples .", "keywords": ["active learning", "supervised learning", "classification"], "combined": "Active learning with direct query construction Active learning may hold the key for solving the data scarcity problem in supervised learning , i.e. , the lack of labeled data . Indeed , labeling data is a costly process , yet an active learner may request labels of only selected instances , thus reducing labeling work dramatically . Most previous works of active learning are , however , pool-based ; that is , a pool of unlabeled examples is given and the learner can only select examples from the pool to query for their labels . This type of active learning has several weaknesses . In this paper we propose novel active learning algorithms that construct examples directly to query for labels . We study both a specific active learner based on the decision tree algorithm , and a general active learner that can work with any base learning algorithm . As there is no restriction on what examples to be queried , our methods are shown to often query fewer examples to reduce the predictive error quickly . This casts doubt on the usefulness of the pool in pool-based active learning . Nevertheless , our methods can be easily adapted to work with a given pool of unlabeled examples . [[EENNDD]] active learning; supervised learning; classification"}, "Pembelajaran aktif dengan pembinaan pertanyaan langsung Pembelajaran aktif dapat menjadi kunci untuk menyelesaikan masalah kekurangan data dalam pembelajaran diawasi, iaitu kekurangan data berlabel. Memang, pelabelan data adalah proses yang mahal, namun pelajar yang aktif mungkin meminta label dengan contoh terpilih sahaja, sehingga mengurangkan pelabelan bekerja secara dramatik. Sebilangan besar karya pembelajaran aktif sebelumnya adalah berdasarkan kumpulan; iaitu kumpulan contoh tidak berlabel diberikan dan pelajar hanya dapat memilih contoh dari kumpulan untuk meminta label mereka. Pembelajaran aktif jenis ini mempunyai beberapa kelemahan. Dalam makalah ini kami mencadangkan algoritma pembelajaran aktif baru yang membina contoh secara langsung untuk meminta label. Kami mengkaji pelajar aktif khusus berdasarkan algoritma pohon keputusan, dan pelajar aktif umum yang dapat bekerja dengan algoritma pembelajaran asas. Oleh kerana tidak ada batasan mengenai contoh apa yang akan ditanyakan, kaedah kami ditunjukkan untuk sering meminta lebih sedikit contoh untuk mengurangkan ralat ramalan dengan cepat. Ini menimbulkan keraguan akan kegunaan kolam dalam pembelajaran aktif berasaskan kolam. Walaupun begitu, kaedah kami dapat disesuaikan dengan mudah untuk berfungsi dengan kumpulan contoh yang tidak berlabel. [[EENNDD]] pembelajaran aktif; pembelajaran yang diselia; pengelasan"], [{"string": "Tracking multiple topics for finding interesting articles We introduce multiple topic tracking MTT for iScore to better recommend news articles for users with multiple interests and to address changes in user interests over time . As an extension of the basic Rocchio algorithm , traditional topic detection and tracking , and single-pass clustering , MTT maintains multiple interest profiles to identify interesting articles for a specific user given user-feedback . Focusing on only interesting topics enables iScore to discard useless profiles to address changes in user interests and to achieve a balance between resource consumption and classification accuracy . Also by relating a topic 's interestingness to an article . s interestingness , iScore is able to achieve higher quality results than traditional methods such as the Rocchio algorithm . We identify several operating parameters that work well for MTT . Using the same parameters , we show that MTT alone yields high quality results for recommending interesting articles from several corpora . The inclusion of MTT improves iScore 's performance by 9 % in recommending news articles from the Yahoo ! News RSS feeds and the TREC11 adaptive filter article collection . And through a small user study , we show that iScore can still perform well when only provided with little user feedback .", "keywords": ["news filtering", "news recommendation", "personalization", "content analysis and indexing"], "combined": "Tracking multiple topics for finding interesting articles We introduce multiple topic tracking MTT for iScore to better recommend news articles for users with multiple interests and to address changes in user interests over time . As an extension of the basic Rocchio algorithm , traditional topic detection and tracking , and single-pass clustering , MTT maintains multiple interest profiles to identify interesting articles for a specific user given user-feedback . Focusing on only interesting topics enables iScore to discard useless profiles to address changes in user interests and to achieve a balance between resource consumption and classification accuracy . Also by relating a topic 's interestingness to an article . s interestingness , iScore is able to achieve higher quality results than traditional methods such as the Rocchio algorithm . We identify several operating parameters that work well for MTT . Using the same parameters , we show that MTT alone yields high quality results for recommending interesting articles from several corpora . The inclusion of MTT improves iScore 's performance by 9 % in recommending news articles from the Yahoo ! News RSS feeds and the TREC11 adaptive filter article collection . And through a small user study , we show that iScore can still perform well when only provided with little user feedback . [[EENNDD]] news filtering; news recommendation; personalization; content analysis and indexing"}, "Menjejaki pelbagai topik untuk mencari artikel menarik Kami memperkenalkan MTT pelacakan pelbagai topik untuk iScore untuk lebih mengesyorkan artikel berita untuk pengguna dengan pelbagai minat dan untuk menangani perubahan minat pengguna dari masa ke masa. Sebagai lanjutan dari algoritma Rocchio asas, pengesanan dan penjejakan topik tradisional, dan pengelompokan single-pass, MTT mengekalkan pelbagai profil minat untuk mengenal pasti artikel menarik bagi pengguna tertentu yang diberi maklum balas pengguna. Memusatkan perhatian hanya pada topik menarik membolehkan iScore membuang profil yang tidak berguna untuk menangani perubahan minat pengguna dan mencapai keseimbangan antara penggunaan sumber dan ketepatan klasifikasi. Juga dengan mengaitkan minat topik dengan artikel. Keseronokan, iScore mampu mencapai hasil berkualiti lebih tinggi daripada kaedah tradisional seperti algoritma Rocchio. Kami mengenal pasti beberapa parameter operasi yang berfungsi dengan baik untuk MTT. Dengan menggunakan parameter yang sama, kami menunjukkan bahawa MTT sendiri memberikan hasil yang berkualiti tinggi untuk mengesyorkan artikel menarik dari beberapa syarikat. Kemasukan MTT meningkatkan prestasi iScore sebanyak 9% dalam mengesyorkan artikel berita dari Yahoo! Suapan RSS berita dan koleksi artikel penapis adaptif TREC11. Dan melalui kajian pengguna kecil, kami menunjukkan bahawa iScore masih dapat menunjukkan prestasi yang baik apabila hanya diberikan sedikit maklum balas pengguna. [[EENNDD]] penapisan berita; cadangan berita; pemperibadian; analisis kandungan dan pengindeksan"], [{"string": "Differentially private recommender systems : building privacy into the net We consider the problem of producing recommendations from collective user behavior while simultaneously providing guarantees of privacy for these users . Specifically , we consider the Netflix Prize data set , and its leading algorithms , adapted to the framework of differential privacy . Unlike prior privacy work concerned with cryptographically securing the computation of recommendations , differential privacy constrains a computation in a way that precludes any inference about the underlying records from its output . Such algorithms necessarily introduce uncertainty -- i.e. , noise -- to computations , trading accuracy for privacy . We find that several of the leading approaches in the Netflix Prize competition can be adapted to provide differential privacy , without significantly degrading their accuracy . To adapt these algorithms , we explicitly factor them into two parts , an aggregation\\/learning phase that can be performed with differential privacy guarantees , and an individual recommendation phase that uses the learned correlations and an individual 's data to provide personalized recommendations . The adaptations are non-trivial , and involve both careful analysis of the per-record sensitivity of the algorithms to calibrate noise , as well as new post-processing steps to mitigate the impact of this noise . We measure the empirical trade-off between accuracy and privacy in these adaptations , and find that we can provide non-trivial formal privacy guarantees while still outperforming the Cinematch baseline Netflix provides .", "keywords": ["recommender systems", "differential privacy", "netflix"], "combined": "Differentially private recommender systems : building privacy into the net We consider the problem of producing recommendations from collective user behavior while simultaneously providing guarantees of privacy for these users . Specifically , we consider the Netflix Prize data set , and its leading algorithms , adapted to the framework of differential privacy . Unlike prior privacy work concerned with cryptographically securing the computation of recommendations , differential privacy constrains a computation in a way that precludes any inference about the underlying records from its output . Such algorithms necessarily introduce uncertainty -- i.e. , noise -- to computations , trading accuracy for privacy . We find that several of the leading approaches in the Netflix Prize competition can be adapted to provide differential privacy , without significantly degrading their accuracy . To adapt these algorithms , we explicitly factor them into two parts , an aggregation\\/learning phase that can be performed with differential privacy guarantees , and an individual recommendation phase that uses the learned correlations and an individual 's data to provide personalized recommendations . The adaptations are non-trivial , and involve both careful analysis of the per-record sensitivity of the algorithms to calibrate noise , as well as new post-processing steps to mitigate the impact of this noise . We measure the empirical trade-off between accuracy and privacy in these adaptations , and find that we can provide non-trivial formal privacy guarantees while still outperforming the Cinematch baseline Netflix provides . [[EENNDD]] recommender systems; differential privacy; netflix"}, "Sistem pengesyorkan peribadi yang berbeza: membina privasi ke dalam jaring Kami menganggap masalah menghasilkan cadangan dari tingkah laku pengguna kolektif dan pada masa yang sama memberikan jaminan privasi untuk pengguna ini. Secara khusus, kami mempertimbangkan set data Hadiah Netflix, dan algoritma terkemuka, disesuaikan dengan kerangka privasi berbeza. Tidak seperti pekerjaan privasi sebelumnya yang berkaitan dengan pengiraan cadangan secara kriptografi, privasi berbeza membatasi pengiraan dengan cara yang menghalang kesimpulan mengenai catatan yang mendasari dari outputnya. Algoritma sedemikian semestinya memperkenalkan ketidakpastian - iaitu kebisingan - kepada pengiraan, ketepatan perdagangan untuk privasi. Kami mendapati bahawa beberapa pendekatan terkemuka dalam pertandingan Hadiah Netflix dapat disesuaikan untuk memberikan privasi berbeza, tanpa merosakkan ketepatan mereka dengan ketara. Untuk menyesuaikan algoritma ini, kami secara eksplisit memfaktorkannya menjadi dua bahagian, fasa agregasi \\ / pembelajaran yang dapat dilakukan dengan jaminan privasi berbeza, dan fasa cadangan individu yang menggunakan korelasi yang dipelajari dan data individu untuk memberikan cadangan yang diperibadikan. Penyesuaiannya tidak sepele, dan melibatkan analisis yang teliti terhadap sensitiviti per-rakaman algoritma untuk menentukurkan bunyi, serta langkah-langkah pasca pemprosesan baru untuk mengurangkan kesan kebisingan ini. Kami mengukur pertukaran empirik antara ketepatan dan privasi dalam penyesuaian ini, dan mendapati bahawa kami dapat memberikan jaminan privasi formal yang tidak remeh sementara masih mengungguli asas yang disediakan oleh Cinematch Netflix. [[EENNDD]] sistem cadangan; privasi berbeza; netflix"], [{"string": "Beyond classification and ranking : constrained optimization of the ROI Classification has been commonly used in many data mining projects in the financial service industry . For instance , to predict collectability of accounts receivable , a binary class label is created based on whether a payment is received within a certain period . However , optimization of the classifier does not necessarily lead to maximization of return on investment ROI , since maximization of the true positive rate is often different from maximization of the collectable amount which determines the ROI under a fixed budget constraint . The typical cost sensitive learning does not solve this problem either since it involves an unknown opportunity cost due to the budget constraint . Learning the ranks of collectable amount would ultimately solve the problem , but it tries to tackle an unnecessarily difficult problem and often results in poorer results for our specific target . We propose a new algorithm that uses gradient descent to directly optimize the related monetary measure under the budget constraint and thus maximizes the ROI . By comparison with several classification , regression , and ranking algorithms , we demonstrate the new algorithm 's substantial improvement of the financial impact on our clients in the financial service industry .", "keywords": ["neural networks", "constrained optimization", "return on investment"], "combined": "Beyond classification and ranking : constrained optimization of the ROI Classification has been commonly used in many data mining projects in the financial service industry . For instance , to predict collectability of accounts receivable , a binary class label is created based on whether a payment is received within a certain period . However , optimization of the classifier does not necessarily lead to maximization of return on investment ROI , since maximization of the true positive rate is often different from maximization of the collectable amount which determines the ROI under a fixed budget constraint . The typical cost sensitive learning does not solve this problem either since it involves an unknown opportunity cost due to the budget constraint . Learning the ranks of collectable amount would ultimately solve the problem , but it tries to tackle an unnecessarily difficult problem and often results in poorer results for our specific target . We propose a new algorithm that uses gradient descent to directly optimize the related monetary measure under the budget constraint and thus maximizes the ROI . By comparison with several classification , regression , and ranking algorithms , we demonstrate the new algorithm 's substantial improvement of the financial impact on our clients in the financial service industry . [[EENNDD]] neural networks; constrained optimization; return on investment"}, "Di luar klasifikasi dan peringkat: pengoptimuman terbatas dari ROI Klasifikasi telah biasa digunakan dalam banyak projek perlombongan data dalam industri perkhidmatan kewangan. Sebagai contoh, untuk memprediksi koleksi akaun yang boleh diterima, label kelas binari dibuat berdasarkan sama ada pembayaran diterima dalam jangka masa tertentu. Walau bagaimanapun, pengoptimuman pengklasifikasi tidak semestinya membawa kepada pemaksimumkan ROI pelaburan, kerana pemaksimumkan kadar positif sebenarnya sering kali berbeza dengan memaksimumkan jumlah terkumpul yang menentukan ROI di bawah batasan anggaran tetap. Pembelajaran sensitif kos biasa tidak menyelesaikan masalah ini kerana melibatkan kos peluang yang tidak diketahui kerana kekangan anggaran. Mempelajari peringkat jumlah terkumpul akhirnya akan menyelesaikan masalah, tetapi ia berusaha mengatasi masalah yang sukar dan sering menghasilkan hasil yang lebih buruk untuk sasaran khusus kita. Kami mencadangkan algoritma baru yang menggunakan penurunan gradien untuk secara langsung mengoptimumkan ukuran kewangan yang berkaitan di bawah kekangan anggaran dan dengan itu memaksimumkan ROI. Dengan membandingkan dengan beberapa algoritma klasifikasi, regresi, dan peringkat, kami menunjukkan peningkatan besar algoritma baru mengenai kesan kewangan kepada pelanggan kami dalam industri perkhidmatan kewangan. [[EENNDD]] rangkaian saraf; pengoptimuman terhad; pulangan pelaburan"], [{"string": "Robust space transformations for distance-based operations For many KDD operations , such as nearest neighbor search , distance-based clustering , and outlier detection , there is an underlying & kgr ;-D data space in which each tuple\\/object is represented as a point in the space . In the presence of differing scales , variability , correlation , and\\/or outliers , we may get unintuitive results if an inappropriate space is used . The fundamental question that this paper addresses is : `` What then is an appropriate space ? '' We propose using a robust space transformation called the Donoho-Stahel estimator . In the first half of the paper , we show the key properties of the estimator . Of particular importance to KDD applications involving databases is the stability property , which says that in spite of frequent updates , the estimator does not : a change much , b lose its usefulness , or c require re-computation . In the second half , we focus on the computation of the estimator for high-dimensional databases . We develop randomized algorithms and evaluate how well they perform empirically . The novel algorithm we develop called the Hybrid-random algorithm is , in most cases , at least an order of magnitude faster than the Fixed-angle and Subsampling algorithms .", "keywords": ["outliers", "data mining", "distance-based operations", "robust estimators", "space transformations", "computation of transforms", "robust statistics"], "combined": "Robust space transformations for distance-based operations For many KDD operations , such as nearest neighbor search , distance-based clustering , and outlier detection , there is an underlying & kgr ;-D data space in which each tuple\\/object is represented as a point in the space . In the presence of differing scales , variability , correlation , and\\/or outliers , we may get unintuitive results if an inappropriate space is used . The fundamental question that this paper addresses is : `` What then is an appropriate space ? '' We propose using a robust space transformation called the Donoho-Stahel estimator . In the first half of the paper , we show the key properties of the estimator . Of particular importance to KDD applications involving databases is the stability property , which says that in spite of frequent updates , the estimator does not : a change much , b lose its usefulness , or c require re-computation . In the second half , we focus on the computation of the estimator for high-dimensional databases . We develop randomized algorithms and evaluate how well they perform empirically . The novel algorithm we develop called the Hybrid-random algorithm is , in most cases , at least an order of magnitude faster than the Fixed-angle and Subsampling algorithms . [[EENNDD]] outliers; data mining; distance-based operations; robust estimators; space transformations; computation of transforms; robust statistics"}, "Transformasi ruang yang kuat untuk operasi berdasarkan jarak Untuk banyak operasi KDD, seperti pencarian tetangga terdekat, pengelompokan berdasarkan jarak, dan pengesanan luar, terdapat ruang data & kgr ;-D yang mendasari di mana setiap tuple \\ / objek diwakili sebagai tunjuk di ruang. Dengan adanya skala, kebolehubahan, korelasi, dan \\ / atau penyekat yang berbeza, kami mungkin mendapat hasil yang tidak disengajakan jika ruang yang tidak sesuai digunakan. Soalan asas yang ditangani oleh makalah ini adalah: `` Lalu, apakah ruang yang sesuai? '' Kami mencadangkan menggunakan transformasi ruang yang kuat yang disebut estimator Donoho-Stahel. Pada separuh pertama kertas, kami menunjukkan sifat utama penganggar. Yang sangat penting bagi aplikasi KDD yang melibatkan pangkalan data adalah sifat kestabilan, yang mengatakan bahawa walaupun sering dikemas kini, penganggar tidak: banyak perubahan, b kehilangan kegunaannya, atau c memerlukan pengiraan semula. Pada separuh masa kedua, kami memberi tumpuan kepada pengiraan penganggar untuk pangkalan data dimensi tinggi. Kami mengembangkan algoritma secara rawak dan menilai sejauh mana prestasi mereka secara empirikal. Algoritma novel yang kami kembangkan disebut algoritma Hybrid-random, dalam kebanyakan kes, sekurang-kurangnya urutan magnitud lebih cepat daripada algoritma Fixed-angle dan Subsampling. [[EENNDD]] penyekat; perlombongan data; operasi berdasarkan jarak; penganggar yang kukuh; transformasi ruang; pengiraan transformasi; statistik yang mantap"], [{"string": "Deformable Markov model templates for time-series pattern matching", "keywords": ["time series", "hidden markov models", "deformable templates", "segmental markov models", "pattern matching"], "combined": "Deformable Markov model templates for time-series pattern matching [[EENNDD]] time series; hidden markov models; deformable templates; segmental markov models; pattern matching"}, "Templat model Markov yang boleh berubah bentuk untuk padanan corak siri masa [[EENNDD]] siri masa; model markov tersembunyi; templat ubah bentuk; model markov segmen; padanan corak"], [{"string": "Reasoning about sets using redescription mining Redescription mining is a newly introduced data mining problem that seeks to find subsets of data that afford multiple definitions . It can be viewed as a generalization of association rule mining , from finding implications to equivalences ; as a form of conceptual clustering , where the goal is to identify clusters that afford dual characterizations ; and as a form of constructive induction , to build features based on given descriptors that mutually reinforce each other . In this paper , we present the use of redescription mining as an important tool to reason about a collection of sets , especially their overlaps , similarities , and differences . We outline algorithms to mine all minimal non-redundant redescriptions underlying a dataset using notions of minimal generators of closed itemsets . We also show the use of these algorithms in an interactive context , supporting constraint-based exploration and querying . Specifically , we showcase a bioinformatics application that empowers the biologist to define a vocabulary of sets underlying a domain of genes and to reason about these sets , yielding significant biological insight .", "keywords": ["redescription", "closed itemsets", "learning", "minimal generators"], "combined": "Reasoning about sets using redescription mining Redescription mining is a newly introduced data mining problem that seeks to find subsets of data that afford multiple definitions . It can be viewed as a generalization of association rule mining , from finding implications to equivalences ; as a form of conceptual clustering , where the goal is to identify clusters that afford dual characterizations ; and as a form of constructive induction , to build features based on given descriptors that mutually reinforce each other . In this paper , we present the use of redescription mining as an important tool to reason about a collection of sets , especially their overlaps , similarities , and differences . We outline algorithms to mine all minimal non-redundant redescriptions underlying a dataset using notions of minimal generators of closed itemsets . We also show the use of these algorithms in an interactive context , supporting constraint-based exploration and querying . Specifically , we showcase a bioinformatics application that empowers the biologist to define a vocabulary of sets underlying a domain of genes and to reason about these sets , yielding significant biological insight . [[EENNDD]] redescription; closed itemsets; learning; minimal generators"}, "Berpikir tentang set menggunakan perlombongan reseskripsi Perlombongan reseskripsi adalah masalah perlombongan data yang baru diperkenalkan yang bertujuan untuk mencari subkumpulan data yang mempunyai banyak definisi. Ini dapat dilihat sebagai generalisasi perlombongan peraturan persatuan, dari mencari implikasi hingga kesetaraan; sebagai satu bentuk pengelompokan konseptual, di mana tujuannya adalah untuk mengenal pasti kelompok yang mempunyai dua ciri; dan sebagai bentuk induksi konstruktif, untuk membina ciri berdasarkan deskriptor yang diberikan yang saling memperkuat antara satu sama lain. Dalam makalah ini, kami menyajikan penggunaan perlombongan reseskripsi sebagai alat penting untuk memberi alasan mengenai koleksi set, terutama pertindihan, persamaan, dan perbezaannya. Kami menggariskan algoritma untuk melombong semua redeskripsi minimum yang tidak berlebihan yang mendasari set data menggunakan konsep penjana minimum itemets tertutup. Kami juga menunjukkan penggunaan algoritma ini dalam konteks interaktif, menyokong penerokaan dan pertanyaan berdasarkan kekangan. Secara khusus, kami mempamerkan aplikasi bioinformatika yang memberi kuasa kepada ahli biologi untuk menentukan perbendaharaan kata set yang mendasari domain gen dan memberi pertimbangan mengenai set ini, menghasilkan wawasan biologi yang signifikan. [[EENNDD]] reka bentuk semula; set barang tertutup; belajar; penjana minimum"], [{"string": "Detecting graph-based spatial outliers : algorithms and applications a summary of results Identification of outliers can lead to the discovery of unexpected , interesting , and useful knowledge . Existing methods are designed for detecting spatial outliers in multidimensional geometric data sets , where a distance metric is available . In this paper , we focus on detecting spatial outliers in graph structured data sets . We define statistical tests , analyze the statistical foundation underlying our approach , design several fast algorithms to detect spatial outliers , and provide a cost model for outlier detection procedures . In addition , we provide experimental results from the application of our algorithms on a Minneapolis-St . Paul Twin Cities traffic dataset to show their effectiveness and usefulness .", "keywords": ["spatial data mining", "spatial graphs", "outlier detection"], "combined": "Detecting graph-based spatial outliers : algorithms and applications a summary of results Identification of outliers can lead to the discovery of unexpected , interesting , and useful knowledge . Existing methods are designed for detecting spatial outliers in multidimensional geometric data sets , where a distance metric is available . In this paper , we focus on detecting spatial outliers in graph structured data sets . We define statistical tests , analyze the statistical foundation underlying our approach , design several fast algorithms to detect spatial outliers , and provide a cost model for outlier detection procedures . In addition , we provide experimental results from the application of our algorithms on a Minneapolis-St . Paul Twin Cities traffic dataset to show their effectiveness and usefulness . [[EENNDD]] spatial data mining; spatial graphs; outlier detection"}, "Mengesan penyusun spasial berasaskan grafik: algoritma dan aplikasi ringkasan hasil Pengenalpastian outlier boleh menyebabkan penemuan pengetahuan yang tidak dijangka, menarik, dan berguna. Kaedah yang ada dirancang untuk mengesan spasial outliers dalam set data geometri multidimensi, di mana metrik jarak tersedia. Dalam makalah ini, kami memberi tumpuan untuk mengesan spasial outliers dalam set data berstruktur grafik. Kami menentukan ujian statistik, menganalisis asas statistik yang mendasari pendekatan kami, merancang beberapa algoritma cepat untuk mengesan spasial outliers, dan menyediakan model kos untuk prosedur pengesanan outlier. Sebagai tambahan, kami memberikan hasil eksperimen dari penerapan algoritma kami di Minneapolis-St. Kumpulan data lalu lintas Paul Twin Cities untuk menunjukkan keberkesanan dan kegunaannya. [[EENNDD]] perlombongan data spatial; grafik ruang; pengesanan luar"], [{"string": "Data selection for support vector machine classifiers", "keywords": ["concave minimization", "data selection", "support vector machines", "data classification"], "combined": "Data selection for support vector machine classifiers [[EENNDD]] concave minimization; data selection; support vector machines; data classification"}, "Pemilihan data untuk pengelasan mesin vektor sokongan [[EENNDD]] pengurangan cekung; pemilihan data; mesin vektor sokongan; pengkelasan data"], [{"string": "Formulating distance functions via the kernel trick Tasks of data mining and information retrieval depend on a good distance function for measuring similarity between data instances . The most effective distance function must be formulated in a context-dependent also application - , data - , and user-dependent way . In this paper , we propose to learn a distance function by capturing the nonlinear relationships among contextual information provided by the application , data , or user . We show that through a process called the `` kernel trick , '' such nonlinear relationships can be learned efficiently in a projected space . Theoretically , we substantiate that our method is both sound and optimal . Empirically , using several datasets and applications , we demonstrate that our method is effective and useful .", "keywords": ["distance function", "information storage and retrieval", "kernel trick"], "combined": "Formulating distance functions via the kernel trick Tasks of data mining and information retrieval depend on a good distance function for measuring similarity between data instances . The most effective distance function must be formulated in a context-dependent also application - , data - , and user-dependent way . In this paper , we propose to learn a distance function by capturing the nonlinear relationships among contextual information provided by the application , data , or user . We show that through a process called the `` kernel trick , '' such nonlinear relationships can be learned efficiently in a projected space . Theoretically , we substantiate that our method is both sound and optimal . Empirically , using several datasets and applications , we demonstrate that our method is effective and useful . [[EENNDD]] distance function; information storage and retrieval; kernel trick"}, "Merumuskan fungsi jarak melalui kernel trick Tugas perlombongan data dan pengambilan maklumat bergantung pada fungsi jarak yang baik untuk mengukur kesamaan antara contoh data. Fungsi jarak yang paling berkesan mesti dirumuskan dengan bergantung pada konteks juga aplikasi -, data -, dan cara yang bergantung pada pengguna. Dalam makalah ini, kami mengusulkan untuk mempelajari fungsi jarak jauh dengan menangkap hubungan tidak linear antara informasi kontekstual yang diberikan oleh aplikasi, data, atau pengguna. Kami menunjukkan bahawa melalui proses yang disebut \"kernel trick,\" hubungan nonlinear seperti itu dapat dipelajari dengan efisien di ruang yang diproyeksikan. Secara teorinya, kami membuktikan bahawa kaedah kami baik dan optimum. Secara empirikal, menggunakan beberapa set data dan aplikasi, kami menunjukkan bahawa kaedah kami berkesan dan berguna. [[EENNDD]] fungsi jarak; penyimpanan dan pengambilan maklumat; muslihat kernel"], [{"string": "Reducing the human overhead in text categorization Many applications in text processing require significant human effort for either labeling large document collections when learning statistical models or extrapolating rules from them when using knowledge engineering . In this work , we describe away to reduce this effort , while retaining the methods ' accuracy , by constructing a hybrid classifier that utilizes human reasoning over automatically discovered text patterns to complement machine learning . Using a standard sentiment-classification dataset and real customer feedback data , we demonstrate that the resulting technique results in significant reduction of the human effort required to obtain a given classification accuracy . Moreover , the hybrid text classifier also results in a significant boost in accuracy over machine-learning based classifiers when a comparable amount of labeled data is used .", "keywords": ["text classification", "text mining", "supervised learning", "classification", "active learning", "support vector machines", "machine learning"], "combined": "Reducing the human overhead in text categorization Many applications in text processing require significant human effort for either labeling large document collections when learning statistical models or extrapolating rules from them when using knowledge engineering . In this work , we describe away to reduce this effort , while retaining the methods ' accuracy , by constructing a hybrid classifier that utilizes human reasoning over automatically discovered text patterns to complement machine learning . Using a standard sentiment-classification dataset and real customer feedback data , we demonstrate that the resulting technique results in significant reduction of the human effort required to obtain a given classification accuracy . Moreover , the hybrid text classifier also results in a significant boost in accuracy over machine-learning based classifiers when a comparable amount of labeled data is used . [[EENNDD]] text classification; text mining; supervised learning; classification; active learning; support vector machines; machine learning"}, "Mengurangkan overhead manusia dalam pengkategorian teks Banyak aplikasi dalam pemprosesan teks memerlukan usaha manusia yang besar untuk melabel koleksi dokumen yang besar ketika mempelajari model statistik atau mengekstrapolasi peraturan dari mereka ketika menggunakan teknik pengetahuan. Dalam karya ini, kami menjelaskan untuk mengurangkan usaha ini, sambil mengekalkan ketepatan kaedah, dengan membina pengkelasan hibrid yang menggunakan penaakulan manusia daripada corak teks yang dijumpai secara automatik untuk melengkapkan pembelajaran mesin. Dengan menggunakan set data klasifikasi sentimen standard dan data maklum balas pelanggan sebenar, kami menunjukkan bahawa teknik yang dihasilkan menghasilkan pengurangan yang signifikan terhadap usaha manusia yang diperlukan untuk mendapatkan ketepatan klasifikasi yang diberikan. Lebih-lebih lagi, pengkelasan teks hibrid juga menghasilkan peningkatan ketepatan yang signifikan berbanding pengklasifikasi berasaskan pembelajaran mesin apabila jumlah data berlabel digunakan sebanding. [[EENNDD]] pengelasan teks; perlombongan teks; pembelajaran yang diselia; pengelasan; pembelajaran aktif; mesin vektor sokongan; pembelajaran mesin"], [{"string": "Real world performance of association rule algorithms This study compares five well-known association rule algorithms using three real-world datasets and an artificial dataset . The experimental results confirm the performance improvements previously claimed by the authors on the artificial data , but some of these gains do not carry over to the real datasets , indicating overfitting of the algorithms to the IBM artificial dataset . More importantly , we found that the choice of algorithm only matters at support levels that generate more rules than would be useful in practice . For support levels that generate less than 1,000,000 rules , which is much more than humans can handle and is sufficient for prediction purposes where data is loaded into RAM , Apriori finishes processing in less than 10 minutes . On our datasets , we observed super-exponential growth in the number of rules . On one of our datasets , a 0.02 % change in the support increased the number of rules from less than a million to over a billion , implying that outside a very narrow range of support values , the choice of algorithm is irrelevant .", "keywords": ["affinity analysis", "frequent itemsets", "association rules", "market basket analysis", "comparisons", "benchmark"], "combined": "Real world performance of association rule algorithms This study compares five well-known association rule algorithms using three real-world datasets and an artificial dataset . The experimental results confirm the performance improvements previously claimed by the authors on the artificial data , but some of these gains do not carry over to the real datasets , indicating overfitting of the algorithms to the IBM artificial dataset . More importantly , we found that the choice of algorithm only matters at support levels that generate more rules than would be useful in practice . For support levels that generate less than 1,000,000 rules , which is much more than humans can handle and is sufficient for prediction purposes where data is loaded into RAM , Apriori finishes processing in less than 10 minutes . On our datasets , we observed super-exponential growth in the number of rules . On one of our datasets , a 0.02 % change in the support increased the number of rules from less than a million to over a billion , implying that outside a very narrow range of support values , the choice of algorithm is irrelevant . [[EENNDD]] affinity analysis; frequent itemsets; association rules; market basket analysis; comparisons; benchmark"}, "Prestasi dunia nyata algoritma peraturan persatuan Kajian ini membandingkan lima algoritma peraturan persatuan terkenal menggunakan tiga set data dunia nyata dan satu set data buatan. Hasil eksperimen mengesahkan peningkatan prestasi yang sebelumnya dituntut oleh pengarang mengenai data tiruan, tetapi beberapa keuntungan ini tidak membawa ke set data sebenar, yang menunjukkan kelebihan algoritma ke set data buatan IBM. Lebih penting lagi, kami mendapati bahawa pilihan algoritma hanya penting pada tahap sokongan yang menghasilkan lebih banyak peraturan daripada yang berguna dalam praktiknya. Untuk tahap sokongan yang menghasilkan kurang dari 1,000,000 peraturan, yang jauh lebih banyak daripada yang dapat ditangani manusia dan mencukupi untuk tujuan ramalan di mana data dimuat ke dalam RAM, Apriori menyelesaikan pemprosesan dalam masa kurang dari 10 minit. Pada set data kami, kami melihat pertumbuhan jumlah peraturan yang sangat eksponensial. Pada salah satu set data kami, perubahan 0.02% dalam sokongan meningkatkan jumlah peraturan dari kurang dari satu juta menjadi lebih dari satu miliar, yang menunjukkan bahawa di luar julat nilai sokongan yang sangat sempit, pilihan algoritma tidak relevan. [[EENNDD]] analisis pertalian; set barang yang kerap; peraturan persatuan; analisis bakul pasaran; perbandingan; penanda aras"], [{"string": "Mining advisor-advisee relationships from research publication networks Information network contains abundant knowledge about relationships among people or entities . Unfortunately , such kind of knowledge is often hidden in a network where different kinds of relationships are not explicitly categorized . For example , in a research publication network , the advisor-advisee relationships among researchers are hidden in the coauthor network . Discovery of those relationships can benefit many interesting applications such as expert finding and research community analysis . In this paper , we take a computer science bibliographic network as an example , to analyze the roles of authors and to discover the likely advisor-advisee relationships . In particular , we propose a time-constrained probabilistic factor graph model TPFG , which takes a research publication network as input and models the advisor-advisee relationship mining problem using a jointly likelihood objective function . We further design an efficient learning algorithm to optimize the objective function . Based on that our model suggests and ranks probable advisors for every author . Experimental results show that the proposed approach infer advisor-advisee relationships efficiently and achieves a state-of-the-art accuracy 80-90 % . We also apply the discovered advisor-advisee relationships to bole search , a specific expert finding task and empirical study shows that the search performance can be effectively improved +4.09 % by NDCG@5 .", "keywords": ["time-constrained factor graph", "coauthor network", "advisor-advisee prediction", "relationship mining"], "combined": "Mining advisor-advisee relationships from research publication networks Information network contains abundant knowledge about relationships among people or entities . Unfortunately , such kind of knowledge is often hidden in a network where different kinds of relationships are not explicitly categorized . For example , in a research publication network , the advisor-advisee relationships among researchers are hidden in the coauthor network . Discovery of those relationships can benefit many interesting applications such as expert finding and research community analysis . In this paper , we take a computer science bibliographic network as an example , to analyze the roles of authors and to discover the likely advisor-advisee relationships . In particular , we propose a time-constrained probabilistic factor graph model TPFG , which takes a research publication network as input and models the advisor-advisee relationship mining problem using a jointly likelihood objective function . We further design an efficient learning algorithm to optimize the objective function . Based on that our model suggests and ranks probable advisors for every author . Experimental results show that the proposed approach infer advisor-advisee relationships efficiently and achieves a state-of-the-art accuracy 80-90 % . We also apply the discovered advisor-advisee relationships to bole search , a specific expert finding task and empirical study shows that the search performance can be effectively improved +4.09 % by NDCG@5 . [[EENNDD]] time-constrained factor graph; coauthor network; advisor-advisee prediction; relationship mining"}, "Perlombongan hubungan penasihat-penasihat dari rangkaian penerbitan penyelidikan Rangkaian maklumat mengandungi banyak pengetahuan mengenai hubungan antara orang atau entiti. Malangnya, pengetahuan semacam itu sering disembunyikan dalam rangkaian di mana pelbagai jenis hubungan tidak dikategorikan secara eksplisit. Sebagai contoh, dalam rangkaian penerbitan penyelidikan, hubungan penasihat-penasihat di antara penyelidik tersembunyi dalam rangkaian pengarang bersama. Penemuan hubungan tersebut dapat memanfaatkan banyak aplikasi menarik seperti penemuan pakar dan analisis komuniti penyelidikan. Dalam makalah ini, kami mengambil rangkaian bibliografi sains komputer sebagai contoh, untuk menganalisis peranan pengarang dan untuk mengetahui kemungkinan hubungan penasihat-penasihat. Secara khusus, kami mencadangkan model grafik faktor probabilistik yang dibatasi masa TPFG, yang menjadikan rangkaian penerbitan penyelidikan sebagai input dan memodelkan masalah penambangan hubungan penasihat-penasihat menggunakan fungsi objektif kemungkinan bersama. Kami seterusnya merancang algoritma pembelajaran yang cekap untuk mengoptimumkan fungsi objektif. Berdasarkan itu, model kami mencadangkan dan memberi barisan penasihat kepada setiap pengarang. Hasil eksperimen menunjukkan bahawa pendekatan yang dicadangkan menyimpulkan hubungan penasihat-penasihat dengan cekap dan mencapai ketepatan terkini 80-90%. Kami juga menerapkan hubungan penasihat-penasihat yang ditemui untuk pencarian bole, tugas penemuan pakar tertentu dan kajian empirik menunjukkan bahawa prestasi carian dapat ditingkatkan dengan berkesan +4.09% oleh NDCG @ 5. [[EENNDD]] graf faktor terhad masa; rangkaian pengarang; ramalan penasihat-penasihat; perlombongan hubungan"], [{"string": "Data mining to predict and prevent errors in health insurance claims processing Health insurance costs across the world have increased alarmingly in recent years . A major cause of this increase are payment errors made by the insurance companies while processing claims . These errors often result in extra administrative effort to re-process or rework the claim which accounts for up to 30 % of the administrative staff in a typical health insurer . We describe a system that helps reduce these errors using machine learning techniques by predicting claims that will need to be reworked , generating explanations to help the auditors correct these claims , and experiment with feature selection , concept drift , and active learning to collect feedback from the auditors to improve over time . We describe our framework , problem formulation , evaluation metrics , and experimental results on claims data from a large US health insurer . We show that our system results in an order of magnitude better precision hit rate over existing approaches which is accurate enough to potentially result in over $ 15-25 million in savings for a typical insurer . We also describe interesting research problems in this domain as well as design choices made to make the system easily deployable across health insurance companies .", "keywords": ["decision support", "claim rework identification", "health insurance claims", "machine learning"], "combined": "Data mining to predict and prevent errors in health insurance claims processing Health insurance costs across the world have increased alarmingly in recent years . A major cause of this increase are payment errors made by the insurance companies while processing claims . These errors often result in extra administrative effort to re-process or rework the claim which accounts for up to 30 % of the administrative staff in a typical health insurer . We describe a system that helps reduce these errors using machine learning techniques by predicting claims that will need to be reworked , generating explanations to help the auditors correct these claims , and experiment with feature selection , concept drift , and active learning to collect feedback from the auditors to improve over time . We describe our framework , problem formulation , evaluation metrics , and experimental results on claims data from a large US health insurer . We show that our system results in an order of magnitude better precision hit rate over existing approaches which is accurate enough to potentially result in over $ 15-25 million in savings for a typical insurer . We also describe interesting research problems in this domain as well as design choices made to make the system easily deployable across health insurance companies . [[EENNDD]] decision support; claim rework identification; health insurance claims; machine learning"}, "Perlombongan data untuk meramalkan dan mencegah kesilapan dalam memproses tuntutan insurans kesihatan Kos insurans kesihatan di seluruh dunia meningkat dengan membimbangkan dalam beberapa tahun terakhir. Penyebab utama kenaikan ini adalah kesilapan pembayaran yang dibuat oleh syarikat insurans semasa memproses tuntutan. Kesalahan ini sering mengakibatkan usaha pentadbiran tambahan untuk memproses semula atau mengolah semula tuntutan yang merangkumi hingga 30% kakitangan pentadbiran di syarikat insurans kesihatan biasa. Kami menerangkan sistem yang membantu mengurangkan kesilapan ini dengan menggunakan teknik pembelajaran mesin dengan meramalkan tuntutan yang perlu dikerjakan semula, menghasilkan penjelasan untuk membantu juruaudit membetulkan tuntutan ini, dan bereksperimen dengan pemilihan ciri, konsep konsep, dan pembelajaran aktif untuk mengumpulkan maklum balas dari juruaudit untuk bertambah baik dari masa ke masa. Kami menerangkan kerangka kerja kami, perumusan masalah, metrik penilaian, dan hasil eksperimen terhadap data tuntutan dari syarikat insurans kesihatan AS yang besar. Kami menunjukkan bahawa sistem kami menghasilkan urutan magnitud yang lebih baik dengan kadar ketepatan berbanding pendekatan yang ada yang cukup tepat sehingga berpotensi menghasilkan lebih dari $ 15-25 juta simpanan untuk syarikat insurans biasa. Kami juga menerangkan masalah penyelidikan yang menarik dalam domain ini serta pilihan reka bentuk yang dibuat untuk menjadikan sistem ini mudah digunakan di seluruh syarikat insurans kesihatan. [[EENNDD]] sokongan keputusan; tuntutan pengenalan semula tuntutan; tuntutan insurans kesihatan; pembelajaran mesin"], [{"string": "Mining a stream of transactions for customer patterns Transaction data can arrive at a ferocious rate in the order that transactions are completed . The data contain an enormous amount of information about customers , not just transactions , but extracting up-to-date customer information from an ever changing stream of data and mining it in real-time is a challenge . This paper describes a statistically principled approach to designing short , accurate summaries or signatures of high dimensional customer behavior that can be kept current with a stream of transactions . A signature database can then be used for data mining and to provide approximate answers to many kinds of queries about current customers quickly and accurately , as an empirical study of the calling patterns of 96,000 wireless customers who made about 18 million wireless calls over a three month period shows .", "keywords": ["dynamic database", "approximate queries", "customer profiles", "histograms", "signatures", "incremental updates", "massive data"], "combined": "Mining a stream of transactions for customer patterns Transaction data can arrive at a ferocious rate in the order that transactions are completed . The data contain an enormous amount of information about customers , not just transactions , but extracting up-to-date customer information from an ever changing stream of data and mining it in real-time is a challenge . This paper describes a statistically principled approach to designing short , accurate summaries or signatures of high dimensional customer behavior that can be kept current with a stream of transactions . A signature database can then be used for data mining and to provide approximate answers to many kinds of queries about current customers quickly and accurately , as an empirical study of the calling patterns of 96,000 wireless customers who made about 18 million wireless calls over a three month period shows . [[EENNDD]] dynamic database; approximate queries; customer profiles; histograms; signatures; incremental updates; massive data"}, "Melombong aliran transaksi untuk corak pelanggan Data transaksi boleh sampai pada kadar yang ganas dalam urutan transaksi selesai. Data tersebut mengandungi sejumlah besar informasi mengenai pelanggan, bukan hanya transaksi, tetapi mengekstrak maklumat pelanggan terkini dari aliran data yang selalu berubah dan menambangnya dalam masa nyata adalah satu cabaran. Makalah ini menerangkan pendekatan berprinsip secara statistik untuk merancang ringkasan pendek atau tepat mengenai tandatangan tingkah laku pelanggan dimensi tinggi yang dapat dikemas kini dengan aliran transaksi. Pangkalan data tandatangan kemudiannya dapat digunakan untuk perlombongan data dan untuk memberikan anggaran jawapan bagi banyak jenis pertanyaan mengenai pelanggan semasa dengan cepat dan tepat, sebagai kajian empirik terhadap pola panggilan 96,000 pelanggan tanpa wayar yang membuat sekitar 18 juta panggilan tanpa wayar selama tiga bulan menunjukkan tempoh. [[EENNDD]] pangkalan data dinamik; pertanyaan anggaran; profil pelanggan; histogram; tandatangan; kemas kini tambahan; data besar-besaran"], [{"string": "Learning the kernel matrix in discriminant analysis via quadratically constrained quadratic programming The kernel function plays a central role in kernel methods . In this paper , we consider the automated learning of the kernel matrix over a convex combination of pre-specified kernel matrices in Regularized Kernel Discriminant Analysis RKDA , which performs lineardiscriminant analysis in the feature space via the kernel trick . Previous studies have shown that this kernel learning problem can be formulated as a semidefinite program SDP , which is however computationally expensive , even with the recent advances in interior point methods . Based on the equivalence relationship between RKDA and least square problems in the binary-class case , we propose a Quadratically Constrained Quadratic Programming QCQP formulation for the kernel learning problem , which can be solved more efficiently than SDP . While most existing work on kernel learning deal with binary-class problems only , we show that our QCQP formulation can be extended naturally to the multi-class case . Experimental results on both binary-class and multi-class benchmarkdata sets show the efficacy of the proposed QCQP formulations .", "keywords": ["model selection", "quadratically constrained quadratic programming", "convex optimization", "kernel learning", "kernel discriminant analysis"], "combined": "Learning the kernel matrix in discriminant analysis via quadratically constrained quadratic programming The kernel function plays a central role in kernel methods . In this paper , we consider the automated learning of the kernel matrix over a convex combination of pre-specified kernel matrices in Regularized Kernel Discriminant Analysis RKDA , which performs lineardiscriminant analysis in the feature space via the kernel trick . Previous studies have shown that this kernel learning problem can be formulated as a semidefinite program SDP , which is however computationally expensive , even with the recent advances in interior point methods . Based on the equivalence relationship between RKDA and least square problems in the binary-class case , we propose a Quadratically Constrained Quadratic Programming QCQP formulation for the kernel learning problem , which can be solved more efficiently than SDP . While most existing work on kernel learning deal with binary-class problems only , we show that our QCQP formulation can be extended naturally to the multi-class case . Experimental results on both binary-class and multi-class benchmarkdata sets show the efficacy of the proposed QCQP formulations . [[EENNDD]] model selection; quadratically constrained quadratic programming; convex optimization; kernel learning; kernel discriminant analysis"}, "Mempelajari matriks kernel dalam analisis diskriminan melalui pengaturcaraan kuadratik yang dibatasi secara kuadratik Fungsi kernel memainkan peranan penting dalam kaedah kernel. Dalam makalah ini, kami mempertimbangkan pembelajaran matriks kernel secara automatik melalui kombinasi cembung matriks kernel yang ditentukan sebelumnya dalam RKDA Analisis Diskriminasi Kernel Teratur, yang melakukan analisis garis diskriminan di ruang fitur melalui trik kernel. Kajian terdahulu menunjukkan bahawa masalah pembelajaran kernel ini dapat dirumuskan sebagai SDP program semidefinite, yang bagaimanapun mahal, bahkan dengan kemajuan dalam kaedah titik dalaman baru-baru ini. Berdasarkan hubungan kesetaraan antara RKDA dan permasalahan paling kecil dalam kes kelas binari, kami mencadangkan rumusan QCQP Pengaturcaraan Kuadratik Terhad Kuadratik untuk masalah pembelajaran kernel, yang dapat diselesaikan dengan lebih efisien daripada SDP. Walaupun kebanyakan karya pembelajaran kernel yang ada hanya menangani masalah kelas binari, kami menunjukkan bahawa rumusan QCQP kami dapat diperluaskan secara semula jadi ke kes berbilang kelas. Hasil eksperimen pada set data penanda aras kelas binari dan berbilang kelas menunjukkan keberkesanan rumusan QCQP yang dicadangkan. [[EENNDD]] pemilihan model; pengaturcaraan kuadratik yang dibatasi secara kuadratik; pengoptimuman cembung; pembelajaran kernel; analisis diskriminasi kernel"], [{"string": "Mining GPS data to augment road models", "keywords": ["background knowledge", "noisy data", "incremental algorithms", "evaluating knowledge and potential discoveries", "case studies", "implementation and use of kdd systems"], "combined": "Mining GPS data to augment road models [[EENNDD]] background knowledge; noisy data; incremental algorithms; evaluating knowledge and potential discoveries; case studies; implementation and use of kdd systems"}, "Perlombongan data GPS untuk menambah model latar belakang [[EENNDD]]; data yang bising; algoritma kenaikan; menilai pengetahuan dan penemuan yang berpotensi; kajian kes; pelaksanaan dan penggunaan sistem kdd"], [{"string": "Passenger-based predictive modeling of airline no-show rates Airlines routinely overbook flights based on the expectation that some fraction of booked passengers will not show for each flight . Accurate forecasts of the expected number of no-shows for each flight can increase airline revenue by reducing the number of spoiled seats empty seats that might otherwise have been sold and the number of involuntary denied boardings at the departure gate . Conventional no-show forecasting methods typically average the no-show rates of historically similar flights , without the use of passenger-specific information . We develop two classes of models to predict cabin-level no-show rates using specific information on the individual passengers booked on each flight . The first of these models computes the no-show probability for each passenger , using both the cabin-level historical forecast and the extracted passenger features as explanatory variables . This passenger-level model is implemented using three different predictive methods : a C4 .5 decision-tree , a segmented Naive Bayes algorithm , and a new aggregation method for an ensemble of probabilistic models . The second cabin-level model is formulated using the desired cabin-level no-show rate as the response variable . Inputs to this model include the predicted cabin-level no-show rates derived from the various passenger-level models , as well as simple statistics of the features of the cabin passenger population . The cabin-level model is implemented using either linear regression , or as a direct probability model with explicit incorporation of the cabin-level no-show rates derived from the passenger-level model outputs . The new passenger-based models are compared to a conventional historical model , using train and evaluation data sets taken from over 1 million passenger name records . Standard metrics such as lift curves and mean-square cabin-level errors establish the improved accuracy of the passenger-based models over the historical model . All models are also evaluated using a simple revenue model , and it is shown that the cabin-level passenger-based model can produce between 0.4 % and 3.2 % revenue gain over the conventional model , depending on the revenue-model parameters .", "keywords": ["model aggregation", "airline overbooking", "classification", "no-show forecasting", "predictive modeling", "probabilistic estimation"], "combined": "Passenger-based predictive modeling of airline no-show rates Airlines routinely overbook flights based on the expectation that some fraction of booked passengers will not show for each flight . Accurate forecasts of the expected number of no-shows for each flight can increase airline revenue by reducing the number of spoiled seats empty seats that might otherwise have been sold and the number of involuntary denied boardings at the departure gate . Conventional no-show forecasting methods typically average the no-show rates of historically similar flights , without the use of passenger-specific information . We develop two classes of models to predict cabin-level no-show rates using specific information on the individual passengers booked on each flight . The first of these models computes the no-show probability for each passenger , using both the cabin-level historical forecast and the extracted passenger features as explanatory variables . This passenger-level model is implemented using three different predictive methods : a C4 .5 decision-tree , a segmented Naive Bayes algorithm , and a new aggregation method for an ensemble of probabilistic models . The second cabin-level model is formulated using the desired cabin-level no-show rate as the response variable . Inputs to this model include the predicted cabin-level no-show rates derived from the various passenger-level models , as well as simple statistics of the features of the cabin passenger population . The cabin-level model is implemented using either linear regression , or as a direct probability model with explicit incorporation of the cabin-level no-show rates derived from the passenger-level model outputs . The new passenger-based models are compared to a conventional historical model , using train and evaluation data sets taken from over 1 million passenger name records . Standard metrics such as lift curves and mean-square cabin-level errors establish the improved accuracy of the passenger-based models over the historical model . All models are also evaluated using a simple revenue model , and it is shown that the cabin-level passenger-based model can produce between 0.4 % and 3.2 % revenue gain over the conventional model , depending on the revenue-model parameters . [[EENNDD]] model aggregation; airline overbooking; classification; no-show forecasting; predictive modeling; probabilistic estimation"}, "Pemodelan ramalan berdasarkan penumpang pada kadar penerbangan tidak hadir Syarikat penerbangan secara rutin menempah penerbangan berdasarkan jangkaan bahawa sebahagian kecil penumpang yang ditempah tidak akan ditunjukkan untuk setiap penerbangan. Ramalan yang tepat mengenai jumlah ketidakhadiran yang dijangkakan untuk setiap penerbangan dapat meningkatkan pendapatan syarikat penerbangan dengan mengurangkan jumlah tempat duduk yang rosak yang mungkin telah dijual dan jumlah penumpang yang ditolak secara tidak sengaja di pintu keberangkatan. Kaedah peramalan tidak hadir secara konvensional biasanya rata-rata kadar ketiadaan penerbangan yang serupa dengan sejarah, tanpa menggunakan maklumat khusus penumpang. Kami mengembangkan dua kelas model untuk meramalkan kadar tidak hadir di tingkat kabin menggunakan maklumat khusus mengenai setiap penumpang yang ditempah pada setiap penerbangan. Model pertama ini mengira kebarangkalian tidak tampil untuk setiap penumpang, menggunakan ramalan sejarah tingkat kabin dan ciri penumpang yang diekstrak sebagai pemboleh ubah penjelasan. Model tahap penumpang ini dilaksanakan menggunakan tiga kaedah ramalan yang berbeza: pohon keputusan C4 .5, algoritma Naive Bayes yang tersegmentasi, dan kaedah agregasi baru untuk kumpulan model probabilistik. Model tahap kabin kedua dirumuskan menggunakan kadar tidak tampil di tingkat kabin yang diinginkan sebagai pemboleh ubah tindak balas. Input untuk model ini merangkumi ramalan kadar tidak menunjukkan tahap kabin yang berasal dari pelbagai model tingkat penumpang, serta statistik sederhana ciri penduduk penumpang kabin. Model tingkat kabin diimplementasikan dengan menggunakan regresi linier, atau sebagai model kebarangkalian langsung dengan penggabungan eksplisit dari tingkat tidak menunjukkan tingkat kabin yang berasal dari output model tingkat penumpang. Model berasaskan penumpang baru dibandingkan dengan model sejarah konvensional, menggunakan set data kereta api dan penilaian yang diambil dari lebih dari 1 juta rekod nama penumpang. Metrik standard seperti keluk angkat dan ralat kabin segi empat sama menunjukkan peningkatan ketepatan model berdasarkan penumpang berbanding model sejarah. Semua model juga dinilai menggunakan model pendapatan sederhana, dan ditunjukkan bahawa model berdasarkan penumpang tingkat kabin dapat menghasilkan keuntungan antara 0,4% dan 3,2% dibandingkan model konvensional, bergantung pada parameter model pendapatan. [[EENNDD]] penggabungan model; tempahan lebih banyak syarikat penerbangan; pengelasan; ramalan tanpa persembahan; pemodelan ramalan; anggaran kebarangkalian"], [{"string": "Effective label acquisition for collective classification Information diffusion , viral marketing , and collective classification all attempt to model and exploit the relationships in a network to make inferences about the labels of nodes . A variety of techniques have been introduced and methods that combine attribute information and neighboring label information have been shown to be effective for collective labeling of the nodes in a network . However , in part because of the correlation between node labels that the techniques exploit , it is easy to find cases in which , once a misclassification is made , incorrect information propagates throughout the network . This problem can be mitigated if the system is allowed to judiciously acquire the labels for a small number of nodes . Unfortunately , under relatively general assumptions , determining the optimal set of labels to acquire is intractable . Here we propose an acquisition method that learns the cases when a given collective classification algorithm makes mistakes , and suggests acquisitions to correct those mistakes . We empirically show on both real and synthetic datasets that this method significantly outperforms a greedy approximate inference approach , a viral marketing approach , and approaches based on network structural measures such as node degree and network clustering . In addition to significantly improving accuracy with just a small amount of labeled data , our method is tractable on large networks .", "keywords": ["label acquisition", "active inference", "collective classification"], "combined": "Effective label acquisition for collective classification Information diffusion , viral marketing , and collective classification all attempt to model and exploit the relationships in a network to make inferences about the labels of nodes . A variety of techniques have been introduced and methods that combine attribute information and neighboring label information have been shown to be effective for collective labeling of the nodes in a network . However , in part because of the correlation between node labels that the techniques exploit , it is easy to find cases in which , once a misclassification is made , incorrect information propagates throughout the network . This problem can be mitigated if the system is allowed to judiciously acquire the labels for a small number of nodes . Unfortunately , under relatively general assumptions , determining the optimal set of labels to acquire is intractable . Here we propose an acquisition method that learns the cases when a given collective classification algorithm makes mistakes , and suggests acquisitions to correct those mistakes . We empirically show on both real and synthetic datasets that this method significantly outperforms a greedy approximate inference approach , a viral marketing approach , and approaches based on network structural measures such as node degree and network clustering . In addition to significantly improving accuracy with just a small amount of labeled data , our method is tractable on large networks . [[EENNDD]] label acquisition; active inference; collective classification"}, "Pemerolehan label yang berkesan untuk klasifikasi kolektif Penyebaran maklumat, pemasaran virus, dan klasifikasi kolektif semuanya berusaha memodelkan dan memanfaatkan hubungan dalam rangkaian untuk membuat kesimpulan mengenai label nod. Pelbagai teknik telah diperkenalkan dan kaedah yang menggabungkan maklumat atribut dan maklumat label tetangga telah terbukti berkesan untuk pelabelan kolektif nod dalam rangkaian. Namun, sebahagiannya kerana korelasi antara label simpul yang dieksploitasi oleh teknik, mudah untuk mencari kes di mana, setelah salah klasifikasi dibuat, maklumat yang salah menyebarkan ke seluruh rangkaian. Masalah ini dapat dikurangkan jika sistem diizinkan memperoleh label dengan bijak untuk sebilangan kecil nod. Sayangnya, dengan anggapan yang agak umum, menentukan sekumpulan label optimum untuk diperoleh adalah sukar. Di sini kami mencadangkan kaedah pemerolehan yang mempelajari kes apabila algoritma klasifikasi kolektif tertentu membuat kesilapan, dan mencadangkan pemerolehan untuk membetulkan kesilapan tersebut. Kami secara empirik menunjukkan pada kumpulan data nyata dan sintetik bahawa kaedah ini secara signifikan mengungguli pendekatan inferensi anggaran tamak, pendekatan pemasaran virus, dan pendekatan berdasarkan langkah-langkah struktur rangkaian seperti tahap nod dan pengelompokan rangkaian. Selain meningkatkan ketepatan secara signifikan dengan hanya sebilangan kecil data berlabel, kaedah kami dapat dijumpai di rangkaian besar. [[EENNDD]] pemerolehan label; inferens aktif; pengelasan kolektif"], [{"string": "Cost-effective outbreak detection in networks Given a water distribution network , where should we place sensors toquickly detect contaminants ? Or , which blogs should we read to avoid missing important stories ? . These seemingly different problems share common structure : Outbreak detection can be modeled as selecting nodes sensor locations , blogs in a network , in order to detect the spreading of a virus or information asquickly as possible . We present a general methodology for near optimal sensor placement in these and related problems . We demonstrate that many realistic outbreak detection objectives e.g. , detection likelihood , population affected exhibit the property of `` submodularity '' . We exploit submodularity to develop an efficient algorithm that scales to large problems , achieving near optimal placements , while being 700 times faster than a simple greedy algorithm . We also derive online bounds on the quality of the placements obtained by any algorithm . Our algorithms and bounds also handle cases where nodes sensor locations , blogs have different costs . We evaluate our approach on several large real-world problems , including a model of a water distribution network from the EPA , andreal blog data . The obtained sensor placements are provably near optimal , providing a constant fraction of the optimal solution . We show that the approach scales , achieving speedups and savings in storage of several orders of magnitude . We also show how the approach leads to deeper insights in both applications , answering multicriteria trade-off , cost-sensitivity and generalization questions .", "keywords": ["virus propagation", "graphs", "information cascades", "submodular functions", "sensor placement"], "combined": "Cost-effective outbreak detection in networks Given a water distribution network , where should we place sensors toquickly detect contaminants ? Or , which blogs should we read to avoid missing important stories ? . These seemingly different problems share common structure : Outbreak detection can be modeled as selecting nodes sensor locations , blogs in a network , in order to detect the spreading of a virus or information asquickly as possible . We present a general methodology for near optimal sensor placement in these and related problems . We demonstrate that many realistic outbreak detection objectives e.g. , detection likelihood , population affected exhibit the property of `` submodularity '' . We exploit submodularity to develop an efficient algorithm that scales to large problems , achieving near optimal placements , while being 700 times faster than a simple greedy algorithm . We also derive online bounds on the quality of the placements obtained by any algorithm . Our algorithms and bounds also handle cases where nodes sensor locations , blogs have different costs . We evaluate our approach on several large real-world problems , including a model of a water distribution network from the EPA , andreal blog data . The obtained sensor placements are provably near optimal , providing a constant fraction of the optimal solution . We show that the approach scales , achieving speedups and savings in storage of several orders of magnitude . We also show how the approach leads to deeper insights in both applications , answering multicriteria trade-off , cost-sensitivity and generalization questions . [[EENNDD]] virus propagation; graphs; information cascades; submodular functions; sensor placement"}, "Pengesanan wabak yang menjimatkan dalam rangkaian Memandangkan rangkaian pengedaran air, di mana kita harus meletakkan sensor untuk mengesan bahan cemar dengan cepat? Atau, blog mana yang harus kita baca untuk mengelakkan kehilangan kisah penting? . Masalah yang kelihatan berbeza ini mempunyai struktur yang sama: Pengesanan wabak dapat dimodelkan sebagai memilih lokasi sensor node, blog dalam rangkaian, untuk mengesan penyebaran virus atau maklumat secepat mungkin. Kami menyajikan metodologi umum untuk penempatan sensor yang hampir optimum dalam masalah ini dan yang berkaitan. Kami menunjukkan bahawa banyak objektif pengesanan wabak yang realistik, mis. , kemungkinan pengesanan, populasi yang terjejas menunjukkan sifat \"submodularity\". Kami mengeksploitasi submodularity untuk mengembangkan algoritma yang cekap untuk mengatasi masalah besar, mencapai penempatan hampir optimum, sementara 700 kali lebih cepat daripada algoritma tamak sederhana. Kami juga memperoleh batasan dalam talian mengenai kualiti penempatan yang diperoleh oleh algoritma apa pun. Algoritma dan batas kami juga menangani kes di mana lokasi sensor nod, blog mempunyai kos yang berbeza. Kami menilai pendekatan kami mengenai beberapa masalah dunia nyata yang besar, termasuk model rangkaian pengedaran air dari EPA, dan data blog sebenar. Penempatan sensor yang diperoleh terbukti hampir optimum, memberikan pecahan tetap dari penyelesaian optimum. Kami menunjukkan bahawa pendekatan itu berskala, mencapai peningkatan dan penjimatan dalam penyimpanan beberapa pesanan besar. Kami juga menunjukkan bagaimana pendekatan itu membawa kepada pandangan yang lebih mendalam dalam kedua-dua aplikasi tersebut, menjawab soalan pertukaran pelbagai kriteria, kepekaan kos dan generalisasi. [[EENNDD]] penyebaran virus; grafik; lata maklumat; fungsi submodular; penempatan sensor"], [{"string": "A general model for clustering binary data Clustering is the problem of identifying the distribution of patterns and intrinsic correlations in large data sets by partitioning the data points into similarity classes . This paper studies the problem of clustering binary data . This is the case for market basket datasets where the transactions contain items and for document datasets where the documents contain `` bag of words '' . The contribution of the paper is three-fold . First a general binary data clustering model is presented . The model treats the data and features equally , based on their symmetric association relations , and explicitly describes the data assignments as well as feature assignments . We characterize several variations with different optimization procedures for the general model . Second , we also establish the connections between our clustering model with other existing clustering methods . Third , we also discuss the problem for determining the number of clusters for binary clustering . Experimental results show the effectiveness of the proposed clustering model .", "keywords": ["binary data", "matrix approximation", "clustering", "general model"], "combined": "A general model for clustering binary data Clustering is the problem of identifying the distribution of patterns and intrinsic correlations in large data sets by partitioning the data points into similarity classes . This paper studies the problem of clustering binary data . This is the case for market basket datasets where the transactions contain items and for document datasets where the documents contain `` bag of words '' . The contribution of the paper is three-fold . First a general binary data clustering model is presented . The model treats the data and features equally , based on their symmetric association relations , and explicitly describes the data assignments as well as feature assignments . We characterize several variations with different optimization procedures for the general model . Second , we also establish the connections between our clustering model with other existing clustering methods . Third , we also discuss the problem for determining the number of clusters for binary clustering . Experimental results show the effectiveness of the proposed clustering model . [[EENNDD]] binary data; matrix approximation; clustering; general model"}, "Model umum untuk pengelompokan data binari Penggabungan adalah masalah mengenal pasti taburan corak dan korelasi intrinsik dalam kumpulan data besar dengan memisahkan titik data ke dalam kelas kesamaan. Makalah ini mengkaji masalah pengelompokan data binari. Ini berlaku untuk kumpulan data keranjang pasar di mana transaksi mengandungi item dan untuk kumpulan data dokumen di mana dokumen tersebut mengandungi \"beg kata\". Sumbangan kertas adalah tiga kali ganda. Pertama, model pengelompokan data binari umum dibentangkan. Model tersebut memperlakukan data dan ciri secara sama rata, berdasarkan hubungan asosiasi simetri mereka, dan secara eksplisit menerangkan penugasan data serta penugasan ciri. Kami mencirikan beberapa variasi dengan prosedur pengoptimuman yang berbeza untuk model umum. Kedua, kami juga menjalin hubungan antara model pengelompokan kami dengan kaedah pengelompokan lain yang ada. Ketiga, kami juga membincangkan masalah untuk menentukan jumlah kluster untuk pengelompokan binari. Hasil eksperimen menunjukkan keberkesanan model pengelompokan yang dicadangkan. [[EENNDD]] data binari; penghampiran matriks; pengelompokan; model am"], [{"string": "Collaborative crawling : mining user experiences for topical resource discovery The rapid growth of the world wide web had made the problem of topic specific resource discovery an important one in recent years . In this problem , it is desired to find web pages which satisfy a predicate specified by the user . Such a predicate could be a keyword query , a topical query , or some arbitrary contraint . Several techniques such as focussed crawling and intelligent crawling have recently been proposed for topic specific resource discovery . All these crawlers are linkage based , since they use the hyperlink behavior in order to perform resource discovery . Recent studies have shown that the topical correlations in hyperlinks are quite noisy and may not always show the consistency necessary for a reliable resource discovery process . In this paper , we will approach the problem of resource discovery from an entirely different perspective ; we will mine the significant browsing patterns of world wide web users in order to model the likelihood of web pages belonging to a specified predicate . This user behavior can be mined from the freely available traces of large public domain proxies on the world wide web . We refer to this technique as collaborative crawling because it mines the collective user experiences in order to find topical resources . Such a strategy is extremely effective because the topical consistency in world wide web browsing patterns turns out to very reliable . In addition , the user-centered crawling system can be combined with linkage based systems to create an overall system which works more effectively than a system based purely on either user behavior or hyperlinks .", "keywords": ["world wide web"], "combined": "Collaborative crawling : mining user experiences for topical resource discovery The rapid growth of the world wide web had made the problem of topic specific resource discovery an important one in recent years . In this problem , it is desired to find web pages which satisfy a predicate specified by the user . Such a predicate could be a keyword query , a topical query , or some arbitrary contraint . Several techniques such as focussed crawling and intelligent crawling have recently been proposed for topic specific resource discovery . All these crawlers are linkage based , since they use the hyperlink behavior in order to perform resource discovery . Recent studies have shown that the topical correlations in hyperlinks are quite noisy and may not always show the consistency necessary for a reliable resource discovery process . In this paper , we will approach the problem of resource discovery from an entirely different perspective ; we will mine the significant browsing patterns of world wide web users in order to model the likelihood of web pages belonging to a specified predicate . This user behavior can be mined from the freely available traces of large public domain proxies on the world wide web . We refer to this technique as collaborative crawling because it mines the collective user experiences in order to find topical resources . Such a strategy is extremely effective because the topical consistency in world wide web browsing patterns turns out to very reliable . In addition , the user-centered crawling system can be combined with linkage based systems to create an overall system which works more effectively than a system based purely on either user behavior or hyperlinks . [[EENNDD]] world wide web"}, "Perayapan kolaboratif: pengalaman pengguna perlombongan untuk penemuan sumber daya topikal Pertumbuhan pesat di seluruh dunia menjadikan masalah penemuan sumber daya topik yang penting dalam beberapa tahun kebelakangan ini. Dalam masalah ini, dikehendaki mencari laman web yang memenuhi predikat yang ditentukan oleh pengguna. Predikat seperti itu boleh menjadi pertanyaan kata kunci, pertanyaan topikal, atau beberapa kontroversi sewenang-wenangnya. Beberapa teknik seperti merangkak fokus dan merangkak pintar baru-baru ini telah dicadangkan untuk penemuan sumber khusus topik. Semua perayap ini berdasarkan tautan, kerana mereka menggunakan tingkah laku hiperpautan untuk melakukan penemuan sumber. Kajian terkini menunjukkan bahawa korelasi topikal dalam pautan sangat bising dan mungkin tidak selalu menunjukkan konsistensi yang diperlukan untuk proses penemuan sumber yang boleh dipercayai. Dalam makalah ini, kita akan mendekati masalah penemuan sumber dari perspektif yang sama sekali berbeza; kami akan meneliti corak penyemakan imbas yang ketara bagi pengguna web seluruh dunia untuk memodelkan kemungkinan halaman web tergolong dalam predikat yang ditentukan. Tingkah laku pengguna ini dapat diambil dari jejak proksi domain awam yang tersedia secara bebas di web seluruh dunia. Kami merujuk teknik ini sebagai perayapan kolaboratif kerana ia mengurangkan pengalaman pengguna kolektif untuk mencari sumber topikal. Strategi seperti ini sangat berkesan kerana konsistensi topikal dalam corak melayari laman web seluruh dunia ternyata sangat dipercayai. Sebagai tambahan, sistem perayapan yang berpusat pada pengguna dapat digabungkan dengan sistem berasaskan hubungan untuk membuat sistem keseluruhan yang berfungsi lebih efektif daripada sistem yang berdasarkan pada tingkah laku pengguna atau hyperlink. [[EENNDD]] web seluruh dunia"], [{"string": "A fast kernel-based multilevel algorithm for graph clustering Graph clustering also called graph partitioning -- clustering the nodes of a graph -- is an important problem in diverse data mining applications . Traditional approaches involve optimization of graph clustering objectives such as normalized cut or ratio association ; spectral methods are widely used for these objectives , but they require eigenvector computation which can be slow . Recently , graph clustering with a general cut objective has been shown to be mathematically equivalent to an appropriate weighted kernel k-means objective function . In this paper , we exploit this equivalence to develop a very fast multilevel algorithm for graph clustering . Multilevel approaches involve coarsening , initial partitioning and refinement phases , all of which may be specialized to different graph clustering objectives . Unlike existing multilevel clustering approaches , such as METIS , our algorithm does not constrain the cluster sizes to be nearly equal . Our approach gives a theoretical guarantee that the refinement step decreases the graph cut objective under consideration . Experiments show that we achieve better final objective function values as compared to a state-of-the-art spectral clustering algorithm : on a series of benchmark test graphs with up to thirty thousand nodes and one million edges , our algorithm achieves lower normalized cut values in 67 % of our experiments and higher ratio association values in 100 % of our experiments . Furthermore , on large graphs , our algorithm is significantly faster than spectral methods . Finally , our algorithm requires far less memory than spectral methods ; we cluster a 1.2 million node movie network into 5000 clusters , which due to memory requirements can not be done directly with spectral methods .", "keywords": ["kernel methods", "graph clustering", "spectral clustering", "multilevel methods"], "combined": "A fast kernel-based multilevel algorithm for graph clustering Graph clustering also called graph partitioning -- clustering the nodes of a graph -- is an important problem in diverse data mining applications . Traditional approaches involve optimization of graph clustering objectives such as normalized cut or ratio association ; spectral methods are widely used for these objectives , but they require eigenvector computation which can be slow . Recently , graph clustering with a general cut objective has been shown to be mathematically equivalent to an appropriate weighted kernel k-means objective function . In this paper , we exploit this equivalence to develop a very fast multilevel algorithm for graph clustering . Multilevel approaches involve coarsening , initial partitioning and refinement phases , all of which may be specialized to different graph clustering objectives . Unlike existing multilevel clustering approaches , such as METIS , our algorithm does not constrain the cluster sizes to be nearly equal . Our approach gives a theoretical guarantee that the refinement step decreases the graph cut objective under consideration . Experiments show that we achieve better final objective function values as compared to a state-of-the-art spectral clustering algorithm : on a series of benchmark test graphs with up to thirty thousand nodes and one million edges , our algorithm achieves lower normalized cut values in 67 % of our experiments and higher ratio association values in 100 % of our experiments . Furthermore , on large graphs , our algorithm is significantly faster than spectral methods . Finally , our algorithm requires far less memory than spectral methods ; we cluster a 1.2 million node movie network into 5000 clusters , which due to memory requirements can not be done directly with spectral methods . [[EENNDD]] kernel methods; graph clustering; spectral clustering; multilevel methods"}, "Algoritma bertingkat yang berasaskan kernel cepat untuk pengelompokan grafik Penggabungan grafik juga disebut partisi grafik - pengelompokan nod graf - adalah masalah penting dalam pelbagai aplikasi perlombongan data. Pendekatan tradisional melibatkan pengoptimuman objektif pengelompokan grafik seperti persatuan pemotongan atau nisbah yang dinormalisasi; kaedah spektrum digunakan secara meluas untuk objektif ini, tetapi memerlukan pengiraan eigenvector yang boleh menjadi lambat. Baru-baru ini, pengelompokan grafik dengan objektif potong umum telah ditunjukkan secara matematik setara dengan fungsi objektif k-bermaksud kernel berwajaran yang sesuai. Dalam makalah ini, kami memanfaatkan kesetaraan ini untuk mengembangkan algoritma bertingkat yang sangat cepat untuk pengelompokan grafik. Pendekatan bertingkat merangkumi fasa penyempitan, pemisahan awal dan penyempurnaan, yang semuanya mungkin khusus untuk objektif pengelompokan grafik yang berbeza. Tidak seperti pendekatan pengelompokan bertingkat yang ada, seperti METIS, algoritma kami tidak mengekang ukuran kelompok hampir sama. Pendekatan kami memberikan jaminan teoretikal bahawa langkah penyempurnaan mengurangkan objektif pemotongan grafik yang sedang dipertimbangkan. Eksperimen menunjukkan bahawa kami mencapai nilai fungsi objektif akhir yang lebih baik berbanding dengan algoritma pengelompokan spektrum canggih: pada siri grafik ujian penanda aras dengan hingga tiga puluh ribu nod dan satu juta tepi, algoritma kami mencapai nilai potong normal yang lebih rendah dalam 67% eksperimen kami dan nilai perkaitan nisbah yang lebih tinggi dalam 100% eksperimen kami. Selanjutnya, pada graf besar, algoritma kami jauh lebih cepat daripada kaedah spektrum. Akhirnya, algoritma kami memerlukan memori yang jauh lebih sedikit daripada kaedah spektrum; kami mengumpulkan rangkaian filem 1.2 juta simpul menjadi 5000 kelompok, yang kerana keperluan memori tidak dapat dilakukan secara langsung dengan kaedah spektrum. [[EENNDD]] kaedah kernel; pengelompokan grafik; pengelompokan spektrum; kaedah bertingkat"], [{"string": "Experimental design for solicitation campaigns Data mining techniques are routinely used by fundraisers to select those prospects from a large pool of candidates who are most likely to make a financial contribution . These techniques often rely on statistical models based on trial performance data . This trial performance data is typically obtained by soliciting a smaller sample of the possible prospect pool . Collecting this trial data involves a cost ; therefore the fundraiser is interested in keeping the trial size small while still collecting enough data to build a reliable statistical model that will be used to evaluate the remainder of the prospects . We describe an experimental design approach to optimally choose the trial prospects from an existing large pool of prospects . Prospects are clustered to render the problem practically tractable . We modify the standard D-optimality algorithm to prevent repeated selection of the same prospect cluster , since each prospect can only be solicited at most once . We assess the benefits of this approach on the KDD-98 data set by comparing the performance of the model based on the optimal trial data set with that of a model based on a randomly selected trial data set of equal size .", "keywords": ["data collection", "experimental design", "probability and statistics", "solicitation campaign"], "combined": "Experimental design for solicitation campaigns Data mining techniques are routinely used by fundraisers to select those prospects from a large pool of candidates who are most likely to make a financial contribution . These techniques often rely on statistical models based on trial performance data . This trial performance data is typically obtained by soliciting a smaller sample of the possible prospect pool . Collecting this trial data involves a cost ; therefore the fundraiser is interested in keeping the trial size small while still collecting enough data to build a reliable statistical model that will be used to evaluate the remainder of the prospects . We describe an experimental design approach to optimally choose the trial prospects from an existing large pool of prospects . Prospects are clustered to render the problem practically tractable . We modify the standard D-optimality algorithm to prevent repeated selection of the same prospect cluster , since each prospect can only be solicited at most once . We assess the benefits of this approach on the KDD-98 data set by comparing the performance of the model based on the optimal trial data set with that of a model based on a randomly selected trial data set of equal size . [[EENNDD]] data collection; experimental design; probability and statistics; solicitation campaign"}, "Reka bentuk eksperimental untuk kempen permintaan Teknik perlombongan data secara rutin digunakan oleh pengumpul dana untuk memilih prospek tersebut dari sejumlah besar calon yang kemungkinan besar akan memberikan sumbangan kewangan. Teknik-teknik ini sering bergantung pada model statistik berdasarkan data prestasi percubaan. Data prestasi percubaan ini biasanya diperoleh dengan meminta sampel yang lebih kecil dari kumpulan prospek yang mungkin. Mengumpulkan data percubaan ini melibatkan kos; oleh itu penggalangan dana berminat untuk menjaga ukuran percubaan tetap kecil sementara masih mengumpulkan cukup data untuk membangun model statistik yang dapat dipercayai yang akan digunakan untuk menilai sisa prospek. Kami menerangkan pendekatan reka bentuk eksperimental untuk memilih prospek percubaan secara optimum dari kumpulan prospek yang ada. Prospek dikumpulkan untuk menjadikan masalah ini dapat diselesaikan secara praktikal. Kami mengubah algoritma D-optimum standard untuk mengelakkan pemilihan berulang dari kelompok prospek yang sama, kerana setiap prospek hanya dapat diminta paling banyak sekali. Kami menilai faedah pendekatan ini pada kumpulan data KDD-98 dengan membandingkan prestasi model berdasarkan set data percubaan yang optimum dengan model berdasarkan pada kumpulan data percubaan yang dipilih secara rawak dengan ukuran yang sama. [[EENNDD]] pengumpulan data; reka bentuk eksperimen; kebarangkalian dan statistik; kempen permohonan"], [{"string": "CFI-Stream : mining closed frequent itemsets in data streams Mining frequent closed itemsets provides complete and condensed information for non-redundant association rules generation . Extensive studies have been done on mining frequent closed itemsets , but they are mainly intended for traditional transaction databases and thus do not take data stream characteristics into consideration . In this paper , we propose a novel approach for mining closed frequent itemsets over data streams . It computes and maintains closed itemsets online and incrementally , and can output the current closed frequent itemsets in real time based on users ' specified thresholds . Experimental results show that our proposed method is both time and space efficient , has good scalability as the number of transactions processed increases and adapts very rapidly to the change in data streams .", "keywords": ["data stream", "frequent closed itemsets", "association rules"], "combined": "CFI-Stream : mining closed frequent itemsets in data streams Mining frequent closed itemsets provides complete and condensed information for non-redundant association rules generation . Extensive studies have been done on mining frequent closed itemsets , but they are mainly intended for traditional transaction databases and thus do not take data stream characteristics into consideration . In this paper , we propose a novel approach for mining closed frequent itemsets over data streams . It computes and maintains closed itemsets online and incrementally , and can output the current closed frequent itemsets in real time based on users ' specified thresholds . Experimental results show that our proposed method is both time and space efficient , has good scalability as the number of transactions processed increases and adapts very rapidly to the change in data streams . [[EENNDD]] data stream; frequent closed itemsets; association rules"}, "CFI-Stream: melombong kumpulan barang yang kerap ditutup dalam aliran data Melombong set barang yang sering ditutup memberikan maklumat lengkap dan padat untuk penjanaan peraturan persatuan yang tidak berlebihan. Kajian ekstensif telah dilakukan mengenai penambangan kumpulan barang yang sering ditutup, tetapi kebanyakannya ditujukan untuk pangkalan data transaksi tradisional dan dengan demikian tidak mempertimbangkan ciri aliran data. Dalam makalah ini, kami mencadangkan pendekatan baru untuk melombong kumpulan barang yang sering ditutup melalui aliran data. Ia mengira dan mengekalkan set item tertutup secara dalam talian dan bertahap, dan dapat menghasilkan kumpulan barang tertutup semasa dalam masa nyata berdasarkan ambang yang ditentukan pengguna. Hasil eksperimen menunjukkan bahawa kaedah yang dicadangkan kami adalah masa dan ruang yang cekap, mempunyai skalabilitas yang baik kerana jumlah transaksi yang diproses meningkat dan menyesuaikan diri dengan cepat terhadap perubahan aliran data. [[EENNDD]] aliran data; set barang yang kerap ditutup; peraturan persatuan"], [{"string": "Unifying dependent clustering and disparate clustering for non-homogeneous data Modern data mining settings involve a combination of attribute-valued descriptors over entities as well as specified relationships between these entities . We present an approach to cluster such non-homogeneous datasets by using the relationships to impose either dependent clustering or disparate clustering constraints . Unlike prior work that views constraints as boolean criteria , we present a formulation that allows constraints to be satisfied or violated in a smooth manner . This enables us to achieve dependent clustering and disparate clustering using the same optimization framework by merely maximizing versus minimizing the objective function . We present results on both synthetic data as well as several real-world datasets .", "keywords": ["relational clustering", "learning", "multi-criteria optimization.", "contingency tables", "clustering"], "combined": "Unifying dependent clustering and disparate clustering for non-homogeneous data Modern data mining settings involve a combination of attribute-valued descriptors over entities as well as specified relationships between these entities . We present an approach to cluster such non-homogeneous datasets by using the relationships to impose either dependent clustering or disparate clustering constraints . Unlike prior work that views constraints as boolean criteria , we present a formulation that allows constraints to be satisfied or violated in a smooth manner . This enables us to achieve dependent clustering and disparate clustering using the same optimization framework by merely maximizing versus minimizing the objective function . We present results on both synthetic data as well as several real-world datasets . [[EENNDD]] relational clustering; learning; multi-criteria optimization.; contingency tables; clustering"}, "Menyatukan penggabungan yang bergantung dan pengelompokan yang berbeza untuk data yang tidak homogen Tetapan perlombongan data moden melibatkan gabungan penerangan bernilai atribut terhadap entiti serta hubungan yang ditentukan antara entiti ini. Kami menyajikan pendekatan untuk mengelompokkan set data yang tidak homogen dengan menggunakan hubungan untuk menerapkan pengelompokan bergantung atau sekatan pengelompokan yang berbeza. Tidak seperti karya sebelumnya yang melihat batasan sebagai kriteria boolean, kami menyajikan rumusan yang membolehkan batasan dipenuhi atau dilanggar dengan lancar. Ini membolehkan kita mencapai pengelompokan bergantung dan pengelompokan yang berbeza menggunakan kerangka pengoptimuman yang sama dengan hanya memaksimumkan berbanding meminimumkan fungsi objektif. Kami membentangkan hasil pada kedua-dua data sintetik dan juga beberapa set data dunia nyata. [[EENNDD]] pengelompokan hubungan; belajar; pengoptimuman pelbagai kriteria .; jadual kontingensi; pengelompokan"], [{"string": "Visual classification : an interactive approach to decision tree construction", "keywords": ["decision support"], "combined": "Visual classification : an interactive approach to decision tree construction [[EENNDD]] decision support"}, "Klasifikasi visual: pendekatan interaktif untuk pembinaan keputusan keputusan [[EENNDD]] sokongan keputusan"], [{"string": "Rule extraction from linear support vector machines We describe an algorithm for converting linear support vector machines and any other arbitrary hyperplane-based linear classifiers into a set of non-overlapping rules that , unlike the original classifier , can be easily interpreted by humans . Each iteration of the rule extraction algorithm is formulated as a constrained optimization problem that is computationally inexpensive to solve . We discuss various properties of the algorithm and provide proof of convergence for two different optimization criteria We demonstrate the performance and the speed of the algorithm on linear classifiers learned from real-world datasets , including a medical dataset on detection of lung cancer from medical images . The ability to convert SVM 's and other `` black-box '' classifiers into a set of human-understandable rules , is critical not only for physician acceptance , but also to reducing the regulatory barrier for medical-decision support systems based on such classifiers .", "keywords": ["rule extraction", "medical decision-support", "miscellaneous", "mathematical programming", "linear classifiers"], "combined": "Rule extraction from linear support vector machines We describe an algorithm for converting linear support vector machines and any other arbitrary hyperplane-based linear classifiers into a set of non-overlapping rules that , unlike the original classifier , can be easily interpreted by humans . Each iteration of the rule extraction algorithm is formulated as a constrained optimization problem that is computationally inexpensive to solve . We discuss various properties of the algorithm and provide proof of convergence for two different optimization criteria We demonstrate the performance and the speed of the algorithm on linear classifiers learned from real-world datasets , including a medical dataset on detection of lung cancer from medical images . The ability to convert SVM 's and other `` black-box '' classifiers into a set of human-understandable rules , is critical not only for physician acceptance , but also to reducing the regulatory barrier for medical-decision support systems based on such classifiers . [[EENNDD]] rule extraction; medical decision-support; miscellaneous; mathematical programming; linear classifiers"}, "Pengekstrakan peraturan dari mesin vektor sokongan linier Kami menerangkan algoritma untuk menukar mesin vektor sokongan linear dan pengelasan linear berasaskan hiperplan sewenang-wenangnya menjadi satu set peraturan yang tidak bertindih yang, tidak seperti pengklasifikasi asal, dapat dengan mudah ditafsirkan oleh manusia. Setiap iterasi algoritma pengekstrakan aturan dirumuskan sebagai masalah pengoptimuman terkendali yang tidak dapat diselesaikan secara komputasi. Kami membincangkan pelbagai sifat algoritma dan memberikan bukti penumpuan untuk dua kriteria pengoptimuman yang berbeza. Kami menunjukkan prestasi dan kepantasan algoritma pada pengkelasan linear yang dipelajari dari kumpulan data dunia nyata, termasuk set data perubatan mengenai pengesanan barah paru-paru dari gambar perubatan. Keupayaan untuk menukar pengkelasan SVM dan \"kotak hitam\" lain menjadi sekumpulan peraturan yang dapat difahami manusia, sangat penting bukan hanya untuk penerimaan doktor, tetapi juga untuk mengurangkan halangan peraturan untuk sistem sokongan keputusan perubatan berdasarkan pengelasan. [[EENNDD]] pengekstrakan peraturan; sokongan keputusan perubatan; pelbagai; pengaturcaraan matematik; pengkelasan linear"], [{"string": "Efficient mining of iterative patterns for software specification discovery Studies have shown that program comprehension takes up to 45 % of software development costs . Such high costs are caused by the lack-of documented specification and further aggravated by the phenomenon of software evolution . There is a need for automated tools to extract specifications to aid program comprehension . In this paper , a novel technique to efficiently mine common software temporal patterns from traces is proposed . These patterns shed light on program behaviors , and are termed iterative patterns . They capture unique characteristic of software traces , typically not found in arbitrary sequences . Specifically , due to loops , interesting iterative patterns can occur multiple times within a trace . Furthermore , an occurrence of an iterative pattern in a trace can extend across a sequence of indefinite length . Since a program behavior can be manifested in numerous ways , analyzing a single trace will not be sufficient . Iterative pattern mining extends sequential pattern and episode minings to discover frequent iterative patterns which occur repetitively both within a program trace and across multiple traces . In this paper , we present CLIPER CLosed Iterative Pattern minER to efficiently mine a closed set of iterative patterns . A performance study on several simulated and real datasets shows the efficiency of our mining algorithm and effectiveness of our pruning strategy . Our case study on JBoss Application Server confirms the usefulness of mined patterns in discovering interesting software behavioral specification .", "keywords": ["software specification discovery", "closed iterative patterns"], "combined": "Efficient mining of iterative patterns for software specification discovery Studies have shown that program comprehension takes up to 45 % of software development costs . Such high costs are caused by the lack-of documented specification and further aggravated by the phenomenon of software evolution . There is a need for automated tools to extract specifications to aid program comprehension . In this paper , a novel technique to efficiently mine common software temporal patterns from traces is proposed . These patterns shed light on program behaviors , and are termed iterative patterns . They capture unique characteristic of software traces , typically not found in arbitrary sequences . Specifically , due to loops , interesting iterative patterns can occur multiple times within a trace . Furthermore , an occurrence of an iterative pattern in a trace can extend across a sequence of indefinite length . Since a program behavior can be manifested in numerous ways , analyzing a single trace will not be sufficient . Iterative pattern mining extends sequential pattern and episode minings to discover frequent iterative patterns which occur repetitively both within a program trace and across multiple traces . In this paper , we present CLIPER CLosed Iterative Pattern minER to efficiently mine a closed set of iterative patterns . A performance study on several simulated and real datasets shows the efficiency of our mining algorithm and effectiveness of our pruning strategy . Our case study on JBoss Application Server confirms the usefulness of mined patterns in discovering interesting software behavioral specification . [[EENNDD]] software specification discovery; closed iterative patterns"}, "Penambangan corak berulang yang berkesan untuk penemuan spesifikasi perisian Kajian telah menunjukkan bahawa pemahaman program memerlukan sehingga 45% dari kos pengembangan perisian. Kos yang tinggi itu disebabkan oleh kekurangan spesifikasi yang didokumentasikan dan semakin diperburuk oleh fenomena evolusi perisian. Ada keperluan alat automatik untuk mengekstrak spesifikasi untuk membantu pemahaman program. Dalam makalah ini, teknik baru untuk menambang secara berkesan corak temporal perisian biasa dari jejak dicadangkan. Pola-pola ini menjelaskan tingkah laku program, dan disebut sebagai pola berulang. Mereka menangkap ciri unik jejak perisian, biasanya tidak terdapat dalam urutan sewenang-wenangnya. Khususnya, kerana gelung, corak berulang yang menarik dapat terjadi berkali-kali dalam jejak. Selanjutnya, berlakunya corak berulang dalam jejak dapat melintasi urutan panjang yang tidak tentu. Oleh kerana tingkah laku program dapat dimanifestasikan dalam pelbagai cara, analisis satu jejak tidak akan mencukupi. Perlombongan corak berulang meluaskan corak dan urutan episod untuk mencari corak berulang berulang yang berlaku berulang kali dalam jejak program dan melintasi pelbagai jejak. Dalam makalah ini, kami menyajikan CLERER CLOSER Iterative Pattern minER untuk melombong satu set corak berulang berulang secara cekap. Kajian prestasi pada beberapa set data simulasi dan nyata menunjukkan kecekapan algoritma perlombongan dan keberkesanan strategi pemangkasan kami. Kajian kes kami di JBoss Application Server mengesahkan kegunaan corak yang dilombong dalam menemui spesifikasi tingkah laku perisian yang menarik. [[EENNDD]] penemuan spesifikasi perisian; corak berulang berulang"], [{"string": "Discovering associations with numeric variables This paper further develops Aumann and Lindell 's 3 proposal for a variant of association rules for which the consequent is a numeric variable . It is argued that these rules can discover useful interactions with numeric data that can not be discovered directly using traditional association rules with discretization . Alternative measures for identifying interesting rules are proposed . Efficient algorithms are presented that enable these rules to be discovered for dense data sets for which application of Auman and Lindell 's algorithm is infeasible .", "keywords": ["impact rule", "information search and retrieval", "learning", "association rule", "numeric data", "search"], "combined": "Discovering associations with numeric variables This paper further develops Aumann and Lindell 's 3 proposal for a variant of association rules for which the consequent is a numeric variable . It is argued that these rules can discover useful interactions with numeric data that can not be discovered directly using traditional association rules with discretization . Alternative measures for identifying interesting rules are proposed . Efficient algorithms are presented that enable these rules to be discovered for dense data sets for which application of Auman and Lindell 's algorithm is infeasible . [[EENNDD]] impact rule; information search and retrieval; learning; association rule; numeric data; search"}, "Menemui kaitan dengan pemboleh ubah berangka Makalah ini selanjutnya mengembangkan cadangan 3 Aumann dan Lindell untuk varian peraturan persatuan yang akibatnya adalah pemboleh ubah berangka. Dikatakan bahawa peraturan ini dapat menemui interaksi yang berguna dengan data berangka yang tidak dapat dijumpai secara langsung menggunakan peraturan persatuan tradisional dengan diskritisasi. Langkah-langkah alternatif untuk mengenal pasti peraturan yang menarik dicadangkan. Algoritma yang cekap disajikan yang memungkinkan peraturan ini dapat dijumpai untuk set data padat yang mana aplikasi algoritma Auman dan Lindell tidak dapat dilaksanakan. [[EENNDD]] peraturan kesan; pencarian dan pengambilan maklumat; belajar; peraturan persatuan; data berangka; cari"], [{"string": "A unifying framework for detecting outliers and change points from non-stationary time series data We are concerned with the issues of outlier detection and change point detection from a data stream . In the area of data mining , there have been increased interest in these issues since the former is related to fraud detection , rare event discovery , etc. , while the latter is related to event\\/trend by change detection , activity monitoring , etc. . Specifically , it is important to consider the situation where the data source is non-stationary , since the nature of data source may change over time in real applications . Although in most previous work outlier detection and change point detection have not been related explicitly , this paper presents a unifying framework for dealing with both of them on the basis of the theory of on-line learning of non-stationary time series . In this framework a probabilistic model of the data source is incrementally learned using an on-line discounting learning algorithm , which can track the changing data source adaptively by forgetting the effect of past data gradually . Then the score for any given data is calculated to measure its deviation from the learned model , with a higher score indicating a high possibility of being an outlier . Further change points in a data stream are detected by applying this scoring method into a time series of moving averaged losses for prediction using the learned model . Specifically we develop an efficient algorithms for on-line discounting learning of auto-regression models from time series data , and demonstrate the validity of our framework through simulation and experimental applications to stock market data analysis .", "keywords": ["probabilistic algorithms"], "combined": "A unifying framework for detecting outliers and change points from non-stationary time series data We are concerned with the issues of outlier detection and change point detection from a data stream . In the area of data mining , there have been increased interest in these issues since the former is related to fraud detection , rare event discovery , etc. , while the latter is related to event\\/trend by change detection , activity monitoring , etc. . Specifically , it is important to consider the situation where the data source is non-stationary , since the nature of data source may change over time in real applications . Although in most previous work outlier detection and change point detection have not been related explicitly , this paper presents a unifying framework for dealing with both of them on the basis of the theory of on-line learning of non-stationary time series . In this framework a probabilistic model of the data source is incrementally learned using an on-line discounting learning algorithm , which can track the changing data source adaptively by forgetting the effect of past data gradually . Then the score for any given data is calculated to measure its deviation from the learned model , with a higher score indicating a high possibility of being an outlier . Further change points in a data stream are detected by applying this scoring method into a time series of moving averaged losses for prediction using the learned model . Specifically we develop an efficient algorithms for on-line discounting learning of auto-regression models from time series data , and demonstrate the validity of our framework through simulation and experimental applications to stock market data analysis . [[EENNDD]] probabilistic algorithms"}, "Kerangka penyatuan untuk mengesan outliers dan titik perubahan dari data siri masa tidak bergerak Kami prihatin dengan masalah pengesanan outlier dan pengesanan titik perubahan dari aliran data. Di bidang perlombongan data, ada peningkatan minat terhadap masalah ini karena yang pertama berkaitan dengan pengesanan penipuan, penemuan peristiwa yang jarang berlaku, dll., Sementara yang terakhir berkaitan dengan peristiwa \\ / tren oleh pengesanan perubahan, pemantauan aktiviti, dll. . Secara khusus, penting untuk mempertimbangkan situasi di mana sumber data tidak bergerak, kerana sifat sumber data dapat berubah dari masa ke masa dalam aplikasi nyata. Walaupun dalam kebanyakan pengesanan hasil kerja terdahulu dan pengesanan titik perubahan tidak berkaitan secara eksplisit, makalah ini menyajikan kerangka penyatuan untuk menangani kedua-duanya berdasarkan teori pembelajaran dalam talian siri masa tidak bergerak. Dalam kerangka ini, model probabilistik sumber data dipelajari secara bertahap menggunakan algoritma pembelajaran diskaun on-line, yang dapat melacak perubahan sumber data secara adaptif dengan melupakan pengaruh data masa lalu secara beransur-ansur. Kemudian skor untuk setiap data yang diberikan dikira untuk mengukur penyimpangannya dari model yang dipelajari, dengan skor yang lebih tinggi menunjukkan kemungkinan yang tinggi untuk menjadi orang luar. Titik perubahan lebih lanjut dalam aliran data dikesan dengan menerapkan metode pemarkahan ini ke dalam rangkaian waktu kerugian rata-rata bergerak untuk ramalan menggunakan model yang dipelajari. Secara khusus kami mengembangkan algoritma yang cekap untuk pembelajaran on-line model auto-regresi dari data siri waktu, dan menunjukkan keabsahan kerangka kami melalui simulasi dan aplikasi eksperimental untuk analisis data pasar saham. [[EENNDD]] algoritma probabilistik"], [{"string": "Evolutionary clustering We consider the problem of clustering data over time . An evolutionary clustering should simultaneously optimize two potentially conflicting criteria : first , the clustering at any point in time should remain faithful to the current data as much as possible ; and second , the clustering should not shift dramatically from one timestep to the next . We present a generic framework for this problem , and discuss evolutionary versions of two widely-used clustering algorithms within this framework : k-means and agglomerative hierarchical clustering . We extensively evaluate these algorithms on real data sets and show that our algorithms can simultaneously attain both high accuracy in capturing today 's data , and high fidelity in reflecting yesterday 's clustering .", "keywords": ["k-means", "temporal evolution", "information search and retrieval", "agglomerative", "clustering"], "combined": "Evolutionary clustering We consider the problem of clustering data over time . An evolutionary clustering should simultaneously optimize two potentially conflicting criteria : first , the clustering at any point in time should remain faithful to the current data as much as possible ; and second , the clustering should not shift dramatically from one timestep to the next . We present a generic framework for this problem , and discuss evolutionary versions of two widely-used clustering algorithms within this framework : k-means and agglomerative hierarchical clustering . We extensively evaluate these algorithms on real data sets and show that our algorithms can simultaneously attain both high accuracy in capturing today 's data , and high fidelity in reflecting yesterday 's clustering . [[EENNDD]] k-means; temporal evolution; information search and retrieval; agglomerative; clustering"}, "Pengelompokan evolusi Kami menganggap masalah pengelompokan data dari masa ke masa. Pengelompokan evolusi secara serentak harus mengoptimumkan dua kriteria yang berpotensi bertentangan: pertama, pengelompokan pada bila-bila masa harus tetap setia pada data terkini sebanyak mungkin; dan kedua, pengelompokan tidak boleh beralih secara dramatik dari satu langkah masa ke tahap berikutnya. Kami membentangkan kerangka umum untuk masalah ini, dan membincangkan versi evolusi dua algoritma pengelompokan yang digunakan secara meluas dalam kerangka ini: k-cara dan pengelompokan hierarki aglomeratif. Kami secara mendalam menilai algoritma ini pada set data sebenar dan menunjukkan bahawa algoritma kami dapat secara serentak mencapai ketepatan tinggi dalam menangkap data hari ini, dan kesetiaan yang tinggi dalam mencerminkan pengelompokan semalam. [[EENNDD]] k-bermaksud; evolusi temporal; pencarian dan pengambilan maklumat; aglomeratif; pengelompokan"], [{"string": "Xproj : a framework for projected structural clustering of xml documents XML has become a popular method of data representation both on the web and in databases in recent years . One of the reasons for the popularity of XML has been its ability to encode structural information about data records . However , this structural characteristic of data sets also makes it a challenging problem for a variety of data mining problems . One such problem is that of clustering , in which the structural aspects of the data result in a high implicit dimensionality of the data representation . As a result , it becomes more difficult to cluster the data in a meaningful way . In this paper , we propose an effective clustering algorithm for XML data which uses substructures of the documents in order to gain insights about the important underlying structures . We propose new ways of using multiple sub-structuralinformation in XML documents to evaluate the quality of intermediate cluster solutions , and guide the algorithms to a final solution which reflects the true structural behavior in individual partitions . We test the algorithm on a variety of real and synthetic data sets .", "keywords": ["xml", "clustering"], "combined": "Xproj : a framework for projected structural clustering of xml documents XML has become a popular method of data representation both on the web and in databases in recent years . One of the reasons for the popularity of XML has been its ability to encode structural information about data records . However , this structural characteristic of data sets also makes it a challenging problem for a variety of data mining problems . One such problem is that of clustering , in which the structural aspects of the data result in a high implicit dimensionality of the data representation . As a result , it becomes more difficult to cluster the data in a meaningful way . In this paper , we propose an effective clustering algorithm for XML data which uses substructures of the documents in order to gain insights about the important underlying structures . We propose new ways of using multiple sub-structuralinformation in XML documents to evaluate the quality of intermediate cluster solutions , and guide the algorithms to a final solution which reflects the true structural behavior in individual partitions . We test the algorithm on a variety of real and synthetic data sets . [[EENNDD]] xml; clustering"}, "Xproj: kerangka kerja untuk pengelompokan struktur yang diproyeksikan dari dokumen xml XML telah menjadi kaedah representasi data yang popular baik di web dan di pangkalan data dalam beberapa tahun terakhir. Salah satu sebab populariti XML adalah kemampuannya untuk menyandikan maklumat struktur mengenai rekod data. Walau bagaimanapun, ciri struktur set data ini juga menjadikannya masalah yang mencabar untuk pelbagai masalah perlombongan data. Salah satu masalah seperti itu adalah pengelompokan, di mana aspek struktur data menghasilkan dimensi implisit tinggi perwakilan data. Akibatnya, menjadi lebih sukar untuk mengumpulkan data dengan cara yang bermakna. Dalam makalah ini, kami mencadangkan algoritma pengelompokan yang berkesan untuk data XML yang menggunakan substruktur dokumen untuk mendapatkan pandangan mengenai struktur asas yang penting. Kami mencadangkan cara baru menggunakan beberapa sub-struktur maklumat dalam dokumen XML untuk menilai kualiti penyelesaian kluster pertengahan, dan membimbing algoritma ke penyelesaian akhir yang mencerminkan tingkah laku struktur sebenar dalam partisi individu. Kami menguji algoritma pada pelbagai set data sebenar dan sintetik. [[EENNDD]] xml; pengelompokan"], [{"string": "Effective and efficient itemset pattern summarization : regression-based approaches In this paper , we propose a set of novel regression-based approaches to effectively and efficiently summarize frequent itemset patterns . Specifically , we show that the problem of minimizing the restoration error for a set of itemsets based on a probabilistic model corresponds to a non-linear regression problem . We show that under certain conditions , we can transform the nonlinear regression problem to a linear regression problem . We propose two new methods , k-regression and tree-regression , to partition the entire collection of frequent itemsets in order to minimize the restoration error . The K-regression approach , employing a K-means type clustering method , guarantees that the total restoration error achieves a local minimum . The tree-regression approach employs a decision-tree type of top-down partition process . In addition , we discuss alternatives to estimate the frequency for the collection of itemsets being covered by the k representative itemsets . The experimental evaluation on both real and synthetic datasets demonstrates that our approaches significantly improve the summarization performance in terms of both accuracy restoration error , and computational cost .", "keywords": ["frequency restoration", "pattern summarization", "regression"], "combined": "Effective and efficient itemset pattern summarization : regression-based approaches In this paper , we propose a set of novel regression-based approaches to effectively and efficiently summarize frequent itemset patterns . Specifically , we show that the problem of minimizing the restoration error for a set of itemsets based on a probabilistic model corresponds to a non-linear regression problem . We show that under certain conditions , we can transform the nonlinear regression problem to a linear regression problem . We propose two new methods , k-regression and tree-regression , to partition the entire collection of frequent itemsets in order to minimize the restoration error . The K-regression approach , employing a K-means type clustering method , guarantees that the total restoration error achieves a local minimum . The tree-regression approach employs a decision-tree type of top-down partition process . In addition , we discuss alternatives to estimate the frequency for the collection of itemsets being covered by the k representative itemsets . The experimental evaluation on both real and synthetic datasets demonstrates that our approaches significantly improve the summarization performance in terms of both accuracy restoration error , and computational cost . [[EENNDD]] frequency restoration; pattern summarization; regression"}, "Ringkasan corak itemet yang berkesan dan cekap: pendekatan berasaskan regresi Dalam makalah ini, kami mencadangkan satu set pendekatan berasaskan regresi novel untuk meringkaskan corak itemet yang kerap dan berkesan. Secara khusus, kami menunjukkan bahawa masalah meminimumkan kesalahan pemulihan untuk sekumpulan item berdasarkan model probabilistik sesuai dengan masalah regresi non-linear. Kami menunjukkan bahawa dalam keadaan tertentu, kita dapat mengubah masalah regresi nonlinear menjadi masalah regresi linear. Kami mencadangkan dua kaedah baru, k-regresi dan regresi pohon, untuk membahagikan keseluruhan koleksi itemets yang kerap untuk mengurangkan kesalahan pemulihan. Pendekatan regresi K, menggunakan kaedah pengelompokan jenis K-berarti, menjamin bahawa keseluruhan kesalahan pemulihan mencapai minimum tempatan. Pendekatan regresi pokok menggunakan proses partisi top-down jenis pohon keputusan. Di samping itu, kami membincangkan alternatif untuk menganggarkan kekerapan pengumpulan kumpulan barang yang dilindungi oleh kumpulan item perwakilan k. Penilaian eksperimental pada kumpulan data nyata dan sintetik menunjukkan bahawa pendekatan kami meningkatkan prestasi ringkasan dengan ketara dari segi kedua-dua kesalahan pemulihan ketepatan, dan kos pengiraan. [[EENNDD]] pemulihan frekuensi; ringkasan corak; regresi"], [{"string": "Mining for misconfigured machines in grid systems Grid systems are proving increasingly useful for managing the batch computing jobs of organizations . One well-known example is Intel , whose internally developed NetBatch system manages tens of thousands of machines . The size , heterogeneity , and complexity of grid systems make them very difficult , however , to configure . This often results in misconfigured machines , which may adversely affect the entire system . We investigate a distributed data mining approach for detection of misconfigured machines . Our Grid Monitoring System GMS non-intrusively collects data from all sources log files , system services , etc. available throughout the grid system . It converts raw data to semantically meaningful data and stores this data on the machine it was obtained from , limiting incurred overhead and allowing scalability . Afterwards , when analysis is requested , a distributed outliers detection algorithm is employed to identify misconfigured machines . The algorithm itself is implemented as a recursive workflow of grid jobs . It is especially suited to grid systems , in which the machines might be unavailable most of the time and often fail altogether .", "keywords": ["grid systems", "system monitoring", "grid information system", "outliers detection", "distributed systems", "distributed data mining"], "combined": "Mining for misconfigured machines in grid systems Grid systems are proving increasingly useful for managing the batch computing jobs of organizations . One well-known example is Intel , whose internally developed NetBatch system manages tens of thousands of machines . The size , heterogeneity , and complexity of grid systems make them very difficult , however , to configure . This often results in misconfigured machines , which may adversely affect the entire system . We investigate a distributed data mining approach for detection of misconfigured machines . Our Grid Monitoring System GMS non-intrusively collects data from all sources log files , system services , etc. available throughout the grid system . It converts raw data to semantically meaningful data and stores this data on the machine it was obtained from , limiting incurred overhead and allowing scalability . Afterwards , when analysis is requested , a distributed outliers detection algorithm is employed to identify misconfigured machines . The algorithm itself is implemented as a recursive workflow of grid jobs . It is especially suited to grid systems , in which the machines might be unavailable most of the time and often fail altogether . [[EENNDD]] grid systems; system monitoring; grid information system; outliers detection; distributed systems; distributed data mining"}, "Perlombongan untuk mesin yang salah dikonfigurasi dalam sistem grid Sistem grid terbukti semakin berguna untuk menguruskan pekerjaan komputasi kumpulan organisasi. Salah satu contoh yang terkenal ialah Intel, yang sistem NetBatch yang dikembangkan secara dalaman menguruskan puluhan ribu mesin. Ukuran, heterogenitas, dan kerumitan sistem grid menjadikannya sangat sukar, bagaimanapun, untuk dikonfigurasi. Ini sering mengakibatkan mesin yang salah dikonfigurasi, yang boleh menjejaskan keseluruhan sistem. Kami menyiasat pendekatan perlombongan data yang diedarkan untuk mengesan mesin yang salah konfigurasi. Sistem Pemantauan Grid kami GMS secara tidak sengaja mengumpulkan data dari semua fail log sumber, perkhidmatan sistem, dan lain-lain yang terdapat di seluruh sistem grid. Ia menukar data mentah menjadi data yang bermakna secara semantik dan menyimpan data ini pada mesin yang diperolehnya, membatasi overhead yang dikeluarkan dan memungkinkan skalabilitas. Selepas itu, apabila analisis diminta, algoritma pengesanan outliers diedarkan digunakan untuk mengenal pasti mesin yang salah konfigurasi. Algoritma itu sendiri dilaksanakan sebagai aliran kerja grid kerja rekursif. Ini sangat sesuai untuk sistem grid, di mana mesin mungkin tidak dapat digunakan hampir sepanjang masa dan sering gagal sama sekali. [[EENNDD]] sistem grid; pemantauan sistem; sistem maklumat grid; pengesanan outliers; sistem yang diedarkan; perlombongan data yang diedarkan"], [{"string": "Scalable mining of large disk-based graph databases Mining frequent structural patterns from graph databases is an interesting problem with broad applications . Most of the previous studies focus on pruning unfruitful search subspaces effectively , but few of them address the mining on large , disk-based databases . As many graph databases in applications can not be held into main memory , scalable mining of large , disk-based graph databases remains a challenging problem . In this paper , we develop an effective index structure , ADI for u ad \\/ u jacency u i \\/ u ndex , to support mining various graph patterns over large databases that can not be held into main memory . The index is simple and efficient to build . Moreover , the new index structure can be easily adopted in various existing graph pattern mining algorithms . As an example , we adapt the well-known gSpan algorithm by using the ADI structure . The experimental results show that the new index structure enables the scalable graph pattern mining over large databases . In one set of the experiments , the new disk-based method can mine graph databases with one million graphs , while the original gSpan algorithm can only handle databases of up to 300 thousand graphs . Moreover , our new method is faster than gSpan when both can run in main memory .", "keywords": ["graph database", "index", "graph mining", "frequent graph pattern"], "combined": "Scalable mining of large disk-based graph databases Mining frequent structural patterns from graph databases is an interesting problem with broad applications . Most of the previous studies focus on pruning unfruitful search subspaces effectively , but few of them address the mining on large , disk-based databases . As many graph databases in applications can not be held into main memory , scalable mining of large , disk-based graph databases remains a challenging problem . In this paper , we develop an effective index structure , ADI for u ad \\/ u jacency u i \\/ u ndex , to support mining various graph patterns over large databases that can not be held into main memory . The index is simple and efficient to build . Moreover , the new index structure can be easily adopted in various existing graph pattern mining algorithms . As an example , we adapt the well-known gSpan algorithm by using the ADI structure . The experimental results show that the new index structure enables the scalable graph pattern mining over large databases . In one set of the experiments , the new disk-based method can mine graph databases with one million graphs , while the original gSpan algorithm can only handle databases of up to 300 thousand graphs . Moreover , our new method is faster than gSpan when both can run in main memory . [[EENNDD]] graph database; index; graph mining; frequent graph pattern"}, "Perlombongan skala besar dari pangkalan data grafik berasaskan cakera Melombong corak struktur kerap dari pangkalan data grafik adalah masalah menarik dengan aplikasi yang luas. Sebilangan besar kajian sebelumnya menumpukan pada pemangkasan ruang carian yang tidak berbuah dengan berkesan, tetapi sebilangan kecil daripadanya menangani penambangan pada pangkalan data berasaskan cakera yang besar. Oleh kerana banyak pangkalan data grafik dalam aplikasi tidak dapat disimpan dalam memori utama, perlombongan pangkalan data grafik berasaskan disk yang besar dan berskala tetap menjadi masalah yang mencabar. Dalam makalah ini, kami mengembangkan struktur indeks yang berkesan, ADI untuk u ad \\ / u jacency u i \\ / u ndex, untuk menyokong perlombongan pelbagai corak grafik di atas pangkalan data besar yang tidak dapat disimpan dalam memori utama. Indeksnya mudah dan cekap dibina. Lebih-lebih lagi, struktur indeks baru dapat diterima dengan mudah dalam pelbagai algoritma perlombongan corak grafik yang ada. Sebagai contoh, kami menyesuaikan algoritma gSpan yang terkenal dengan menggunakan struktur ADI. Hasil eksperimen menunjukkan bahawa struktur indeks baru memungkinkan perlombongan corak grafik yang dapat diskalakan ke atas pangkalan data yang besar. Dalam satu set eksperimen, kaedah berasaskan cakera baru dapat menambang pangkalan data grafik dengan satu juta grafik, sementara algoritma gSpan yang asli hanya dapat menangani pangkalan data hingga 300 ribu grafik. Lebih-lebih lagi, kaedah baru kami lebih pantas daripada gSpan apabila kedua-duanya dapat berjalan dalam memori utama. [[EENNDD]] pangkalan data grafik; indeks; perlombongan grafik; corak graf yang kerap"], [{"string": "A general approach to incorporate data quality matrices into data mining algorithms Data quality is a central issue for many information-oriented organizations . Recent advances in the data quality field reflect the view that a database is the product of a manufacturing process . While routine errors , such as non-existent zip codes , can be detected and corrected using traditional data cleansing tools , many errors systemic to the manufacturing process can not be addressed . Therefore , the product of the data manufacturing process is an imprecise recording of information about the entities of interest i.e. customers , transactions or assets . In this way , the database is only one flawed version of the entities it is supposed to represent . Quality assurance systems such as Motorola 's Six-Sigma and other continuous improvement methods document the data manufacturing process 's shortcomings . A widespread method of documentation is quality matrices . In this paper , we explore the use of the readily available data quality matrices for the data mining classification task . We first illustrate that if we do not factor in these quality matrices , then our results for prediction are sub-optimal . We then suggest a general-purpose ensemble approach that perturbs the data according to these quality matrices to improve the predictive accuracy and show the improvement is due to a reduction in variance .", "keywords": ["decision trees", "six-sigma", "classification", "data quality", "ensemble approaches"], "combined": "A general approach to incorporate data quality matrices into data mining algorithms Data quality is a central issue for many information-oriented organizations . Recent advances in the data quality field reflect the view that a database is the product of a manufacturing process . While routine errors , such as non-existent zip codes , can be detected and corrected using traditional data cleansing tools , many errors systemic to the manufacturing process can not be addressed . Therefore , the product of the data manufacturing process is an imprecise recording of information about the entities of interest i.e. customers , transactions or assets . In this way , the database is only one flawed version of the entities it is supposed to represent . Quality assurance systems such as Motorola 's Six-Sigma and other continuous improvement methods document the data manufacturing process 's shortcomings . A widespread method of documentation is quality matrices . In this paper , we explore the use of the readily available data quality matrices for the data mining classification task . We first illustrate that if we do not factor in these quality matrices , then our results for prediction are sub-optimal . We then suggest a general-purpose ensemble approach that perturbs the data according to these quality matrices to improve the predictive accuracy and show the improvement is due to a reduction in variance . [[EENNDD]] decision trees; six-sigma; classification; data quality; ensemble approaches"}, "Pendekatan umum untuk memasukkan matriks kualiti data ke dalam algoritma perlombongan data Kualiti data adalah isu utama bagi banyak organisasi yang berorientasikan maklumat. Kemajuan terkini dalam bidang kualiti data mencerminkan pandangan bahawa pangkalan data adalah produk dari proses pembuatan. Walaupun kesalahan rutin, seperti kod zip yang tidak ada, dapat dikesan dan diperbaiki dengan menggunakan alat pembersih data tradisional, banyak kesalahan yang sistemik terhadap proses pembuatannya tidak dapat ditangani. Oleh itu, produk dari proses pembuatan data adalah rakaman maklumat yang tidak tepat mengenai entiti menarik iaitu pelanggan, transaksi atau aset. Dengan cara ini, pangkalan data hanya satu versi entiti cacat yang seharusnya diwakilinya. Sistem jaminan kualiti seperti Motorola's Six-Sigma dan kaedah peningkatan berterusan yang lain mendokumentasikan kekurangan proses pembuatan data. Kaedah dokumentasi yang meluas adalah matriks berkualiti. Dalam makalah ini, kami meneroka penggunaan matriks kualiti data yang tersedia untuk tugas pengelasan data mining. Mula-mula kita menggambarkan bahawa jika kita tidak memperhitungkan matriks berkualiti ini, maka hasil ramalan kita kurang optimum. Kami kemudian mencadangkan pendekatan ensemble tujuan umum yang mengganggu data mengikut matriks kualiti ini untuk meningkatkan ketepatan ramalan dan menunjukkan peningkatan disebabkan oleh penurunan varians. [[EENNDD]] pokok keputusan; enam-sigma; pengelasan; kualiti data; pendekatan ensemble"], [{"string": "Query , analysis , and visualization of hierarchically structured data using Polaris In the last several years , large OLAP databases have become common in a variety of applications such as corporate data warehouses and scientific computing . To support interactive analysis , many of these databases are augmented with hierarchical structures that provide meaningful levels of abstraction that can be leveraged by both the computer and analyst . This hierarchical structure generates many challenges and opportunities in the design of systems for the query , analysis , and visualization of these databases . In this paper , we present an interactive visual exploration tool that facilitates exploratory analysis of data warehouses with rich hierarchical structure , such as might be stored in data cubes . We base this tool on Polaris , a system for rapidly constructing table-based graphical displays of multidimensional databases . Polaris builds visualizations using an algebraic formalism derived from the interface and interpreted as a set of queries to a database . We extend the user interface , algebraic formalism , and generation of data queries in Polaris to expose and take advantage of hierarchical structure . In the resulting system , analysts can navigate through the hierarchical projections of a database , rapidly and incrementally generating visualizations for each projection .", "keywords": ["graphical user interfaces"], "combined": "Query , analysis , and visualization of hierarchically structured data using Polaris In the last several years , large OLAP databases have become common in a variety of applications such as corporate data warehouses and scientific computing . To support interactive analysis , many of these databases are augmented with hierarchical structures that provide meaningful levels of abstraction that can be leveraged by both the computer and analyst . This hierarchical structure generates many challenges and opportunities in the design of systems for the query , analysis , and visualization of these databases . In this paper , we present an interactive visual exploration tool that facilitates exploratory analysis of data warehouses with rich hierarchical structure , such as might be stored in data cubes . We base this tool on Polaris , a system for rapidly constructing table-based graphical displays of multidimensional databases . Polaris builds visualizations using an algebraic formalism derived from the interface and interpreted as a set of queries to a database . We extend the user interface , algebraic formalism , and generation of data queries in Polaris to expose and take advantage of hierarchical structure . In the resulting system , analysts can navigate through the hierarchical projections of a database , rapidly and incrementally generating visualizations for each projection . [[EENNDD]] graphical user interfaces"}, "Pertanyaan, analisis, dan visualisasi data berstruktur hierarki menggunakan Polaris Dalam beberapa tahun kebelakangan ini, pangkalan data OLAP yang besar telah menjadi hal biasa dalam pelbagai aplikasi seperti gudang data korporat dan pengkomputeran saintifik. Untuk menyokong analisis interaktif, banyak pangkalan data ini ditambah dengan struktur hierarki yang memberikan tahap abstraksi yang bermakna yang dapat dimanfaatkan oleh komputer dan penganalisis. Struktur hierarki ini menghasilkan banyak cabaran dan peluang dalam reka bentuk sistem untuk pertanyaan, analisis, dan visualisasi pangkalan data ini. Dalam makalah ini, kami menyajikan alat eksplorasi visual interaktif yang memfasilitasi analisis eksplorasi gudang data dengan struktur hierarki yang kaya, seperti mungkin disimpan dalam kubus data. Kami mendasarkan alat ini pada Polaris, sebuah sistem untuk membuat paparan grafik pangkalan data multidimensi berasaskan jadual dengan pantas. Polaris membina visualisasi menggunakan formalisme algebra yang berasal dari antara muka dan ditafsirkan sebagai satu set pertanyaan ke pangkalan data. Kami memperluaskan antara muka pengguna, formalisme aljabar, dan penjanaan pertanyaan data di Polaris untuk mendedahkan dan memanfaatkan struktur hierarki. Dalam sistem yang dihasilkan, penganalisis dapat menavigasi unjuran hierarki pangkalan data, menghasilkan visualisasi dengan cepat dan bertahap untuk setiap unjuran. [[EENNDD]] antara muka pengguna grafik"], [{"string": "Spatial scan statistics : approximations and performance study Spatial scan statistics are used to determine hotspots in spatial data , and are widely used in epidemiology and biosurveillance . In recent years , there has been much effort invested in designing efficient algorithms for finding such `` high discrepancy '' regions , with methods ranging from fast heuristics for special cases , to general grid-based methods , and to efficient approximation algorithms with provable guarantees on performance and quality . In this paper , we make a number of contributions to the computational study of spatial scan statistics . First , we describe a simple exact algorithm for finding the largest discrepancy region in a domain . Second , we propose a new approximation algorithm for a large class of discrepancy functions including the Kulldorff scan statistic that improves the approximation versus run time trade-off of prior methods . Third , we extend our simple exact and our approximation algorithms to data sets which lie naturally on a grid or are accumulated onto a grid . Fourth , we conduct a detailed experimental comparison of these methods with a number of known methods , demonstrating that our approximation algorithm has far superior performance in practice to prior methods , and exhibits a good performance-accuracy trade-off . All extant methods including those in this paper are suitable for data sets that are modestly sized ; if data sets are of the order of millions of data points , none of these methods scale well . For such massive data settings , it is natural to examine whether small-space streaming algorithms might yield accurate answers . Here , we provide some negative results , showing that any streaming algorithms that even provide approximately optimal answers to the discrepancy maximization problem must use space linear in the input .", "keywords": ["spatial scan statistics", "kulldorff scan statistic", "discrepancy", "probability and statistics"], "combined": "Spatial scan statistics : approximations and performance study Spatial scan statistics are used to determine hotspots in spatial data , and are widely used in epidemiology and biosurveillance . In recent years , there has been much effort invested in designing efficient algorithms for finding such `` high discrepancy '' regions , with methods ranging from fast heuristics for special cases , to general grid-based methods , and to efficient approximation algorithms with provable guarantees on performance and quality . In this paper , we make a number of contributions to the computational study of spatial scan statistics . First , we describe a simple exact algorithm for finding the largest discrepancy region in a domain . Second , we propose a new approximation algorithm for a large class of discrepancy functions including the Kulldorff scan statistic that improves the approximation versus run time trade-off of prior methods . Third , we extend our simple exact and our approximation algorithms to data sets which lie naturally on a grid or are accumulated onto a grid . Fourth , we conduct a detailed experimental comparison of these methods with a number of known methods , demonstrating that our approximation algorithm has far superior performance in practice to prior methods , and exhibits a good performance-accuracy trade-off . All extant methods including those in this paper are suitable for data sets that are modestly sized ; if data sets are of the order of millions of data points , none of these methods scale well . For such massive data settings , it is natural to examine whether small-space streaming algorithms might yield accurate answers . Here , we provide some negative results , showing that any streaming algorithms that even provide approximately optimal answers to the discrepancy maximization problem must use space linear in the input . [[EENNDD]] spatial scan statistics; kulldorff scan statistic; discrepancy; probability and statistics"}, "Statistik imbasan spasial: pendekatan dan kajian prestasi Statistik imbasan spasial digunakan untuk menentukan titik panas dalam data spasial, dan banyak digunakan dalam epidemiologi dan pengawasan biosur. Dalam tahun-tahun kebelakangan ini, ada banyak usaha yang dilaburkan dalam merancang algoritma yang cekap untuk mencari wilayah \"perbezaan tinggi\" seperti itu, dengan kaedah mulai dari heuristik cepat untuk kes khas, hingga kaedah berdasarkan grid umum, dan algoritma penghampiran yang efisien dengan jaminan yang dapat dibuktikan pada prestasi dan kualiti. Dalam makalah ini, kami membuat sejumlah sumbangan untuk kajian komputasi statistik imbasan spasial. Pertama, kami menerangkan algoritma tepat yang tepat untuk mencari kawasan perbezaan terbesar dalam domain. Kedua, kami mencadangkan algoritma penghampiran baru untuk kelas fungsi perbezaan yang besar termasuk statistik imbasan Kulldorff yang meningkatkan penghampiran berbanding jangka masa pertukaran kaedah sebelumnya. Ketiga, kami memperluas algoritma tepat dan tepat kami ke set data yang terletak secara semula jadi pada grid atau terkumpul ke grid. Keempat, kami melakukan perbandingan eksperimental terperinci kaedah ini dengan sebilangan kaedah yang diketahui, menunjukkan bahawa algoritma penghampiran kami mempunyai prestasi jauh lebih unggul dalam praktik berbanding kaedah sebelumnya, dan menunjukkan pertukaran ketepatan prestasi yang baik. Semua kaedah yang ada termasuk yang terdapat dalam makalah ini sesuai untuk set data yang bersaiz sederhana; jika set data berada dalam urutan berjuta-juta titik data, tidak ada kaedah ini dengan skala yang baik. Untuk tetapan data yang begitu besar, wajar untuk memeriksa sama ada algoritma streaming ruang kecil mungkin menghasilkan jawapan yang tepat. Di sini, kami memberikan beberapa hasil negatif, menunjukkan bahawa mana-mana algoritma streaming yang bahkan memberikan kira-kira jawapan yang optimum untuk masalah pemaksimumkan perbezaan mesti menggunakan ruang linear dalam input. [[EENNDD]] statistik imbasan ruang; statistik imbasan kulldorff; percanggahan; kebarangkalian dan statistik"], [{"string": "Towards parameter-free data mining Most data mining algorithms require the setting of many input parameters . Two main dangers of working with parameter-laden algorithms are the following . First , incorrect settings may cause an algorithm to fail in finding the true patterns . Second , a perhaps more insidious problem is that the algorithm may report spurious patterns that do not really exist , or greatly overestimate the significance of the reported patterns . This is especially likely when the user fails to understand the role of parameters in the data mining process . Data mining algorithms should have as few parameters as possible , ideally none . A parameter-free algorithm would limit our ability to impose our prejudices , expectations , and presumptions on the problem at hand , and would let the data itself speak to us . In this work , we show that recent results in bioinformatics and computational theory hold great promise for a parameter-free data-mining paradigm . The results are motivated by observations in Kolmogorov complexity theory . However , as a practical matter , they can be implemented using any off-the-shelf compression algorithm with the addition of just a dozen or so lines of code . We will show that this approach is competitive or superior to the state-of-the-art approaches in anomaly\\/interestingness detection , classification , and clustering with empirical tests on time series\\/DNA\\/text \\/ video datasets .", "keywords": ["anomaly detection", "parameter-free data mining", "clustering"], "combined": "Towards parameter-free data mining Most data mining algorithms require the setting of many input parameters . Two main dangers of working with parameter-laden algorithms are the following . First , incorrect settings may cause an algorithm to fail in finding the true patterns . Second , a perhaps more insidious problem is that the algorithm may report spurious patterns that do not really exist , or greatly overestimate the significance of the reported patterns . This is especially likely when the user fails to understand the role of parameters in the data mining process . Data mining algorithms should have as few parameters as possible , ideally none . A parameter-free algorithm would limit our ability to impose our prejudices , expectations , and presumptions on the problem at hand , and would let the data itself speak to us . In this work , we show that recent results in bioinformatics and computational theory hold great promise for a parameter-free data-mining paradigm . The results are motivated by observations in Kolmogorov complexity theory . However , as a practical matter , they can be implemented using any off-the-shelf compression algorithm with the addition of just a dozen or so lines of code . We will show that this approach is competitive or superior to the state-of-the-art approaches in anomaly\\/interestingness detection , classification , and clustering with empirical tests on time series\\/DNA\\/text \\/ video datasets . [[EENNDD]] anomaly detection; parameter-free data mining; clustering"}, "Ke arah perlombongan data tanpa parameter Sebilangan besar algoritma perlombongan data memerlukan penetapan banyak parameter input. Dua bahaya utama bekerja dengan algoritma yang dipenuhi parameter adalah berikut. Pertama, tetapan yang salah boleh menyebabkan algoritma gagal dalam mencari corak yang sebenarnya. Kedua, masalah yang mungkin lebih berbahaya ialah algoritma mungkin melaporkan corak palsu yang tidak benar-benar wujud, atau terlalu menilai kepentingan corak yang dilaporkan. Ini berkemungkinan besar apabila pengguna gagal memahami peranan parameter dalam proses perlombongan data. Algoritma perlombongan data harus mempunyai seberapa sedikit parameter, idealnya tidak ada. Algoritma bebas parameter akan membatasi kemampuan kita untuk memaksakan prasangka, harapan, dan anggapan kita terhadap masalah yang dihadapi, dan membiarkan data itu sendiri berbicara kepada kita. Dalam karya ini, kami menunjukkan bahawa hasil terkini dalam bioinformatik dan teori komputasi sangat menjanjikan untuk paradigma perlombongan data tanpa parameter. Hasilnya dimotivasi oleh pemerhatian dalam teori kerumitan Kolmogorov. Namun, sebagai perkara praktikal, mereka dapat dilaksanakan dengan menggunakan algoritma pemampatan di luar rak dengan penambahan hanya selusin baris kod. Kami akan menunjukkan bahawa pendekatan ini berdaya saing atau unggul daripada pendekatan canggih dalam pengesanan, klasifikasi, dan pengelompokan anomali, / pengelompokan, dan pengumpulan dengan ujian empirik pada siri data \\ / DNA \\ / teks \\ / video. [[EENNDD]] pengesanan anomali; perlombongan data tanpa parameter; pengelompokan"], [{"string": "Diagnosing memory leaks using graph mining on heap dumps Memory leaks are caused by software programs that prevent the reclamation of memory that is no longer in use . They can cause significant slowdowns , exhaustion of available storage space and , eventually , application crashes . Detecting memory leaks is challenging because real-world applications are built on multiple layers of software frameworks , making it difficult for a developer to know whether observed references to objects are legitimate or the cause of a leak . We present a graph mining solution to this problem wherein we analyze heap dumps to automatically identify subgraphs which could represent potential memory leak sources . Although heap dumps are commonly analyzed in existing heap profiling tools , our work is the first to apply a graph grammar mining solution to this problem . Unlike classical graph mining work , we show that it suffices to mine the dominator tree of the heap dump , which is significantly smaller than the underlying graph . Our approach identifies not just leaking candidates and their structure , but also provides aggregate information about the access path to the leaks . We demonstrate several synthetic as well as real-world examples of heap dumps for which our approach provides more insight into the problem than state-of-the-art tools such as Eclipse 's MAT .", "keywords": ["memory leaks", "heap profiling", "graph mining", "dominator tree", "graph grammars"], "combined": "Diagnosing memory leaks using graph mining on heap dumps Memory leaks are caused by software programs that prevent the reclamation of memory that is no longer in use . They can cause significant slowdowns , exhaustion of available storage space and , eventually , application crashes . Detecting memory leaks is challenging because real-world applications are built on multiple layers of software frameworks , making it difficult for a developer to know whether observed references to objects are legitimate or the cause of a leak . We present a graph mining solution to this problem wherein we analyze heap dumps to automatically identify subgraphs which could represent potential memory leak sources . Although heap dumps are commonly analyzed in existing heap profiling tools , our work is the first to apply a graph grammar mining solution to this problem . Unlike classical graph mining work , we show that it suffices to mine the dominator tree of the heap dump , which is significantly smaller than the underlying graph . Our approach identifies not just leaking candidates and their structure , but also provides aggregate information about the access path to the leaks . We demonstrate several synthetic as well as real-world examples of heap dumps for which our approach provides more insight into the problem than state-of-the-art tools such as Eclipse 's MAT . [[EENNDD]] memory leaks; heap profiling; graph mining; dominator tree; graph grammars"}, "Mendiagnosis kebocoran memori menggunakan penambangan grafik pada timbunan timbunan Kebocoran memori disebabkan oleh program perisian yang menghalang penambakan memori yang tidak lagi digunakan. Mereka boleh menyebabkan penurunan yang ketara, kehabisan ruang simpanan yang ada dan, akhirnya, aplikasi mengalami kerosakan. Mengesan kebocoran memori adalah sesuatu yang mencabar kerana aplikasi dunia nyata dibina pada pelbagai lapisan kerangka perisian, sehingga menyukarkan pembangun untuk mengetahui sama ada rujukan yang diperhatikan pada objek adalah sah atau penyebab kebocoran. Kami mengemukakan penyelesaian perlombongan grafik untuk masalah ini di mana kami menganalisis timbunan timbunan untuk mengenal pasti subgraf secara automatik yang dapat mewakili sumber kebocoran memori yang berpotensi. Walaupun timbunan timbunan biasanya dianalisis dalam alat profil timbunan yang ada, kerja kami adalah yang pertama menerapkan penyelesaian perlombongan tatabahasa grafik untuk masalah ini. Tidak seperti pekerjaan perlombongan grafik klasik, kami menunjukkan bahawa memadai untuk menambang pokok dominasi timbunan timbunan, yang jauh lebih kecil daripada graf yang mendasari. Pendekatan kami mengenal pasti bukan hanya calon yang bocor dan strukturnya, tetapi juga memberikan maklumat agregat mengenai jalan masuk ke kebocoran. Kami menunjukkan beberapa contoh timbunan timbunan sintetik dan nyata di mana pendekatan kami memberikan lebih banyak pandangan mengenai masalah daripada alat canggih seperti Eclipse's MAT. [[EENNDD]] kebocoran memori; profil timbunan; perlombongan grafik; pokok dominasi; tatabahasa grafik"], [{"string": "Quantifying trends accurately despite classifier error and class imbalance This paper promotes a new task for supervised machine learning research : quantification - the pursuit of learning methods for accurately estimating the class distribution of a test set , with no concern for predictions on individual cases . A variant for cost quantification addresses the need to total up costs according to categories predicted by imperfect classifiers . These tasks cover a large and important family of applications that measure trends over time . The paper establishes a research methodology , and uses it to evaluate several proposed methods that involve selecting the classification threshold in a way that would spoil the accuracy of individual classifications . In empirical tests , Median Sweep methods show outstanding ability to estimate the class distribution , despite wide disparity in testing and training conditions . The paper addresses shifting class priors and costs , but not concept drift in general .", "keywords": ["text mining", "classification", "cost quantification", "decision support", "quantification"], "combined": "Quantifying trends accurately despite classifier error and class imbalance This paper promotes a new task for supervised machine learning research : quantification - the pursuit of learning methods for accurately estimating the class distribution of a test set , with no concern for predictions on individual cases . A variant for cost quantification addresses the need to total up costs according to categories predicted by imperfect classifiers . These tasks cover a large and important family of applications that measure trends over time . The paper establishes a research methodology , and uses it to evaluate several proposed methods that involve selecting the classification threshold in a way that would spoil the accuracy of individual classifications . In empirical tests , Median Sweep methods show outstanding ability to estimate the class distribution , despite wide disparity in testing and training conditions . The paper addresses shifting class priors and costs , but not concept drift in general . [[EENNDD]] text mining; classification; cost quantification; decision support; quantification"}, "Mengira tren dengan tepat walaupun terdapat kesalahan pengkelasan dan ketidakseimbangan kelas Kertas ini mempromosikan tugas baru untuk penyelidikan pembelajaran mesin yang diawasi: kuantifikasi - pengejaran kaedah pembelajaran untuk menganggarkan secara tepat taburan kelas satu set ujian, tanpa mempedulikan ramalan pada setiap kes. Varian untuk kuantifikasi kos menangani keperluan untuk menambah kos mengikut kategori yang diramalkan oleh pengklasifikasi yang tidak sempurna. Tugas-tugas ini merangkumi sekumpulan aplikasi besar dan penting yang mengukur tren dari masa ke masa. Makalah ini menetapkan metodologi penyelidikan, dan menggunakannya untuk menilai beberapa kaedah yang dicadangkan yang melibatkan memilih ambang klasifikasi dengan cara yang akan merosakkan ketepatan klasifikasi individu. Dalam ujian empirikal, kaedah Sapuan Median menunjukkan kemampuan yang luar biasa untuk menganggarkan taburan kelas, walaupun terdapat perbezaan yang besar dalam keadaan ujian dan latihan. Makalah ini membahas perubahan kelas sebelum dan kos, tetapi bukan konsep konsep secara umum. [[EENNDD]] perlombongan teks; pengelasan; pengukuran kos; sokongan keputusan; pengukuran"], [{"string": "CLOPE : a fast and effective clustering algorithm for transactional data This paper studies the problem of categorical data clustering , especially for transactional data characterized by high dimensionality and large volume . Starting from a heuristic method of increasing the height-to-width ratio of the cluster histogram , we develop a novel algorithm -- CLOPE , which is very fast and scalable , while being quite effective . We demonstrate the performance of our algorithm on two real world datasets , and compare CLOPE with the state-of-art algorithms .", "keywords": ["scalability", "categorical data"], "combined": "CLOPE : a fast and effective clustering algorithm for transactional data This paper studies the problem of categorical data clustering , especially for transactional data characterized by high dimensionality and large volume . Starting from a heuristic method of increasing the height-to-width ratio of the cluster histogram , we develop a novel algorithm -- CLOPE , which is very fast and scalable , while being quite effective . We demonstrate the performance of our algorithm on two real world datasets , and compare CLOPE with the state-of-art algorithms . [[EENNDD]] scalability; categorical data"}, "CLOPE: algoritma pengelompokan cepat dan berkesan untuk data transaksional Makalah ini mengkaji masalah pengelompokan data kategorik, terutamanya untuk data transaksi yang dicirikan oleh dimensi tinggi dan isi padu yang besar. Bermula dari kaedah heuristik untuk meningkatkan nisbah tinggi ke lebar histogram kluster, kami mengembangkan algoritma baru - CLOPE, yang sangat cepat dan berskala, sementara cukup berkesan. Kami menunjukkan prestasi algoritma kami pada dua set data dunia nyata, dan membandingkan CLOPE dengan algoritma canggih. [[EENNDD]] skalabiliti; data kategori"], [{"string": "A GPU-tailored approach for training kernelized SVMs We present a method for efficiently training binary and multiclass kernelized SVMs on a Graphics Processing Unit GPU . Our methods apply to a broad range of kernels , including the popular Gaus - sian kernel , on datasets as large as the amount of available memory on the graphics card . Our approach is distinguished from earlier work in that it cleanly and efficiently handles sparse datasets through the use of a novel clustering technique . Our optimization algorithm is also specifically designed to take advantage of the graphics hardware . This leads to different algorithmic choices then those preferred in serial implementations . Our easy-to-use library is orders of magnitude faster then existing CPU libraries , and several times faster than prior GPU approaches .", "keywords": ["gpgpu", "concurrent programming"], "combined": "A GPU-tailored approach for training kernelized SVMs We present a method for efficiently training binary and multiclass kernelized SVMs on a Graphics Processing Unit GPU . Our methods apply to a broad range of kernels , including the popular Gaus - sian kernel , on datasets as large as the amount of available memory on the graphics card . Our approach is distinguished from earlier work in that it cleanly and efficiently handles sparse datasets through the use of a novel clustering technique . Our optimization algorithm is also specifically designed to take advantage of the graphics hardware . This leads to different algorithmic choices then those preferred in serial implementations . Our easy-to-use library is orders of magnitude faster then existing CPU libraries , and several times faster than prior GPU approaches . [[EENNDD]] gpgpu; concurrent programming"}, "Pendekatan yang disesuaikan dengan GPU untuk melatih SVM kernelized Kami menyajikan satu kaedah untuk melatih SVM kernelized binari dan multikelas dengan cekap pada GPU Unit Pemprosesan Grafik. Kaedah kami berlaku untuk pelbagai kernel, termasuk kernel Gaus - sian yang popular, pada set data sebesar jumlah memori yang ada pada kad grafik. Pendekatan kami dibezakan dari pekerjaan sebelumnya kerana ia secara bersih dan cekap menangani set data yang jarang melalui penggunaan teknik pengelompokan baru. Algoritma pengoptimuman kami juga direka khusus untuk memanfaatkan perkakasan grafik. Ini membawa kepada pilihan algoritma yang berbeza daripada yang disukai dalam pelaksanaan bersiri. Perpustakaan kami yang mudah digunakan adalah pesanan besarnya lebih cepat daripada perpustakaan CPU yang ada, dan beberapa kali lebih cepat daripada pendekatan GPU sebelumnya. [[EENNDD]] gpgpu; pengaturcaraan serentak"], [{"string": "Efficient methods for topic model inference on streaming document collections Topic models provide a powerful tool for analyzing large text collections by representing high dimensional data in a low dimensional subspace . Fitting a topic model given a set of training documents requires approximate inference techniques that are computationally expensive . With today 's large-scale , constantly expanding document collections , it is useful to be able to infer topic distributions for new documents without retraining the model . In this paper , we empirically evaluate the performance of several methods for topic inference in previously unseen documents , including methods based on Gibbs sampling , variational inference , and a new method inspired by text classification . The classification-based inference method produces results similar to iterative inference methods , but requires only a single matrix multiplication . In addition to these inference methods , we present SparseLDA , an algorithm and data structure for evaluating Gibbs sampling distributions . Empirical results indicate that SparseLDA can be approximately 20 times faster than traditional LDA and provide twice the speedup of previously published fast sampling methods , while also using substantially less memory .", "keywords": ["inference", "topic modeling", "miscellaneous"], "combined": "Efficient methods for topic model inference on streaming document collections Topic models provide a powerful tool for analyzing large text collections by representing high dimensional data in a low dimensional subspace . Fitting a topic model given a set of training documents requires approximate inference techniques that are computationally expensive . With today 's large-scale , constantly expanding document collections , it is useful to be able to infer topic distributions for new documents without retraining the model . In this paper , we empirically evaluate the performance of several methods for topic inference in previously unseen documents , including methods based on Gibbs sampling , variational inference , and a new method inspired by text classification . The classification-based inference method produces results similar to iterative inference methods , but requires only a single matrix multiplication . In addition to these inference methods , we present SparseLDA , an algorithm and data structure for evaluating Gibbs sampling distributions . Empirical results indicate that SparseLDA can be approximately 20 times faster than traditional LDA and provide twice the speedup of previously published fast sampling methods , while also using substantially less memory . [[EENNDD]] inference; topic modeling; miscellaneous"}, "Kaedah yang cekap untuk inferensi model topik pada streaming dokumen koleksi Model topik menyediakan alat yang kuat untuk menganalisis koleksi teks besar dengan mewakili data dimensi tinggi dalam ruang bawah dimensi rendah. Memadankan model topik yang diberi satu set dokumen latihan memerlukan teknik inferensi anggaran yang sangat mahal. Dengan koleksi dokumen berskala besar dan terus berkembang hari ini, adalah berguna untuk dapat menyimpulkan pembahagian topik untuk dokumen baru tanpa melatih modelnya. Dalam makalah ini, kami secara empirikal menilai prestasi beberapa kaedah untuk inferensi topik dalam dokumen yang sebelumnya tidak terlihat, termasuk kaedah berdasarkan pengambilan sampel Gibbs, inferensi variasi, dan kaedah baru yang diilhami oleh klasifikasi teks. Kaedah inferensi berdasarkan klasifikasi menghasilkan hasil yang serupa dengan kaedah inferensi berulang, tetapi hanya memerlukan pendaraban matriks tunggal. Sebagai tambahan kepada kaedah inferensi ini, kami menyajikan SparseLDA, algoritma dan struktur data untuk menilai taburan persampelan Gibbs. Hasil empirik menunjukkan bahawa SparseLDA dapat lebih kurang 20 kali lebih cepat daripada LDA tradisional dan memberikan dua kali lebih cepat kaedah persampelan cepat yang diterbitkan sebelumnya, sementara juga menggunakan memori yang jauh lebih sedikit. [[EENNDD]] inferens; pemodelan topik; pelbagai"], [{"string": "Compressed data cubes for OLAP aggregate query approximation on continuous dimensions", "keywords": ["approximate query answering", "olap", "data cubes", "decision support", "density estimation"], "combined": "Compressed data cubes for OLAP aggregate query approximation on continuous dimensions [[EENNDD]] approximate query answering; olap; data cubes; decision support; density estimation"}, "Kiub data yang dimampatkan untuk penghampiran pertanyaan agregat OLAP pada dimensi berterusan [[EENNDD]] anggaran jawapan pertanyaan; olap; kiub data; sokongan keputusan; anggaran ketumpatan"], [{"string": "Mining scale-free networks using geodesic clustering Many real-world graphs have been shown to be scale-free -- vertex degrees follow power law distributions , vertices tend to cluster , and the average length of all shortest paths is small . We present a new model for understanding scale-free networks based on multilevel geodesic approximation , using a new data structure called a multilevel mesh . Using this multilevel framework , we propose a new kind of graph clustering for data reduction of very large graph systems such as social , biological , or electronic networks . Finally , we apply our algorithms to real-world social networks and protein interaction graphs to show that they can reveal knowledge embedded in underlying graph structures . We also demonstrate how our data structures can be used to quickly answer approximate distance and shortest path queries on scale-free networks .", "keywords": ["social networks", "graphs", "scale-free networks", "clustering"], "combined": "Mining scale-free networks using geodesic clustering Many real-world graphs have been shown to be scale-free -- vertex degrees follow power law distributions , vertices tend to cluster , and the average length of all shortest paths is small . We present a new model for understanding scale-free networks based on multilevel geodesic approximation , using a new data structure called a multilevel mesh . Using this multilevel framework , we propose a new kind of graph clustering for data reduction of very large graph systems such as social , biological , or electronic networks . Finally , we apply our algorithms to real-world social networks and protein interaction graphs to show that they can reveal knowledge embedded in underlying graph structures . We also demonstrate how our data structures can be used to quickly answer approximate distance and shortest path queries on scale-free networks . [[EENNDD]] social networks; graphs; scale-free networks; clustering"}, "Rangkaian bebas skala penambangan menggunakan pengelompokan geodesi Banyak grafik dunia nyata terbukti bebas skala - darjah puncak mengikuti pembahagian undang-undang kuasa, bucu cenderung berkerumun, dan panjang rata-rata semua jalur terpendek adalah kecil. Kami menyajikan model baru untuk memahami rangkaian bebas skala berdasarkan pendekatan geodesi bertingkat, menggunakan struktur data baru yang disebut jala bertingkat. Dengan menggunakan kerangka bertingkat ini, kami mencadangkan pengelompokan grafik baru untuk pengurangan data sistem grafik yang sangat besar seperti rangkaian sosial, biologi, atau elektronik. Akhirnya, kami menerapkan algoritma kami ke rangkaian sosial dunia nyata dan grafik interaksi protein untuk menunjukkan bahawa mereka dapat mendedahkan pengetahuan yang tertanam dalam struktur grafik yang mendasari. Kami juga menunjukkan bagaimana struktur data kami dapat digunakan untuk cepat menjawab anggaran jarak dan pertanyaan jalan terpendek pada rangkaian bebas skala. [[EENNDD]] rangkaian sosial; grafik; rangkaian bebas skala; pengelompokan"], [{"string": "Efficient search for association rules", "keywords": ["information search and retrieval", "learning", "miscellaneous", "association rule", "search"], "combined": "Efficient search for association rules [[EENNDD]] information search and retrieval; learning; miscellaneous; association rule; search"}, "Pencarian cekap untuk peraturan persatuan [[EENNDD]] carian dan pengambilan maklumat; belajar; pelbagai; peraturan persatuan; cari"], [{"string": "A model for discovering customer value for E-content There exists a huge demand for multimedia goods and services in the Internet . Currently available bandwidth speeds can support sale of downloadable content like CDs , e-books , etc. as well as services like video-on-demand . In the future , such services will be prevalent in the Internet . Since costs are typically fixed , maximizing revenue can maximize profits . A primary determinant of revenue in such e-content markets is how much value the customers associate with the content . Though marketing surveys are useful , they can not adapt to the dynamic nature of the Internet market . In this work , we examine how to learn customer valuations in close to real-time . Our contributions in this paper are threefold : 1 we develop a probabilistic model to describe customer behavior , 2 we develop a framework for pricing e-content based on basic economic principles , and 3 we propose a price discovering algorithm that learns customer behavior parameters and suggests prices to an e-content provider . We validate our algorithm using simulations . Our simulations indicate that our algorithm generates revenue close to the maximum expectation . Further , they also indicate that the algorithm is robust to transient customer behavior .", "keywords": ["probabilistic algorithms"], "combined": "A model for discovering customer value for E-content There exists a huge demand for multimedia goods and services in the Internet . Currently available bandwidth speeds can support sale of downloadable content like CDs , e-books , etc. as well as services like video-on-demand . In the future , such services will be prevalent in the Internet . Since costs are typically fixed , maximizing revenue can maximize profits . A primary determinant of revenue in such e-content markets is how much value the customers associate with the content . Though marketing surveys are useful , they can not adapt to the dynamic nature of the Internet market . In this work , we examine how to learn customer valuations in close to real-time . Our contributions in this paper are threefold : 1 we develop a probabilistic model to describe customer behavior , 2 we develop a framework for pricing e-content based on basic economic principles , and 3 we propose a price discovering algorithm that learns customer behavior parameters and suggests prices to an e-content provider . We validate our algorithm using simulations . Our simulations indicate that our algorithm generates revenue close to the maximum expectation . Further , they also indicate that the algorithm is robust to transient customer behavior . [[EENNDD]] probabilistic algorithms"}, "Model untuk mengetahui nilai pelanggan untuk kandungan E Terdapat permintaan yang besar untuk barangan dan perkhidmatan multimedia di Internet. Kelajuan lebar jalur yang tersedia pada masa ini dapat menyokong penjualan kandungan yang boleh dimuat turun seperti CD, e-buku, dan lain-lain serta perkhidmatan seperti video atas permintaan. Pada masa akan datang, perkhidmatan tersebut akan berlaku di Internet. Oleh kerana kos biasanya tetap, memaksimumkan pendapatan dapat memaksimumkan keuntungan. Penentu utama pendapatan di pasaran e-kandungan seperti itu adalah berapa banyak nilai yang dihubungkan oleh pelanggan dengan kandungan tersebut. Walaupun tinjauan pemasaran berguna, mereka tidak dapat menyesuaikan diri dengan sifat pasar Internet yang dinamis. Dalam karya ini, kami mengkaji bagaimana mempelajari penilaian pelanggan hampir dengan masa nyata. Sumbangan kami dalam makalah ini tiga kali ganda: 1 kami mengembangkan model probabilistik untuk menggambarkan tingkah laku pelanggan, 2 kami mengembangkan kerangka kerja untuk menentukan harga e-konten berdasarkan prinsip ekonomi asas, dan 3 kami mencadangkan algoritma penemuan harga yang mempelajari parameter tingkah laku pelanggan dan mencadangkan harga kepada pembekal e-kandungan. Kami mengesahkan algoritma kami menggunakan simulasi. Simulasi kami menunjukkan bahawa algoritma kami menjana pendapatan hampir dengan jangkaan maksimum. Selanjutnya, mereka juga menunjukkan bahawa algoritma itu kuat untuk tingkah laku pelanggan sementara. [[EENNDD]] algoritma probabilistik"], [{"string": "Temporal causal modeling with graphical granger methods The need for mining causality , beyond mere statistical correlations , for real world problems has been recognized widely . Many of these applications naturally involve temporal data , which raises the challenge of how best to leverage the temporal information for causal modeling . Recently graphical modeling with the concept of `` Granger causality '' , based on the intuition that a cause helps predict its effects in the future , has gained attention in many domains involving time series data analysis . With the surge of interest in model selection methodologies for regression , such as the Lasso , as practical alternatives to solving structural learning of graphical models , the question arises whether and how to combine these two notions into a practically viable approach for temporal causal modeling . In this paper , we examine a host of related algorithms that , loosely speaking , fall under the category of graphical Granger methods , and characterize their relative performance from multiple viewpoints . Our experiments show , for instance , that the Lasso algorithm exhibits consistent gain over the canonical pairwise graphical Granger method . We also characterize conditions under which these variants of graphical Granger methods perform well in comparison to other benchmark methods . Finally , we apply these methods to a real world data set involving key performance indicators of corporations , and present some concrete results .", "keywords": ["graphical models", "time series data", "causal modeling"], "combined": "Temporal causal modeling with graphical granger methods The need for mining causality , beyond mere statistical correlations , for real world problems has been recognized widely . Many of these applications naturally involve temporal data , which raises the challenge of how best to leverage the temporal information for causal modeling . Recently graphical modeling with the concept of `` Granger causality '' , based on the intuition that a cause helps predict its effects in the future , has gained attention in many domains involving time series data analysis . With the surge of interest in model selection methodologies for regression , such as the Lasso , as practical alternatives to solving structural learning of graphical models , the question arises whether and how to combine these two notions into a practically viable approach for temporal causal modeling . In this paper , we examine a host of related algorithms that , loosely speaking , fall under the category of graphical Granger methods , and characterize their relative performance from multiple viewpoints . Our experiments show , for instance , that the Lasso algorithm exhibits consistent gain over the canonical pairwise graphical Granger method . We also characterize conditions under which these variants of graphical Granger methods perform well in comparison to other benchmark methods . Finally , we apply these methods to a real world data set involving key performance indicators of corporations , and present some concrete results . [[EENNDD]] graphical models; time series data; causal modeling"}, "Pemodelan sebab-akibat sementara dengan kaedah penyusun grafik Keperluan untuk sebab-sebab perlombongan, di luar korelasi statistik semata-mata, untuk masalah dunia nyata telah diakui secara meluas. Sebilangan besar aplikasi ini secara alami melibatkan data temporal, yang menimbulkan tantangan bagaimana cara terbaik untuk memanfaatkan maklumat temporal untuk pemodelan kausal. Baru-baru ini pemodelan grafik dengan konsep \"Granger causality\", berdasarkan intuisi bahawa penyebab membantu meramalkan kesannya di masa depan, telah mendapat perhatian dalam banyak domain yang melibatkan analisis data siri masa. Dengan adanya minat terhadap metodologi pemilihan model untuk regresi, seperti Lasso, sebagai alternatif praktikal untuk menyelesaikan pembelajaran struktur model grafik, timbul persoalan sama ada dan bagaimana menggabungkan kedua-dua gagasan ini menjadi pendekatan praktikal untuk modeling sebab-akibat temporal. Dalam makalah ini, kami meneliti sejumlah algoritma yang berkaitan, yang secara longgar, tergolong dalam kategori kaedah Granger grafik, dan mencirikan prestasi relatifnya dari pelbagai sudut pandangan. Contohnya, eksperimen kami menunjukkan bahawa algoritma Lasso menunjukkan keuntungan yang konsisten berbanding kaedah Granger grafik berpasangan kanonik. Kami juga mencirikan keadaan di mana varian kaedah Granger grafik ini berfungsi dengan baik berbanding dengan kaedah penanda aras yang lain. Akhirnya, kami menerapkan kaedah ini pada kumpulan data dunia nyata yang melibatkan petunjuk prestasi utama syarikat, dan membentangkan beberapa hasil yang konkrit. [[EENNDD]] model grafik; data siri masa; pemodelan kausal"], [{"string": "Trajectory clustering with mixtures of regression models", "keywords": ["video"], "combined": "Trajectory clustering with mixtures of regression models [[EENNDD]] video"}, "Pengelompokan lintasan dengan campuran video model regresi [[EENNDD]]"], [{"string": "IMDS : intelligent malware detection system The proliferation of malware has presented a serious threat to the security of computer systems . Traditional signature-based anti-virus systems fail to detect polymorphic and new , previously unseen malicious executables . In this paper , resting on the analysis of Windows API execution sequences called by PE files , we develop the Intelligent Malware Detection System IMDS using Objective-Oriented Association OOA mining based classification . IMDS is an integrated system consisting of three major modules : PE parser , OOA rule generator , and rule based classifier . An OOA algorithm is adapted to efficiently generate OOA rules for classification . A comprehensive experimental study on a large collection of PE files obtained from the anti-virus laboratory of King-Soft Corporation is performed to compare various malware detection approaches . Promising experimental results demonstrate that the accuracy and efficiency of our IMDS system out perform popular anti-virus software such as Norton AntiVirus and McAfee VirusScan , as well as previous data mining based detection systems which employed Naive Bayes , Support Vector Machine SVM and Decision Tree techniques .", "keywords": ["pe file", "learning", "malware", "windows api sequence", "ooa mining"], "combined": "IMDS : intelligent malware detection system The proliferation of malware has presented a serious threat to the security of computer systems . Traditional signature-based anti-virus systems fail to detect polymorphic and new , previously unseen malicious executables . In this paper , resting on the analysis of Windows API execution sequences called by PE files , we develop the Intelligent Malware Detection System IMDS using Objective-Oriented Association OOA mining based classification . IMDS is an integrated system consisting of three major modules : PE parser , OOA rule generator , and rule based classifier . An OOA algorithm is adapted to efficiently generate OOA rules for classification . A comprehensive experimental study on a large collection of PE files obtained from the anti-virus laboratory of King-Soft Corporation is performed to compare various malware detection approaches . Promising experimental results demonstrate that the accuracy and efficiency of our IMDS system out perform popular anti-virus software such as Norton AntiVirus and McAfee VirusScan , as well as previous data mining based detection systems which employed Naive Bayes , Support Vector Machine SVM and Decision Tree techniques . [[EENNDD]] pe file; learning; malware; windows api sequence; ooa mining"}, "IMDS: sistem pengesanan perisian hasad pintar Penyebaran perisian hasad telah menimbulkan ancaman serius terhadap keselamatan sistem komputer. Sistem anti-virus berasaskan tanda tangan tradisional gagal mengesan polimorfik dan yang baru, yang sebelumnya tidak dapat dilaksanakan yang berniat jahat. Dalam makalah ini, berdasarkan analisis urutan pelaksanaan API Windows yang dipanggil oleh fail PE, kami mengembangkan Sistem Pengesanan Malware Cerdas IMDS menggunakan klasifikasi berasaskan perlombongan OOA Persatuan Berorientasi Objektif. IMDS adalah sistem bersepadu yang terdiri daripada tiga modul utama: penghurai PE, penjana peraturan OOA, dan pengelasan berdasarkan peraturan. Algoritma OOA disesuaikan untuk menghasilkan peraturan OOA untuk pengelasan secara cekap. Kajian eksperimental komprehensif mengenai koleksi besar fail PE yang diperoleh dari makmal anti-virus King-Soft Corporation dilakukan untuk membandingkan pelbagai pendekatan pengesanan malware. Hasil eksperimen yang menjanjikan menunjukkan bahawa ketepatan dan kecekapan sistem IMDS kami melaksanakan perisian anti-virus yang popular seperti Norton AntiVirus dan McAfee VirusScan, serta sistem pengesanan berasaskan perlombongan data sebelumnya yang menggunakan teknik Naive Bayes, Support Vector Machine SVM dan Decision Tree . [[EENNDD]] fail pe; belajar; perisian hasad; urutan api windows; perlombongan ooa"], [{"string": "A new two-phase sampling based algorithm for discovering association rules This paper introduces FAST , a novel two-phase sampling-based algorithm for discovering association rules in large databases . In Phase I a large initial sample of transactions is collected and used to quickly and accurately estimate the support of each individual item in the database . In Phase II these estimated supports are used to either trim `` outlier '' transactions or select `` representative '' transactions from the initial sample , thereby forming a small final sample that more accurately reflects the statistical characteristics i.e. , itemset supports of the entire database . The expensive operation of discovering association rules is then performed on the final sample . In an empirical study , FAST was able to achieve 90 -- 95 % accuracy using a final sample having a size of only 15 -- 33 % of that of a comparable random sample . This efficiency gain resulted in a speedup by roughly a factor of 10 over previous algorithms that require expensive processing of the entire database -- even efficient algorithms that exploit sampling . Our new sampling technique can be used in conjunction with almost any standard association-rule algorithm , and can potentially render scalable other algorithms that mine `` count '' data .", "keywords": ["fast fourier transforms"], "combined": "A new two-phase sampling based algorithm for discovering association rules This paper introduces FAST , a novel two-phase sampling-based algorithm for discovering association rules in large databases . In Phase I a large initial sample of transactions is collected and used to quickly and accurately estimate the support of each individual item in the database . In Phase II these estimated supports are used to either trim `` outlier '' transactions or select `` representative '' transactions from the initial sample , thereby forming a small final sample that more accurately reflects the statistical characteristics i.e. , itemset supports of the entire database . The expensive operation of discovering association rules is then performed on the final sample . In an empirical study , FAST was able to achieve 90 -- 95 % accuracy using a final sample having a size of only 15 -- 33 % of that of a comparable random sample . This efficiency gain resulted in a speedup by roughly a factor of 10 over previous algorithms that require expensive processing of the entire database -- even efficient algorithms that exploit sampling . Our new sampling technique can be used in conjunction with almost any standard association-rule algorithm , and can potentially render scalable other algorithms that mine `` count '' data . [[EENNDD]] fast fourier transforms"}, "Algoritma berasaskan persampelan dua fasa baru untuk menemui peraturan perkaitan Kertas ini memperkenalkan FAST, sebuah algoritma berasaskan pensampelan dua fasa baru untuk mengetahui peraturan pergaulan dalam pangkalan data besar. Pada Tahap I sampel awal transaksi yang besar dikumpulkan dan digunakan untuk menganggarkan sokongan setiap item dalam pangkalan data dengan cepat dan tepat. Pada Tahap II, anggaran sokongan ini digunakan untuk memotong transaksi \"outlier\" atau memilih transaksi \"representatif\" dari sampel awal, sehingga membentuk sampel akhir kecil yang lebih tepat menggambarkan ciri statistik iaitu, sokongan itemet keseluruhan pangkalan data . Operasi yang mahal untuk menemui peraturan persatuan kemudian dilakukan pada sampel akhir. Dalam kajian empirikal, FAST dapat mencapai ketepatan 90 - 95% menggunakan sampel akhir yang mempunyai ukuran hanya 15 - 33% daripada sampel rawak sebanding. Peningkatan kecekapan ini menghasilkan peningkatan oleh kira-kira faktor 10 berbanding algoritma sebelumnya yang memerlukan pemprosesan keseluruhan pangkalan data yang mahal - bahkan algoritma yang cekap yang memanfaatkan persampelan. Teknik pensampelan baru kami dapat digunakan bersama dengan hampir semua algoritma peraturan persatuan standard, dan berpotensi dapat membuat algoritma lain yang dapat diskalakan yang menambang data \"hitung\". [[EENNDD]] transformasi Fourier pantas"], [{"string": "Evaluation of prediction models for marketing campaigns We consider prediction-model evaluation in the context of marketing-campaign planning . In order to evaluate and compare models with specific campaign objectives in mind , we need to concentrate our attention on the appropriate evaluation-criteria . These should portray the model 's ability to score accurately and to identify the relevant target population . In this paper we discuss some applicable model-evaluation and selection criteria , their relevance for campaign planning , their robustness under changing population distributions , and their employment when constructing confidence intervals . We illustrate our results with a case study based on our experience from several projects .", "keywords": ["confidence intervals", "performance measures", "marketing campaigns", "model evaluation"], "combined": "Evaluation of prediction models for marketing campaigns We consider prediction-model evaluation in the context of marketing-campaign planning . In order to evaluate and compare models with specific campaign objectives in mind , we need to concentrate our attention on the appropriate evaluation-criteria . These should portray the model 's ability to score accurately and to identify the relevant target population . In this paper we discuss some applicable model-evaluation and selection criteria , their relevance for campaign planning , their robustness under changing population distributions , and their employment when constructing confidence intervals . We illustrate our results with a case study based on our experience from several projects . [[EENNDD]] confidence intervals; performance measures; marketing campaigns; model evaluation"}, "Penilaian model ramalan untuk kempen pemasaran Kami mempertimbangkan penilaian model ramalan dalam konteks perancangan kampanye pemasaran. Untuk menilai dan membandingkan model dengan mempertimbangkan objektif kempen tertentu, kita perlu memusatkan perhatian kita pada kriteria penilaian yang sesuai. Ini harus menggambarkan kemampuan model untuk menjaring dengan tepat dan mengenal pasti populasi sasaran yang relevan. Dalam makalah ini kita membincangkan beberapa kriteria penilaian dan pemilihan model yang berlaku, relevansinya dengan perencanaan kempen, ketahanan mereka di bawah perubahan taburan penduduk, dan pekerjaan mereka ketika membangun selang keyakinan. Kami menggambarkan hasil kami dengan kajian kes berdasarkan pengalaman kami dari beberapa projek. [[EENNDD]] selang keyakinan; ukuran prestasi; kempen pemasaran; penilaian model"], [{"string": "YALE : rapid prototyping for complex data mining tasks KDD is a complex and demanding task . While a large number of methods has been established for numerous problems , many challenges remain to be solved . New tasks emerge requiring the development of new methods or processing schemes . Like in software development , the development of such solutions demands for careful analysis , specification , implementation , and testing . Rapid prototyping is an approach which allows crucial design decisions as early as possible . A rapid prototyping system should support maximal re-use and innovative combinations of existing methods , as well as simple and quick integration of new ones . This paper describes Yale , a free open-source environment forKDD and machine learning . Yale provides a rich variety of methods whichallows rapid prototyping for new applications and makes costlyre-implementations unnecessary . Additionally , Yale offers extensive functionality for process evaluation and optimization which is a crucial property for any KDD rapid prototyping tool . Following the paradigm of visual programming eases the design of processing schemes . While the graphical user interface supports interactive design , the underlying XML representation enables automated applications after the prototyping phase . After a discussion of the key concepts of Yale , we illustrate the advantages of rapid prototyping for KDD on case studies ranging from data pre-processing to result visualization . These case studies cover tasks like feature engineering , text mining , data stream mining and tracking drifting concepts , ensemble methods and distributed data mining . This variety of applications is also reflected in a broad user base , we counted more than 40,000 downloads during the last twelve months .", "keywords": ["data pre-processing", "data stream mining", "result visualization", "audio and text mining", "design methodology", "feature construction", "multimedia mining", "kdd system", "rapid prototyping", "distributed data mining"], "combined": "YALE : rapid prototyping for complex data mining tasks KDD is a complex and demanding task . While a large number of methods has been established for numerous problems , many challenges remain to be solved . New tasks emerge requiring the development of new methods or processing schemes . Like in software development , the development of such solutions demands for careful analysis , specification , implementation , and testing . Rapid prototyping is an approach which allows crucial design decisions as early as possible . A rapid prototyping system should support maximal re-use and innovative combinations of existing methods , as well as simple and quick integration of new ones . This paper describes Yale , a free open-source environment forKDD and machine learning . Yale provides a rich variety of methods whichallows rapid prototyping for new applications and makes costlyre-implementations unnecessary . Additionally , Yale offers extensive functionality for process evaluation and optimization which is a crucial property for any KDD rapid prototyping tool . Following the paradigm of visual programming eases the design of processing schemes . While the graphical user interface supports interactive design , the underlying XML representation enables automated applications after the prototyping phase . After a discussion of the key concepts of Yale , we illustrate the advantages of rapid prototyping for KDD on case studies ranging from data pre-processing to result visualization . These case studies cover tasks like feature engineering , text mining , data stream mining and tracking drifting concepts , ensemble methods and distributed data mining . This variety of applications is also reflected in a broad user base , we counted more than 40,000 downloads during the last twelve months . [[EENNDD]] data pre-processing; data stream mining; result visualization; audio and text mining; design methodology; feature construction; multimedia mining; kdd system; rapid prototyping; distributed data mining"}, "YALE: prototaip pantas untuk tugas perlombongan data yang kompleks KDD adalah tugas yang kompleks dan menuntut. Walaupun sebilangan besar kaedah telah ditentukan untuk banyak masalah, banyak cabaran masih harus diselesaikan. Tugas baru muncul yang memerlukan pengembangan kaedah baru atau skema pemprosesan. Seperti dalam pengembangan perangkat lunak, pengembangan solusi semacam itu menuntut analisis, spesifikasi, pelaksanaan, dan pengujian yang teliti. Prototaip cepat adalah pendekatan yang memungkinkan keputusan reka bentuk penting seawal mungkin. Sistem prototaip cepat harus menyokong penggunaan semula maksimum dan kombinasi kaedah sedia ada yang inovatif, serta penyatuan yang baru dengan mudah dan cepat. Makalah ini menerangkan Yale, persekitaran sumber terbuka percuma untuk KDD dan pembelajaran mesin. Yale menyediakan pelbagai kaedah yang membolehkan prototaip cepat untuk aplikasi baru dan membuat pelaksanaan yang mahal tidak perlu. Selain itu, Yale menawarkan fungsi yang luas untuk penilaian proses dan pengoptimuman yang merupakan harta penting bagi mana-mana alat prototaip pantas KDD. Mengikuti paradigma pengaturcaraan visual memudahkan reka bentuk skema pemprosesan. Walaupun antara muka pengguna grafik menyokong reka bentuk interaktif, representasi XML yang mendasari membolehkan aplikasi automatik setelah fasa prototaip. Selepas perbincangan mengenai konsep utama Yale, kami menggambarkan kelebihan prototaip pantas untuk KDD pada kajian kes mulai dari pra-pemprosesan data hingga visualisasi hasil. Kajian kes ini merangkumi tugas-tugas seperti rekayasa ciri, perlombongan teks, perlombongan aliran data dan mengesan konsep drifting, kaedah ensemble dan perlombongan data yang diedarkan. Pelbagai aplikasi ini juga tercermin dalam pangkalan pengguna yang luas, kami mengira lebih daripada 40,000 muat turun selama dua belas bulan terakhir. [[EENNDD]] pra-pemprosesan data; perlombongan aliran data; visualisasi hasil; perlombongan audio dan teks; metodologi reka bentuk; pembinaan ciri; perlombongan multimedia; sistem kdd; prototaip cepat; perlombongan data yang diedarkan"], [{"string": "Growing a tree in the forest : constructing folksonomies by integrating structured metadata Many social Web sites allow users to annotate the content with descriptive metadata , such as tags , and more recently to organize content hierarchically . These types of structured metadata provide valuable evidence for learning how a community organizes knowledge . For instance , we can aggregate many personal hierarchies into a common taxonomy , also known as a folksonomy , that will aid users in visualizing and browsing social content , and also to help them in organizing their own content . However , learning from social metadata presents several challenges , since it is sparse , shallow , ambiguous , noisy , and inconsistent . We describe an approach to folksonomy learning based on relational clustering , which exploits structured metadata contained in personal hierarchies . Our approach clusters similar hierarchies using their structure and tag statistics , then incrementally weaves them into a deeper , bushier tree . We study folksonomy learning using social metadata extracted from the photo-sharing site Flickr , and demonstrate that the proposed approach addresses the challenges . Moreover , comparing to previous work , the approach produces larger , more accurate folksonomies , and in addition , scales better .", "keywords": ["taxonomies", "social information processing", "relational clustering", "folksonomies", "collective knowledge", "social metadata"], "combined": "Growing a tree in the forest : constructing folksonomies by integrating structured metadata Many social Web sites allow users to annotate the content with descriptive metadata , such as tags , and more recently to organize content hierarchically . These types of structured metadata provide valuable evidence for learning how a community organizes knowledge . For instance , we can aggregate many personal hierarchies into a common taxonomy , also known as a folksonomy , that will aid users in visualizing and browsing social content , and also to help them in organizing their own content . However , learning from social metadata presents several challenges , since it is sparse , shallow , ambiguous , noisy , and inconsistent . We describe an approach to folksonomy learning based on relational clustering , which exploits structured metadata contained in personal hierarchies . Our approach clusters similar hierarchies using their structure and tag statistics , then incrementally weaves them into a deeper , bushier tree . We study folksonomy learning using social metadata extracted from the photo-sharing site Flickr , and demonstrate that the proposed approach addresses the challenges . Moreover , comparing to previous work , the approach produces larger , more accurate folksonomies , and in addition , scales better . [[EENNDD]] taxonomies; social information processing; relational clustering; folksonomies; collective knowledge; social metadata"}, "Menanam pokok di hutan: membina folksonomies dengan mengintegrasikan metadata berstruktur Banyak laman web sosial membolehkan pengguna memberi penjelasan pada kandungan dengan metadata deskriptif, seperti teg, dan baru-baru ini untuk mengatur kandungan secara hierarki. Jenis metadata berstruktur ini memberikan bukti berharga untuk belajar bagaimana komuniti mengatur pengetahuan. Sebagai contoh, kita dapat mengumpulkan banyak hierarki peribadi menjadi taksonomi biasa, juga dikenal sebagai folksonomi, yang akan membantu pengguna dalam memvisualisasikan dan melayari kandungan sosial, dan juga untuk membantu mereka dalam mengatur kandungan mereka sendiri. Namun, belajar dari metadata sosial menghadirkan beberapa cabaran, kerana jarang, cetek, tidak jelas, bising, dan tidak konsisten. Kami menerangkan pendekatan untuk pembelajaran folksonomi berdasarkan pengelompokan relasional, yang memanfaatkan metadata berstruktur yang terkandung dalam hierarki peribadi. Pendekatan kami mengumpulkan hierarki yang serupa dengan menggunakan struktur dan statistik tag mereka, kemudian secara bertahap menenunkannya menjadi pohon yang lebih dalam dan lebat. Kami mempelajari pembelajaran folksonomi menggunakan metadata sosial yang diekstrak dari laman perkongsian foto Flickr, dan menunjukkan bahawa pendekatan yang dicadangkan menangani cabaran. Lebih-lebih lagi, jika dibandingkan dengan karya sebelumnya, pendekatan ini menghasilkan folksonomi yang lebih besar dan tepat, dan di samping itu, skala lebih baik. [[EENNDD]] taksonomi; pemprosesan maklumat sosial; pengelompokan hubungan; folksonomi; pengetahuan kolektif; metadata sosial"], [{"string": "Online multiscale dynamic topic models We propose an online topic model for sequentially analyzing the time evolution of topics in document collections . Topics naturally evolve with multiple timescales . For example , some words may be used consistently over one hundred years , while other words emerge and disappear over periods of a few days . Thus , in the proposed model , current topic-specific distributions over words are assumed to be generated based on the multiscale word distributions of the previous epoch . Considering both the long-timescale dependency as well as the short-timescale dependency yields a more robust model . We derive efficient online inference procedures based on a stochastic EM algorithm , in which the model is sequentially updated using newly obtained data ; this means that past data are not required to make the inference . We demonstrate the effectiveness of the proposed method in terms of predictive performance and computational efficiency by examining collections of real documents with timestamps .", "keywords": ["online learning", "time-series analysis", "topic model"], "combined": "Online multiscale dynamic topic models We propose an online topic model for sequentially analyzing the time evolution of topics in document collections . Topics naturally evolve with multiple timescales . For example , some words may be used consistently over one hundred years , while other words emerge and disappear over periods of a few days . Thus , in the proposed model , current topic-specific distributions over words are assumed to be generated based on the multiscale word distributions of the previous epoch . Considering both the long-timescale dependency as well as the short-timescale dependency yields a more robust model . We derive efficient online inference procedures based on a stochastic EM algorithm , in which the model is sequentially updated using newly obtained data ; this means that past data are not required to make the inference . We demonstrate the effectiveness of the proposed method in terms of predictive performance and computational efficiency by examining collections of real documents with timestamps . [[EENNDD]] online learning; time-series analysis; topic model"}, "Model topik dinamik pelbagai skala dalam talian Kami mencadangkan model topik dalam talian untuk secara berurutan menganalisis evolusi masa topik dalam koleksi dokumen. Topik secara semula jadi berkembang dengan beberapa skala masa. Contohnya, beberapa perkataan boleh digunakan secara konsisten selama seratus tahun, sementara kata-kata lain muncul dan hilang dalam beberapa hari. Oleh itu, dalam model yang dicadangkan, pembahagian topik-topik semasa mengenai kata-kata diasumsikan dihasilkan berdasarkan distribusi kata pelbagai skala pada zaman sebelumnya. Mengingat ketergantungan skala jangka panjang dan juga ketergantungan skala waktu pendek menghasilkan model yang lebih mantap. Kami memperoleh prosedur inferens dalam talian yang cekap berdasarkan algoritma EM stokastik, di mana modelnya diperbaharui secara berurutan menggunakan data yang baru diperoleh; ini bermaksud bahawa data masa lalu tidak diperlukan untuk membuat kesimpulan. Kami menunjukkan keberkesanan kaedah yang dicadangkan dari segi prestasi ramalan dan kecekapan komputasi dengan memeriksa koleksi dokumen sebenar dengan cap waktu. [[EENNDD]] pembelajaran dalam talian; analisis siri masa; model topik"], [{"string": "DivRank : the interplay of prestige and diversity in information networks Information networks are widely used to characterize the relationships between data items such as text documents . Many important retrieval and mining tasks rely on ranking the data items based on their centrality or prestige in the network . Beyond prestige , diversity has been recognized as a crucial objective in ranking , aiming at providing a non-redundant and high coverage piece of information in the top ranked results . Nevertheless , existing network-based ranking approaches either disregard the concern of diversity , or handle it with non-optimized heuristics , usually based on greedy vertex selection . We propose a novel ranking algorithm , DivRank , based on a reinforced random walk in an information network . This model automatically balances the prestige and the diversity of the top ranked vertices in a principled way . DivRank not only has a clear optimization explanation , but also well connects to classical models in mathematics and network science . We evaluate DivRank using empirical experiments on three different networks as well as a text summarization task . DivRank outperforms existing network-based ranking methods in terms of enhancing diversity in prestige .", "keywords": ["ranking", "information networks", "reinforced random walk", "diversity"], "combined": "DivRank : the interplay of prestige and diversity in information networks Information networks are widely used to characterize the relationships between data items such as text documents . Many important retrieval and mining tasks rely on ranking the data items based on their centrality or prestige in the network . Beyond prestige , diversity has been recognized as a crucial objective in ranking , aiming at providing a non-redundant and high coverage piece of information in the top ranked results . Nevertheless , existing network-based ranking approaches either disregard the concern of diversity , or handle it with non-optimized heuristics , usually based on greedy vertex selection . We propose a novel ranking algorithm , DivRank , based on a reinforced random walk in an information network . This model automatically balances the prestige and the diversity of the top ranked vertices in a principled way . DivRank not only has a clear optimization explanation , but also well connects to classical models in mathematics and network science . We evaluate DivRank using empirical experiments on three different networks as well as a text summarization task . DivRank outperforms existing network-based ranking methods in terms of enhancing diversity in prestige . [[EENNDD]] ranking; information networks; reinforced random walk; diversity"}, "DivRank: interaksi prestij dan kepelbagaian dalam rangkaian maklumat Rangkaian maklumat banyak digunakan untuk mencirikan hubungan antara item data seperti dokumen teks. Banyak tugas pengambilan dan perlombongan penting bergantung pada pemeringkatan item data berdasarkan pusat atau prestijnya dalam rangkaian. Di luar prestij, kepelbagaian telah diakui sebagai objektif penting dalam peringkat, bertujuan untuk memberikan maklumat yang tidak berlebihan dan liputan tinggi dalam hasil peringkat teratas. Walaupun begitu, pendekatan peringkat berdasarkan rangkaian yang ada sama ada mengabaikan keprihatinan kepelbagaian, atau mengatasinya dengan heuristik yang tidak dioptimumkan, biasanya berdasarkan pemilihan titik tamak. Kami mencadangkan algoritma pemeringkatan baru, DivRank, berdasarkan jalan rawak yang diperkuat dalam rangkaian maklumat. Model ini secara automatik mengimbangi prestij dan kepelbagaian bucu peringkat teratas dengan cara yang berprinsip. DivRank bukan sahaja mempunyai penjelasan pengoptimuman yang jelas, tetapi juga menghubungkan dengan model klasik dalam matematik dan sains rangkaian. Kami menilai DivRank menggunakan eksperimen empirikal di tiga rangkaian yang berbeza serta tugas ringkasan teks. DivRank mengungguli kaedah ranking berdasarkan rangkaian yang ada dari segi meningkatkan kepelbagaian prestij. [[EENNDD]] kedudukan; rangkaian maklumat; jalan rawak yang diperkuat; kepelbagaian"], [{"string": "Learning optimal ranking with tensor factorization for tag recommendation Tag recommendation is the task of predicting a personalized list of tags for a user given an item . This is important for many websites with tagging capabilities like last . fm or delicious . In this paper , we propose a method for tag recommendation based on tensor factorization TF . In contrast to other TF methods like higher order singular value decomposition HOSVD , our method RTF ` ranking with tensor factorization ' directly optimizes the factorization model for the best personalized ranking . RTF handles missing values and learns from pairwise ranking constraints . Our optimization criterion for TF is motivated by a detailed analysis of the problem and of interpretation schemes for the observed data in tagging systems . In all , RTF directly optimizes for the actual problem using a correct interpretation of the data . We provide a gradient descent algorithm to solve our optimization problem . We also provide an improved learning and prediction method with runtime complexity analysis for RTF . The prediction runtime of RTF is independent of the number of observations and only depends on the factorization dimensions . Besides the theoretical analysis , we empirically show that our method outperforms other state-of-the-art tag recommendation methods like FolkRank , PageRank and HOSVD both in quality and prediction runtime .", "keywords": ["ranking", "tag recommendation", "tensor factorization"], "combined": "Learning optimal ranking with tensor factorization for tag recommendation Tag recommendation is the task of predicting a personalized list of tags for a user given an item . This is important for many websites with tagging capabilities like last . fm or delicious . In this paper , we propose a method for tag recommendation based on tensor factorization TF . In contrast to other TF methods like higher order singular value decomposition HOSVD , our method RTF ` ranking with tensor factorization ' directly optimizes the factorization model for the best personalized ranking . RTF handles missing values and learns from pairwise ranking constraints . Our optimization criterion for TF is motivated by a detailed analysis of the problem and of interpretation schemes for the observed data in tagging systems . In all , RTF directly optimizes for the actual problem using a correct interpretation of the data . We provide a gradient descent algorithm to solve our optimization problem . We also provide an improved learning and prediction method with runtime complexity analysis for RTF . The prediction runtime of RTF is independent of the number of observations and only depends on the factorization dimensions . Besides the theoretical analysis , we empirically show that our method outperforms other state-of-the-art tag recommendation methods like FolkRank , PageRank and HOSVD both in quality and prediction runtime . [[EENNDD]] ranking; tag recommendation; tensor factorization"}, "Belajar peringkat optimum dengan faktorisasi tensor untuk cadangan tag Cadangan tag adalah tugas meramalkan senarai tag yang diperibadikan untuk pengguna yang diberi item. Ini penting untuk banyak laman web dengan keupayaan penandaan seperti terakhir. fm atau sedap. Dalam makalah ini, kami mencadangkan kaedah untuk cadangan tag berdasarkan tensor factorization TF. Berbeza dengan kaedah TF lain seperti penguraian nilai tunggal orde yang lebih tinggi HOSVD, kaedah kami RTF \"peringkat dengan pemfaktoran tensor\" secara langsung mengoptimumkan model pemfaktoran untuk pemeringkatan terbaik. RTF menangani nilai yang hilang dan belajar dari kekangan peringkat berpasangan. Kriteria pengoptimuman kami untuk TF didorong oleh analisis terperinci masalah dan skema tafsiran untuk data yang diperhatikan dalam sistem penandaan. Secara keseluruhan, RTF secara langsung mengoptimumkan masalah sebenar dengan menggunakan tafsiran data yang betul. Kami menyediakan algoritma penurunan kecerunan untuk menyelesaikan masalah pengoptimuman kami. Kami juga menyediakan kaedah pembelajaran dan ramalan yang lebih baik dengan analisis kerumitan jangka masa untuk RTF. Jangka masa jangkaan RTF tidak bergantung pada jumlah pemerhatian dan hanya bergantung pada dimensi pemfaktoran. Selain analisis teori, kami secara empirik menunjukkan bahawa kaedah kami mengatasi kaedah cadangan tag canggih lain seperti FolkRank, PageRank dan HOSVD baik dari segi kualiti dan jangka masa jangkaan. [[EENNDD]] kedudukan; cadangan teg; pemfaktoran tensor"], [{"string": "Gaining insights into support vector machine pattern classifiers using projection-based tour methods This paper discusses visual methods that can be used to understand and interpret the results of classification using support vector machines SVM on data with continuous real-valued variables . SVM induction algorithms build pattern classifiers by identifying a maximal margin separating hyperplane from training examples in high dimensional pattern spaces or spaces induced by suitable nonlinear kernel transformations over pattern spaces . SVM have been demonstrated to be quite effective in a number of practical pattern classification tasks . Since the separating hyperplane is defined in terms of more than two variables it is necessary to use visual techniques that can navigate the viewer through high-dimensional spaces . We demonstrate the use of projection-based tour methods to gain useful insights into SVM classifiers with linear kernels on 8-dimensional data .", "keywords": ["tours", "multivariate data", "classification", "visualization", "machine leaning", "dynamic graphics", "support vector machines"], "combined": "Gaining insights into support vector machine pattern classifiers using projection-based tour methods This paper discusses visual methods that can be used to understand and interpret the results of classification using support vector machines SVM on data with continuous real-valued variables . SVM induction algorithms build pattern classifiers by identifying a maximal margin separating hyperplane from training examples in high dimensional pattern spaces or spaces induced by suitable nonlinear kernel transformations over pattern spaces . SVM have been demonstrated to be quite effective in a number of practical pattern classification tasks . Since the separating hyperplane is defined in terms of more than two variables it is necessary to use visual techniques that can navigate the viewer through high-dimensional spaces . We demonstrate the use of projection-based tour methods to gain useful insights into SVM classifiers with linear kernels on 8-dimensional data . [[EENNDD]] tours; multivariate data; classification; visualization; machine leaning; dynamic graphics; support vector machines"}, "Mendapatkan gambaran mengenai pengelasan corak mesin vektor sokongan menggunakan kaedah lawatan berasaskan unjuran Kertas ini membincangkan kaedah visual yang boleh digunakan untuk memahami dan menafsirkan hasil klasifikasi menggunakan mesin vektor sokongan SVM pada data dengan pemboleh ubah bernilai sebenar berterusan. Algoritma induksi SVM membina pengkelasan corak dengan mengenal pasti margin maksimum yang memisahkan hiperplan dari contoh latihan di ruang corak dimensi tinggi atau ruang yang disebabkan oleh transformasi kernel nonlinear yang sesuai di atas ruang pola. SVM telah terbukti cukup berkesan dalam beberapa tugas pengelasan corak praktikal. Oleh kerana hiperplan pemisah ditakrifkan dalam istilah lebih daripada dua pemboleh ubah, perlu menggunakan teknik visual yang dapat menavigasi penonton melalui ruang dimensi tinggi. Kami menunjukkan penggunaan kaedah lawatan berasaskan unjuran untuk mendapatkan pandangan berguna mengenai pengkelasan SVM dengan kernel linear pada data 8 dimensi. [[EENNDD]] lawatan; data multivariate; pengelasan; visualisasi; mesin bersandar; grafik dinamik; mesin vektor sokongan"], [{"string": "The minimum consistent subset cover problem and its applications in data mining In this paper , we introduce and study the Minimum Consistent Subset Cover MCSC problem . Given a finite ground set X and a constraint t , find the minimum number of consistent subsets that cover X , where a subset of X is consistent if it satisfies t. The MCSC problem generalizes the traditional set covering problem and has Minimum Clique Partition , a dual problem of graph coloring , as an instance . Many practical data mining problems in the areas of rule learning , clustering , and frequent pattern mining can be formulated as MCSC instances . In particular , we discuss the Minimum Rule Set problem that minimizes model complexity of decision rules as well as some converse k-clustering problems that minimize the number of clusters satisfying certain distance constraints . We also show how the MCSC problem can find applications in frequent pattern summarization . For any of these MCSC formulations , our proposed novel graph-based generic algorithm CAG can be directly applicable . CAG starts by constructing a maximal optimal partial solution , then performs an example-driven specific-to-general search on a dynamically maintained bipartite assignment graph to simultaneously learn a set of consistent subsets with small cardinality covering the ground set . Our experiments on benchmark datasets show that CAG achieves good results compared to existing popular heuristics .", "keywords": ["minimum consistent subset cover", "pattern summarization", "converse k-clustering", "minimum rule set"], "combined": "The minimum consistent subset cover problem and its applications in data mining In this paper , we introduce and study the Minimum Consistent Subset Cover MCSC problem . Given a finite ground set X and a constraint t , find the minimum number of consistent subsets that cover X , where a subset of X is consistent if it satisfies t. The MCSC problem generalizes the traditional set covering problem and has Minimum Clique Partition , a dual problem of graph coloring , as an instance . Many practical data mining problems in the areas of rule learning , clustering , and frequent pattern mining can be formulated as MCSC instances . In particular , we discuss the Minimum Rule Set problem that minimizes model complexity of decision rules as well as some converse k-clustering problems that minimize the number of clusters satisfying certain distance constraints . We also show how the MCSC problem can find applications in frequent pattern summarization . For any of these MCSC formulations , our proposed novel graph-based generic algorithm CAG can be directly applicable . CAG starts by constructing a maximal optimal partial solution , then performs an example-driven specific-to-general search on a dynamically maintained bipartite assignment graph to simultaneously learn a set of consistent subsets with small cardinality covering the ground set . Our experiments on benchmark datasets show that CAG achieves good results compared to existing popular heuristics . [[EENNDD]] minimum consistent subset cover; pattern summarization; converse k-clustering; minimum rule set"}, "Masalah perlindungan subset minimum yang konsisten dan aplikasinya dalam perlombongan data Dalam makalah ini, kami memperkenalkan dan mengkaji masalah Perlindungan Subset Konsisten Minimum MCSC. Memandangkan himpunan asas terhingga X dan kekangan t, cari bilangan minimum subset konsisten yang merangkumi X, di mana subset X konsisten jika memenuhi t. Masalah MCSC menyamaratakan set tradisional yang meliputi masalah dan mempunyai Partisi Clique Minimum, masalah ganda pewarnaan grafik, sebagai contoh. Banyak masalah perlombongan data praktikal di bidang pembelajaran peraturan, pengelompokan, dan perlombongan corak yang kerap dapat dirumuskan sebagai contoh MCSC. Secara khusus, kami membincangkan masalah Set Peraturan Minimum yang meminimumkan kerumitan model peraturan keputusan dan juga beberapa masalah k-kluster sebaliknya yang meminimumkan jumlah kluster yang memenuhi batasan jarak tertentu. Kami juga menunjukkan bagaimana masalah MCSC dapat mencari aplikasi dalam ringkasan corak yang kerap. Untuk mana-mana formulasi MCSC ini, CAG algoritma generik berasaskan grafik novel yang dicadangkan kami boleh digunakan secara langsung. CAG dimulakan dengan membina penyelesaian separa optimum yang maksimum, kemudian melakukan carian khusus-ke-umum berdasarkan contoh pada graf penugasan bipartit yang dikekalkan secara dinamik untuk mempelajari serentak subset yang konsisten dengan kardinaliti kecil yang meliputi permukaan tanah. Eksperimen kami pada set data penanda aras menunjukkan bahawa CAG mencapai hasil yang baik berbanding dengan heuristik popular yang ada. [[EENNDD]] penutup subset konsisten minimum; ringkasan corak; berkompang k-kluster; peraturan minimum ditetapkan"], [{"string": "Pattern-based similarity search for microarray data One fundamental task in near-neighbor search as well as other similarity matching efforts is to find a distance function that can efficiently quantify the similarity between two objects in a meaningful way . In DNA microarray analysis , the expression levels of two closely related genes may rise and fall synchronously in response to a set of experimental stimuli . Although the magnitude of their expression levels may not be close , the patterns they exhibit can be very similar . Unfortunately , none of the conventional distance metrics such as the Lp norm can model this similarity effectively . In this paper , we study the near-neighbor search problem based on this new type of similarity . We propose to measure the distance between two genes by subspace pattern similarity , i.e. , whether they exhibit a synchronous pattern of rise and fall on a subset of dimensions . We then present an efficient algorithm for subspace near-neighbor search based on pattern similarity distance , and we perform tests on various data sets to show its effectiveness .", "keywords": ["distance function", "pattern recognition", "near neighbor"], "combined": "Pattern-based similarity search for microarray data One fundamental task in near-neighbor search as well as other similarity matching efforts is to find a distance function that can efficiently quantify the similarity between two objects in a meaningful way . In DNA microarray analysis , the expression levels of two closely related genes may rise and fall synchronously in response to a set of experimental stimuli . Although the magnitude of their expression levels may not be close , the patterns they exhibit can be very similar . Unfortunately , none of the conventional distance metrics such as the Lp norm can model this similarity effectively . In this paper , we study the near-neighbor search problem based on this new type of similarity . We propose to measure the distance between two genes by subspace pattern similarity , i.e. , whether they exhibit a synchronous pattern of rise and fall on a subset of dimensions . We then present an efficient algorithm for subspace near-neighbor search based on pattern similarity distance , and we perform tests on various data sets to show its effectiveness . [[EENNDD]] distance function; pattern recognition; near neighbor"}, "Pencarian kesamaan berasaskan corak untuk data microarray Salah satu tugas asas dalam pencarian tetangga dekat dan usaha pencocokan kesamaan lain adalah mencari fungsi jarak yang dapat mengukur persamaan antara dua objek dengan berkesan dengan cara yang bermakna. Dalam analisis mikroarray DNA, tahap ekspresi dua gen yang berkait rapat dapat naik dan turun secara serentak sebagai tindak balas terhadap sekumpulan rangsangan eksperimen. Walaupun besarnya tahap ekspresi mereka mungkin tidak dekat, corak yang mereka tunjukkan sangat serupa. Malangnya, tidak ada metrik jarak konvensional seperti norma Lp yang dapat memodelkan persamaan ini dengan berkesan. Dalam makalah ini, kami mengkaji masalah pencarian tetangga dekat berdasarkan jenis persamaan baru ini. Kami mencadangkan untuk mengukur jarak antara dua gen dengan kesamaan corak ruang bawah, iaitu, sama ada mereka menunjukkan corak kenaikan dan jatuh pada subset dimensi. Kami kemudian membentangkan algoritma yang cekap untuk carian ruang bawah tanah dekat-jiran berdasarkan jarak kesamaan corak, dan kami melakukan ujian pada pelbagai set data untuk menunjukkan keberkesanannya. [[EENNDD]] fungsi jarak; pengecaman corak; berhampiran jiran"], [{"string": "Clustering moving objects Due to the advances in positioning technologies , the real time information of moving objects becomes increasingly available , which has posed new challenges to the database research . As a long-standing technique to identify overall distribution patterns in data , clustering has achieved brilliant successes in analyzing static datasets . In this paper , we study the problem of clustering moving objects , which could catch interesting pattern changes during the motion process and provide better insight into the essence of the mobile data points . In order to catch the spatial-temporal regularities of moving objects and handle large amounts of data , micro-clustering 20 is employed . Efficient techniques are proposed to keep the moving micro-clusters geographically small . Important events such as the collisions among moving micro-clusters are also identified . In this way , high quality moving micro-clusters are dynamically maintained , which leads to fast and competitive clustering result at any given time instance . We validate our approaches with a through experimental evaluation , where orders of magnitude improvement on running time is observed over normal K-Means clustering method 14 .", "keywords": ["moving object", "clustering", "micro-cluster"], "combined": "Clustering moving objects Due to the advances in positioning technologies , the real time information of moving objects becomes increasingly available , which has posed new challenges to the database research . As a long-standing technique to identify overall distribution patterns in data , clustering has achieved brilliant successes in analyzing static datasets . In this paper , we study the problem of clustering moving objects , which could catch interesting pattern changes during the motion process and provide better insight into the essence of the mobile data points . In order to catch the spatial-temporal regularities of moving objects and handle large amounts of data , micro-clustering 20 is employed . Efficient techniques are proposed to keep the moving micro-clusters geographically small . Important events such as the collisions among moving micro-clusters are also identified . In this way , high quality moving micro-clusters are dynamically maintained , which leads to fast and competitive clustering result at any given time instance . We validate our approaches with a through experimental evaluation , where orders of magnitude improvement on running time is observed over normal K-Means clustering method 14 . [[EENNDD]] moving object; clustering; micro-cluster"}, "Menggabungkan objek bergerak Kerana kemajuan teknologi penentududukan, maklumat masa nyata objek bergerak menjadi semakin tersedia, yang telah menimbulkan tantangan baru bagi penyelidikan pangkalan data. Sebagai teknik lama untuk mengenal pasti corak taburan keseluruhan dalam data, pengelompokan telah mencapai kejayaan cemerlang dalam menganalisis set data statik. Dalam makalah ini, kami mengkaji masalah pengelompokan objek bergerak, yang dapat menangkap perubahan corak yang menarik semasa proses gerakan dan memberikan wawasan yang lebih baik mengenai inti dari titik data bergerak. Untuk menangkap keteraturan spasial-temporal objek bergerak dan menangani sejumlah besar data, pengelompokan mikro 20 digunakan. Teknik yang cekap dicadangkan untuk memastikan kelompok mikro bergerak secara geografi tetap kecil. Kejadian penting seperti perlanggaran antara kelompok mikro bergerak juga dikenal pasti. Dengan cara ini, kluster mikro bergerak berkualiti tinggi dikekalkan secara dinamis, yang menghasilkan hasil pengelompokan cepat dan kompetitif pada waktu tertentu. Kami mengesahkan pendekatan kami dengan penilaian eksperimental melalui, di mana pesanan peningkatan magnitud pada masa berjalan diperhatikan berbanding kaedah pengelompokan K-Means 14 yang normal. [[EENNDD]] objek bergerak; pengelompokan; kluster mikro"], [{"string": "Targeting the right students using data mining", "keywords": ["data mining application in education", "target selection", "scoring"], "combined": "Targeting the right students using data mining [[EENNDD]] data mining application in education; target selection; scoring"}, "Menyasarkan pelajar yang betul menggunakan aplikasi perlombongan data [[EENNDD]] dalam pendidikan; pemilihan sasaran; pemarkahan"], [{"string": "DynaMMo : mining and summarization of coevolving sequences with missing values Given multiple time sequences with missing values , we propose DynaMMo which summarizes , compresses , and finds latent variables . The idea is to discover hidden variables and learn their dynamics , making our algorithm able to function even when there are missing values . We performed experiments on both real and synthetic datasets spanning several megabytes , including motion capture sequences and chlorine levels in drinking water . We show that our proposed DynaMMo method a can successfully learn the latent variables and their evolution ; b can provide high compression for little loss of reconstruction accuracy ; c can extract compact but powerful features for segmentation , interpretation , and forecasting ; d has complexity linear on the duration of sequences .", "keywords": ["time series", "missing value", "expectation maximization", "bayesian network"], "combined": "DynaMMo : mining and summarization of coevolving sequences with missing values Given multiple time sequences with missing values , we propose DynaMMo which summarizes , compresses , and finds latent variables . The idea is to discover hidden variables and learn their dynamics , making our algorithm able to function even when there are missing values . We performed experiments on both real and synthetic datasets spanning several megabytes , including motion capture sequences and chlorine levels in drinking water . We show that our proposed DynaMMo method a can successfully learn the latent variables and their evolution ; b can provide high compression for little loss of reconstruction accuracy ; c can extract compact but powerful features for segmentation , interpretation , and forecasting ; d has complexity linear on the duration of sequences . [[EENNDD]] time series; missing value; expectation maximization; bayesian network"}, "DynaMMo: perlombongan dan penjumlahan urutan koevolving dengan nilai yang hilang Memandangkan urutan masa dengan nilai yang hilang, kami mencadangkan DynaMMo yang merangkum, memampatkan, dan mencari pemboleh ubah laten. Ideanya adalah untuk menemui pemboleh ubah tersembunyi dan mempelajari dinamika, menjadikan algoritma kami dapat berfungsi walaupun terdapat nilai yang tidak ada. Kami melakukan eksperimen pada kumpulan data nyata dan sintetik yang merangkumi beberapa megabait, termasuk urutan tangkapan gerakan dan tahap klorin dalam air minum. Kami menunjukkan bahawa kaedah DynaMMo yang kami cadangkan dapat berjaya mempelajari pemboleh ubah pendam dan evolusi mereka; b dapat memberikan mampatan yang tinggi untuk kehilangan ketepatan pembinaan semula; c dapat mengekstrak ciri ringkas tetapi kuat untuk segmentasi, tafsiran, dan ramalan; d mempunyai kerumitan linear pada jangka masa urutan. [[EENNDD]] siri masa; hilang nilai; memaksimumkan jangkaan; rangkaian bayesian"], [{"string": "Multiple domain user personalization Content personalization is a key tool in creating attractive websites . Synergies can be obtained by integrating personalization between several Internet properties . In this paper we propose a hierarchical Bayesian model to address these issues . Our model allows the integration of multiple properties without changing the overall structure , which makes it easily extensible across large Internet portals . It relies at its lowest level on Latent Dirichlet Allocation , while making use of latent side features for cross-property integration . We demonstrate the efficiency of our approach by analyzing data from several properties of a major Internet portal .", "keywords": ["latent dirichlet allocation", "domain integration", "user profiling", "learning", "parallel statistical inference"], "combined": "Multiple domain user personalization Content personalization is a key tool in creating attractive websites . Synergies can be obtained by integrating personalization between several Internet properties . In this paper we propose a hierarchical Bayesian model to address these issues . Our model allows the integration of multiple properties without changing the overall structure , which makes it easily extensible across large Internet portals . It relies at its lowest level on Latent Dirichlet Allocation , while making use of latent side features for cross-property integration . We demonstrate the efficiency of our approach by analyzing data from several properties of a major Internet portal . [[EENNDD]] latent dirichlet allocation; domain integration; user profiling; learning; parallel statistical inference"}, "Pemperibadian pengguna pelbagai domain Pemperibadian kandungan adalah alat utama dalam membuat laman web yang menarik. Sinergi dapat diperoleh dengan mengintegrasikan pemperibadian antara beberapa sifat Internet. Dalam makalah ini kami mencadangkan model Bayesian yang hierarki untuk mengatasi masalah ini. Model kami membolehkan penyatuan pelbagai sifat tanpa mengubah keseluruhan struktur, yang menjadikannya mudah diperluas di portal Internet yang besar. Ia bergantung pada tahap terendah pada Latent Dirichlet Allocation, sambil menggunakan ciri sisi laten untuk integrasi lintas harta tanah. Kami menunjukkan kecekapan pendekatan kami dengan menganalisis data dari beberapa sifat sebuah portal Internet utama. [[EENNDD]] peruntukan laten laten; penyatuan domain; profil pengguna; belajar; inferens statistik selari"], [{"string": "Robust boosting and its relation to bagging Several authors have suggested viewing boosting as a gradient descent search for a good fit in function space . At each iteration observations are re-weighted using the gradient of the underlying loss function . We present an approach of weight decay for observation weights which is equivalent to `` robustifying '' the underlying loss function . At the extreme end of decay this approach converges to Bagging , which can be viewed as boosting with a linear underlying loss function . We illustrate the practical usefulness of weight decay for improving prediction performance and present an equivalence between one form of weight decay and `` Huberizing '' -- a statistical method for making loss functions more robust .", "keywords": ["boosting", "robust fitting", "design methodology", "bagging"], "combined": "Robust boosting and its relation to bagging Several authors have suggested viewing boosting as a gradient descent search for a good fit in function space . At each iteration observations are re-weighted using the gradient of the underlying loss function . We present an approach of weight decay for observation weights which is equivalent to `` robustifying '' the underlying loss function . At the extreme end of decay this approach converges to Bagging , which can be viewed as boosting with a linear underlying loss function . We illustrate the practical usefulness of weight decay for improving prediction performance and present an equivalence between one form of weight decay and `` Huberizing '' -- a statistical method for making loss functions more robust . [[EENNDD]] boosting; robust fitting; design methodology; bagging"}, "Peningkatan yang kuat dan kaitannya dengan pengantungan Beberapa penulis mencadangkan peningkatan tontonan sebagai penelusuran kecerunan untuk mencari ruang yang sesuai. Pada setiap pemerhatian pemerhatian ditimbang semula menggunakan kecerunan fungsi kerugian yang mendasari. Kami menyajikan pendekatan penurunan berat badan untuk berat pemerhatian yang setara dengan \"memperkuat\" fungsi kehilangan yang mendasari. Pada akhir pembusukan yang melampau, pendekatan ini menyatu dengan Bagging, yang dapat dilihat sebagai peningkatan dengan fungsi kerugian yang mendasari linear. Kami menggambarkan kegunaan praktikal penurunan berat badan untuk meningkatkan prestasi ramalan dan menunjukkan persamaan antara satu bentuk penurunan berat badan dan \"Huberizing\" - kaedah statistik untuk menjadikan fungsi penurunan lebih kuat. [[EENNDD]] meningkatkan; pemasangan yang kukuh; metodologi reka bentuk; mengantong"], [{"string": "Variable latent semantic indexing Latent Semantic Indexing is a classical method to produce optimal low-rank approximations of a term-document matrix . However , in the context of a particular query distribution , the approximation thus produced need not be optimal . We propose VLSI , a new query-dependent or `` variable '' low-rank approximation that minimizes approximation error for any specified query distribution . With this tool , it is possible to tailor the LSI technique to particular settings , often resulting in vastly improved approximations at much lower dimensionality . We validate this method via a series of experiments on classical corpora , showing that VLSI typically performs similarly to LSI with an order of magnitude fewer dimensions .", "keywords": ["lsi", "linear algebra", "vlsi", "sparse, structured, and very large systems", "matrix approximation", "svd"], "combined": "Variable latent semantic indexing Latent Semantic Indexing is a classical method to produce optimal low-rank approximations of a term-document matrix . However , in the context of a particular query distribution , the approximation thus produced need not be optimal . We propose VLSI , a new query-dependent or `` variable '' low-rank approximation that minimizes approximation error for any specified query distribution . With this tool , it is possible to tailor the LSI technique to particular settings , often resulting in vastly improved approximations at much lower dimensionality . We validate this method via a series of experiments on classical corpora , showing that VLSI typically performs similarly to LSI with an order of magnitude fewer dimensions . [[EENNDD]] lsi; linear algebra; vlsi; sparse, structured, and very large systems; matrix approximation; svd"}, "Pengindeksan semantik laten berubah-ubah Pengindeksan Semantik Laten adalah kaedah klasik untuk menghasilkan penghampiran peringkat rendah optimum dari matriks term-dokumen. Namun, dalam konteks sebaran pertanyaan tertentu, pendekatan yang dihasilkan tidak perlu optimal. Kami mencadangkan VLSI, penghampiran peringkat rendah yang bergantung kepada pertanyaan atau \"variabel\" yang meminimumkan ralat penghampiran untuk sebarang pengedaran pertanyaan yang ditentukan. Dengan alat ini, adalah mungkin untuk menyesuaikan teknik LSI ke tetapan tertentu, yang sering menghasilkan perkiraan yang jauh lebih baik pada dimensi yang jauh lebih rendah. Kami mengesahkan kaedah ini melalui satu siri eksperimen pada corpora klasik, yang menunjukkan bahawa VLSI biasanya berfungsi sama dengan LSI dengan susunan magnitud yang lebih sedikit dimensi. [[EENNDD]] lsi; aljabar linear; vlsi; sistem yang jarang, tersusun, dan sangat besar; penghampiran matriks; svd"], [{"string": "Statistical entity-topic models The primary purpose of news articles is to convey information about who , what , when and where . But learning and summarizing these relationships for collections of thousands to millions of articles is difficult . While statistical topic models have been highly successful at topically summarizing huge collections of text documents , they do not explicitly address the textual interactions between who\\/where , i.e. named entities persons , organizations , locations and what , i.e. the topics . We present new graphical models that directly learn the relationship between topics discussed in news articles and entities mentioned in each article . We show how these entity-topic models , through a better understanding of the entity-topic relationships , are better at making predictions about entities .", "keywords": ["probabilistic algorithms", "topic modeling", "miscellaneous", "text modeling", "entity recognition"], "combined": "Statistical entity-topic models The primary purpose of news articles is to convey information about who , what , when and where . But learning and summarizing these relationships for collections of thousands to millions of articles is difficult . While statistical topic models have been highly successful at topically summarizing huge collections of text documents , they do not explicitly address the textual interactions between who\\/where , i.e. named entities persons , organizations , locations and what , i.e. the topics . We present new graphical models that directly learn the relationship between topics discussed in news articles and entities mentioned in each article . We show how these entity-topic models , through a better understanding of the entity-topic relationships , are better at making predictions about entities . [[EENNDD]] probabilistic algorithms; topic modeling; miscellaneous; text modeling; entity recognition"}, "Model statistik entiti-topik Tujuan utama artikel berita adalah untuk menyampaikan maklumat mengenai siapa, apa, kapan dan di mana. Tetapi belajar dan meringkaskan hubungan ini untuk koleksi beribu-ribu hingga berjuta-juta artikel adalah sukar. Walaupun model topik statistik sangat berjaya dalam merumuskan koleksi dokumen teks secara topikal, mereka tidak secara eksplisit menangani interaksi teks antara siapa \\ / di mana, iaitu menamakan entiti orang, organisasi, lokasi dan apa topiknya. Kami menyajikan model grafik baru yang secara langsung mempelajari hubungan antara topik yang dibincangkan dalam artikel berita dan entiti yang disebutkan dalam setiap artikel. Kami menunjukkan bagaimana model topik entiti ini, melalui pemahaman yang lebih baik mengenai hubungan topik-entiti, lebih baik dalam membuat ramalan mengenai entiti. [[EENNDD]] algoritma probabilistik; pemodelan topik; pelbagai; pemodelan teks; pengiktirafan entiti"], [{"string": "Collective annotation of Wikipedia entities in web text To take the first step beyond keyword-based search toward entity-based search , suitable token spans `` spots '' on documents must be identified as references to real-world entities from an entity catalog . Several systems have been proposed to link spots on Web pages to entities in Wikipedia . They are largely based on local compatibility between the text around the spot and textual metadata associated with the entity . Two recent systems exploit inter-label dependencies , but in limited ways . We propose a general collective disambiguation approach . Our premise is that coherent documents refer to entities from one or a few related topics or domains . We give formulations for the trade-off between local spot-to-entity compatibility and measures of global coherence between entities . Optimizing the overall entity assignment is NP-hard . We investigate practical solutions based on local hill-climbing , rounding integer linear programs , and pre-clustering entities followed by local optimization within clusters . In experiments involving over a hundred manually-annotated Web pages and tens of thousands of spots , our approaches significantly outperform recently-proposed algorithms .", "keywords": ["wikipedia", "collective inference", "information search and retrieval", "entity annotation/disambiguation"], "combined": "Collective annotation of Wikipedia entities in web text To take the first step beyond keyword-based search toward entity-based search , suitable token spans `` spots '' on documents must be identified as references to real-world entities from an entity catalog . Several systems have been proposed to link spots on Web pages to entities in Wikipedia . They are largely based on local compatibility between the text around the spot and textual metadata associated with the entity . Two recent systems exploit inter-label dependencies , but in limited ways . We propose a general collective disambiguation approach . Our premise is that coherent documents refer to entities from one or a few related topics or domains . We give formulations for the trade-off between local spot-to-entity compatibility and measures of global coherence between entities . Optimizing the overall entity assignment is NP-hard . We investigate practical solutions based on local hill-climbing , rounding integer linear programs , and pre-clustering entities followed by local optimization within clusters . In experiments involving over a hundred manually-annotated Web pages and tens of thousands of spots , our approaches significantly outperform recently-proposed algorithms . [[EENNDD]] wikipedia; collective inference; information search and retrieval; entity annotation/disambiguation"}, "Anotasi kolektif entiti Wikipedia dalam teks web Untuk mengambil langkah pertama melampaui carian berdasarkan kata kunci ke arah pencarian berdasarkan entiti, tanda token yang sesuai merangkumi \"tempat\" pada dokumen mesti dikenal pasti sebagai rujukan kepada entiti dunia nyata dari katalog entiti. Beberapa sistem telah diusulkan untuk menghubungkan tempat di laman Web dengan entiti di Wikipedia. Ini sebahagian besarnya berdasarkan keserasian tempatan antara teks di sekitar tempat dan metadata teks yang berkaitan dengan entiti. Dua sistem baru-baru ini memanfaatkan kebergantungan antara label, tetapi dengan cara yang terhad. Kami mencadangkan pendekatan disambiguasi kolektif umum. Premis kami adalah bahawa dokumen yang koheren merujuk kepada entiti dari satu atau beberapa topik atau domain yang berkaitan. Kami memberikan rumusan untuk pertukaran antara keserasian spot-to-entiti tempatan dan langkah-langkah kesatuan global antara entiti. Mengoptimumkan keseluruhan penugasan entiti sukar dilakukan. Kami menyiasat penyelesaian praktikal berdasarkan pendakian bukit tempatan, program linear integer bulat, dan entiti pra-kluster diikuti dengan pengoptimuman tempatan dalam kelompok. Dalam eksperimen yang melibatkan lebih dari seratus halaman Web yang diberi anotasi secara manual dan berpuluh-puluh ribu tempat, pendekatan kami mengatasi algoritma yang baru dicadangkan. [[EENNDD]] wikipedia; inferens kolektif; carian dan pengambilan maklumat; anotasi entiti / disambiguasi"], [{"string": "Empirical Bayesian data mining for discovering patterns in post-marketing drug safety Because of practical limits in characterizing the safety profiles of therapeutic products prior to marketing , manufacturers and regulatory agencies perform post-marketing surveillance based on the collection of adverse reaction reports `` pharmacovigilance '' . The resulting databases , while rich in real-world information , are notoriously difficult to analyze using traditional techniques . Each report may involve multiple medicines , symptoms , and demographic factors , and there is no easily linked information on drug exposure in the reporting population . KDD techniques , such as association finding , are well-matched to the problem , but are difficult for medical staff to apply and interpret . To deploy KDD effectively for pharmacovigilance , Lincoln Technologies and GlaxoSmithKline collaborated to create a webbased safety data mining web environment . The analytical core is a high-performance implementation of the MGPS Multi-Item Gamma Poisson Shrinker algorithm described previously by DuMouchel and Pregibon , with several significant extensions and enhancements . The environment offers an interface for specifying data mining runs , a batch execution facility , tabular and graphical methods for exploring associations , and drilldown to case details . Substantial work was involved in preparing the raw adverse event data for mining , including harmonization of drug names and removal of duplicate reports . The environment can be used to explore both drug-event and multi-way associations interactions , syndromes . It has been used to study age\\/gender effects , to predict the safety profiles of proposed combination drugs , and to separate contributions of individual drugs to safety problems in polytherapy situations .", "keywords": ["post-marketing surveillance", "empirical bayes methods", "pharmacovigilance", "association rules"], "combined": "Empirical Bayesian data mining for discovering patterns in post-marketing drug safety Because of practical limits in characterizing the safety profiles of therapeutic products prior to marketing , manufacturers and regulatory agencies perform post-marketing surveillance based on the collection of adverse reaction reports `` pharmacovigilance '' . The resulting databases , while rich in real-world information , are notoriously difficult to analyze using traditional techniques . Each report may involve multiple medicines , symptoms , and demographic factors , and there is no easily linked information on drug exposure in the reporting population . KDD techniques , such as association finding , are well-matched to the problem , but are difficult for medical staff to apply and interpret . To deploy KDD effectively for pharmacovigilance , Lincoln Technologies and GlaxoSmithKline collaborated to create a webbased safety data mining web environment . The analytical core is a high-performance implementation of the MGPS Multi-Item Gamma Poisson Shrinker algorithm described previously by DuMouchel and Pregibon , with several significant extensions and enhancements . The environment offers an interface for specifying data mining runs , a batch execution facility , tabular and graphical methods for exploring associations , and drilldown to case details . Substantial work was involved in preparing the raw adverse event data for mining , including harmonization of drug names and removal of duplicate reports . The environment can be used to explore both drug-event and multi-way associations interactions , syndromes . It has been used to study age\\/gender effects , to predict the safety profiles of proposed combination drugs , and to separate contributions of individual drugs to safety problems in polytherapy situations . [[EENNDD]] post-marketing surveillance; empirical bayes methods; pharmacovigilance; association rules"}, "Perlombongan data Empirical Bayesian untuk mengetahui corak keselamatan ubat pasca pemasaran Kerana had praktikal dalam mencirikan profil keselamatan produk terapi sebelum pemasaran, pengeluar dan agensi pengawalseliaan melakukan pengawasan pasca pemasaran berdasarkan pengumpulan laporan reaksi buruk \"farmakovigilance\" '. Pangkalan data yang dihasilkan, walaupun kaya dengan maklumat dunia nyata, sukar untuk dianalisis menggunakan teknik tradisional. Setiap laporan mungkin melibatkan pelbagai ubat, gejala, dan faktor demografi, dan tidak ada maklumat yang mudah dihubungkan mengenai pendedahan ubat pada populasi pelapor. Teknik KDD, seperti penemuan persatuan, cocok dengan masalah tersebut, tetapi sukar bagi staf perubatan untuk menerapkan dan menafsirkan. Untuk menyebarkan KDD secara berkesan untuk farmakovigilance, Lincoln Technologies dan GlaxoSmithKline bekerjasama untuk mewujudkan persekitaran web perlombongan data keselamatan berasaskan web. Inti analitik adalah pelaksanaan berprestasi tinggi dari algoritma MGPS Multi-Item Gamma Poisson Shrinker yang dijelaskan sebelumnya oleh DuMouchel dan Pregibon, dengan beberapa peluasan dan peningkatan yang ketara. Persekitaran menawarkan antara muka untuk menentukan jalan keluar data mining, kemudahan pelaksanaan kumpulan, kaedah jadual dan grafik untuk meneroka persatuan, dan penjelasan mengenai perincian kes. Kerja penting terlibat dalam penyediaan data kejadian buruk mentah untuk perlombongan, termasuk penyelarasan nama ubat dan penghapusan laporan pendua. Persekitaran boleh digunakan untuk meneroka interaksi, sindrom peristiwa-dadah dan persatuan pelbagai arah. Telah digunakan untuk mengkaji kesan usia / jantina, untuk meramalkan profil keselamatan ubat gabungan yang dicadangkan, dan untuk memisahkan sumbangan ubat individu terhadap masalah keselamatan dalam situasi politerapi. [[EENNDD]] pengawasan pasca pemasaran; kaedah bayes empirikal; farmakovigilance; peraturan persatuan"], [{"string": "Locating secret messages in images Steganography involves hiding messages in innocuous media such as images , while steganalysis is the field of detecting these secret messages . The ultimate goal of steganalysis is two-fold : making a binary classification of a file as stego-bearing or innocent , and secondly , locating the hidden message with an aim to extracting , sterilizing or manipulating it . Almost all steganalysis approaches known as attacks focus on the first of these two issues . In this paper , we explore the difficult related problem : given that we know an image file contains steganography , locate which pixels contain the message . We treat the hidden message location problem as outlier detection using probability\\/energy measures of images motivated by the image restoration community . Pixels contributing the most to the energy calculations of an image are deemed outliers . Typically , of the top third of one percent of most energized pixels outliers , we find that 87 % are stego-bearing in color images and 61 % in grayscale images . In all image types only 1 % of all pixels are stego-bearing indicating our techniques provides a substantial lift over random guessing .", "keywords": ["steganalysis", "outlier detection", "steganography"], "combined": "Locating secret messages in images Steganography involves hiding messages in innocuous media such as images , while steganalysis is the field of detecting these secret messages . The ultimate goal of steganalysis is two-fold : making a binary classification of a file as stego-bearing or innocent , and secondly , locating the hidden message with an aim to extracting , sterilizing or manipulating it . Almost all steganalysis approaches known as attacks focus on the first of these two issues . In this paper , we explore the difficult related problem : given that we know an image file contains steganography , locate which pixels contain the message . We treat the hidden message location problem as outlier detection using probability\\/energy measures of images motivated by the image restoration community . Pixels contributing the most to the energy calculations of an image are deemed outliers . Typically , of the top third of one percent of most energized pixels outliers , we find that 87 % are stego-bearing in color images and 61 % in grayscale images . In all image types only 1 % of all pixels are stego-bearing indicating our techniques provides a substantial lift over random guessing . [[EENNDD]] steganalysis; outlier detection; steganography"}, "Mencari mesej rahsia dalam gambar Steganografi melibatkan penyembunyian mesej di media yang tidak berbahaya seperti gambar, sementara steganalisis adalah bidang mengesan mesej rahsia ini. Matlamat utama steganalisis adalah dua kali ganda: menjadikan klasifikasi binari fail sebagai stego-bearings atau tidak bersalah, dan kedua, mencari mesej tersembunyi dengan tujuan untuk mengekstrak, mensterilkan atau memanipulasinya. Hampir semua pendekatan steganalisis yang dikenali sebagai serangan memfokuskan pada yang pertama dari dua masalah ini. Dalam makalah ini, kami meneroka masalah yang sukar: kerana kami tahu fail gambar mengandungi steganografi, cari piksel mana yang mengandungi mesej. Kami menganggap masalah lokasi mesej tersembunyi sebagai pengesanan luar menggunakan kebarangkalian \\ / tindakan tenaga gambar yang dimotivasi oleh komuniti pemulihan imej. Piksel yang paling banyak menyumbang kepada pengiraan tenaga pada gambar dianggap sebagai garis besar. Lazimnya, dari sepertiga teratas dari satu peratus daripada pangkalan piksel yang paling bertenaga, kami mendapati bahawa 87% mempunyai gambar warna yang stego dan 61% pada gambar skala kelabu. Dalam semua jenis gambar, hanya 1% dari semua piksel yang mengandungi stego yang menunjukkan teknik kami memberikan peningkatan yang besar berbanding dengan tekaan rawak. [[EENNDD]] steganalisis; pengesanan luar; steganografi"], [{"string": "Factorization meets the neighborhood : a multifaceted collaborative filtering model Recommender systems provide users with personalized suggestions for products or services . These systems often rely on Collaborating Filtering CF , where past transactions are analyzed in order to establish connections between users and products . The two more successful approaches to CF are latent factor models , which directly profile both users and products , and neighborhood models , which analyze similarities between products or users . In this work we introduce some innovations to both approaches . The factor and neighborhood models can now be smoothly merged , thereby building a more accurate combined model . Further accuracy improvements are achieved by extending the models to exploit both explicit and implicit feedback by the users . The methods are tested on the Netflix data . Results are better than those previously published on that dataset . In addition , we suggest a new evaluation metric , which highlights the differences among methods , based on their performance at a top-K recommendation task .", "keywords": ["collaborative filtering", "recommender systems"], "combined": "Factorization meets the neighborhood : a multifaceted collaborative filtering model Recommender systems provide users with personalized suggestions for products or services . These systems often rely on Collaborating Filtering CF , where past transactions are analyzed in order to establish connections between users and products . The two more successful approaches to CF are latent factor models , which directly profile both users and products , and neighborhood models , which analyze similarities between products or users . In this work we introduce some innovations to both approaches . The factor and neighborhood models can now be smoothly merged , thereby building a more accurate combined model . Further accuracy improvements are achieved by extending the models to exploit both explicit and implicit feedback by the users . The methods are tested on the Netflix data . Results are better than those previously published on that dataset . In addition , we suggest a new evaluation metric , which highlights the differences among methods , based on their performance at a top-K recommendation task . [[EENNDD]] collaborative filtering; recommender systems"}, "Pemfaktoran memenuhi kejiranan: model penapisan kolaboratif pelbagai aspek Sistem penyedia menyediakan pengguna dengan cadangan yang diperibadikan untuk produk atau perkhidmatan. Sistem ini sering bergantung pada Collaborating Filtering CF, di mana transaksi masa lalu dianalisis untuk menjalin hubungan antara pengguna dan produk. Dua pendekatan yang lebih berjaya untuk CF adalah model faktor pendam, yang secara langsung profil pengguna dan produk, dan model kejiranan, yang menganalisis persamaan antara produk atau pengguna. Dalam karya ini kami memperkenalkan beberapa inovasi untuk kedua pendekatan. Model faktor dan kejiranan kini dapat digabungkan dengan lancar, dengan itu membina model gabungan yang lebih tepat. Peningkatan ketepatan selanjutnya dicapai dengan memperluas model untuk memanfaatkan maklum balas eksplisit dan implisit oleh pengguna. Kaedah diuji pada data Netflix. Hasilnya lebih baik daripada yang diterbitkan sebelumnya pada set data tersebut. Sebagai tambahan, kami mencadangkan metrik penilaian baru, yang menyoroti perbezaan antara kaedah, berdasarkan prestasi mereka pada tugas cadangan K-top. [[EENNDD]] penapisan kolaboratif; sistem cadangan"], [{"string": "Discovering evolutionary theme patterns from text : an exploration of temporal text mining Temporal Text Mining TTM is concerned with discovering temporal patterns in text information collected over time . Since most text information bears some time stamps , TTM has many applications in multiple domains , such as summarizing events in news articles and revealing research trends in scientific literature . In this paper , we study a particular TTM task -- discovering and summarizing the evolutionary patterns of themes in a text stream . We define this new text mining problem and present general probabilistic methods for solving this problem through 1 discovering latent themes from text ; 2 constructing an evolution graph of themes ; and 3 analyzing life cycles of themes . Evaluation of the proposed methods on two different domains i.e. , news articles and literature shows that the proposed methods can discover interesting evolutionary theme patterns effectively .", "keywords": ["theme threads", "temporal text mining", "evolutionary theme patterns"], "combined": "Discovering evolutionary theme patterns from text : an exploration of temporal text mining Temporal Text Mining TTM is concerned with discovering temporal patterns in text information collected over time . Since most text information bears some time stamps , TTM has many applications in multiple domains , such as summarizing events in news articles and revealing research trends in scientific literature . In this paper , we study a particular TTM task -- discovering and summarizing the evolutionary patterns of themes in a text stream . We define this new text mining problem and present general probabilistic methods for solving this problem through 1 discovering latent themes from text ; 2 constructing an evolution graph of themes ; and 3 analyzing life cycles of themes . Evaluation of the proposed methods on two different domains i.e. , news articles and literature shows that the proposed methods can discover interesting evolutionary theme patterns effectively . [[EENNDD]] theme threads; temporal text mining; evolutionary theme patterns"}, "Mencari corak tema evolusi dari teks: penerokaan perlombongan teks temporal Temporal Text Mining TTM berkaitan dengan penemuan corak temporal dalam maklumat teks yang dikumpulkan dari masa ke masa. Oleh kerana kebanyakan maklumat teks mempunyai cap waktu, TTM memiliki banyak aplikasi dalam beberapa domain, seperti meringkas peristiwa dalam artikel berita dan mengungkapkan trend penyelidikan dalam literatur ilmiah. Dalam makalah ini, kami mempelajari tugas TTM tertentu - mencari dan meringkaskan pola evolusi tema dalam aliran teks. Kami menentukan masalah perlombongan teks baru ini dan mengemukakan kaedah probabilistik umum untuk menyelesaikan masalah ini melalui 1 menemui tema terpendam dari teks; 2 membina grafik evolusi tema; dan 3 menganalisis kitaran hidup tema. Penilaian kaedah yang dicadangkan pada dua domain berbeza, artikel berita dan literatur menunjukkan bahawa kaedah yang dicadangkan dapat menemui corak tema evolusi yang menarik dengan berkesan. [[EENNDD]] utas tema; perlombongan teks temporal; corak tema evolusi"], [{"string": "Event detection from evolution of click-through data Previous efforts on event detection from the web have focused primarily on web content and structure data ignoring the rich collection of web log data . In this paper , we propose the first approach to detect events from the click-through data , which is the log data of web search engines . The intuition behind event detection from click-through data is that such data is often event-driven and each event can be represented as a set ofquery-page pairs that are not only semantically similar but also have similar evolution pattern over time . Given the click-through data , in our proposed approach , we first segment it into a sequence of bipartite graphs based on theuser-defined time granularity . Next , the sequence of bipartite graphs is represented as a vector-based graph , which records the semantic and evolutionary relationships between queries and pages . After that , the vector-based graph is transformed into its dual graph , where each node is a query-page pair that will be used to represent real world events . Then , the problem of event detection is equivalent to the problem of clustering the dual graph of the vector-based graph . The clustering process is based on a two-phase graph cut algorithm . In the first phase , query-page pairs are clustered based on thesemantic-based similarity such that each cluster in the result corresponds to a specific topic . In the second phase , query-page pairs related to the same topic are further clustered based on the evolution pattern-based similarity such that each cluster is expected to represent a specific event under the specific topic . Experiments with real click-through data collected from a commercial web search engine show that the proposed approach produces high quality results .", "keywords": ["evolution pattern", "event detection", "dynamic web", "click-through data"], "combined": "Event detection from evolution of click-through data Previous efforts on event detection from the web have focused primarily on web content and structure data ignoring the rich collection of web log data . In this paper , we propose the first approach to detect events from the click-through data , which is the log data of web search engines . The intuition behind event detection from click-through data is that such data is often event-driven and each event can be represented as a set ofquery-page pairs that are not only semantically similar but also have similar evolution pattern over time . Given the click-through data , in our proposed approach , we first segment it into a sequence of bipartite graphs based on theuser-defined time granularity . Next , the sequence of bipartite graphs is represented as a vector-based graph , which records the semantic and evolutionary relationships between queries and pages . After that , the vector-based graph is transformed into its dual graph , where each node is a query-page pair that will be used to represent real world events . Then , the problem of event detection is equivalent to the problem of clustering the dual graph of the vector-based graph . The clustering process is based on a two-phase graph cut algorithm . In the first phase , query-page pairs are clustered based on thesemantic-based similarity such that each cluster in the result corresponds to a specific topic . In the second phase , query-page pairs related to the same topic are further clustered based on the evolution pattern-based similarity such that each cluster is expected to represent a specific event under the specific topic . Experiments with real click-through data collected from a commercial web search engine show that the proposed approach produces high quality results . [[EENNDD]] evolution pattern; event detection; dynamic web; click-through data"}, "Pengesanan peristiwa dari evolusi data klik-tayang Usaha sebelumnya dalam pengesanan peristiwa dari web telah memfokuskan terutamanya pada kandungan web dan data struktur mengabaikan koleksi data log web yang kaya. Dalam makalah ini, kami mencadangkan pendekatan pertama untuk mengesan peristiwa dari data klik-tayang, yang merupakan data log dari mesin pencari web. Intuisi di sebalik pengesanan peristiwa dari data klik-tayang adalah bahawa data tersebut sering didorong oleh peristiwa dan setiap peristiwa dapat ditunjukkan sebagai sekumpulan pasangan halaman pertanyaan yang tidak hanya semantik mirip tetapi juga mempunyai corak evolusi yang serupa dari masa ke masa. Memandangkan data klik-tayang, dalam pendekatan yang dicadangkan, pertama-tama kami membahagikannya kepada urutan graf bipartit berdasarkan butiran masa yang ditentukan pengguna. Seterusnya, urutan graf bipartit ditunjukkan sebagai grafik berasaskan vektor, yang mencatat hubungan semantik dan evolusi antara pertanyaan dan halaman. Selepas itu, grafik berasaskan vektor diubah menjadi dua grafiknya, di mana setiap simpul adalah pasangan halaman pertanyaan yang akan digunakan untuk mewakili peristiwa dunia nyata. Kemudian, masalah pengesanan peristiwa bersamaan dengan masalah pengelompokan graf dua dari grafik berasaskan vektor. Proses pengelompokan dibuat berdasarkan algoritma pemotongan graf dua fasa. Pada fasa pertama, pasangan halaman pertanyaan dikelompokkan berdasarkan kesamaan berdasarkan tematik sehingga setiap kelompok dalam hasilnya sesuai dengan topik tertentu. Pada fasa kedua, pasangan halaman pertanyaan yang berkaitan dengan topik yang sama selanjutnya dikelompokkan berdasarkan kesamaan berdasarkan pola evolusi sehingga setiap kelompok diharapkan dapat mewakili peristiwa tertentu di bawah topik tertentu. Eksperimen dengan data klik-tayang sebenar yang dikumpulkan dari mesin carian web komersial menunjukkan bahawa pendekatan yang dicadangkan menghasilkan hasil yang berkualiti tinggi. [[EENNDD]] corak evolusi; pengesanan peristiwa; web dinamik; data klik-tayang"], [{"string": "Generative model-based clustering of directional data High dimensional directional data is becoming increasingly important in contemporary applications such as analysis of text and gene-expression data . A natural model for multi-variate directional data is provided by the von Mises-Fisher vMF distribution on the unit hypersphere that is analogous to the multi-variate Gaussian distribution in Rd. . In this paper , we propose modeling complex directional data as a mixture of vMF distributions . We derive and analyze two variants of the Expectation Maximization EM framework for estimating the parameters of this mixture . We also propose two clustering algorithms corresponding to these variants . An interesting aspect of our methodology is that the spherical kmeans algorithm kmeans with cosine similarity can be shown to be a special case of both our algorithms . Thus , modeling text data by vMF distributions lends theoretical validity to the use of cosine similarity which has been widely used by the information retrieval community . As part of experimental validation , we present results on modeling high-dimensional text and gene-expression data as a mixture of vMF distributions . The results indicate that our approach yields superior clusterings especially for difficult clustering tasks in high-dimensional spaces .", "keywords": ["von mises-fisher", "information search and retrieval", "em", "directional data", "mixtures", "clustering"], "combined": "Generative model-based clustering of directional data High dimensional directional data is becoming increasingly important in contemporary applications such as analysis of text and gene-expression data . A natural model for multi-variate directional data is provided by the von Mises-Fisher vMF distribution on the unit hypersphere that is analogous to the multi-variate Gaussian distribution in Rd. . In this paper , we propose modeling complex directional data as a mixture of vMF distributions . We derive and analyze two variants of the Expectation Maximization EM framework for estimating the parameters of this mixture . We also propose two clustering algorithms corresponding to these variants . An interesting aspect of our methodology is that the spherical kmeans algorithm kmeans with cosine similarity can be shown to be a special case of both our algorithms . Thus , modeling text data by vMF distributions lends theoretical validity to the use of cosine similarity which has been widely used by the information retrieval community . As part of experimental validation , we present results on modeling high-dimensional text and gene-expression data as a mixture of vMF distributions . The results indicate that our approach yields superior clusterings especially for difficult clustering tasks in high-dimensional spaces . [[EENNDD]] von mises-fisher; information search and retrieval; em; directional data; mixtures; clustering"}, "Penggabungan data arah berasaskan model generatif Data arah dimensi tinggi menjadi semakin penting dalam aplikasi kontemporari seperti analisis teks dan data ekspresi gen. Model semula jadi untuk data arah pelbagai-variasi disediakan oleh taburan von Mises-Fisher vMF pada unit hipersfera yang serupa dengan taburan Gaussian pelbagai-variasi dalam Rd. . Dalam makalah ini, kami mencadangkan pemodelan data arah kompleks sebagai campuran pengedaran vMF. Kami memperoleh dan menganalisis dua varian kerangka EM Pemaksaan Jangkaan untuk menganggarkan parameter campuran ini. Kami juga mencadangkan dua algoritma pengelompokan yang sesuai dengan varian ini. Aspek yang menarik dari metodologi kami adalah bahawa algoritma kmeans sfera kmeans dengan kesamaan kosinus dapat ditunjukkan sebagai kes khas kedua algoritma kami. Oleh itu, pemodelan data teks oleh distribusi vMF memberikan kesahan teori terhadap penggunaan kesamaan kosinus yang telah banyak digunakan oleh komuniti pencarian maklumat. Sebagai sebahagian daripada pengesahan eksperimen, kami menyajikan hasil pemodelan teks dimensi tinggi dan data ekspresi gen sebagai campuran pengedaran vMF. Hasilnya menunjukkan bahawa pendekatan kami menghasilkan pengelompokan yang unggul terutama untuk tugas pengelompokan yang sukar di ruang dimensi tinggi. [[EENNDD]] von mises-fisher; carian dan pengambilan maklumat; em; data arah; campuran; pengelompokan"], [{"string": "Adaptive duplicate detection using learnable string similarity measures The problem of identifying approximately duplicate records in databases is an essential step for data cleaning and data integration processes . Most existing approaches have relied on generic or manually tuned distance metrics for estimating the similarity of potential duplicates . In this paper , we present a framework for improving duplicate detection using trainable measures of textual similarity . We propose to employ learnable text distance functions for each database field , and show that such measures are capable of adapting to the specific notion of similarity that is appropriate for the field 's domain . We present two learnable text similarity measures suitable for this task : an extended variant of learnable string edit distance , and a novel vector-space based measure that employs a Support Vector Machine SVM for training . Experimental results on a range of datasets show that our framework can improve duplicate detection accuracy over traditional techniques .", "keywords": ["distance metric learning", "svm applications", "learning", "database applications", "data cleaning", "string edit distance", "trained similarity measures", "record linkage"], "combined": "Adaptive duplicate detection using learnable string similarity measures The problem of identifying approximately duplicate records in databases is an essential step for data cleaning and data integration processes . Most existing approaches have relied on generic or manually tuned distance metrics for estimating the similarity of potential duplicates . In this paper , we present a framework for improving duplicate detection using trainable measures of textual similarity . We propose to employ learnable text distance functions for each database field , and show that such measures are capable of adapting to the specific notion of similarity that is appropriate for the field 's domain . We present two learnable text similarity measures suitable for this task : an extended variant of learnable string edit distance , and a novel vector-space based measure that employs a Support Vector Machine SVM for training . Experimental results on a range of datasets show that our framework can improve duplicate detection accuracy over traditional techniques . [[EENNDD]] distance metric learning; svm applications; learning; database applications; data cleaning; string edit distance; trained similarity measures; record linkage"}, "Pengesanan pendua adaptif menggunakan langkah-langkah kesamaan rentetan yang dapat dipelajari Masalah untuk mengenal pasti kira-kira pendua catatan dalam pangkalan data adalah langkah penting untuk proses pembersihan data dan penyatuan data. Sebilangan besar pendekatan yang ada bergantung pada metrik jarak generik atau manual yang ditala untuk menganggar kesamaan potensi pendua. Dalam makalah ini, kami memaparkan kerangka kerja untuk meningkatkan pengesanan pendua menggunakan ukuran kesamaan teks yang dapat dilatih. Kami mengusulkan untuk menggunakan fungsi jarak teks yang dapat dipelajari untuk setiap bidang pangkalan data, dan menunjukkan bahawa langkah-langkah tersebut dapat menyesuaikan diri dengan gagasan kesamaan tertentu yang sesuai untuk domain bidang. Kami membentangkan dua ukuran kesamaan teks yang dapat dipelajari yang sesuai untuk tugas ini: varian jarak jauh dari rentetan rentetan yang dapat dipelajari, dan ukuran berdasarkan ruang vektor baru yang menggunakan SVM Mesin Vektor Sokongan untuk latihan. Hasil eksperimen pada pelbagai set data menunjukkan bahawa kerangka kerja kami dapat meningkatkan ketepatan pengesanan pendua berbanding teknik tradisional. [[EENNDD]] pembelajaran metrik jarak jauh; aplikasi svm; belajar; aplikasi pangkalan data; pembersihan data; jarak edit rentetan; langkah persamaan terlatih; pautan rakaman"], [{"string": "Logical-shapelets : an expressive primitive for time series classification Time series shapelets are small , local patterns in a time series that are highly predictive of a class and are thus very useful features for building classifiers and for certain visualization and summarization tasks . While shapelets were introduced only recently , they have already seen significant adoption and extension in the community . Despite their immense potential as a data mining primitive , there are two important limitations of shapelets . First , their expressiveness is limited to simple binary presence\\/absence questions . Second , even though shapelets are computed offline , the time taken to compute them is significant . In this work , we address the latter problem by introducing a novel algorithm that finds shapelets in less time than current methods by an order of magnitude . Our algorithm is based on intelligent caching and reuse of computations , and the admissible pruning of the search space . Because our algorithm is so fast , it creates an opportunity to consider more expressive shapelet queries . In particular , we show for the first time an augmented shapelet representation that distinguishes the data based on conjunctions or disjunctions of shapelets . We call our novel representation Logical-Shapelets . We demonstrate the efficiency of our approach on the classic benchmark datasets used for these problems , and show several case studies where logical shapelets significantly outperform the original shapelet representation and other time series classification techniques . We demonstrate the utility of our ideas in domains as diverse as gesture recognition , robotics , and biometrics .", "keywords": ["information gain", "decision tree", "time series", "classification"], "combined": "Logical-shapelets : an expressive primitive for time series classification Time series shapelets are small , local patterns in a time series that are highly predictive of a class and are thus very useful features for building classifiers and for certain visualization and summarization tasks . While shapelets were introduced only recently , they have already seen significant adoption and extension in the community . Despite their immense potential as a data mining primitive , there are two important limitations of shapelets . First , their expressiveness is limited to simple binary presence\\/absence questions . Second , even though shapelets are computed offline , the time taken to compute them is significant . In this work , we address the latter problem by introducing a novel algorithm that finds shapelets in less time than current methods by an order of magnitude . Our algorithm is based on intelligent caching and reuse of computations , and the admissible pruning of the search space . Because our algorithm is so fast , it creates an opportunity to consider more expressive shapelet queries . In particular , we show for the first time an augmented shapelet representation that distinguishes the data based on conjunctions or disjunctions of shapelets . We call our novel representation Logical-Shapelets . We demonstrate the efficiency of our approach on the classic benchmark datasets used for these problems , and show several case studies where logical shapelets significantly outperform the original shapelet representation and other time series classification techniques . We demonstrate the utility of our ideas in domains as diverse as gesture recognition , robotics , and biometrics . [[EENNDD]] information gain; decision tree; time series; classification"}, "Logical-shapelets: primitif ekspresif untuk klasifikasi siri masa Shapelet siri masa kecil, corak tempatan dalam siri masa yang sangat meramalkan kelas dan dengan itu merupakan ciri yang sangat berguna untuk membina pengklasifikasi dan untuk tugas visualisasi dan ringkasan tertentu. Walaupun shapelets diperkenalkan baru-baru ini, mereka telah melihat penggunaan dan pengembangan yang signifikan dalam masyarakat. Walaupun potensinya sangat besar sebagai primitif perlombongan data, ada dua batasan penting dari kapel. Pertama, ekspresi mereka terhad kepada soalan kehadiran / ketiadaan perduaan sederhana. Kedua, walaupun shapelets dihitung di luar talian, masa yang diperlukan untuk menghitungnya adalah penting. Dalam karya ini, kita mengatasi masalah yang terakhir dengan memperkenalkan algoritma novel yang menemukan shapelets dalam masa yang lebih sedikit daripada kaedah semasa dengan susunan besarnya. Algoritma kami berdasarkan caching pintar dan penggunaan semula pengiraan, dan pemangkasan ruang carian yang boleh diterima. Oleh kerana algoritma kami begitu pantas, ini mewujudkan peluang untuk mempertimbangkan pertanyaan shapelet yang lebih ekspresif. Khususnya, kami menunjukkan untuk pertama kalinya perwakilan shapelet tambahan yang membezakan data berdasarkan konjungsi atau gangguan shapelet. Kami memanggil representasi novel kami Logical-Shapelets. Kami menunjukkan kecekapan pendekatan kami pada set data penanda aras klasik yang digunakan untuk masalah ini, dan menunjukkan beberapa kajian kes di mana shapelets logik secara signifikan mengatasi perwakilan shapelet yang asli dan teknik klasifikasi siri masa yang lain. Kami menunjukkan kegunaan idea kami dalam domain yang beragam seperti pengenalan isyarat, robotik, dan biometrik. [[EENNDD]] perolehan maklumat; pokok keputusan; siri masa; pengelasan"], [{"string": "Practical guide to controlled experiments on the web : listen to your customers not to the hippo The web provides an unprecedented opportunity to evaluate ideas quickly using controlled experiments , also called randomized experiments single factor or factorial designs , A\\/B tests and their generalizations , split tests , Control\\/Treatment tests , and parallel flights . Controlled experiments embody the best scientific design for establishing a causal relationship between changes and their influence on user-observable behavior . We provide a practical guide to conducting online experiments , where end-users can help guide the development of features . Our experience indicates that significant learning and return-on-investment ROI are seen when development teams listen to their customers , not to the Highest Paid Person 's Opinion HiPPO . We provide several examples of controlled experiments with surprising results . We review the important ingredients of running controlled experiments , and discuss their limitations both technical and organizational . We focus on several areas that are critical to experimentation , including statistical power , sample size , and techniques for variance reduction . We describe common architectures for experimentation systems and analyze their advantages and disadvantages . We evaluate randomization and hashing techniques , which we show are not as simple in practice as is often assumed . Controlled experiments typically generate large amounts of data , which can be analyzed using data mining techniques to gain deeper understanding of the factors influencing the outcome of interest , leading to new hypotheses and creating a virtuous cycle of improvements . Organizations that embrace controlled experiments with clear evaluation criteria can evolve their systems with automated optimizations and real-time analyses . Based on our extensive practical experience with multiple systems and organizations , we share key lessons that will help practitioners in running trustworthy controlled experiments .", "keywords": ["e-commerce", "learning", "a/b testing", "controlled experiments"], "combined": "Practical guide to controlled experiments on the web : listen to your customers not to the hippo The web provides an unprecedented opportunity to evaluate ideas quickly using controlled experiments , also called randomized experiments single factor or factorial designs , A\\/B tests and their generalizations , split tests , Control\\/Treatment tests , and parallel flights . Controlled experiments embody the best scientific design for establishing a causal relationship between changes and their influence on user-observable behavior . We provide a practical guide to conducting online experiments , where end-users can help guide the development of features . Our experience indicates that significant learning and return-on-investment ROI are seen when development teams listen to their customers , not to the Highest Paid Person 's Opinion HiPPO . We provide several examples of controlled experiments with surprising results . We review the important ingredients of running controlled experiments , and discuss their limitations both technical and organizational . We focus on several areas that are critical to experimentation , including statistical power , sample size , and techniques for variance reduction . We describe common architectures for experimentation systems and analyze their advantages and disadvantages . We evaluate randomization and hashing techniques , which we show are not as simple in practice as is often assumed . Controlled experiments typically generate large amounts of data , which can be analyzed using data mining techniques to gain deeper understanding of the factors influencing the outcome of interest , leading to new hypotheses and creating a virtuous cycle of improvements . Organizations that embrace controlled experiments with clear evaluation criteria can evolve their systems with automated optimizations and real-time analyses . Based on our extensive practical experience with multiple systems and organizations , we share key lessons that will help practitioners in running trustworthy controlled experiments . [[EENNDD]] e-commerce; learning; a/b testing; controlled experiments"}, "Panduan praktikal untuk eksperimen terkawal di web: dengarkan pelanggan anda dan jangan sampai hippo. Web memberikan peluang yang belum pernah terjadi sebelumnya untuk menilai idea dengan cepat menggunakan eksperimen terkawal, juga disebut eksperimen rawak faktor tunggal atau reka bentuk faktorial, ujian A \\ / B dan generalisasi mereka, ujian perpecahan, ujian Kawalan / / rawatan, dan penerbangan selari. Eksperimen terkawal merangkumi reka bentuk saintifik terbaik untuk mewujudkan hubungan kausal antara perubahan dan pengaruhnya terhadap tingkah laku yang dapat dilihat oleh pengguna. Kami menyediakan panduan praktikal untuk menjalankan eksperimen dalam talian, di mana pengguna akhir dapat membantu memandu pengembangan ciri. Pengalaman kami menunjukkan bahawa ROI pembelajaran dan pengembalian pelaburan yang ketara dilihat ketika pasukan pengembangan mendengarkan pelanggan mereka, bukan dengan HiPPO Pendapat Orang Berbayar Tertinggi. Kami memberikan beberapa contoh eksperimen terkawal dengan hasil yang mengejutkan. Kami mengkaji bahan penting dalam menjalankan eksperimen terkawal, dan membincangkan batasannya baik dari segi teknikal dan organisasi. Kami memberi tumpuan kepada beberapa bidang yang sangat penting untuk eksperimen, termasuk kekuatan statistik, ukuran sampel, dan teknik untuk pengurangan varians. Kami menerangkan seni bina biasa untuk sistem eksperimen dan menganalisis kelebihan dan kekurangannya. Kami menilai teknik rawak dan hash, yang kami tunjukkan tidak sesederhana dalam praktik seperti yang sering diandaikan. Eksperimen terkawal biasanya menghasilkan sejumlah besar data, yang dapat dianalisis menggunakan teknik perlombongan data untuk mendapatkan pemahaman yang lebih mendalam mengenai faktor-faktor yang mempengaruhi hasil minat, yang membawa kepada hipotesis baru dan membuat kitaran peningkatan yang baik. Organisasi yang merangkumi eksperimen terkawal dengan kriteria penilaian yang jelas dapat mengembangkan sistem mereka dengan pengoptimuman automatik dan analisis masa nyata. Berdasarkan pengalaman praktikal kami yang luas dengan pelbagai sistem dan organisasi, kami berkongsi pelajaran penting yang akan membantu pengamal menjalankan eksperimen terkawal yang boleh dipercayai. [[EENNDD]] e-dagang; belajar; ujian a / b; eksperimen terkawal"], [{"string": "GBASE : a scalable and general graph management system Graphs appear in numerous applications including cyber-security , the Internet , social networks , protein networks , recommendation systems , and many more . Graphs with millions or even billions of nodes and edges are common-place . How to store such large graphs efficiently ? What are the core operations\\/queries on those graph ? How to answer the graph queries quickly ? We propose GBASE , a scalable and general graph management and mining system . The key novelties lie in 1 our storage and compression scheme for a parallel setting and 2 the carefully chosen graph operations and their efficient implementation . We designed and implemented an instance of GBASE using MapReduce\\/Hadoop . GBASE provides a parallel indexing mechanism for graph mining operations that both saves storage space , as well as accelerates queries . We ran numerous experiments on real graphs , spanning billions of nodes and edges , and we show that our proposed GBASE is indeed fast , scalable and nimble , with significant savings in space and time .", "keywords": ["graph", "compression", "indexing", "distributed computing"], "combined": "GBASE : a scalable and general graph management system Graphs appear in numerous applications including cyber-security , the Internet , social networks , protein networks , recommendation systems , and many more . Graphs with millions or even billions of nodes and edges are common-place . How to store such large graphs efficiently ? What are the core operations\\/queries on those graph ? How to answer the graph queries quickly ? We propose GBASE , a scalable and general graph management and mining system . The key novelties lie in 1 our storage and compression scheme for a parallel setting and 2 the carefully chosen graph operations and their efficient implementation . We designed and implemented an instance of GBASE using MapReduce\\/Hadoop . GBASE provides a parallel indexing mechanism for graph mining operations that both saves storage space , as well as accelerates queries . We ran numerous experiments on real graphs , spanning billions of nodes and edges , and we show that our proposed GBASE is indeed fast , scalable and nimble , with significant savings in space and time . [[EENNDD]] graph; compression; indexing; distributed computing"}, "GBASE: sistem pengurusan grafik yang berskala dan umum Grafik muncul dalam banyak aplikasi termasuk keselamatan siber, Internet, rangkaian sosial, rangkaian protein, sistem cadangan, dan banyak lagi. Grafik dengan jutaan atau bahkan berbilion nod dan tepi adalah tempat yang biasa. Bagaimana untuk menyimpan graf besar seperti itu dengan cekap? Apakah inti operasi \\ / pertanyaan pada grafik tersebut? Bagaimana cara menjawab pertanyaan grafik dengan cepat? Kami mencadangkan GBASE, sistem grafik dan pengurusan perlombongan yang berskala dan umum. Perkara baru terletak pada 1 skema penyimpanan dan pemampatan kami untuk pengaturan selari dan 2 operasi grafik yang dipilih dengan teliti dan pelaksanaannya yang cekap. Kami merancang dan melaksanakan contoh GBASE menggunakan MapReduce \\ / Hadoop. GBASE menyediakan mekanisme pengindeksan selari untuk operasi perlombongan grafik yang menjimatkan ruang simpanan, dan juga mempercepat pertanyaan. Kami menjalankan banyak eksperimen pada grafik nyata, merangkumi berbilion nod dan pinggir, dan kami menunjukkan bahawa GBASE yang dicadangkan kami memang pantas, berskala dan pantas, dengan penjimatan ruang dan masa yang ketara. [[EENNDD]] graf; pemampatan; pengindeksan; pengkomputeran diedarkan"], [{"string": "Towards bounding sequential patterns Given a sequence database , can we have a non-trivial upper bound on the number of sequential patterns ? The problem of bounding sequential patterns is very challenging in theory due to the combinatorial complexity of sequences , even given some inspiring results on bounding itemsets in frequent itemset mining . Moreover , the problem is highly meaningful in practice , since the upper bound can be used in many applications such as space allocation in building sequence data warehouses . In this paper , we tackle the problem of bounding sequential patterns by presenting , for the first time in the field of sequential pattern mining , strong combinatorial results on computing the number of possible sequential patterns that can be generated at a given length k. We introduce , as a case study , two novel techniques to estimate the number of candidate sequences . An extensive empirical study on both real data and synthetic data verifies the effectiveness of our methods .", "keywords": ["sequential pattern mining", "combinatorics"], "combined": "Towards bounding sequential patterns Given a sequence database , can we have a non-trivial upper bound on the number of sequential patterns ? The problem of bounding sequential patterns is very challenging in theory due to the combinatorial complexity of sequences , even given some inspiring results on bounding itemsets in frequent itemset mining . Moreover , the problem is highly meaningful in practice , since the upper bound can be used in many applications such as space allocation in building sequence data warehouses . In this paper , we tackle the problem of bounding sequential patterns by presenting , for the first time in the field of sequential pattern mining , strong combinatorial results on computing the number of possible sequential patterns that can be generated at a given length k. We introduce , as a case study , two novel techniques to estimate the number of candidate sequences . An extensive empirical study on both real data and synthetic data verifies the effectiveness of our methods . [[EENNDD]] sequential pattern mining; combinatorics"}, "Menuju pola urutan yang berurutan Diberi pangkalan data urutan, dapatkah kita memiliki batas atas yang tidak sepele pada jumlah pola urutan? Masalah mengikat corak jujukan sangat mencabar dari segi teori kerana kerumitan urutan gabungan, bahkan diberikan beberapa hasil yang menginspirasi untuk mengikat itemset dalam perlombongan itemet yang kerap. Lebih-lebih lagi, masalah ini sangat bermakna dalam praktiknya, kerana batas atas dapat digunakan dalam banyak aplikasi seperti peruntukan ruang dalam gudang data urutan bangunan. Dalam makalah ini, kami mengatasi masalah mengikat pola berurutan dengan membentangkan, untuk pertama kalinya dalam bidang perlombongan pola berurutan, hasil gabungan yang kuat untuk menghitung jumlah kemungkinan pola berurutan yang dapat dihasilkan pada panjang tertentu k. Kami memperkenalkan, sebagai kajian kes, dua teknik baru untuk menganggarkan jumlah urutan calon. Kajian empirikal yang luas mengenai data sebenar dan data sintetik mengesahkan keberkesanan kaedah kami. [[EENNDD]] perlombongan corak jujukan; penggabungan"], [{"string": "Extracting shared subspace for multi-label classification Multi-label problems arise in various domains such as multi-topic document categorization and protein function prediction . One natural way to deal with such problems is to construct a binary classifier for each label , resulting in a set of independent binary classification problems . Since the multiple labels share the same input space , and the semantics conveyed by different labels are usually correlated , it is essential to exploit the correlation information contained in different labels . In this paper , we consider a general framework for extracting shared structures in multi-label classification . In this framework , a common subspace is assumed to be shared among multiple labels . We show that the optimal solution to the proposed formulation can be obtained by solving a generalized eigenvalue problem , though the problem is non-convex . For high-dimensional problems , direct computation of the solution is expensive , and we develop an efficient algorithm for this case . One appealing feature of the proposed framework is that it includes several well-known algorithms as special cases , thus elucidating their intrinsic relationships . We have conducted extensive experiments on eleven multi-topic web page categorization tasks , and results demonstrate the effectiveness of the proposed formulation in comparison with several representative algorithms .", "keywords": ["least squares", "shared subspace", "multi-label classification"], "combined": "Extracting shared subspace for multi-label classification Multi-label problems arise in various domains such as multi-topic document categorization and protein function prediction . One natural way to deal with such problems is to construct a binary classifier for each label , resulting in a set of independent binary classification problems . Since the multiple labels share the same input space , and the semantics conveyed by different labels are usually correlated , it is essential to exploit the correlation information contained in different labels . In this paper , we consider a general framework for extracting shared structures in multi-label classification . In this framework , a common subspace is assumed to be shared among multiple labels . We show that the optimal solution to the proposed formulation can be obtained by solving a generalized eigenvalue problem , though the problem is non-convex . For high-dimensional problems , direct computation of the solution is expensive , and we develop an efficient algorithm for this case . One appealing feature of the proposed framework is that it includes several well-known algorithms as special cases , thus elucidating their intrinsic relationships . We have conducted extensive experiments on eleven multi-topic web page categorization tasks , and results demonstrate the effectiveness of the proposed formulation in comparison with several representative algorithms . [[EENNDD]] least squares; shared subspace; multi-label classification"}, "Mengekstrak ruang bawah yang dikongsi untuk klasifikasi pelbagai label Masalah pelbagai label timbul dalam pelbagai domain seperti pengkategorian dokumen berbilang topik dan ramalan fungsi protein. Salah satu cara semula jadi untuk mengatasi masalah tersebut adalah dengan membina pengkelasan binari untuk setiap label, yang menghasilkan satu set masalah pengelasan binari bebas. Oleh kerana pelbagai label mempunyai ruang input yang sama, dan semantik yang disampaikan oleh label yang berlainan biasanya berkorelasi, sangat penting untuk memanfaatkan maklumat korelasi yang terdapat dalam label yang berlainan. Dalam makalah ini, kami mempertimbangkan kerangka umum untuk mengekstraksi struktur bersama dalam klasifikasi pelbagai label. Dalam kerangka ini, ruang bawah biasa dianggap dapat dikongsi di antara beberapa label. Kami menunjukkan bahawa penyelesaian optimum untuk rumusan yang diusulkan dapat diperoleh dengan menyelesaikan masalah nilai eigen yang umum, walaupun masalahnya tidak cembung. Untuk masalah dimensi tinggi, pengiraan langsung penyelesaiannya mahal, dan kami mengembangkan algoritma yang cekap untuk kes ini. Satu ciri menarik dari kerangka kerja yang dicadangkan adalah bahawa ia merangkumi beberapa algoritma terkenal sebagai kes khas, sehingga menjelaskan hubungan intrinsik mereka. Kami telah melakukan eksperimen yang luas pada sebelas tugas pengkategorian halaman web berbilang topik, dan hasilnya menunjukkan keberkesanan rumusan yang dicadangkan dibandingkan dengan beberapa algoritma perwakilan. [[EENNDD]] petak paling sedikit; ruang bawah kongsi; klasifikasi pelbagai label"], [{"string": "Designing efficient cascaded classifiers : tradeoff between accuracy and cost We propose a method to train a cascade of classifiers by simultaneously optimizing all its stages . The approach relies on the idea of optimizing soft cascades . In particular , instead of optimizing a deterministic hard cascade , we optimize a stochastic soft cascade where each stage accepts or rejects samples according to a probability distribution induced by the previous stage-specific classifier . The overall system accuracy is maximized while explicitly controlling the expected cost for feature acquisition . Experimental results on three clinically relevant problems show the effectiveness of our proposed approach in achieving the desired tradeoff between accuracy and feature acquisition cost .", "keywords": ["cost sensitive learning", "accuracy vs cost", "cascade design"], "combined": "Designing efficient cascaded classifiers : tradeoff between accuracy and cost We propose a method to train a cascade of classifiers by simultaneously optimizing all its stages . The approach relies on the idea of optimizing soft cascades . In particular , instead of optimizing a deterministic hard cascade , we optimize a stochastic soft cascade where each stage accepts or rejects samples according to a probability distribution induced by the previous stage-specific classifier . The overall system accuracy is maximized while explicitly controlling the expected cost for feature acquisition . Experimental results on three clinically relevant problems show the effectiveness of our proposed approach in achieving the desired tradeoff between accuracy and feature acquisition cost . [[EENNDD]] cost sensitive learning; accuracy vs cost; cascade design"}, "Merancang pengklasifikasi lata cekap: pertukaran antara ketepatan dan kos Kami mencadangkan kaedah untuk melatih lata pengklasifikasi dengan mengoptimumkan semua peringkatnya secara serentak. Pendekatan itu bergantung pada idea mengoptimumkan kaskade lembut. Khususnya, daripada mengoptimumkan kaskade keras deterministik, kita mengoptimumkan kaskade lembut stokastik di mana setiap peringkat menerima atau menolak sampel mengikut taburan kebarangkalian yang disebabkan oleh pengklasifikasi tahap-tahap sebelumnya. Ketepatan keseluruhan sistem dimaksimumkan sambil secara eksplisit mengawal jangkaan kos untuk pemerolehan ciri. Hasil eksperimen pada tiga masalah yang berkaitan secara klinikal menunjukkan keberkesanan pendekatan yang dicadangkan kami dalam mencapai pertukaran yang diinginkan antara ketepatan dan kos pemerolehan ciri. [[EENNDD]] pembelajaran sensitif kos; ketepatan vs kos; reka bentuk lata"], [{"string": "Indexing multi-dimensional time-series with support for multiple distance measures Although most time-series data mining research has concentrated on providing solutions for a single distance function , in this work we motivate the need for a single index structure that can support multiple distance measures . Our specific area of interest is the efficient retrieval and analysis of trajectory similarities . Trajectory datasets are very common in environmental applications , mobility experiments , video surveillance and are especially important for the discovery of certain biological patterns . Our primary similarity measure is based on the Longest Common Subsequence LCSS model , that offers enhanced robustness , particularly for noisy data , which are encountered very often in real world applications . However , our index is able to accommodate other distance measures as well , including the ubiquitous Euclidean distance , and the increasingly popular Dynamic Time Warping DTW . While other researchers have advocated one or other of these similarity measures , a major contribution of our work is the ability to support all these measures without the need to restructure the index . Our framework guarantees no false dismissals and can also be tailored to provide much faster response time at the expense of slightly reduced precision\\/recall . The experimental results demonstrate that our index can help speed-up the computation of expensive similarity measures such as the LCSS and the DTW .", "keywords": ["dynamic time warping", "longest common subsequence", "trajectories", "database applications"], "combined": "Indexing multi-dimensional time-series with support for multiple distance measures Although most time-series data mining research has concentrated on providing solutions for a single distance function , in this work we motivate the need for a single index structure that can support multiple distance measures . Our specific area of interest is the efficient retrieval and analysis of trajectory similarities . Trajectory datasets are very common in environmental applications , mobility experiments , video surveillance and are especially important for the discovery of certain biological patterns . Our primary similarity measure is based on the Longest Common Subsequence LCSS model , that offers enhanced robustness , particularly for noisy data , which are encountered very often in real world applications . However , our index is able to accommodate other distance measures as well , including the ubiquitous Euclidean distance , and the increasingly popular Dynamic Time Warping DTW . While other researchers have advocated one or other of these similarity measures , a major contribution of our work is the ability to support all these measures without the need to restructure the index . Our framework guarantees no false dismissals and can also be tailored to provide much faster response time at the expense of slightly reduced precision\\/recall . The experimental results demonstrate that our index can help speed-up the computation of expensive similarity measures such as the LCSS and the DTW . [[EENNDD]] dynamic time warping; longest common subsequence; trajectories; database applications"}, "Mengindeks siri masa pelbagai dimensi dengan sokongan untuk ukuran jarak jauh Walaupun kebanyakan penyelidikan perlombongan data siri masa telah menumpukan perhatian pada penyediaan penyelesaian untuk fungsi jarak satu, dalam karya ini kami memotivasi perlunya struktur indeks tunggal yang dapat menyokong beberapa ukuran jarak . Bidang minat khusus kami adalah pengambilan dan analisis persamaan lintasan yang cekap. Set data lintasan sangat umum dalam aplikasi persekitaran, eksperimen mobiliti, pengawasan video dan sangat penting untuk penemuan corak biologi tertentu. Ukuran kesamaan utama kami didasarkan pada model LCSS Pengikut Umum Terpanjang, yang menawarkan kekuatan yang lebih baik, terutama untuk data yang bising, yang sering ditemui dalam aplikasi dunia nyata. Walau bagaimanapun, indeks kami juga dapat menampung ukuran jarak lain, termasuk jarak Euclidean di mana-mana, dan Dynamic Time Warping DTW yang semakin popular. Walaupun penyelidik lain telah menganjurkan satu atau lain dari langkah-langkah kesamaan ini, sumbangan utama pekerjaan kami adalah kemampuan untuk menyokong semua langkah ini tanpa perlu menyusun semula indeks. Kerangka kerja kami menjamin tidak ada pemecatan yang salah dan juga dapat disesuaikan untuk memberikan masa tindak balas yang lebih cepat dengan mengorbankan ketepatan yang sedikit berkurang \\ / ingat. Hasil eksperimen menunjukkan bahawa indeks kami dapat membantu mempercepat pengiraan ukuran kesamaan yang mahal seperti LCSS dan DTW. [[EENNDD]] masa yang dinamik melengkung; kebiasaan paling lama; lintasan; aplikasi pangkalan data"], [{"string": "Constant-factor approximation algorithms for identifying dynamic communities We propose two approximation algorithms for identifying communities in dynamic social networks . Communities are intuitively characterized as `` unusually densely knit '' subsets of a social network . This notion becomes more problematic if the social interactions change over time . Aggregating social networks over time can radically misrepresent the existing and changing community structure . Recently , we have proposed an optimization-based framework for modeling dynamic community structure . Also , we have proposed an algorithm for finding such structure based on maximum weight bipartite matching . In this paper , we analyze its performance guarantee for a special case where all actors can be observed at all times . In such instances , we show that the algorithm is a small constant factor approximation of the optimum . We use a similar idea to design an approximation algorithm for the general case where some individuals are possibly unobserved at times , and to show that the approximation factor increases twofold but remains a constant regardless of the input size . This is the first algorithm for inferring communities in dynamic networks with a provable approximation guarantee . We demonstrate the general algorithm on real data sets . The results confirm the efficiency and effectiveness of the algorithm in identifying dynamic communities .", "keywords": ["community identification", "approximation algorithms", "dynamic social networks", "general"], "combined": "Constant-factor approximation algorithms for identifying dynamic communities We propose two approximation algorithms for identifying communities in dynamic social networks . Communities are intuitively characterized as `` unusually densely knit '' subsets of a social network . This notion becomes more problematic if the social interactions change over time . Aggregating social networks over time can radically misrepresent the existing and changing community structure . Recently , we have proposed an optimization-based framework for modeling dynamic community structure . Also , we have proposed an algorithm for finding such structure based on maximum weight bipartite matching . In this paper , we analyze its performance guarantee for a special case where all actors can be observed at all times . In such instances , we show that the algorithm is a small constant factor approximation of the optimum . We use a similar idea to design an approximation algorithm for the general case where some individuals are possibly unobserved at times , and to show that the approximation factor increases twofold but remains a constant regardless of the input size . This is the first algorithm for inferring communities in dynamic networks with a provable approximation guarantee . We demonstrate the general algorithm on real data sets . The results confirm the efficiency and effectiveness of the algorithm in identifying dynamic communities . [[EENNDD]] community identification; approximation algorithms; dynamic social networks; general"}, "Algoritma penghampiran faktor berterusan untuk mengenal pasti komuniti dinamik Kami mencadangkan dua algoritma penghampiran untuk mengenal pasti komuniti dalam rangkaian sosial dinamik. Komuniti secara intuitif dicirikan sebagai subkumpulan jaringan sosial yang \"sangat rajin\". Gagasan ini menjadi lebih bermasalah sekiranya interaksi sosial berubah dari masa ke masa. Gabungan rangkaian sosial dari masa ke masa secara radikal dapat memberi gambaran yang salah mengenai struktur masyarakat yang ada dan berubah. Baru-baru ini, kami telah mencadangkan kerangka kerja berasaskan pengoptimuman untuk memodelkan struktur komuniti yang dinamik. Juga, kami telah mencadangkan algoritma untuk mencari struktur sedemikian berdasarkan padanan bipartit berat maksimum. Dalam makalah ini, kami menganalisis jaminan prestasinya untuk kes khas di mana semua pelakon dapat diperhatikan setiap saat. Dalam keadaan seperti ini, kami menunjukkan bahawa algoritma adalah penghampiran faktor pemalar kecil yang optimum. Kami menggunakan idea yang serupa untuk merancang algoritma penghampiran untuk kes umum di mana sebilangan individu mungkin tidak diperhatikan kadang-kadang, dan untuk menunjukkan bahawa faktor penghampiran meningkat dua kali ganda tetapi tetap tetap tanpa mengira ukuran input. Ini adalah algoritma pertama untuk menyimpulkan komuniti dalam rangkaian dinamik dengan jaminan penghampiran yang dapat dibuktikan. Kami menunjukkan algoritma umum pada set data sebenar. Hasilnya mengesahkan kecekapan dan keberkesanan algoritma dalam mengenal pasti komuniti yang dinamik. [[EENNDD]] pengenalan masyarakat; algoritma penghampiran; rangkaian sosial yang dinamik; umum"], [{"string": "Mining data records in Web pages A large amount of information on the Web is contained in regularly structured objects , which we call data records . Such data records are important because they often present the essential information of their host pages , e.g. , lists of products or services . It is useful to mine such data records in order to extract information from them to provide value-added services . Existing automatic techniques are not satisfactory because of their poor accuracies . In this paper , we propose a more effective technique to perform the task . The technique is based on two observations about data records on the Web and a string matching algorithm . The proposed technique is able to mine both contiguous and non-contiguous data records . Our experimental results show that the proposed technique outperforms existing techniques substantially .", "keywords": ["web information integration", "web mining", "web data records", "database applications"], "combined": "Mining data records in Web pages A large amount of information on the Web is contained in regularly structured objects , which we call data records . Such data records are important because they often present the essential information of their host pages , e.g. , lists of products or services . It is useful to mine such data records in order to extract information from them to provide value-added services . Existing automatic techniques are not satisfactory because of their poor accuracies . In this paper , we propose a more effective technique to perform the task . The technique is based on two observations about data records on the Web and a string matching algorithm . The proposed technique is able to mine both contiguous and non-contiguous data records . Our experimental results show that the proposed technique outperforms existing techniques substantially . [[EENNDD]] web information integration; web mining; web data records; database applications"}, "Melombong catatan data di halaman Web Sejumlah besar maklumat di Web terkandung dalam objek berstruktur secara teratur, yang kami sebut sebagai catatan data. Rekod data sedemikian penting kerana mereka sering menyampaikan maklumat penting dari halaman hos mereka, misalnya , senarai produk atau perkhidmatan. Adalah berguna untuk melombong rekod data tersebut untuk mengekstrak maklumat dari mereka untuk memberikan perkhidmatan tambah nilai. Teknik automatik yang ada tidak memuaskan kerana ketepatannya yang kurang baik. Dalam makalah ini, kami mencadangkan teknik yang lebih berkesan untuk melaksanakan tugas. Teknik ini berdasarkan dua pemerhatian mengenai catatan data di Web dan algoritma pemadanan tali. Teknik yang dicadangkan dapat melengkapkan rekod data bersebelahan dan tidak bersebelahan. Hasil eksperimen kami menunjukkan bahawa teknik yang dicadangkan mengatasi teknik yang ada dengan ketara. [[EENNDD]] penyatuan maklumat web; perlombongan web; rekod data web; aplikasi pangkalan data"], [{"string": "Generating succinct titles for web URLs How can a search engine automatically provide the best and most appropriate title for a result URL link-title so that users will be persuaded to click on the URL ? We consider the problem of automatically generating link-titles for URLs and propose a general statistical framework for solving this problem . The framework is based on using information from a diverse collection of sources , each of which can be thought of as contributing one or more candidate link-titles for the URL . It can also incorporate the context in which the link-title will be used , along with constraints on its length . Our framework is applicable to several scenarios : obtaining succinct titles for displaying quicklinks , obtaining titles for URLs that lack a good title , constructing succinct sitemaps , etc. . Extensive experiments show that our method is very effective , producing results that are at least 20 % better than non-trivial baselines .", "keywords": ["web page title generation", "quicklinks", "information search and retrieval", "sitemaps"], "combined": "Generating succinct titles for web URLs How can a search engine automatically provide the best and most appropriate title for a result URL link-title so that users will be persuaded to click on the URL ? We consider the problem of automatically generating link-titles for URLs and propose a general statistical framework for solving this problem . The framework is based on using information from a diverse collection of sources , each of which can be thought of as contributing one or more candidate link-titles for the URL . It can also incorporate the context in which the link-title will be used , along with constraints on its length . Our framework is applicable to several scenarios : obtaining succinct titles for displaying quicklinks , obtaining titles for URLs that lack a good title , constructing succinct sitemaps , etc. . Extensive experiments show that our method is very effective , producing results that are at least 20 % better than non-trivial baselines . [[EENNDD]] web page title generation; quicklinks; information search and retrieval; sitemaps"}, "Menjana tajuk ringkas untuk URL web Bagaimana mesin pencari dapat secara automatik memberikan tajuk yang terbaik dan paling sesuai untuk tajuk-link URL hasil sehingga pengguna akan dipujuk untuk mengklik URL tersebut? Kami mempertimbangkan masalah menghasilkan tajuk pautan untuk URL secara automatik dan mencadangkan kerangka statistik umum untuk menyelesaikan masalah ini. Kerangka ini didasarkan pada penggunaan maklumat dari koleksi sumber yang beragam, yang masing-masing dapat dianggap menyumbang satu atau lebih judul pautan calon untuk URL. Ini juga dapat memasukkan konteks di mana judul pautan akan digunakan, bersama dengan batasan panjangnya. Kerangka kerja kami berlaku untuk beberapa senario: mendapatkan tajuk ringkas untuk memaparkan pautan cepat, mendapatkan tajuk untuk URL yang tidak mempunyai tajuk yang baik, membuat peta laman ringkas, dll. Eksperimen yang meluas menunjukkan bahawa kaedah kami sangat berkesan, menghasilkan hasil yang sekurang-kurangnya 20% lebih baik daripada garis dasar bukan remeh. [[EENNDD]] penjanaan tajuk laman web; pautan pantas; carian dan pengambilan maklumat; peta laman"], [{"string": "Scaling up dynamic time warping for datamining applications", "keywords": ["dynamic time warping", "time series"], "combined": "Scaling up dynamic time warping for datamining applications [[EENNDD]] dynamic time warping; time series"}, "Meningkatkan penyeragaman waktu dinamik untuk aplikasi perlombongan data [[DAN]] melengkapkan masa dinamik; siri masa"], [{"string": "Mining reference tables for automatic text segmentation Automatically segmenting unstructured text strings into structured records is necessary for importing the information contained in legacy sources and text collections into a data warehouse for subsequent querying , analysis , mining and integration . In this paper , we mine tables present in data warehouses and relational databases to develop an automatic segmentation system . Thus , we overcome limitations of existing supervised text segmentation approaches , which require comprehensive manually labeled training data . Our segmentation system is robust , accurate , and efficient , and requires no additional manual effort . Thorough evaluation on real datasets demonstrates the robustness and accuracy of our system , with segmentation accuracy exceeding state of the art supervised approaches .", "keywords": ["text management", "information extraction", "learning", "data cleaning", "text segmentation", "machine learning"], "combined": "Mining reference tables for automatic text segmentation Automatically segmenting unstructured text strings into structured records is necessary for importing the information contained in legacy sources and text collections into a data warehouse for subsequent querying , analysis , mining and integration . In this paper , we mine tables present in data warehouses and relational databases to develop an automatic segmentation system . Thus , we overcome limitations of existing supervised text segmentation approaches , which require comprehensive manually labeled training data . Our segmentation system is robust , accurate , and efficient , and requires no additional manual effort . Thorough evaluation on real datasets demonstrates the robustness and accuracy of our system , with segmentation accuracy exceeding state of the art supervised approaches . [[EENNDD]] text management; information extraction; learning; data cleaning; text segmentation; machine learning"}, "Jadual rujukan perlombongan untuk segmentasi teks automatik Menyegmentasikan rentetan teks tidak berstruktur secara automatik ke dalam rekod berstruktur diperlukan untuk mengimport maklumat yang terdapat dalam sumber warisan dan koleksi teks ke gudang data untuk pertanyaan, analisis, perlombongan dan integrasi berikutnya. Dalam makalah ini, kami melombong jadual yang terdapat di gudang data dan pangkalan data relasional untuk mengembangkan sistem segmentasi automatik. Oleh itu, kami mengatasi batasan pendekatan segmentasi teks yang diselia yang ada, yang memerlukan data latihan berlabel manual yang komprehensif. Sistem segmentasi kami kuat, tepat, dan efisien, dan tidak memerlukan usaha manual tambahan. Penilaian menyeluruh pada set data sebenar menunjukkan ketahanan dan ketepatan sistem kami, dengan ketepatan segmentasi melebihi pendekatan terkini yang diawasi. [[EENNDD]] pengurusan teks; pengekstrakan maklumat; belajar; pembersihan data; segmentasi teks; pembelajaran mesin"], [{"string": "Finding low-entropy sets and trees from binary data The discovery of subsets with special properties from binary data hasbeen one of the key themes in pattern discovery . Pattern classes suchas frequent itemsets stress the co-occurrence of the value 1 in the data . While this choice makes sense in the context of sparse binary data , it disregards potentially interesting subsets of attributes that have some other type of dependency structure . We consider the problem of finding all subsets of attributes that have low complexity . The complexity is measured by either the entropy of the projection of the data on the subset , or the entropy of the data for the subset when modeled using a Bayesian tree , with downward or upward pointing edges . We show that the entropy measure on sets has a monotonicity property , and thus a levelwise approach can find all low-entropy itemsets . We also show that the tree-based measures are bounded above by the entropy of the corresponding itemset , allowing similar algorithms to be used for finding low-entropy trees . We describe algorithms for finding all subsets satisfying an entropy condition . We give an extensive empirical evaluation of the performance of the methods both on synthetic and on real data . We also discuss the search for high-entropy subsets and the computation of the Vapnik-Chervonenkis dimension of the data .", "keywords": ["local models", "pattern discovery"], "combined": "Finding low-entropy sets and trees from binary data The discovery of subsets with special properties from binary data hasbeen one of the key themes in pattern discovery . Pattern classes suchas frequent itemsets stress the co-occurrence of the value 1 in the data . While this choice makes sense in the context of sparse binary data , it disregards potentially interesting subsets of attributes that have some other type of dependency structure . We consider the problem of finding all subsets of attributes that have low complexity . The complexity is measured by either the entropy of the projection of the data on the subset , or the entropy of the data for the subset when modeled using a Bayesian tree , with downward or upward pointing edges . We show that the entropy measure on sets has a monotonicity property , and thus a levelwise approach can find all low-entropy itemsets . We also show that the tree-based measures are bounded above by the entropy of the corresponding itemset , allowing similar algorithms to be used for finding low-entropy trees . We describe algorithms for finding all subsets satisfying an entropy condition . We give an extensive empirical evaluation of the performance of the methods both on synthetic and on real data . We also discuss the search for high-entropy subsets and the computation of the Vapnik-Chervonenkis dimension of the data . [[EENNDD]] local models; pattern discovery"}, "Mencari set dan pokok entropi rendah dari data binari Penemuan subset dengan sifat khas dari data binari telah menjadi salah satu tema penting dalam penemuan corak. Kelas corak seperti itemets yang kerap menekankan wujudnya nilai 1 dalam data. Walaupun pilihan ini masuk akal dalam konteks data binari yang jarang, ia mengabaikan subkumpulan atribut yang berpotensi menarik yang mempunyai beberapa jenis struktur pergantungan yang lain. Kami mempertimbangkan masalah mencari semua subset atribut yang mempunyai kerumitan rendah. Kerumitan diukur dengan entropion unjuran data pada subset, atau entropi data untuk subset ketika dimodelkan menggunakan pohon Bayesian, dengan ujung menunjuk ke bawah atau ke atas. Kami menunjukkan bahawa ukuran entropi pada set mempunyai sifat monotonik, dan dengan itu pendekatan secara berperingkat dapat menemukan semua set entropi rendah. Kami juga menunjukkan bahawa langkah-langkah berdasarkan pohon dibatasi di atas oleh entropi itemet yang sesuai, yang memungkinkan algoritma serupa digunakan untuk mencari pokok entropi rendah. Kami menerangkan algoritma untuk mencari semua subset yang memenuhi syarat entropi. Kami memberikan penilaian empirikal yang luas terhadap prestasi kaedah baik pada data sintetik dan data sebenar. Kami juga membincangkan pencarian subset entropi tinggi dan pengiraan dimensi Vapnik-Chervonenkis data. [[EENNDD]] model tempatan; penemuan corak"], [{"string": "Practical learning from one-sided feedback In many data mining applications , online labeling feedback is only available for examples which were predicted to belong to the positive class . Such applications includespam filtering in the case where users never checkemails marked `` spam '' , document retrieval where users cannotgive relevance feedback on unretrieved documents , and online advertising where user behavior can not beobserved for unshown advertisements . One-sided feedback can cripple the performance of classical mistake-driven online learners such as Perceptron . Previous work under the Apple Tasting framework showed how to transform standard online learners into successful learners from one sided feedback . However , we find in practice that this transformation may request more labels than necessary to achieve strong performance . In this paper , we employ two active learning methods which reduce the number of labels requested in practice . One method is the use of Label Efficient active learning . The other method , somewhat surprisingly , is the use of margin-based learners without modification , which we show combines implicit active learning and a greedy strategy to managing the exploration exploitation tradeoff . Experimental results show that these methods can be significantly more effective in practice than those using the Apple Tasting transformation , even on minority class problems .", "keywords": ["streaming data", "general", "active learning", "apple tasting", "online learning"], "combined": "Practical learning from one-sided feedback In many data mining applications , online labeling feedback is only available for examples which were predicted to belong to the positive class . Such applications includespam filtering in the case where users never checkemails marked `` spam '' , document retrieval where users cannotgive relevance feedback on unretrieved documents , and online advertising where user behavior can not beobserved for unshown advertisements . One-sided feedback can cripple the performance of classical mistake-driven online learners such as Perceptron . Previous work under the Apple Tasting framework showed how to transform standard online learners into successful learners from one sided feedback . However , we find in practice that this transformation may request more labels than necessary to achieve strong performance . In this paper , we employ two active learning methods which reduce the number of labels requested in practice . One method is the use of Label Efficient active learning . The other method , somewhat surprisingly , is the use of margin-based learners without modification , which we show combines implicit active learning and a greedy strategy to managing the exploration exploitation tradeoff . Experimental results show that these methods can be significantly more effective in practice than those using the Apple Tasting transformation , even on minority class problems . [[EENNDD]] streaming data; general; active learning; apple tasting; online learning"}, "Pembelajaran praktikal dari maklum balas sepihak Dalam banyak aplikasi perlombongan data, maklum balas pelabelan dalam talian hanya tersedia untuk contoh yang diramalkan termasuk dalam kelas positif. Aplikasi seperti itu termasuk penapisan spam dalam hal pengguna tidak pernah memeriksa kotak surat bertanda \"spam\", pengambilan dokumen di mana pengguna tidak dapat memberikan maklum balas yang relevan pada dokumen yang tidak diambil, dan iklan dalam talian di mana tingkah laku pengguna tidak dapat diperhatikan untuk iklan yang tidak ditampilkan. Maklum balas sepihak dapat melumpuhkan prestasi pelajar dalam talian berdasarkan kesilapan klasik seperti Perceptron. Kerja sebelumnya di bawah kerangka Apple Tasting menunjukkan bagaimana mengubah pelajar dalam talian standard menjadi pelajar yang berjaya dari maklum balas satu sisi. Walau bagaimanapun, dalam praktiknya kita dapati bahawa transformasi ini mungkin meminta lebih banyak label daripada yang diperlukan untuk mencapai prestasi yang kuat. Dalam makalah ini, kami menggunakan dua kaedah pembelajaran aktif yang mengurangkan jumlah label yang diminta dalam praktik. Salah satu kaedah adalah penggunaan pembelajaran aktif Label Efisien. Kaedah lain, yang agak mengejutkan, adalah penggunaan pelajar berdasarkan margin tanpa pengubahsuaian, yang kami tunjukkan menggabungkan pembelajaran aktif tersirat dan strategi tamak untuk menguruskan pertukaran eksploitasi eksplorasi. Hasil eksperimen menunjukkan bahawa kaedah ini jauh lebih berkesan dalam praktiknya daripada menggunakan transformasi Apple Tasting, walaupun pada masalah kelas minoriti. [[EENNDD]] streaming data; umum; pembelajaran aktif; rasa epal; pembelajaran dalam talian"], [{"string": "Carpenter : finding closed patterns in long biological datasets The growth of bioinformatics has resulted in datasets with new characteristics . These datasets typically contain a large number of columns and a small number of rows . For example , many gene expression datasets may contain 10,000-100 ,000 columns but only 100-1000 rows . Such datasets pose a great challenge for existing closed frequent pattern discovery algorithms , since they have an exponential dependence on the average row length . In this paper , we describe a new algorithm called CARPENTER that is specially designed to handle datasets having a large number of attributes and relatively small number of rows . Several experiments on real bioinformatics datasets show that CARPENTER is orders of magnitude better than previous closed pattern mining algorithms like CLOSET and CHARM .", "keywords": ["row enumeration", "closed pattern", "database applications", "frequent pattern"], "combined": "Carpenter : finding closed patterns in long biological datasets The growth of bioinformatics has resulted in datasets with new characteristics . These datasets typically contain a large number of columns and a small number of rows . For example , many gene expression datasets may contain 10,000-100 ,000 columns but only 100-1000 rows . Such datasets pose a great challenge for existing closed frequent pattern discovery algorithms , since they have an exponential dependence on the average row length . In this paper , we describe a new algorithm called CARPENTER that is specially designed to handle datasets having a large number of attributes and relatively small number of rows . Several experiments on real bioinformatics datasets show that CARPENTER is orders of magnitude better than previous closed pattern mining algorithms like CLOSET and CHARM . [[EENNDD]] row enumeration; closed pattern; database applications; frequent pattern"}, "Tukang kayu: mencari corak tertutup dalam kumpulan data biologi yang panjang Pertumbuhan bioinformatik telah menghasilkan kumpulan data dengan ciri baru. Set data ini biasanya mengandungi sebilangan besar lajur dan sebilangan kecil baris. Sebagai contoh, banyak set data ekspresi gen mungkin mengandungi 10,000-100, 000 lajur tetapi hanya 100-1000 baris. Set data sedemikian menimbulkan cabaran besar untuk algoritma penemuan corak tertutup yang ada, kerana mereka mempunyai ketergantungan eksponensial pada panjang baris rata-rata. Dalam makalah ini, kami menerangkan algoritma baru yang disebut CARPENTER yang direka khas untuk menangani set data yang mempunyai sebilangan besar atribut dan bilangan baris yang agak kecil. Beberapa eksperimen pada kumpulan data bioinformatik sebenar menunjukkan bahawa CARPENTER adalah susunan besarnya lebih baik daripada algoritma perlombongan corak tertutup sebelumnya seperti CLOSET dan CHARM. [[EENNDD]] penghitungan baris; corak tertutup; aplikasi pangkalan data; corak yang kerap"], [{"string": "An integrated machine learning approach to stroke prediction Stroke is the third leading cause of death and the principal cause of serious long-term disability in the United States . Accurate prediction of stroke is highly valuable for early intervention and treatment . In this study , we compare the Cox proportional hazards model with a machine learning approach for stroke prediction on the Cardiovascular Health Study CHS dataset . Specifically , we consider the common problems of data imputation , feature selection , and prediction in medical datasets . We propose a novel automatic feature selection algorithm that selects robust features based on our proposed heuristic : conservative mean . Combined with Support Vector Machines SVMs , our proposed feature selection algorithm achieves a greater area under the ROC curve AUC as compared to the Cox proportional hazards model and L1 regularized Cox feature selection algorithm . Furthermore , we present a margin-based censored regression algorithm that combines the concept of margin-based classifiers with censored regression to achieve a better concordance index than the Cox model . Overall , our approach outperforms the current state-of-the-art in both metrics of AUC and concordance index . In addition , our work has also identified potential risk factors that have not been discovered by traditional approaches . Our method can be applied to clinical prediction of other diseases , where missing data are common and risk factors are not well understood .", "keywords": ["roc", "classification", "stroke", "concordance index", "healthcare", "data analysis", "medical data analysis", "stroke prediction", "prediction", "svm", "feature selection", "benchmark"], "combined": "An integrated machine learning approach to stroke prediction Stroke is the third leading cause of death and the principal cause of serious long-term disability in the United States . Accurate prediction of stroke is highly valuable for early intervention and treatment . In this study , we compare the Cox proportional hazards model with a machine learning approach for stroke prediction on the Cardiovascular Health Study CHS dataset . Specifically , we consider the common problems of data imputation , feature selection , and prediction in medical datasets . We propose a novel automatic feature selection algorithm that selects robust features based on our proposed heuristic : conservative mean . Combined with Support Vector Machines SVMs , our proposed feature selection algorithm achieves a greater area under the ROC curve AUC as compared to the Cox proportional hazards model and L1 regularized Cox feature selection algorithm . Furthermore , we present a margin-based censored regression algorithm that combines the concept of margin-based classifiers with censored regression to achieve a better concordance index than the Cox model . Overall , our approach outperforms the current state-of-the-art in both metrics of AUC and concordance index . In addition , our work has also identified potential risk factors that have not been discovered by traditional approaches . Our method can be applied to clinical prediction of other diseases , where missing data are common and risk factors are not well understood . [[EENNDD]] roc; classification; stroke; concordance index; healthcare; data analysis; medical data analysis; stroke prediction; prediction; svm; feature selection; benchmark"}, "Pendekatan pembelajaran mesin bersepadu untuk ramalan strok Strok adalah penyebab utama kematian ketiga dan penyebab utama kecacatan jangka panjang yang serius di Amerika Syarikat. Ramalan strok yang tepat sangat berharga untuk intervensi dan rawatan awal. Dalam kajian ini, kami membandingkan model bahaya proporsional Cox dengan pendekatan pembelajaran mesin untuk ramalan strok pada set data Kardiovaskular Health Study CHS. Secara khusus, kami mempertimbangkan masalah umum pengiraan data, pemilihan ciri, dan ramalan dalam set data perubatan. Kami mencadangkan algoritma pemilihan ciri automatik baru yang memilih ciri kuat berdasarkan heuristik yang dicadangkan: min konservatif. Dikombinasikan dengan SVM Mesin Vektor Sokongan, algoritma pemilihan ciri yang kami cadangkan mencapai kawasan yang lebih luas di bawah AUC kurva ROC berbanding model bahaya berkadar Cox dan algoritma pemilihan ciri Cox teratur L1. Selanjutnya, kami menyajikan algoritma regresi disensor berdasarkan margin yang menggabungkan konsep pengkelasan berdasarkan margin dengan regresi disensor untuk mencapai indeks konkordans yang lebih baik daripada model Cox. Secara keseluruhan, pendekatan kami mengatasi keadaan terkini dalam metrik AUC dan indeks kesesuaian. Di samping itu, kerja kami juga telah mengenal pasti faktor risiko berpotensi yang belum ditemui oleh pendekatan tradisional. Kaedah kami dapat digunakan untuk ramalan klinikal penyakit lain, di mana data yang hilang adalah perkara biasa dan faktor risiko tidak difahami dengan baik. [[EENNDD]] roc; pengelasan; strok; indeks kesesuaian; penjagaan kesihatan; analisis data; analisis data perubatan; ramalan strok; ramalan; svm; pemilihan ciri; penanda aras"], [{"string": "Utility-based anonymization using local recoding Privacy becomes a more and more serious concern in applications involving microdata . Recently , efficient anonymization has attracted much research work . Most of the previous methods use global recoding , which maps the domains of the quasi-identifier attributes to generalized or changed values . However , global recoding may not always achieve effective anonymization in terms of discernability and query answering accuracy using the anonymized data . Moreover , anonymized data is often for analysis . As well accepted in many analytical applications , different attributes in a data set may have different utility in the analysis . The utility of attributes has not been considered in the previous methods . In this paper , we study the problem of utility-based anonymization . First , we propose a simple framework to specify utility of attributes . The framework covers both numeric and categorical data . Second , we develop two simple yet efficient heuristic local recoding methods for utility-based anonymization . Our extensive performance study using both real data sets and synthetic data sets shows that our methods outperform the state-of-the-art multidimensional global recoding methods in both discernability and query answering accuracy . Furthermore , our utility-based method can boost the quality of analysis using the anonymized data .", "keywords": ["local recoding", "privacy preservation", "utility", "k-anonymity"], "combined": "Utility-based anonymization using local recoding Privacy becomes a more and more serious concern in applications involving microdata . Recently , efficient anonymization has attracted much research work . Most of the previous methods use global recoding , which maps the domains of the quasi-identifier attributes to generalized or changed values . However , global recoding may not always achieve effective anonymization in terms of discernability and query answering accuracy using the anonymized data . Moreover , anonymized data is often for analysis . As well accepted in many analytical applications , different attributes in a data set may have different utility in the analysis . The utility of attributes has not been considered in the previous methods . In this paper , we study the problem of utility-based anonymization . First , we propose a simple framework to specify utility of attributes . The framework covers both numeric and categorical data . Second , we develop two simple yet efficient heuristic local recoding methods for utility-based anonymization . Our extensive performance study using both real data sets and synthetic data sets shows that our methods outperform the state-of-the-art multidimensional global recoding methods in both discernability and query answering accuracy . Furthermore , our utility-based method can boost the quality of analysis using the anonymized data . [[EENNDD]] local recoding; privacy preservation; utility; k-anonymity"}, "Anonimisasi berasaskan utiliti menggunakan pengekodan semula Privasi menjadi perhatian yang semakin serius dalam aplikasi yang melibatkan mikrodata. Baru-baru ini, penyebutan yang cekap telah menarik banyak kerja penyelidikan. Sebilangan besar kaedah sebelumnya menggunakan pengekodan semula global, yang memetakan domain atribut kuasi-pengenal kepada nilai umum atau berubah. Walau bagaimanapun, pengekodan semula global mungkin tidak selalu mencapai anonimisasi yang berkesan dari segi kebolehlihatan dan ketepatan menjawab pertanyaan menggunakan data tanpa nama. Lebih-lebih lagi, data tanpa nama sering untuk analisis. Seperti yang diterima baik dalam banyak aplikasi analitik, atribut yang berlainan dalam kumpulan data mungkin memiliki kegunaan yang berbeda dalam analisis. Utiliti atribut belum dipertimbangkan dalam kaedah sebelumnya. Dalam makalah ini, kami mengkaji masalah anonimasi berasaskan utiliti. Pertama, kami mencadangkan kerangka mudah untuk menentukan utiliti atribut. Rangka kerja merangkumi data berangka dan data. Kedua, kami mengembangkan dua kaedah pengkodan semula tempatan heuristik yang mudah tetapi cekap untuk penyebutan berdasarkan utiliti. Kajian prestasi kami yang luas menggunakan kedua-dua set data sebenar dan kumpulan data sintetik menunjukkan bahawa kaedah kami mengatasi kaedah pengkodan global multidimensi canggih dalam kedua-dua kebolehpercayaan dan ketepatan menjawab pertanyaan. Selanjutnya, kaedah berasaskan utiliti kami dapat meningkatkan kualiti analisis menggunakan data tanpa nama. [[EENNDD]] pengekodan semula tempatan; pemeliharaan privasi; utiliti; k-anonimiti"], [{"string": "Exploiting Wikipedia as external knowledge for document clustering In traditional text clustering methods , documents are represented as `` bags of words '' without considering the semantic information of each document . For instance , if two documents use different collections of core words to represent the same topic , they may be falsely assigned to different clusters due to the lack of shared core words , although the core words they use are probably synonyms or semantically associated in other forms . The most common way to solve this problem is to enrich document representation with the background knowledge in an ontology . There are two major issues for this approach : 1 the coverage of the ontology is limited , even for WordNet or Mesh , 2 using ontology terms as replacement or additional features may cause information loss , or introduce noise . In this paper , we present a novel text clustering method to address these two issues by enriching document representation with Wikipedia concept and category information . We develop two approaches , exact match and relatedness-match , to map text documents to Wikipedia concepts , and further to Wikipedia categories . Then the text documents are clustered based on a similarity metric which combines document content information , concept information as well as category information . The experimental results using the proposed clustering framework on three datasets 20-newsgroup , TDT2 , and LA Times show that clustering performance improves significantly by enriching document representation with Wikipedia concepts and categories .", "keywords": ["document representation", "wikipedia", "text clustering"], "combined": "Exploiting Wikipedia as external knowledge for document clustering In traditional text clustering methods , documents are represented as `` bags of words '' without considering the semantic information of each document . For instance , if two documents use different collections of core words to represent the same topic , they may be falsely assigned to different clusters due to the lack of shared core words , although the core words they use are probably synonyms or semantically associated in other forms . The most common way to solve this problem is to enrich document representation with the background knowledge in an ontology . There are two major issues for this approach : 1 the coverage of the ontology is limited , even for WordNet or Mesh , 2 using ontology terms as replacement or additional features may cause information loss , or introduce noise . In this paper , we present a novel text clustering method to address these two issues by enriching document representation with Wikipedia concept and category information . We develop two approaches , exact match and relatedness-match , to map text documents to Wikipedia concepts , and further to Wikipedia categories . Then the text documents are clustered based on a similarity metric which combines document content information , concept information as well as category information . The experimental results using the proposed clustering framework on three datasets 20-newsgroup , TDT2 , and LA Times show that clustering performance improves significantly by enriching document representation with Wikipedia concepts and categories . [[EENNDD]] document representation; wikipedia; text clustering"}, "Mengeksploitasi Wikipedia sebagai pengetahuan luaran untuk pengelompokan dokumen Dalam kaedah pengelompokan teks tradisional, dokumen digambarkan sebagai \"beg kata\" tanpa mempertimbangkan maklumat semantik setiap dokumen. Sebagai contoh, jika dua dokumen menggunakan koleksi kata inti yang berlainan untuk mewakili topik yang sama, mereka mungkin salah ditugaskan ke kelompok yang berbeza kerana kurangnya kata inti yang dikongsi, walaupun kata inti yang mereka gunakan mungkin sinonim atau semantik dikaitkan dalam bentuk lain . Cara yang paling biasa untuk menyelesaikan masalah ini adalah dengan memperkaya perwakilan dokumen dengan pengetahuan latar belakang dalam ontologi. Terdapat dua masalah utama untuk pendekatan ini: 1 liputan ontologi adalah terhad, bahkan untuk WordNet atau Mesh, 2 menggunakan istilah ontologi sebagai penggantian atau ciri tambahan dapat menyebabkan kehilangan maklumat, atau menimbulkan kebisingan. Dalam makalah ini, kami menyajikan metode pengelompokan teks novel untuk mengatasi dua masalah ini dengan memperkaya representasi dokumen dengan konsep Wikipedia dan maklumat kategori. Kami mengembangkan dua pendekatan, pencocokan tepat dan hubungan-persamaan, untuk memetakan dokumen teks ke konsep Wikipedia, dan seterusnya ke kategori Wikipedia. Kemudian dokumen teks dikelompokkan berdasarkan metrik kesamaan yang menggabungkan maklumat kandungan dokumen, maklumat konsep dan juga maklumat kategori. Hasil eksperimen menggunakan kerangka pengelompokan yang dicadangkan pada tiga kumpulan data 20-newsgroup, TDT2, dan LA Times menunjukkan bahawa prestasi pengelompokan meningkat dengan ketara dengan memperkaya perwakilan dokumen dengan konsep dan kategori Wikipedia. [[EENNDD]] perwakilan dokumen; wikipedia; pengelompokan teks"], [{"string": "Coherent closed quasi-clique discovery from large dense graph databases Frequent coherent subgraphs can provide valuable knowledge about the underlying internal structure of a graph database , and mining frequently occurring coherent subgraphs from large dense graph databases has been witnessed several applications and received considerable attention in the graph mining community recently . In this paper , we study how to efficiently mine the complete set of coherent closed quasi-cliques from large dense graph databases , which is an especially challenging task due to the downward-closure property no longer holds . By fully exploring some properties of quasi-cliques , we propose several novel optimization techniques , which can prune the unpromising and redundant sub-search spaces effectively . Meanwhile , we devise an efficient closure checking scheme to facilitate the discovery of only closed quasi-cliques . We also develop a coherent closed quasi-clique mining algorithm , B Cocain \\/ B 1 Thorough performance study shows that Cocain is very efficient and scalable for large dense graph databases .", "keywords": ["quasi-clique", "coherent subgraph", "graph mining"], "combined": "Coherent closed quasi-clique discovery from large dense graph databases Frequent coherent subgraphs can provide valuable knowledge about the underlying internal structure of a graph database , and mining frequently occurring coherent subgraphs from large dense graph databases has been witnessed several applications and received considerable attention in the graph mining community recently . In this paper , we study how to efficiently mine the complete set of coherent closed quasi-cliques from large dense graph databases , which is an especially challenging task due to the downward-closure property no longer holds . By fully exploring some properties of quasi-cliques , we propose several novel optimization techniques , which can prune the unpromising and redundant sub-search spaces effectively . Meanwhile , we devise an efficient closure checking scheme to facilitate the discovery of only closed quasi-cliques . We also develop a coherent closed quasi-clique mining algorithm , B Cocain \\/ B 1 Thorough performance study shows that Cocain is very efficient and scalable for large dense graph databases . [[EENNDD]] quasi-clique; coherent subgraph; graph mining"}, "Penemuan quasi-clique tertutup yang koheren dari pangkalan data grafik padat yang besar Subgraf koheren yang kerap dapat memberikan pengetahuan berharga mengenai struktur dalaman asas pangkalan data grafik, dan penambangan subgraf koheren yang sering berlaku dari pangkalan data grafik padat yang besar telah menyaksikan beberapa aplikasi dan mendapat perhatian yang besar di komuniti perlombongan grafik baru-baru ini. Dalam makalah ini, kami mengkaji bagaimana menambang secara efisien set lengkap kuasi tertutup koheren dari pangkalan data grafik padat yang besar, yang merupakan tugas yang sangat mencabar kerana harta penutupan ke bawah tidak lagi dipegang. Dengan meneroka sepenuhnya beberapa sifat quasi-clik, kami mencadangkan beberapa teknik pengoptimuman baru, yang dapat memangkas ruang sub-carian tanpa kompromi dan berlebihan dengan berkesan. Sementara itu, kami merancang skema pemeriksaan penutupan yang cekap untuk memudahkan penemuan hanya kuasi tertutup. Kami juga mengembangkan algoritma perlombongan quasi-clique tertutup yang koheren, B Cocain \\ / B 1 Kajian prestasi menyeluruh menunjukkan bahawa Cocain sangat cekap dan berskala untuk pangkalan data grafik padat yang besar. [[EENNDD]] semu-klik; subgraf yang koheren; perlombongan grafik"], [{"string": "Maximum profit mining and its application in software development While most software defects i.e. , bugs are corrected and tested as part of the lengthy software development cycle , enterprise software vendors often have to release software products before all reported defects are corrected , due to deadlines and limited resources . A small number of these defects will be escalated by customers and they must be resolved immediately by the software vendors at a very high cost . In this paper , we develop an Escalation Prediction EP system that mines historic defect report data and predict the escalation risk of the defects for maximum net profit . More specifically , we first describe a simple and general framework to convert the maximum net profit problem to cost-sensitive learning . We then apply and compare several well-known cost-sensitive learning approaches for EP . Our experiments suggest that the cost-sensitive decision tree is the best method for producing the highest positive net profit and comprehensible results . The EP system has been deployed successfully in the product group of an enterprise software vendor .", "keywords": ["cost-sensitive learning", "data mining", "learning", "escalation prediction"], "combined": "Maximum profit mining and its application in software development While most software defects i.e. , bugs are corrected and tested as part of the lengthy software development cycle , enterprise software vendors often have to release software products before all reported defects are corrected , due to deadlines and limited resources . A small number of these defects will be escalated by customers and they must be resolved immediately by the software vendors at a very high cost . In this paper , we develop an Escalation Prediction EP system that mines historic defect report data and predict the escalation risk of the defects for maximum net profit . More specifically , we first describe a simple and general framework to convert the maximum net profit problem to cost-sensitive learning . We then apply and compare several well-known cost-sensitive learning approaches for EP . Our experiments suggest that the cost-sensitive decision tree is the best method for producing the highest positive net profit and comprehensible results . The EP system has been deployed successfully in the product group of an enterprise software vendor . [[EENNDD]] cost-sensitive learning; data mining; learning; escalation prediction"}, "Perlombongan keuntungan maksimum dan aplikasinya dalam pembangunan perisian Walaupun sebahagian besar kecacatan perisian, bug diperbaiki dan diuji sebagai sebahagian daripada kitaran pengembangan perisian yang panjang, vendor perisian perusahaan sering kali harus melepaskan produk perisian sebelum semua kerosakan yang dilaporkan diperbaiki, kerana tarikh akhir dan terhad sumber. Sebilangan kecil kecacatan ini akan ditingkatkan oleh pelanggan dan ia mesti segera diatasi oleh vendor perisian dengan kos yang sangat tinggi. Dalam makalah ini, kami mengembangkan sistem EP Prediksi Eskalasi yang menambang data laporan kecacatan bersejarah dan meramalkan peningkatan risiko kecacatan untuk keuntungan bersih maksimum. Lebih khusus lagi, pertama-tama kami menerangkan kerangka kerja sederhana dan umum untuk menukar masalah keuntungan bersih maksimum kepada pembelajaran yang sensitif kos. Kami kemudian menggunakan dan membandingkan beberapa pendekatan pembelajaran sensitif kos untuk EP. Eksperimen kami menunjukkan bahawa keputusan keputusan sensitif kos adalah kaedah terbaik untuk menghasilkan keuntungan bersih positif tertinggi dan hasil yang dapat difahami. Sistem EP berjaya digunakan dalam kumpulan produk vendor perisian perusahaan. [[EENNDD]] pembelajaran sensitif kos; perlombongan data; belajar; ramalan peningkatan"], [{"string": "Compression of weighted graphs We propose to compress weighted graphs networks , motivated by the observation that large networks of social , biological , or other relations can be complex to handle and visualize . In the process also known as graph simplification , nodes and unweighted edges are grouped to supernodes and superedges , respectively , to obtain a smaller graph . We propose models and algorithms for weighted graphs . The interpretation i.e. decompression of a compressed , weighted graph is that a pair of original nodes is connected by an edge if their supernodes are connected by one , and that the weight of an edge is approximated to be the weight of the superedge . The compression problem now consists of choosing supernodes , superedges , and superedge weights so that the approximation error is minimized while the amount of compression is maximized . In this paper , we formulate this task as the ` simple weighted graph compression problem ' . We then propose a much wider class of tasks under the name of ` generalized weighted graph compression problem ' . The generalized task extends the optimization to preserve longer-range connectivities between nodes , not just individual edge weights . We study the properties of these problems and propose a range of algorithms to solve them , with different balances between complexity and quality of the result . We evaluate the problems and algorithms experimentally on real networks . The results indicate that weighted graphs can be compressed efficiently with relatively little compression error .", "keywords": ["compression", "weighted graph", "network", "graph mining"], "combined": "Compression of weighted graphs We propose to compress weighted graphs networks , motivated by the observation that large networks of social , biological , or other relations can be complex to handle and visualize . In the process also known as graph simplification , nodes and unweighted edges are grouped to supernodes and superedges , respectively , to obtain a smaller graph . We propose models and algorithms for weighted graphs . The interpretation i.e. decompression of a compressed , weighted graph is that a pair of original nodes is connected by an edge if their supernodes are connected by one , and that the weight of an edge is approximated to be the weight of the superedge . The compression problem now consists of choosing supernodes , superedges , and superedge weights so that the approximation error is minimized while the amount of compression is maximized . In this paper , we formulate this task as the ` simple weighted graph compression problem ' . We then propose a much wider class of tasks under the name of ` generalized weighted graph compression problem ' . The generalized task extends the optimization to preserve longer-range connectivities between nodes , not just individual edge weights . We study the properties of these problems and propose a range of algorithms to solve them , with different balances between complexity and quality of the result . We evaluate the problems and algorithms experimentally on real networks . The results indicate that weighted graphs can be compressed efficiently with relatively little compression error . [[EENNDD]] compression; weighted graph; network; graph mining"}, "Pemampatan grafik berwajaran Kami mencadangkan untuk memampatkan rangkaian grafik berwajaran, didorong oleh pemerhatian bahawa rangkaian hubungan sosial, biologi, atau yang lain boleh menjadi kompleks untuk ditangani dan digambarkan. Dalam proses yang juga dikenal sebagai penyederhanaan grafik, nod dan tepi tidak berbobot dikelompokkan ke supernod dan superedges, masing-masing, untuk mendapatkan graf yang lebih kecil. Kami mencadangkan model dan algoritma untuk grafik berwajaran. Tafsirannya iaitu penyahmampatan graf yang dimampatkan dan berwajaran adalah bahawa sepasang nod asal disambungkan dengan tepi jika supernod mereka disambungkan oleh satu, dan bahawa berat suatu pinggir didekati menjadi berat superge. Masalah pemampatan sekarang terdiri daripada memilih supernode, superedges, dan superedge weight sehingga kesalahan penghampiran diminimumkan sementara jumlah pemampatan dimaksimumkan. Dalam makalah ini, kami merumuskan tugas ini sebagai \"masalah pemampatan grafik berwajaran sederhana\". Kami kemudian mencadangkan tugas yang lebih luas dengan nama \"masalah pemampatan grafik berwajaran umum\". Tugas umum memperluas pengoptimuman untuk mengekalkan hubungan jarak jauh antara nod, bukan hanya berat tepi individu. Kami mengkaji sifat masalah ini dan mencadangkan pelbagai algoritma untuk menyelesaikannya, dengan keseimbangan yang berbeza antara kerumitan dan kualiti hasilnya. Kami menilai masalah dan algoritma secara eksperimen pada rangkaian sebenar. Hasilnya menunjukkan bahawa graf berwajaran dapat dikompres dengan cekap dengan ralat mampatan yang agak kecil. [[EENNDD]] pemampatan; graf berwajaran; rangkaian; perlombongan grafik"], [{"string": "Exploiting duality in summarization with deterministic guarantees Summarization is an important task in data mining . A major challenge over the past years has been the efficient construction of fixed-space synopses that provide a deterministic quality guarantee , often expressed in terms of a maximum-error metric . Histograms and several hierarchical techniques have been proposed for this problem . However , their time and\\/or space complexities remain impractically high and depend not only on the data set size n , but also on the space budget B. These handicaps stem from a requirement to tabulate all allocations of synopsis space to different regions of the data . In this paper we develop an alternative methodology that dispels these deficiencies , thanks to a fruitful application of the solution to the dual problem : given a maximum allowed error , determine the minimum-space synopsis that achieves it . Compared to the state-of-the-art , our histogram construction algorithm reduces time complexity by at least a Blog2n over log\u03b5 \\* factor and our hierarchical synopsis algorithm reduces the complexity by at least a factor of log2B over log\u03b5 \\* + logn in time and B 1-log B over log n in space , where \u03b5 \\* is the optimal error . These complexity advantages offer both a space-efficiency and a scalability that previous approaches lacked . We verify the benefits of our approach in practice by experimentation .", "keywords": ["wavelets", "miscellaneous", "histograms", "synopses", "efficiency"], "combined": "Exploiting duality in summarization with deterministic guarantees Summarization is an important task in data mining . A major challenge over the past years has been the efficient construction of fixed-space synopses that provide a deterministic quality guarantee , often expressed in terms of a maximum-error metric . Histograms and several hierarchical techniques have been proposed for this problem . However , their time and\\/or space complexities remain impractically high and depend not only on the data set size n , but also on the space budget B. These handicaps stem from a requirement to tabulate all allocations of synopsis space to different regions of the data . In this paper we develop an alternative methodology that dispels these deficiencies , thanks to a fruitful application of the solution to the dual problem : given a maximum allowed error , determine the minimum-space synopsis that achieves it . Compared to the state-of-the-art , our histogram construction algorithm reduces time complexity by at least a Blog2n over log\u03b5 \\* factor and our hierarchical synopsis algorithm reduces the complexity by at least a factor of log2B over log\u03b5 \\* + logn in time and B 1-log B over log n in space , where \u03b5 \\* is the optimal error . These complexity advantages offer both a space-efficiency and a scalability that previous approaches lacked . We verify the benefits of our approach in practice by experimentation . [[EENNDD]] wavelets; miscellaneous; histograms; synopses; efficiency"}, "Mengeksploitasi dualitas dalam ringkasan dengan jaminan deterministik Ringkasan adalah tugas penting dalam perlombongan data. Cabaran utama selama bertahun-tahun terakhir adalah pembinaan sinopsis ruang tetap yang cekap yang memberikan jaminan kualiti deterministik, yang sering dinyatakan dalam metrik ralat maksimum. Histogram dan beberapa teknik hierarki telah dicadangkan untuk masalah ini. Walau bagaimanapun, kerumitan masa dan \\ / atau ruang mereka tetap tinggi dan tidak hanya bergantung pada ukuran data n, tetapi juga pada anggaran ruang B. Kekurangan ini berpunca dari keperluan untuk menjabarkan semua peruntukan ruang sinopsis ke kawasan yang berlainan data. Dalam makalah ini kami mengembangkan metodologi alternatif yang menghilangkan kekurangan ini, berkat aplikasi yang bermanfaat untuk penyelesaian masalah ganda: dengan kesalahan maksimum yang dibenarkan, tentukan sinopsis ruang minimum yang mencapainya. Berbanding dengan yang canggih, algoritma pembinaan histogram kami mengurangkan kerumitan masa dengan sekurang-kurangnya faktor Blog2n over log\u03b5 \\ * dan algoritma sinopsis hierarki kami mengurangkan kerumitan dengan sekurang-kurangnya faktor log2B berbanding log\u03b5 \\ * + log masuk masa dan B 1-log B melebihi log n di ruang, di mana \u03b5 \\ * adalah kesalahan optimum. Kelebihan kerumitan ini menawarkan kecekapan ruang dan skalabiliti yang kekurangan pendekatan sebelumnya. Kami mengesahkan faedah pendekatan kami dalam praktik dengan percubaan. [[EENNDD]] gelombang gelombang; pelbagai; histogram; sinopsis; kecekapan"], [{"string": "Serendipitous learning : learning beyond the predefined label space Most traditional supervised learning methods are developed to learn a model from labeled examples and use this model to classify the unlabeled ones into the same label space predefined by the models . However , in many real world applications , the label spaces for both the labeled\\/training and unlabeled\\/testing examples can be different . To solve this problem , this paper proposes a novel notion of Serendipitous Learning SL , which is defined to address the learning scenarios in which the label space can be enlarged during the testing phase . In particular , a large margin approach is proposed to solve SL . The basic idea is to leverage the knowledge in the labeled examples to help identify novel\\/unknown classes , and the large margin formulation is proposed to incorporate both the classification loss on the examples within the known categories , as well as the clustering loss on the examples in unknown categories . An efficient optimization algorithm based on CCCP and the bundle method is proposed to solve the optimization problem of the large margin formulation of SL . Moreover , an efficient online learning method is proposed to address the issue of large scale data in online learning scenario , which has been shown to have a guaranteed learning regret . An extensive set of experimental results on two synthetic datasets and two datasets from real world applications demonstrate the advantages of the proposed method over several other baseline algorithms . One limitation of the proposed method is that the number of unknown classes is given in advance . It may be possible to remove this constraint if we model it by using a non-parametric way . We also plan to do experiments on more real world applications in the future .", "keywords": ["serendipitous learning", "label space", "maximum margin classification"], "combined": "Serendipitous learning : learning beyond the predefined label space Most traditional supervised learning methods are developed to learn a model from labeled examples and use this model to classify the unlabeled ones into the same label space predefined by the models . However , in many real world applications , the label spaces for both the labeled\\/training and unlabeled\\/testing examples can be different . To solve this problem , this paper proposes a novel notion of Serendipitous Learning SL , which is defined to address the learning scenarios in which the label space can be enlarged during the testing phase . In particular , a large margin approach is proposed to solve SL . The basic idea is to leverage the knowledge in the labeled examples to help identify novel\\/unknown classes , and the large margin formulation is proposed to incorporate both the classification loss on the examples within the known categories , as well as the clustering loss on the examples in unknown categories . An efficient optimization algorithm based on CCCP and the bundle method is proposed to solve the optimization problem of the large margin formulation of SL . Moreover , an efficient online learning method is proposed to address the issue of large scale data in online learning scenario , which has been shown to have a guaranteed learning regret . An extensive set of experimental results on two synthetic datasets and two datasets from real world applications demonstrate the advantages of the proposed method over several other baseline algorithms . One limitation of the proposed method is that the number of unknown classes is given in advance . It may be possible to remove this constraint if we model it by using a non-parametric way . We also plan to do experiments on more real world applications in the future . [[EENNDD]] serendipitous learning; label space; maximum margin classification"}, "Pembelajaran serendipitous: belajar di luar ruang label yang telah ditentukan Sebilangan besar kaedah pembelajaran diselia tradisional dikembangkan untuk mempelajari model dari contoh berlabel dan menggunakan model ini untuk mengklasifikasikan yang tidak berlabel ke dalam ruang label yang sama yang telah ditentukan oleh model. Walau bagaimanapun, dalam banyak aplikasi dunia nyata, ruang label untuk contoh ujian \\ / training dan tidak berlabel \\ / boleh berbeza. Untuk menyelesaikan masalah ini, makalah ini mengemukakan gagasan novel Serendipitous Learning SL, yang ditakrifkan untuk menangani senario pembelajaran di mana ruang label dapat diperbesar semasa fasa pengujian. Khususnya, pendekatan margin besar dicadangkan untuk menyelesaikan SL. Idea asasnya adalah untuk memanfaatkan pengetahuan dalam contoh berlabel untuk membantu mengenal pasti kelas novel / tidak diketahui, dan rumusan margin besar dicadangkan untuk memasukkan kedua-dua kehilangan klasifikasi pada contoh dalam kategori yang diketahui, serta kerugian pengelompokan pada contoh dalam kategori yang tidak diketahui. Algoritma pengoptimuman yang cekap berdasarkan CCCP dan kaedah bundle dicadangkan untuk menyelesaikan masalah pengoptimuman formulasi margin besar SL. Lebih-lebih lagi, kaedah pembelajaran dalam talian yang cekap dicadangkan untuk mengatasi masalah data berskala besar dalam senario pembelajaran dalam talian, yang telah terbukti memiliki penyesalan pembelajaran yang dijamin. Satu set hasil eksperimen yang luas pada dua set data sintetik dan dua set data dari aplikasi dunia nyata menunjukkan kelebihan kaedah yang dicadangkan berbanding beberapa algoritma asas lain. Satu batasan kaedah yang dicadangkan adalah bahawa jumlah kelas yang tidak diketahui diberikan terlebih dahulu. Mungkin boleh menghilangkan kekangan ini jika kita memodelkannya dengan menggunakan cara bukan parametrik. Kami juga merancang untuk melakukan eksperimen pada lebih banyak aplikasi dunia nyata pada masa akan datang. [[EENNDD]] pembelajaran serendipitous; ruang label; klasifikasi margin maksimum"], [{"string": "Predicting rare classes : can boosting make any weak learner strong ? Boosting is a strong ensemble-based learning algorithm with the promise of iteratively improving the classification accuracy using any base learner , as long as it satisfies the condition of yielding weighted accuracy 0.5 . In this paper , we analyze boosting with respect to this basic condition on the base learner , to see if boosting ensures prediction of rarely occurring events with high recall and precision . First we show that a base learner can satisfy the required condition even for poor recall or precision levels , especially for very rare classes . Furthermore , we show that the intelligent weight updating mechanism in boosting , even in its strong cost-sensitive form , does not prevent cases where the base learner always achieves high precision but poor recall or high recall but poor precision , when mapped to the original distribution . In either of these cases , we show that the voting mechanism of boosting falls to achieve good overall recall and precision for the ensemble . In effect , our analysis indicates that one can not be blind to the base learner performance , and just rely on the boosting mechanism to take care of its weakness . We validate our arguments empirically on variety of real and synthetic rare class problems . In particular , using AdaCost as the boosting algorithm , and variations of PNrule and RIPPER as the base learners , we show that if algorithm A achieves better recall-precision balance than algorithm B , then using A as the base learner in AdaCost yields significantly better performance than using B as the base learner .", "keywords": ["analyze"], "combined": "Predicting rare classes : can boosting make any weak learner strong ? Boosting is a strong ensemble-based learning algorithm with the promise of iteratively improving the classification accuracy using any base learner , as long as it satisfies the condition of yielding weighted accuracy 0.5 . In this paper , we analyze boosting with respect to this basic condition on the base learner , to see if boosting ensures prediction of rarely occurring events with high recall and precision . First we show that a base learner can satisfy the required condition even for poor recall or precision levels , especially for very rare classes . Furthermore , we show that the intelligent weight updating mechanism in boosting , even in its strong cost-sensitive form , does not prevent cases where the base learner always achieves high precision but poor recall or high recall but poor precision , when mapped to the original distribution . In either of these cases , we show that the voting mechanism of boosting falls to achieve good overall recall and precision for the ensemble . In effect , our analysis indicates that one can not be blind to the base learner performance , and just rely on the boosting mechanism to take care of its weakness . We validate our arguments empirically on variety of real and synthetic rare class problems . In particular , using AdaCost as the boosting algorithm , and variations of PNrule and RIPPER as the base learners , we show that if algorithm A achieves better recall-precision balance than algorithm B , then using A as the base learner in AdaCost yields significantly better performance than using B as the base learner . [[EENNDD]] analyze"}, "Meramalkan kelas yang jarang berlaku: boleh meningkatkan menjadikan pelajar lemah menjadi kuat? Meningkatkan adalah algoritma pembelajaran berasaskan ensemble yang kuat dengan janji untuk meningkatkan ketepatan klasifikasi secara berulang-ulang menggunakan mana-mana pelajar asas, asalkan memenuhi syarat menghasilkan ketepatan berwajaran 0.5. Dalam makalah ini, kami menganalisis peningkatan sehubungan dengan kondisi dasar ini pada pelajar dasar, untuk melihat apakah peningkatan memastikan ramalan peristiwa yang jarang terjadi dengan penarikan dan ketepatan yang tinggi. Mula-mula kami menunjukkan bahawa pelajar asas dapat memenuhi syarat yang diperlukan walaupun untuk tahap penarikan atau ketepatan yang buruk, terutama untuk kelas yang sangat jarang berlaku. Selanjutnya, kami menunjukkan bahawa mekanisme pengemaskinian berat yang cerdas dalam meningkatkan, walaupun dalam bentuk sensitif kosnya yang kuat, tidak menghalang kes di mana pelajar asas selalu mencapai ketepatan tinggi tetapi penarikan balik yang rendah atau penarikan balik yang tinggi tetapi ketepatan yang buruk, ketika dipetakan ke pengedaran asal . Dalam kedua-dua kes ini, kami menunjukkan bahawa mekanisme pemungutan suara untuk meningkatkan jatuh untuk mencapai penarikan dan ketepatan keseluruhan keseluruhan yang baik untuk ensemble. Akibatnya, analisis kami menunjukkan bahawa seseorang tidak boleh mengetahui prestasi pelajar asas, dan hanya bergantung pada mekanisme peningkatan untuk mengatasi kelemahannya. Kami mengesahkan hujah kami secara empirik mengenai pelbagai masalah kelas jarang dan sintetik. Khususnya, menggunakan AdaCost sebagai algoritma penambahbaikan, dan variasi PNrule dan RIPPER sebagai pelajar asas, kami menunjukkan bahawa jika algoritma A mencapai keseimbangan ketepatan ingat yang lebih baik daripada algoritma B, maka menggunakan A sebagai pelajar asas di AdaCost menghasilkan prestasi yang jauh lebih baik daripada menggunakan B sebagai pelajar asas. [[EENNDD]] analisis"], [{"string": "Structured learning for non-smooth ranking losses Learning to rank from relevance judgment is an active research area . Itemwise score regression , pairwise preference satisfaction , and listwise structured learning are the major techniques in use . Listwise structured learning has been applied recently to optimize important non-decomposable ranking criteria like AUC area under ROC curve and MAP mean average precision . We propose new , almost-linear-time algorithms to optimize for two other criteria widely used to evaluate search systems : MRR mean reciprocal rank and NDCG normalized discounted cumulative gain in the max-margin structured learning framework . We also demonstrate that , for different ranking criteria , one may need to use different feature maps . Search applications should not be optimized in favor of a single criterion , because they need to cater to a variety of queries . E.g. , MRR is best for navigational queries , while NDCG is best for informational queries . A key contribution of this paper is to fold multiple ranking loss functions into a multi-criteria max-margin optimization . The result is a single , robust ranking model that is close to the best accuracy of learners trained on individual criteria . In fact , experiments over the popular LETOR and TREC data sets show that , contrary to conventional wisdom , a test criterion is often not best served by training with the same individual criterion .", "keywords": ["max-margin structured learning to rank", "non-decomposable loss functions"], "combined": "Structured learning for non-smooth ranking losses Learning to rank from relevance judgment is an active research area . Itemwise score regression , pairwise preference satisfaction , and listwise structured learning are the major techniques in use . Listwise structured learning has been applied recently to optimize important non-decomposable ranking criteria like AUC area under ROC curve and MAP mean average precision . We propose new , almost-linear-time algorithms to optimize for two other criteria widely used to evaluate search systems : MRR mean reciprocal rank and NDCG normalized discounted cumulative gain in the max-margin structured learning framework . We also demonstrate that , for different ranking criteria , one may need to use different feature maps . Search applications should not be optimized in favor of a single criterion , because they need to cater to a variety of queries . E.g. , MRR is best for navigational queries , while NDCG is best for informational queries . A key contribution of this paper is to fold multiple ranking loss functions into a multi-criteria max-margin optimization . The result is a single , robust ranking model that is close to the best accuracy of learners trained on individual criteria . In fact , experiments over the popular LETOR and TREC data sets show that , contrary to conventional wisdom , a test criterion is often not best served by training with the same individual criterion . [[EENNDD]] max-margin structured learning to rank; non-decomposable loss functions"}, "Pembelajaran berstruktur untuk kerugian peringkat yang tidak lancar Belajar memberi peringkat dari pertimbangan relevan adalah bidang penyelidikan yang aktif. Regresi skor itemwise, kepuasan preferensi berpasangan, dan pembelajaran tersusun mengikut arah adalah teknik utama yang digunakan. Pembelajaran berstruktur mengikut arah telah diterapkan baru-baru ini untuk mengoptimumkan kriteria peringkat penting yang tidak dapat diuraikan seperti kawasan AUC di bawah keluk ROC dan ketepatan purata purata MAP. Kami mencadangkan algoritma baru hampir linear untuk mengoptimumkan dua kriteria lain yang digunakan secara meluas untuk menilai sistem carian: MRR peringkat timbal balik rata-rata dan NDCG dinaikkan keuntungan kumulatif potongan dalam kerangka pembelajaran berstruktur margin maksimum. Kami juga menunjukkan bahawa, untuk kriteria peringkat yang berbeza, seseorang mungkin perlu menggunakan peta ciri yang berbeza. Aplikasi carian tidak boleh dioptimalkan untuk mendukung satu kriteria, kerana mereka perlu memenuhi berbagai pertanyaan. Cth. , MRR terbaik untuk pertanyaan navigasi, sementara NDCG paling baik untuk pertanyaan maklumat. Sumbangan utama makalah ini adalah melipatgandakan fungsi kehilangan kedudukan ke dalam pengoptimuman margin maksimum pelbagai kriteria. Hasilnya adalah model peringkat tunggal yang mantap dan hampir tepat dengan ketepatan terbaik pelajar yang dilatih berdasarkan kriteria individu. Sebenarnya, eksperimen ke atas set data LETOR dan TREC yang popular menunjukkan bahawa, bertentangan dengan kebijaksanaan konvensional, kriteria ujian sering kali tidak terbaik dilayan dengan latihan dengan kriteria individu yang sama. [[EENNDD]] pembelajaran berstruktur margin maksimum untuk diberi peringkat; fungsi kerugian yang tidak dapat diuraikan"], [{"string": "Webpage understanding : an integrated approach Recent work has shown the effectiveness of leveraging layout and tag-tree structure for segmenting webpages and labeling HTML elements . However , how to effectively segment and label the text contents inside HTML elements is still an open problem . Since many text contents on a webpage are often text fragments and not strictly grammatical , traditional natural language processing techniques , that typically expect grammatical sentences , are no longer directly applicable . In this paper , we examine how to use layout and tag-tree structure in a principled way to help understand text contents on webpages . We propose to segment and label the page structure and the text content of a webpage in a joint discriminative probabilistic model . In this model , semantic labels of page structure can be leveraged to help text content understanding , and semantic labels ofthe text phrases can be used in page structure understanding tasks such as data record detection . Thus , integration of both page structure and text content understanding leads to an integrated solution of webpage understanding . Experimental results on research homepage extraction show the feasibility and promise of our approach .", "keywords": ["webpage understanding", "text processing", "conditional random fields"], "combined": "Webpage understanding : an integrated approach Recent work has shown the effectiveness of leveraging layout and tag-tree structure for segmenting webpages and labeling HTML elements . However , how to effectively segment and label the text contents inside HTML elements is still an open problem . Since many text contents on a webpage are often text fragments and not strictly grammatical , traditional natural language processing techniques , that typically expect grammatical sentences , are no longer directly applicable . In this paper , we examine how to use layout and tag-tree structure in a principled way to help understand text contents on webpages . We propose to segment and label the page structure and the text content of a webpage in a joint discriminative probabilistic model . In this model , semantic labels of page structure can be leveraged to help text content understanding , and semantic labels ofthe text phrases can be used in page structure understanding tasks such as data record detection . Thus , integration of both page structure and text content understanding leads to an integrated solution of webpage understanding . Experimental results on research homepage extraction show the feasibility and promise of our approach . [[EENNDD]] webpage understanding; text processing; conditional random fields"}, "Pemahaman laman web: pendekatan bersepadu Karya terkini telah menunjukkan keberkesanan memanfaatkan susun atur dan struktur pohon tag untuk menyegmentasikan laman web dan melabel elemen HTML. Walau bagaimanapun, cara membahagikan dan melabel kandungan teks di dalam elemen HTML dengan berkesan masih menjadi masalah terbuka. Oleh kerana banyak kandungan teks di laman web selalunya merupakan pecahan teks dan tidak sepenuhnya tatabahasa, teknik pemprosesan bahasa semula jadi tradisional, yang biasanya mengharapkan kalimat tatabahasa, tidak lagi langsung berlaku. Dalam makalah ini, kami mengkaji bagaimana menggunakan susun atur dan struktur pohon tag dengan cara yang berprinsip untuk membantu memahami isi teks di laman web. Kami mencadangkan segmen dan label struktur halaman dan kandungan teks laman web dalam model probabilistik diskriminatif bersama. Dalam model ini, label semantik struktur halaman dapat dimanfaatkan untuk membantu pemahaman kandungan teks, dan label semantik frasa teks dapat digunakan dalam tugas memahami struktur halaman seperti pengesanan rekod data. Oleh itu, penyatuan struktur halaman dan pemahaman kandungan teks membawa kepada penyelesaian pemahaman laman web yang bersepadu. Hasil eksperimen pengekstrakan halaman utama penyelidikan menunjukkan kemungkinan dan janji pendekatan kami. [[EENNDD]] pemahaman laman web; pemprosesan teks; medan rawak bersyarat"], [{"string": "A scalable modular convex solver for regularized risk minimization A wide variety of machine learning problems can be described as minimizing a regularized risk functional , with different algorithms using different notions of risk and different regularizers . Examples include linear Support Vector Machines SVMs , Logistic Regression , Conditional Random Fields CRFs , and Lasso amongst others . This paper describes the theory and implementation of a highly scalable and modular convex solver which solves all these estimation problems . It can be parallelized on a cluster of workstations , allows for data-locality , and can deal with regularizers such as l1 and l2 penalties . At present , our solver implements 20 different estimation problems , can be easily extended , scales to millions of observations , and is up to 10 times faster than specialized solvers for many applications . The open source code is freely available as part of the ELEFANT toolbox .", "keywords": ["optimization", "convexity"], "combined": "A scalable modular convex solver for regularized risk minimization A wide variety of machine learning problems can be described as minimizing a regularized risk functional , with different algorithms using different notions of risk and different regularizers . Examples include linear Support Vector Machines SVMs , Logistic Regression , Conditional Random Fields CRFs , and Lasso amongst others . This paper describes the theory and implementation of a highly scalable and modular convex solver which solves all these estimation problems . It can be parallelized on a cluster of workstations , allows for data-locality , and can deal with regularizers such as l1 and l2 penalties . At present , our solver implements 20 different estimation problems , can be easily extended , scales to millions of observations , and is up to 10 times faster than specialized solvers for many applications . The open source code is freely available as part of the ELEFANT toolbox . [[EENNDD]] optimization; convexity"}, "Penyelesai cembung modular yang dapat diskalakan untuk meminimumkan risiko terkawal Berbagai masalah pembelajaran mesin dapat digambarkan sebagai meminimumkan risiko teratur yang berfungsi, dengan algoritma yang berbeda menggunakan pengertian risiko yang berbeza dan pengatur yang berbeza. Contohnya termasuk SVM Mesin Vektor Sokongan linear, Regresi Logistik, CRF Medan Rawak Bersyarat, dan Lasso antara lain. Makalah ini menerangkan teori dan pelaksanaan pemecah cembung modular yang sangat berskala dan menyelesaikan semua masalah anggaran ini. Ia dapat diselaraskan pada sekumpulan stesen kerja, memungkinkan untuk mendapatkan data-lokasi, dan dapat menangani pengatur seperti hukuman l1 dan l2. Pada masa ini, penyelesai kami melaksanakan 20 masalah anggaran yang berbeza, dapat diperluas dengan mudah, skala hingga berjuta-juta pemerhatian, dan hingga 10 kali lebih cepat daripada pemecah khusus untuk banyak aplikasi. Kod sumber terbuka boleh didapati secara bebas sebagai sebahagian daripada kotak alat ELEFANT. [[EENNDD]] pengoptimuman; cembung"], [{"string": "Learning and making decisions when costs and probabilities are both unknown In many data mining domains , misclassification costs are different for different examples , in the same way that class membership probabilities are example-dependent . In these domains , both costs and probabilities are unknown for test examples , so both cost estimators and probability estimators must be learned . After discussing how to make optimal decisions given cost and probability estimates , we present decision tree and naive Bayesian learning methods for obtaining well-calibrated probability estimates . We then explain how to obtain unbiased estimators for example-dependent costs , taking into account the difficulty that in general , probabilities and costs are not independent random variables , and the training examples for which costs are known are not representative of all examples . The latter problem is called sample selection bias in econometrics . Our solution to it is based on Nobel prize-winning work due to the economist James Heckman . We show that the methods we propose perform better than MetaCost and all other known methods , in a comprehensive experimental comparison that uses the well-known , large , and challenging dataset from the KDD '98 data mining contest .", "keywords": ["probabilistic algorithms"], "combined": "Learning and making decisions when costs and probabilities are both unknown In many data mining domains , misclassification costs are different for different examples , in the same way that class membership probabilities are example-dependent . In these domains , both costs and probabilities are unknown for test examples , so both cost estimators and probability estimators must be learned . After discussing how to make optimal decisions given cost and probability estimates , we present decision tree and naive Bayesian learning methods for obtaining well-calibrated probability estimates . We then explain how to obtain unbiased estimators for example-dependent costs , taking into account the difficulty that in general , probabilities and costs are not independent random variables , and the training examples for which costs are known are not representative of all examples . The latter problem is called sample selection bias in econometrics . Our solution to it is based on Nobel prize-winning work due to the economist James Heckman . We show that the methods we propose perform better than MetaCost and all other known methods , in a comprehensive experimental comparison that uses the well-known , large , and challenging dataset from the KDD '98 data mining contest . [[EENNDD]] probabilistic algorithms"}, "Belajar dan membuat keputusan apabila kos dan kebarangkalian tidak diketahui. Dalam banyak domain perlombongan data, kos salah klasifikasi berbeza untuk contoh yang berbeza, dengan cara yang sama bahawa kebarangkalian keahlian kelas bergantung pada contoh. Dalam domain ini, kedua-dua kos dan kebarangkalian tidak diketahui sebagai contoh ujian, jadi kedua-dua penganggar kos dan penganggar kebarangkalian mesti dipelajari. Setelah membincangkan bagaimana membuat keputusan yang optimum dengan mengira anggaran kos dan kebarangkalian, kami memaparkan kaedah pembelajaran pohon keputusan dan Bayesian yang naif untuk memperoleh anggaran kebarangkalian yang dikalibrasi dengan baik. Kami kemudian menerangkan bagaimana mendapatkan penganggar yang tidak berat sebelah untuk kos bergantung pada contoh, dengan mengambil kira kesukaran bahawa secara amnya, kebarangkalian dan kos bukan pemboleh ubah rawak bebas, dan contoh latihan yang diketahui kosnya tidak mewakili semua contoh. Masalah terakhir disebut bias pemilihan sampel dalam ekonometrik. Penyelesaian kami terhadapnya adalah berdasarkan karya pemenang hadiah Nobel kerana ahli ekonomi James Heckman. Kami menunjukkan bahawa kaedah yang kami cadangkan berkinerja lebih baik daripada MetaCost dan semua kaedah lain yang diketahui, dalam perbandingan eksperimen komprehensif yang menggunakan set data yang terkenal, besar, dan mencabar dari pertandingan perlombongan data KDD '98. [[EENNDD]] algoritma probabilistik"], [{"string": "Reconstructing chemical reaction networks : data mining meets system identification We present an approach to reconstructing chemical reaction networks from time series measurements of the concentrations of the molecules involved . Our solution strategy combines techniques from numerical sensitivity analysis and probabilistic graphical models . By modeling a chemical reaction system as a Markov network undirected graphical model , we show how systematically probing for sensitivities between molecular species can identify the topology of the network . Given the topology , our approach next uses detailed sensitivity profiles to characterize properties of reactions such as reversibility , enzyme-catalysis , and the precise stoichiometries of the reactants and products . We demonstrate applications to reconstructing key biological systems including the yeast cell cycle . In addition to network reconstruction , our algorithm finds applications in model reduction and model comprehension . We argue that our reconstruction algorithm can serve as an important primitive for data mining in systems biology applications .", "keywords": ["graphical models", "systems biology", "ordinary differential equations", "markov networks", "network reconstruction"], "combined": "Reconstructing chemical reaction networks : data mining meets system identification We present an approach to reconstructing chemical reaction networks from time series measurements of the concentrations of the molecules involved . Our solution strategy combines techniques from numerical sensitivity analysis and probabilistic graphical models . By modeling a chemical reaction system as a Markov network undirected graphical model , we show how systematically probing for sensitivities between molecular species can identify the topology of the network . Given the topology , our approach next uses detailed sensitivity profiles to characterize properties of reactions such as reversibility , enzyme-catalysis , and the precise stoichiometries of the reactants and products . We demonstrate applications to reconstructing key biological systems including the yeast cell cycle . In addition to network reconstruction , our algorithm finds applications in model reduction and model comprehension . We argue that our reconstruction algorithm can serve as an important primitive for data mining in systems biology applications . [[EENNDD]] graphical models; systems biology; ordinary differential equations; markov networks; network reconstruction"}, "Membina semula rangkaian tindak balas kimia: perlombongan data memenuhi pengenalan sistem. Kami menyajikan pendekatan untuk membina semula rangkaian reaksi kimia dari pengukuran siri masa kepekatan molekul yang terlibat. Strategi penyelesaian kami menggabungkan teknik dari analisis kepekaan berangka dan model grafik probabilistik. Dengan memodelkan sistem tindak balas kimia sebagai model grafik tidak terarah rangkaian Markov, kami menunjukkan bagaimana secara sistematik menyelidiki kepekaan antara spesies molekul dapat mengenal pasti topologi rangkaian. Mengingat topologi, pendekatan kami seterusnya menggunakan profil kepekaan terperinci untuk mencirikan sifat tindak balas seperti kebolehbalikan, pemangkinan enzim, dan stokikiometri reaktan dan produk yang tepat. Kami menunjukkan aplikasi untuk menyusun semula sistem biologi utama termasuk kitaran sel ragi. Selain pembinaan semula rangkaian, algoritma kami menemui aplikasi dalam pengurangan model dan pemahaman model. Kami berpendapat bahawa algoritma pembinaan semula kami dapat berfungsi sebagai primitif penting untuk perlombongan data dalam aplikasi biologi sistem. [[EENNDD]] model grafik; biologi sistem; persamaan pembezaan biasa; rangkaian markov; pembinaan semula rangkaian"], [{"string": "Explicitly representing expected cost : an alternative to ROC representation", "keywords": ["roc analysis", "cost sensitive learning"], "combined": "Explicitly representing expected cost : an alternative to ROC representation [[EENNDD]] roc analysis; cost sensitive learning"}, "Secara jelas menunjukkan jangkaan kos: alternatif untuk perwakilan ROC [[EENNDD]] analisis roc; pembelajaran sensitif kos"], [{"string": "Feature bagging for outlier detection Outlier detection has recently become an important problem in many industrial and financial applications . In this paper , a novel feature bagging approach for detecting outliers in very large , high dimensional and noisy databases is proposed . It combines results from multiple outlier detection algorithms that are applied using different set of features . Every outlier detection algorithm uses a small subset of features that are randomly selected from the original feature set . As a result , each outlier detector identifies different outliers , and thus assigns to all data records outlier scores that correspond to their probability of being outliers . The outlier scores computed by the individual outlier detection algorithms are then combined in order to find the better quality outliers . Experiments performed on several synthetic and real life data sets show that the proposed methods for combining outputs from multiple outlier detection algorithms provide non-trivial improvements over the base algorithm .", "keywords": ["detection rate", "false alarm", "feature subsets", "outlier detection", "bagging", "integration"], "combined": "Feature bagging for outlier detection Outlier detection has recently become an important problem in many industrial and financial applications . In this paper , a novel feature bagging approach for detecting outliers in very large , high dimensional and noisy databases is proposed . It combines results from multiple outlier detection algorithms that are applied using different set of features . Every outlier detection algorithm uses a small subset of features that are randomly selected from the original feature set . As a result , each outlier detector identifies different outliers , and thus assigns to all data records outlier scores that correspond to their probability of being outliers . The outlier scores computed by the individual outlier detection algorithms are then combined in order to find the better quality outliers . Experiments performed on several synthetic and real life data sets show that the proposed methods for combining outputs from multiple outlier detection algorithms provide non-trivial improvements over the base algorithm . [[EENNDD]] detection rate; false alarm; feature subsets; outlier detection; bagging; integration"}, "Fitur pengantian untuk pengesanan outlier Pengesanan outlier baru-baru ini menjadi masalah penting dalam banyak aplikasi industri dan kewangan. Dalam makalah ini, dicadangkan pendekatan penekanan fitur baru untuk mengesan outliers dalam pangkalan data dimensi tinggi dan bising yang sangat besar. Ini menggabungkan hasil dari beberapa algoritma pengesanan outlier yang diterapkan menggunakan rangkaian fitur yang berbeza. Setiap algoritma pengesanan outlier menggunakan sekumpulan kecil fitur yang dipilih secara rawak dari set ciri asal. Akibatnya, setiap pengesan outlier mengenal pasti outlier yang berlainan, dan dengan demikian memberikan kepada semua data data skor outlier yang sesuai dengan kebarangkalian mereka menjadi outlier. Skor outlier yang dikira oleh algoritma pengesanan outlier individu kemudian digabungkan untuk mencari outlier berkualiti yang lebih baik. Eksperimen yang dilakukan pada beberapa kumpulan data kehidupan sintetik dan nyata menunjukkan bahawa kaedah yang dicadangkan untuk menggabungkan output dari pelbagai algoritma pengesanan outlier memberikan peningkatan yang tidak remeh berbanding algoritma asas. [[EENNDD]] kadar pengesanan; penggera palsu; subset ciri; pengesanan luar; mengemas; penyatuan"], [{"string": "Distributed multivariate regression based on influential observations Large-scale data sets are sometimes logically and physically distributed in separate databases . The issues of mining these data sets are not just their sizes , but also the distributed nature . The complication is that communicating all the data to a central database would be too slow . To reduce communication costs , one could compress the data during transmission . Another method is random sampling . We propose an approach for distributed multivariate regression based on sampling and discuss its relationship with the compression method . The central idea is motivated by the observation that , although communication is limited , each individual site can still scan and process all the data it holds . Thus it is possible for the site to communicate only influential samples without seeing data in other sites . We exploit this observation and derive a method that provides tradeoff between communication cost and accuracy . Experimental results show that it is better than the compression method and random sampling .", "keywords": ["multivariate linear regression", "sampling", "learning curve", "distributed data mining"], "combined": "Distributed multivariate regression based on influential observations Large-scale data sets are sometimes logically and physically distributed in separate databases . The issues of mining these data sets are not just their sizes , but also the distributed nature . The complication is that communicating all the data to a central database would be too slow . To reduce communication costs , one could compress the data during transmission . Another method is random sampling . We propose an approach for distributed multivariate regression based on sampling and discuss its relationship with the compression method . The central idea is motivated by the observation that , although communication is limited , each individual site can still scan and process all the data it holds . Thus it is possible for the site to communicate only influential samples without seeing data in other sites . We exploit this observation and derive a method that provides tradeoff between communication cost and accuracy . Experimental results show that it is better than the compression method and random sampling . [[EENNDD]] multivariate linear regression; sampling; learning curve; distributed data mining"}, "Regresi multivariat yang diedarkan berdasarkan pemerhatian berpengaruh Kumpulan data berskala besar kadangkala diedarkan secara logik dan fizikal dalam pangkalan data yang berasingan. Masalah perlombongan set data ini bukan hanya ukurannya, tetapi juga sifat yang diedarkan. Komplikasinya adalah bahawa komunikasi semua data ke pangkalan data pusat akan menjadi terlalu lambat. Untuk mengurangkan kos komunikasi, seseorang dapat memampatkan data semasa penghantaran. Kaedah lain ialah persampelan rawak. Kami mencadangkan pendekatan untuk regresi multivariat terdistribusi berdasarkan pensampelan dan membincangkan hubungannya dengan kaedah pemampatan. Idea utamanya didorong oleh pemerhatian bahawa, walaupun komunikasi terbatas, setiap laman web individu masih dapat mengimbas dan memproses semua data yang disimpannya. Oleh itu, laman web hanya boleh berkomunikasi dengan sampel yang berpengaruh tanpa melihat data di laman web lain. Kami memanfaatkan pemerhatian ini dan mendapatkan kaedah yang memberikan pertukaran antara kos komunikasi dan ketepatan. Hasil eksperimen menunjukkan bahawa lebih baik daripada kaedah pemampatan dan persampelan rawak. [[EENNDD]] regresi linear multivariate; persampelan; keluk pembelajaran; perlombongan data yang diedarkan"], [{"string": "PROXIMUS : a framework for analyzing very high dimensional discrete-attributed datasets This paper presents an efficient framework for error-bounded compression of high-dimensional discrete attributed datasets . Such datasets , which frequently arise in a wide variety of applications , pose some of the most significant challenges in data analysis . Subsampling and compression are two key technologies for analyzing these datasets . PROXIMUS provides a technique for reducing large datasets into a much smaller set of representative patterns , on which traditional expensive analysis algorithms can be applied with minimal loss of accuracy . We show desirable properties of PROXIMUS in terms of runtime , scalability to large datasets , and performance in terms of capability to represent data in a compact form . We also demonstrate applications of PROXIMUS in association rule mining . In doing so , we establish PROXIMUS as a tool for preprocessing data before applying computationally expensive algorithms or as a tool for directly extracting correlated patterns . Our experimental results show that use of the compressed data for association rule mining provides excellent precision and recall values near 100 % across a range of support thresholds while reducing the time required for association rule mining drastically .", "keywords": ["compressing discrete-valued vectors", "non-orthogonal matrix decompositions", "semi-discrete decomposition", "clustering"], "combined": "PROXIMUS : a framework for analyzing very high dimensional discrete-attributed datasets This paper presents an efficient framework for error-bounded compression of high-dimensional discrete attributed datasets . Such datasets , which frequently arise in a wide variety of applications , pose some of the most significant challenges in data analysis . Subsampling and compression are two key technologies for analyzing these datasets . PROXIMUS provides a technique for reducing large datasets into a much smaller set of representative patterns , on which traditional expensive analysis algorithms can be applied with minimal loss of accuracy . We show desirable properties of PROXIMUS in terms of runtime , scalability to large datasets , and performance in terms of capability to represent data in a compact form . We also demonstrate applications of PROXIMUS in association rule mining . In doing so , we establish PROXIMUS as a tool for preprocessing data before applying computationally expensive algorithms or as a tool for directly extracting correlated patterns . Our experimental results show that use of the compressed data for association rule mining provides excellent precision and recall values near 100 % across a range of support thresholds while reducing the time required for association rule mining drastically . [[EENNDD]] compressing discrete-valued vectors; non-orthogonal matrix decompositions; semi-discrete decomposition; clustering"}, "PROXIMUS: kerangka kerja untuk menganalisis set data diskrit yang mempunyai dimensi yang sangat tinggi. Makalah ini menyajikan kerangka kerja yang cekap untuk pemampatan terikat ralat dari set data diskrit berdimensi tinggi. Set data seperti itu, yang sering muncul dalam berbagai aplikasi, menimbulkan beberapa tantangan paling penting dalam analisis data. Pengambilan sampel dan pemampatan adalah dua teknologi utama untuk menganalisis set data ini. PROXIMUS menyediakan teknik untuk mengurangkan set data yang besar menjadi satu set corak perwakilan yang jauh lebih kecil, di mana algoritma analisis mahal tradisional dapat diterapkan dengan kehilangan ketepatan yang minimum. Kami menunjukkan sifat PROXIMUS yang diingini dari segi jangka masa, skalabiliti ke set data yang besar, dan prestasi dari segi kemampuan untuk mewakili data dalam bentuk yang ringkas. Kami juga menunjukkan aplikasi PROXIMUS dalam perlombongan peraturan persatuan. Dengan berbuat demikian, kami menetapkan PROXIMUS sebagai alat untuk memproses data sebelum menggunakan algoritma yang mahal atau sebagai alat untuk mengekstrak corak yang berkorelasi secara langsung. Hasil eksperimen kami menunjukkan bahawa penggunaan data yang dimampatkan untuk perlombongan peraturan persatuan memberikan nilai ketepatan dan penarikan yang sangat baik hampir 100% merentasi ambang sokongan sambil mengurangkan masa yang diperlukan untuk perlombongan peraturan persatuan secara drastik. [[EENNDD]] memampatkan vektor bernilai diskrit; penguraian matriks bukan ortogonal; penguraian separa diskrit; pengelompokan"], [{"string": "Beyond blacklists : learning to detect malicious web sites from suspicious URLs Malicious Web sites are a cornerstone of Internet criminal activities . As a result , there has been broad interest in developing systems to prevent the end user from visiting such sites . In this paper , we describe an approach to this problem based on automated URL classification , using statistical methods to discover the tell-tale lexical and host-based properties of malicious Web site URLs . These methods are able to learn highly predictive models by extracting and automatically analyzing tens of thousands of features potentially indicative of suspicious URLs . The resulting classifiers obtain 95-99 % accuracy , detecting large numbers of malicious Web sites from their URLs , with only modest false positives .", "keywords": ["supervised learning", "l1-regularization", "security and protection", "abuse and crime involving computers", "malicious web sites"], "combined": "Beyond blacklists : learning to detect malicious web sites from suspicious URLs Malicious Web sites are a cornerstone of Internet criminal activities . As a result , there has been broad interest in developing systems to prevent the end user from visiting such sites . In this paper , we describe an approach to this problem based on automated URL classification , using statistical methods to discover the tell-tale lexical and host-based properties of malicious Web site URLs . These methods are able to learn highly predictive models by extracting and automatically analyzing tens of thousands of features potentially indicative of suspicious URLs . The resulting classifiers obtain 95-99 % accuracy , detecting large numbers of malicious Web sites from their URLs , with only modest false positives . [[EENNDD]] supervised learning; l1-regularization; security and protection; abuse and crime involving computers; malicious web sites"}, "Di luar senarai hitam: belajar mengesan laman web berniat jahat dari URL yang mencurigakan Laman web berniat jahat adalah tonggak aktiviti jenayah Internet. Akibatnya, ada minat yang luas dalam mengembangkan sistem untuk mencegah pengguna akhir mengunjungi laman web tersebut. Dalam makalah ini, kami menerangkan pendekatan untuk masalah ini berdasarkan klasifikasi URL automatik, menggunakan kaedah statistik untuk mengetahui sifat leksikal bercerita dan hos URL laman web berbahaya. Kaedah ini dapat mempelajari model yang sangat meramal dengan mengekstrak dan menganalisis secara automatik puluhan ribu ciri yang berpotensi menunjukkan URL yang mencurigakan. Pengklasifikasi yang dihasilkan memperoleh ketepatan 95-99%, mengesan sebilangan besar laman web jahat dari URL mereka, dengan hanya positif palsu yang sederhana. [[EENNDD]] pembelajaran yang diselia; l1-regularisasi; keselamatan dan perlindungan; penyalahgunaan dan jenayah yang melibatkan komputer; laman web berniat jahat"], [{"string": "Simultaneous record detection and attribute labeling in web data extraction Recent work has shown the feasibility and promise of template-independent Web data extraction . However , existing approaches use decoupled strategies - attempting to do data record detection and attribute labeling in two separate phases . In this paper , we show that separately extracting data records and attributes is highly ineffective and propose a probabilistic model to perform these two tasks simultaneously . In our approach , record detection can benefit from the availability of semantics required in attribute labeling and , at the same time , the accuracy of attribute labeling can be improved when data records are labeled in a collective manner . The proposed model is called Hierarchical Conditional Random Fields . It can efficiently integrate all useful features by learning their importance , and it can also incorporate hierarchical interactions which are very important for Web data extraction . We empirically compare the proposed model with existing decoupled approaches for product information extraction , and the results show significant improvements in both record detection and attribute labeling .", "keywords": ["attribute labeling", "conditional random fields", "data record detection", "web page segmentation", "hierarchical conditional random fields"], "combined": "Simultaneous record detection and attribute labeling in web data extraction Recent work has shown the feasibility and promise of template-independent Web data extraction . However , existing approaches use decoupled strategies - attempting to do data record detection and attribute labeling in two separate phases . In this paper , we show that separately extracting data records and attributes is highly ineffective and propose a probabilistic model to perform these two tasks simultaneously . In our approach , record detection can benefit from the availability of semantics required in attribute labeling and , at the same time , the accuracy of attribute labeling can be improved when data records are labeled in a collective manner . The proposed model is called Hierarchical Conditional Random Fields . It can efficiently integrate all useful features by learning their importance , and it can also incorporate hierarchical interactions which are very important for Web data extraction . We empirically compare the proposed model with existing decoupled approaches for product information extraction , and the results show significant improvements in both record detection and attribute labeling . [[EENNDD]] attribute labeling; conditional random fields; data record detection; web page segmentation; hierarchical conditional random fields"}, "Pengesanan rekod dan pelabelan atribut serentak dalam pengekstrakan data web Karya terbaru menunjukkan kemungkinan dan janji pengekstrakan data Web bebas templat. Walau bagaimanapun, pendekatan yang ada menggunakan strategi yang dipisahkan - cuba melakukan pengesanan rekod data dan pelabelan atribut dalam dua fasa yang terpisah. Dalam makalah ini, kami menunjukkan bahawa mengekstrak data dan atribut data secara berasingan sangat tidak berkesan dan mencadangkan model probabilistik untuk melaksanakan kedua tugas ini secara serentak. Dalam pendekatan kami, pengesanan rekod dapat memanfaatkan ketersediaan semantik yang diperlukan dalam pelabelan atribut dan, pada masa yang sama, ketepatan pelabelan atribut dapat ditingkatkan ketika catatan data dilabel secara kolektif. Model yang dicadangkan dipanggil Medan Rawak Bersyarat Hierarki. Ia dapat menggabungkan semua ciri berguna dengan berkesan dengan mengetahui kepentingannya, dan juga dapat menggabungkan interaksi hirarki yang sangat penting untuk pengekstrakan data Web. Kami secara empirik membandingkan model yang dicadangkan dengan pendekatan yang sudah dipisahkan untuk pengekstrakan maklumat produk, dan hasilnya menunjukkan peningkatan yang signifikan dalam pengesanan rekod dan pelabelan atribut. [[EENNDD]] pelabelan atribut; medan rawak bersyarat; pengesanan rekod data; segmentasi halaman web; bidang rawak bersyarat hierarki"], [{"string": "LungCAD : a clinically approved , machine learning system for lung cancer detection We present LungCAD , a computer aided diagnosis CAD system that employs a classification algorithm for detecting solid pulmonary nodules from CT thorax studies . We briefly describe some of the machine learning techniques developed to overcome the real world challenges in this medical domain . The most significant hurdle in transitioning from a machine learning research prototype that performs well on an in-house dataset into a clinically deployable system , is the requirement that the CAD system be tested in a clinical trial . We describe the clinical trial in which LungCAD was tested : a large scale multi-reader , multi-case MRMC retrospective observational study to evaluate the effect of CAD in clinical practice for detecting solid pulmonary nodules from CT thorax studies . The clinical trial demonstrates that every radiologist that participated in the trial had a significantly greater accuracy with LungCAD , both for detecting nodules and identifying potentially actionable nodules ; this , along with other findings from the trial , has resulted in FDA approval for LungCAD in late 2006 .", "keywords": ["classification", "computer aided detection", "miscellaneous", "lung cancer prognosis", "clinical trial"], "combined": "LungCAD : a clinically approved , machine learning system for lung cancer detection We present LungCAD , a computer aided diagnosis CAD system that employs a classification algorithm for detecting solid pulmonary nodules from CT thorax studies . We briefly describe some of the machine learning techniques developed to overcome the real world challenges in this medical domain . The most significant hurdle in transitioning from a machine learning research prototype that performs well on an in-house dataset into a clinically deployable system , is the requirement that the CAD system be tested in a clinical trial . We describe the clinical trial in which LungCAD was tested : a large scale multi-reader , multi-case MRMC retrospective observational study to evaluate the effect of CAD in clinical practice for detecting solid pulmonary nodules from CT thorax studies . The clinical trial demonstrates that every radiologist that participated in the trial had a significantly greater accuracy with LungCAD , both for detecting nodules and identifying potentially actionable nodules ; this , along with other findings from the trial , has resulted in FDA approval for LungCAD in late 2006 . [[EENNDD]] classification; computer aided detection; miscellaneous; lung cancer prognosis; clinical trial"}, "LungCAD: sistem pembelajaran mesin yang diluluskan secara klinikal untuk pengesanan barah paru-paru Kami menghadirkan LungCAD, sistem CAD diagnosis bantuan komputer yang menggunakan algoritma klasifikasi untuk mengesan nodul paru pepejal dari kajian CT toraks. Kami menerangkan secara ringkas beberapa teknik pembelajaran mesin yang dikembangkan untuk mengatasi cabaran dunia nyata dalam domain perubatan ini. Halangan yang paling ketara dalam peralihan dari prototaip penyelidikan pembelajaran mesin yang berkinerja baik pada set data dalaman ke dalam sistem yang dapat digunakan secara klinikal, adalah syarat sistem CAD diuji dalam percubaan klinikal. Kami menerangkan percubaan klinikal di mana LungCAD diuji: kajian pemerhatian retrospektif MRMC pelbagai pembaca skala besar, untuk menilai kesan CAD dalam amalan klinikal untuk mengesan nodul paru pepejal dari kajian CT toraks. Percubaan klinikal menunjukkan bahawa setiap ahli radiologi yang mengambil bahagian dalam percubaan mempunyai ketepatan yang jauh lebih besar dengan LungCAD, baik untuk mengesan nodul dan mengenal pasti nodul yang berpotensi dapat bertindak; ini, bersama dengan penemuan lain dari perbicaraan, telah menghasilkan persetujuan FDA untuk LungCAD pada akhir tahun 2006. [[EENNDD]] klasifikasi; pengesanan berbantukan komputer; pelbagai; prognosis kanser paru-paru; percubaan klinikal"], [{"string": "Issues in evaluation of stream learning algorithms Learning from data streams is a research area of increasing importance . Nowadays , several stream learning algorithms have been developed . Most of them learn decision models that continuously evolve over time , run in resource-aware environments , detect and react to changes in the environment generating data . One important issue , not yet conveniently addressed , is the design of experimental work to evaluate and compare decision models that evolve over time . There are no golden standards for assessing performance in non-stationary environments . This paper proposes a general framework for assessing predictive stream learning algorithms . We defend the use of Predictive Sequential methods for error estimate - the prequential error . The prequential error allows us to monitor the evolution of the performance of models that evolve over time . Nevertheless , it is known to be a pessimistic estimator in comparison to holdout estimates . To obtain more reliable estimators we need some forgetting mechanism . Two viable alternatives are : sliding windows and fading factors . We observe that the prequential error converges to an holdout estimator when estimated over a sliding window or using fading factors . We present illustrative examples of the use of prequential error estimators , using fading factors , for the tasks of : i assessing performance of a learning algorithm ; ii comparing learning algorithms ; iii hypothesis testing using McNemar test ; and iv change detection using Page-Hinkley test . In these tasks , the prequential error estimated using fading factors provide reliable estimators . In comparison to sliding windows , fading factors are faster and memory-less , a requirement for streaming applications . This paper is a contribution to a discussion in the good-practices on performance assessment when learning dynamic models that evolve over time .", "keywords": ["data streams", "evaluation design"], "combined": "Issues in evaluation of stream learning algorithms Learning from data streams is a research area of increasing importance . Nowadays , several stream learning algorithms have been developed . Most of them learn decision models that continuously evolve over time , run in resource-aware environments , detect and react to changes in the environment generating data . One important issue , not yet conveniently addressed , is the design of experimental work to evaluate and compare decision models that evolve over time . There are no golden standards for assessing performance in non-stationary environments . This paper proposes a general framework for assessing predictive stream learning algorithms . We defend the use of Predictive Sequential methods for error estimate - the prequential error . The prequential error allows us to monitor the evolution of the performance of models that evolve over time . Nevertheless , it is known to be a pessimistic estimator in comparison to holdout estimates . To obtain more reliable estimators we need some forgetting mechanism . Two viable alternatives are : sliding windows and fading factors . We observe that the prequential error converges to an holdout estimator when estimated over a sliding window or using fading factors . We present illustrative examples of the use of prequential error estimators , using fading factors , for the tasks of : i assessing performance of a learning algorithm ; ii comparing learning algorithms ; iii hypothesis testing using McNemar test ; and iv change detection using Page-Hinkley test . In these tasks , the prequential error estimated using fading factors provide reliable estimators . In comparison to sliding windows , fading factors are faster and memory-less , a requirement for streaming applications . This paper is a contribution to a discussion in the good-practices on performance assessment when learning dynamic models that evolve over time . [[EENNDD]] data streams; evaluation design"}, "Isu dalam penilaian algoritma pembelajaran aliran Belajar dari aliran data adalah bidang penyelidikan yang semakin penting. Pada masa kini, beberapa algoritma pembelajaran aliran telah dikembangkan. Sebilangan besar dari mereka mempelajari model keputusan yang terus berkembang dari masa ke masa, berjalan di persekitaran yang menyedari sumber, mengesan dan bertindak balas terhadap perubahan dalam persekitaran yang menghasilkan data. Satu masalah penting, yang belum dapat ditangani dengan mudah, adalah reka bentuk kerja eksperimen untuk menilai dan membandingkan model keputusan yang berkembang dari masa ke masa. Tidak ada piawaian emas untuk menilai prestasi dalam persekitaran yang tidak bergerak. Makalah ini mencadangkan kerangka umum untuk menilai algoritma pembelajaran aliran ramalan. Kami mempertahankan penggunaan kaedah Predictive Sequential untuk perkiraan ralat - ralat prequential. Kesalahan prekuensial membolehkan kita memantau evolusi prestasi model yang berkembang dari masa ke masa. Walaupun begitu, ia diketahui sebagai penganggar pesimis berbanding dengan anggaran penangguhan. Untuk mendapatkan penganggar yang lebih dipercayai, kita memerlukan beberapa mekanisme pelupa. Dua alternatif yang boleh dilaksanakan adalah: tingkap gelongsor dan faktor pudar. Kami memerhatikan bahawa ralat prekuensial bertumpu kepada penganggar penahan apabila dianggarkan melalui tingkap gelongsor atau menggunakan faktor pudar. Kami menyajikan contoh-contoh ilustrasi penggunaan penaksir ralat prekuensial, dengan menggunakan faktor pudar, untuk tugas-tugas: i menilai prestasi algoritma pembelajaran; ii membandingkan algoritma pembelajaran; iii ujian hipotesis menggunakan ujian McNemar; dan iv pengesanan perubahan menggunakan ujian Page-Hinkley. Dalam tugas-tugas ini, ralat prekuensial yang dianggarkan menggunakan faktor pudar memberikan penganggar yang boleh dipercayai. Sebagai perbandingan dengan tingkap gelongsor, faktor pudar lebih cepat dan kurang memori, syarat untuk aplikasi streaming. Makalah ini merupakan sumbangan untuk perbincangan dalam amalan baik penilaian prestasi ketika belajar model dinamik yang berkembang dari masa ke masa. [[EENNDD]] aliran data; reka bentuk penilaian"], [{"string": "On the tradeoff between privacy and utility in data publishing In data publishing , anonymization techniques such as generalization and bucketization have been designed to provide privacy protection . In the meanwhile , they reduce the utility of the data . It is important to consider the tradeoff between privacy and utility . In a paper that appeared in KDD 2008 , Brickell and Shmatikov proposed an evaluation methodology by comparing privacy gain with utility gain resulted from anonymizing the data , and concluded that `` even modest privacy gains require almost complete destruction of the data-mining utility '' . This conclusion seems to undermine existing work on data anonymization . In this paper , we analyze the fundamental characteristics of privacy and utility , and show that it is inappropriate to directly compare privacy with utility . We then observe that the privacy-utility tradeoff in data publishing is similar to the risk-return tradeoff in financial investment , and propose an integrated framework for considering privacy-utility tradeoff , borrowing concepts from the Modern Portfolio Theory for financial investment . Finally , we evaluate our methodology on the Adult dataset from the UCI machine learning repository . Our results clarify several common misconceptions about data utility and provide data publishers useful guidelines on choosing the right tradeoff between privacy and utility .", "keywords": ["anonymity", "data publishing", "privacy"], "combined": "On the tradeoff between privacy and utility in data publishing In data publishing , anonymization techniques such as generalization and bucketization have been designed to provide privacy protection . In the meanwhile , they reduce the utility of the data . It is important to consider the tradeoff between privacy and utility . In a paper that appeared in KDD 2008 , Brickell and Shmatikov proposed an evaluation methodology by comparing privacy gain with utility gain resulted from anonymizing the data , and concluded that `` even modest privacy gains require almost complete destruction of the data-mining utility '' . This conclusion seems to undermine existing work on data anonymization . In this paper , we analyze the fundamental characteristics of privacy and utility , and show that it is inappropriate to directly compare privacy with utility . We then observe that the privacy-utility tradeoff in data publishing is similar to the risk-return tradeoff in financial investment , and propose an integrated framework for considering privacy-utility tradeoff , borrowing concepts from the Modern Portfolio Theory for financial investment . Finally , we evaluate our methodology on the Adult dataset from the UCI machine learning repository . Our results clarify several common misconceptions about data utility and provide data publishers useful guidelines on choosing the right tradeoff between privacy and utility . [[EENNDD]] anonymity; data publishing; privacy"}, "Mengenai pertukaran antara privasi dan utiliti dalam penerbitan data Dalam penerbitan data, teknik anonimisasi seperti generalisasi dan bucketisasi telah dirancang untuk memberikan perlindungan privasi. Sementara itu, mereka mengurangkan kegunaan data. Penting untuk mempertimbangkan pertukaran antara privasi dan utiliti. Dalam makalah yang muncul dalam KDD 2008, Brickell dan Shmatikov mengusulkan metodologi penilaian dengan membandingkan keuntungan privasi dengan keuntungan utiliti yang dihasilkan dari menganonimkan data, dan menyimpulkan bahawa \"keuntungan privasi yang sederhana bahkan memerlukan pemusnahan utiliti perlombongan data yang hampir sepenuhnya\" . Kesimpulan ini seolah-olah melemahkan kerja yang ada pada anonimisasi data. Dalam makalah ini, kami menganalisis ciri-ciri asas privasi dan utiliti, dan menunjukkan bahawa tidak tepat membandingkan privasi dengan utiliti secara langsung. Kami kemudian melihat bahawa pertukaran privasi-utiliti dalam penerbitan data serupa dengan pertukaran risiko-risiko dalam pelaburan kewangan, dan mencadangkan kerangka kerja bersepadu untuk mempertimbangkan pertukaran privasi-utiliti, meminjam konsep dari Teori Portofolio Moden untuk pelaburan kewangan. Akhirnya, kami menilai metodologi kami pada set data Dewasa dari repositori pembelajaran mesin UCI. Hasil kami menjelaskan beberapa kesalahpahaman umum mengenai utiliti data dan memberikan panduan berguna kepada penerbit data mengenai memilih pertukaran yang betul antara privasi dan utiliti. [[EENNDD]] tanpa nama; penerbitan data; privasi"], [{"string": "Probabilistic query models for transaction data We investigate the application of Bayesian networks , Markov random fields , and mixture models to the problem of query answering for transaction data sets . We formulate two versions of the querying problem : the query selectivity estimation i.e. , finding exact counts for tuples in a data set and the query generalization problem i.e. , computing the probability that a tuple will occur in new data . We show that frequent itemsets are useful for reducing the original data to a compressed representation and introduce a method to store them using an ADTree data structure . In an extension of our earlier work on this topic we propose several new schemes for query answering based on the compressed representation that avoid direct scans of the data at query time . Experimental results on real-world transaction data sets provide insights into various tradeoffs involving the offline time for model-building , the online time for query-answering , the memory footprint of the compressed data , and the accuracy of the estimate provided to the query .", "keywords": ["probabilistic algorithms"], "combined": "Probabilistic query models for transaction data We investigate the application of Bayesian networks , Markov random fields , and mixture models to the problem of query answering for transaction data sets . We formulate two versions of the querying problem : the query selectivity estimation i.e. , finding exact counts for tuples in a data set and the query generalization problem i.e. , computing the probability that a tuple will occur in new data . We show that frequent itemsets are useful for reducing the original data to a compressed representation and introduce a method to store them using an ADTree data structure . In an extension of our earlier work on this topic we propose several new schemes for query answering based on the compressed representation that avoid direct scans of the data at query time . Experimental results on real-world transaction data sets provide insights into various tradeoffs involving the offline time for model-building , the online time for query-answering , the memory footprint of the compressed data , and the accuracy of the estimate provided to the query . [[EENNDD]] probabilistic algorithms"}, "Model pertanyaan probabilistik untuk data transaksi Kami menyelidiki aplikasi jaringan Bayesian, medan rawak Markov, dan model campuran terhadap masalah menjawab pertanyaan untuk set data transaksi. Kami merumuskan dua versi masalah pertanyaan: anggaran selektiviti pertanyaan, iaitu mencari jumlah tepat untuk tupel dalam satu set data dan masalah generalisasi pertanyaan, iaitu, mengira kebarangkalian tupel akan terjadi dalam data baru. Kami menunjukkan bahawa set item yang kerap berguna untuk mengurangkan data asal menjadi representasi yang dimampatkan dan memperkenalkan kaedah untuk menyimpannya menggunakan struktur data ADTree. Sebagai lanjutan dari karya kami yang lebih awal mengenai topik ini, kami mencadangkan beberapa skema baru untuk menjawab pertanyaan berdasarkan perwakilan yang dimampatkan yang mengelakkan pengimbasan data secara langsung pada waktu pertanyaan. Hasil eksperimen pada set data transaksi dunia nyata memberikan gambaran mengenai pelbagai pertukaran yang melibatkan masa luar talian untuk pembuatan model, masa dalam talian untuk menjawab pertanyaan, jejak memori data yang dimampatkan, dan ketepatan anggaran yang diberikan untuk pertanyaan. [[EENNDD]] algoritma probabilistik"], [{"string": "A microeconomic data mining problem : customer-oriented catalog segmentation The microeconomic framework for data mining 7 assumes that an enterprise chooses a decision maximizing the overall utility over all customers where the contribution of a customer is a function of the data available on that customer . In Catalog Segmentation , the enterprise wants to design k product catalogs of size r that maximize the overall number of catalog products purchased . However , there are many applications where a customer , once attracted to an enterprise , would purchase more products beyond the ones contained in the catalog . Therefore , in this paper , we investigate an alternative problem formulation , that we call Customer-Oriented Catalog Segmentation , where the overall utility is measured by the number of customers that have at least a specified minimum interest t in the catalogs . We formally introduce the Customer-Oriented Catalog Segmentation problem and discuss its complexity . Then we investigate two different paradigms to design efficient , approximate algorithms for the Customer-Oriented Catalog Segmentation problem , greedy deterministic and randomized algorithms . Since greedy algorithms may be trapped in a local optimum and randomized algorithms crucially depend on a reasonable initial solution , we explore a combination of these two paradigms . Our experimental evaluation on synthetic and real data demonstrates that the new algorithms yield catalogs of significantly higher utility compared to classical Catalog Segmentation algorithms .", "keywords": ["clustering", "catalog segmentation", "microeconomic data mining"], "combined": "A microeconomic data mining problem : customer-oriented catalog segmentation The microeconomic framework for data mining 7 assumes that an enterprise chooses a decision maximizing the overall utility over all customers where the contribution of a customer is a function of the data available on that customer . In Catalog Segmentation , the enterprise wants to design k product catalogs of size r that maximize the overall number of catalog products purchased . However , there are many applications where a customer , once attracted to an enterprise , would purchase more products beyond the ones contained in the catalog . Therefore , in this paper , we investigate an alternative problem formulation , that we call Customer-Oriented Catalog Segmentation , where the overall utility is measured by the number of customers that have at least a specified minimum interest t in the catalogs . We formally introduce the Customer-Oriented Catalog Segmentation problem and discuss its complexity . Then we investigate two different paradigms to design efficient , approximate algorithms for the Customer-Oriented Catalog Segmentation problem , greedy deterministic and randomized algorithms . Since greedy algorithms may be trapped in a local optimum and randomized algorithms crucially depend on a reasonable initial solution , we explore a combination of these two paradigms . Our experimental evaluation on synthetic and real data demonstrates that the new algorithms yield catalogs of significantly higher utility compared to classical Catalog Segmentation algorithms . [[EENNDD]] clustering; catalog segmentation; microeconomic data mining"}, "Masalah perlombongan data mikroekonomi: segmentasi katalog berorientasikan pelanggan Rangka kerja mikroekonomi untuk perlombongan data 7 menganggap bahawa perusahaan memilih keputusan yang memaksimumkan keseluruhan utiliti ke atas semua pelanggan di mana sumbangan pelanggan adalah fungsi dari data yang tersedia pada pelanggan tersebut. Dalam Segmentasi Katalog, perusahaan ingin merancang katalog produk k dengan ukuran r yang memaksimumkan jumlah keseluruhan produk katalog yang dibeli. Namun, ada banyak aplikasi di mana pelanggan, setelah tertarik dengan perusahaan, akan membeli lebih banyak produk di luar yang terdapat dalam katalog. Oleh itu, dalam makalah ini, kami menyelidiki rumusan masalah alternatif, yang kami sebut sebagai Segmentasi Katalog Berorientasikan Pelanggan, di mana utiliti keseluruhan diukur dengan jumlah pelanggan yang memiliki minat minimum tertentu dalam katalog. Kami secara rasmi memperkenalkan masalah Segmentasi Katalog Berorientasikan Pelanggan dan membincangkan kerumitannya. Kemudian kami menyiasat dua paradigma yang berbeza untuk merancang algoritma yang cekap dan tepat untuk masalah Segmentasi Katalog Berorientasikan Pelanggan, algoritma deterministik tamak dan rawak. Oleh kerana algoritma tamak mungkin terperangkap dalam algoritma optimum tempatan dan rawak sangat bergantung pada penyelesaian awal yang munasabah, kami meneroka gabungan kedua-dua paradigma ini. Penilaian eksperimental kami terhadap data sintetik dan nyata menunjukkan bahawa algoritma baru menghasilkan katalog utiliti yang jauh lebih tinggi berbanding dengan algoritma Segmentasi Katalog klasik. [[EENNDD]] pengelompokan; segmentasi katalog; perlombongan data mikroekonomi"], [{"string": "On string classification in data streams String data has recently become important because of its use in a number of applications such as computational and molecular biology , protein analysis , and market basket data . In many cases , these strings contain a wide variety of substructures which may have physical significance for that application . For example , such substructures could represent important fragments of a DNA string or an interesting portion of a fraudulent transaction . In such a case , it is desirable to determine the identity , location , and extent of that substructure in the data . This is a much more difficult generalization of the classification problem , since the latter problem labels entire strings rather than deal with the more complex task of determining string fragments with a particular kind of behavior . The problem becomes even more complicated when different kinds of substrings show complicated nesting patterns . Therefore , we define a somewhat different problem which we refer to as the generalized classification problem . We propose a scalable approach based on hidden markov models for this problem . We show how to implement the generalized string classification procedure for very large data bases and data streams . We present experimental results over a number of large data sets and data streams .", "keywords": ["hidden markov models", "string", "classification"], "combined": "On string classification in data streams String data has recently become important because of its use in a number of applications such as computational and molecular biology , protein analysis , and market basket data . In many cases , these strings contain a wide variety of substructures which may have physical significance for that application . For example , such substructures could represent important fragments of a DNA string or an interesting portion of a fraudulent transaction . In such a case , it is desirable to determine the identity , location , and extent of that substructure in the data . This is a much more difficult generalization of the classification problem , since the latter problem labels entire strings rather than deal with the more complex task of determining string fragments with a particular kind of behavior . The problem becomes even more complicated when different kinds of substrings show complicated nesting patterns . Therefore , we define a somewhat different problem which we refer to as the generalized classification problem . We propose a scalable approach based on hidden markov models for this problem . We show how to implement the generalized string classification procedure for very large data bases and data streams . We present experimental results over a number of large data sets and data streams . [[EENNDD]] hidden markov models; string; classification"}, "Mengenai klasifikasi tali dalam aliran data Data rentetan baru-baru ini menjadi penting kerana penggunaannya dalam sejumlah aplikasi seperti biologi komputasi dan molekul, analisis protein, dan data keranjang pasar. Dalam banyak kes, tali ini mengandungi pelbagai jenis struktur yang mungkin mempunyai kepentingan fizikal untuk aplikasi tersebut. Sebagai contoh, substruktur seperti itu dapat mewakili serpihan penting dari rentetan DNA atau bahagian menarik dari transaksi penipuan. Dalam kes sedemikian, adalah wajar untuk menentukan identiti, lokasi, dan luasnya substruktur tersebut dalam data. Ini adalah generalisasi yang lebih sukar bagi masalah klasifikasi, kerana masalah yang terakhir melabel keseluruhan rentetan daripada menangani tugas yang lebih kompleks untuk menentukan pecahan tali dengan jenis tingkah laku tertentu. Masalahnya menjadi lebih rumit apabila pelbagai jenis substring menunjukkan corak bersarang yang rumit. Oleh itu, kita menentukan masalah yang agak berbeza yang kita sebut sebagai masalah klasifikasi umum. Kami mencadangkan pendekatan berskala berdasarkan model markov tersembunyi untuk masalah ini. Kami menunjukkan cara melaksanakan prosedur pengkelasan string umum untuk pangkalan data dan aliran data yang sangat besar. Kami membentangkan hasil eksperimen ke atas sejumlah kumpulan data dan aliran data yang besar. [[EENNDD]] model markov tersembunyi; tali; pengelasan"], [{"string": "Clustering event logs using iterative partitioning The importance of event logs , as a source of information in systems and network management can not be overemphasized . With the ever increasing size and complexity of today 's event logs , the task of analyzing event logs has become cumbersome to carry out manually . For this reason recent research has focused on the automatic analysis of these log files . In this paper we present IPLoM Iterative Partitioning Log Mining , a novel algorithm for the mining of clusters from event logs . Through a 3-Step hierarchical partitioning process IPLoM partitions log data into its respective clusters . In its 4th and final stage IPLoM produces cluster descriptions or line formats for each of the clusters produced . Unlike other similar algorithms IPLoM is not based on the Apriori algorithm and it is able to find clusters in data whether or not its instances appear frequently . Evaluations show that IPLoM outperforms the other algorithms statistically significantly , and it is also able to achieve an average F-Measure performance 78 % when the closest other algorithm achieves an F-Measure performance of 10 % .", "keywords": ["telecommunications", "fault management", "event log mining"], "combined": "Clustering event logs using iterative partitioning The importance of event logs , as a source of information in systems and network management can not be overemphasized . With the ever increasing size and complexity of today 's event logs , the task of analyzing event logs has become cumbersome to carry out manually . For this reason recent research has focused on the automatic analysis of these log files . In this paper we present IPLoM Iterative Partitioning Log Mining , a novel algorithm for the mining of clusters from event logs . Through a 3-Step hierarchical partitioning process IPLoM partitions log data into its respective clusters . In its 4th and final stage IPLoM produces cluster descriptions or line formats for each of the clusters produced . Unlike other similar algorithms IPLoM is not based on the Apriori algorithm and it is able to find clusters in data whether or not its instances appear frequently . Evaluations show that IPLoM outperforms the other algorithms statistically significantly , and it is also able to achieve an average F-Measure performance 78 % when the closest other algorithm achieves an F-Measure performance of 10 % . [[EENNDD]] telecommunications; fault management; event log mining"}, "Pengelompokan log peristiwa menggunakan partisi iteratif Kepentingan log peristiwa, sebagai sumber maklumat dalam sistem dan pengurusan rangkaian tidak dapat terlalu ditekankan. Dengan semakin bertambahnya ukuran dan kerumitan log peristiwa hari ini, tugas untuk menganalisis log peristiwa menjadi sukar dilakukan secara manual. Atas sebab ini, penyelidikan baru-baru ini telah menumpukan pada analisis automatik fail log ini. Dalam makalah ini kami menyajikan Perlombongan Log Pemisahan Iterative IPLoM, algoritma baru untuk perlombongan kelompok dari log peristiwa. Melalui proses pemisahan hierarki 3 Langkah IPLoM membahagikan data log ke kluster masing-masing. Pada peringkat ke-4 dan terakhir IPLoM menghasilkan deskripsi kluster atau format baris untuk setiap kluster yang dihasilkan. Tidak seperti algoritma lain yang serupa IPLoM tidak berdasarkan algoritma Apriori dan ia dapat mencari kelompok dalam data sama ada kejadiannya kerap muncul atau tidak. Penilaian menunjukkan bahawa IPLoM mengatasi algoritma lain secara statistik secara signifikan, dan ia juga dapat mencapai prestasi F-Measure rata-rata 78% apabila algoritma terdekat yang terdekat mencapai prestasi F-Measure 10%. [[EENNDD]] telekomunikasi; pengurusan kesalahan; perlombongan log peristiwa"], [{"string": "Learning from multi-topic web documents for contextual advertisement Contextual advertising on web pages has become very popular recently and it poses its own set of unique text mining challenges . Often advertisers wish to either target or avoid some specific content on web pages which may appear only in a small part of the page . Learning for these targeting tasks is difficult since most training pages are multi-topic and need expensive human labeling at the sub-document level for accurate training . In this paper we investigate ways to learn for sub-document classification when only page level labels are available - these labels only indicate if the relevant content exists in the given page or not . We propose the application of multiple-instance learning to this task to improve the effectiveness of traditional methods . We apply sub-document classification to two different problems in contextual advertising . One is `` sensitive content detection '' where the advertiser wants to avoid content relating to war , violence , pornography , etc. even if they occur only in a small part of a page . The second problem involves opinion mining from review sites - the advertiser wants to detect and avoid negative opinion about their product when positive , negative and neutral sentiments co-exist on a page . In both these scenarios we present experimental results to show that our proposed system is able to get good block level labeling for free and improve the performance of traditional learning methods .", "keywords": ["general", "contextual advertising", "sensitive content detection", "sub-document classification", "opinion mining"], "combined": "Learning from multi-topic web documents for contextual advertisement Contextual advertising on web pages has become very popular recently and it poses its own set of unique text mining challenges . Often advertisers wish to either target or avoid some specific content on web pages which may appear only in a small part of the page . Learning for these targeting tasks is difficult since most training pages are multi-topic and need expensive human labeling at the sub-document level for accurate training . In this paper we investigate ways to learn for sub-document classification when only page level labels are available - these labels only indicate if the relevant content exists in the given page or not . We propose the application of multiple-instance learning to this task to improve the effectiveness of traditional methods . We apply sub-document classification to two different problems in contextual advertising . One is `` sensitive content detection '' where the advertiser wants to avoid content relating to war , violence , pornography , etc. even if they occur only in a small part of a page . The second problem involves opinion mining from review sites - the advertiser wants to detect and avoid negative opinion about their product when positive , negative and neutral sentiments co-exist on a page . In both these scenarios we present experimental results to show that our proposed system is able to get good block level labeling for free and improve the performance of traditional learning methods . [[EENNDD]] general; contextual advertising; sensitive content detection; sub-document classification; opinion mining"}, "Belajar dari dokumen web berbilang topik untuk iklan kontekstual Pengiklanan kontekstual di laman web menjadi sangat popular baru-baru ini dan menimbulkan pelbagai cabaran penambangan teks yang tersendiri. Selalunya pengiklan ingin menyasarkan atau mengelakkan beberapa kandungan tertentu di laman web yang mungkin hanya muncul di sebahagian kecil halaman. Belajar untuk tugas penargetan ini sukar kerana kebanyakan halaman latihan adalah pelbagai topik dan memerlukan pelabelan manusia yang mahal pada tahap sub-dokumen untuk latihan yang tepat. Dalam makalah ini kami menyiasat cara belajar untuk klasifikasi sub-dokumen apabila hanya label tingkat halaman yang tersedia - label ini hanya menunjukkan jika kandungan yang relevan ada di halaman yang diberikan atau tidak. Kami mencadangkan penerapan pembelajaran pelbagai contoh untuk tugas ini untuk meningkatkan keberkesanan kaedah tradisional. Kami menerapkan klasifikasi sub-dokumen kepada dua masalah yang berbeza dalam pengiklanan kontekstual. Salah satunya adalah \"pengesanan kandungan sensitif\" di mana pengiklan ingin mengelakkan kandungan yang berkaitan dengan perang, keganasan, pornografi, dan lain-lain walaupun hanya berlaku di bahagian kecil halaman. Masalah kedua melibatkan pencarian pendapat dari laman web tinjauan - pengiklan ingin mengesan dan mengelakkan pendapat negatif mengenai produk mereka apabila sentimen positif, negatif dan neutral wujud di halaman. Dalam kedua-dua senario ini, kami menyajikan hasil eksperimen untuk menunjukkan bahawa sistem yang dicadangkan kami dapat memperoleh pelabelan tahap blok yang baik secara percuma dan meningkatkan prestasi kaedah pembelajaran tradisional. [[EENNDD]] umum; pengiklanan kontekstual; pengesanan kandungan sensitif; klasifikasi sub-dokumen; perlombongan pendapat"], [{"string": "Frequent-subsequence-based prediction of outer membrane proteins A number of medically important disease-causing bacteria collectively called Gram-negative bacteria are noted for the extra `` outer '' membrane that surrounds their cell . Proteins resident in this membrane outer membrane proteins , or OMPs are of primary research interest for antibiotic and vaccine drug design as they are on the surface of the bacteria and so are the most accessible targets to develop new drugs against . With the development of genome sequencing technology and bioinformatics , biologists can now deduce all the proteins that are likely produced in a given bacteria and have attempted to classify where proteins are located in a bacterial cell . However such protein localization programs are currently least accurate when predicting OMPs , and so there is a current need for the development of a better OMP classifier . Data mining research suggests that the use of frequent patterns has good performance in aiding the development of accurate and efficient classification algorithms . In this paper , we present two methods to identify OMPs based on frequent subsequences and test them on all Gram-negative bacterial proteins whose localizations have been determined by biological experiments . One classifier follows an association rule approach , while the other is based on support vector machines SVMs . We compare the proposed methods with the state-of-the-art methods in the biological domain . The results demonstrate that our methods are better both in terms of accurately identifying OMPs and providing biological insights that increase our understanding of the structures and functions of these important proteins .", "keywords": ["support vector machine", "classification", "subcellular localization", "database applications", "outer membrane protein", "association rule"], "combined": "Frequent-subsequence-based prediction of outer membrane proteins A number of medically important disease-causing bacteria collectively called Gram-negative bacteria are noted for the extra `` outer '' membrane that surrounds their cell . Proteins resident in this membrane outer membrane proteins , or OMPs are of primary research interest for antibiotic and vaccine drug design as they are on the surface of the bacteria and so are the most accessible targets to develop new drugs against . With the development of genome sequencing technology and bioinformatics , biologists can now deduce all the proteins that are likely produced in a given bacteria and have attempted to classify where proteins are located in a bacterial cell . However such protein localization programs are currently least accurate when predicting OMPs , and so there is a current need for the development of a better OMP classifier . Data mining research suggests that the use of frequent patterns has good performance in aiding the development of accurate and efficient classification algorithms . In this paper , we present two methods to identify OMPs based on frequent subsequences and test them on all Gram-negative bacterial proteins whose localizations have been determined by biological experiments . One classifier follows an association rule approach , while the other is based on support vector machines SVMs . We compare the proposed methods with the state-of-the-art methods in the biological domain . The results demonstrate that our methods are better both in terms of accurately identifying OMPs and providing biological insights that increase our understanding of the structures and functions of these important proteins . [[EENNDD]] support vector machine; classification; subcellular localization; database applications; outer membrane protein; association rule"}, "Ramalan yang kerap dilakukan berdasarkan protein membran luar Sejumlah bakteria penyebab penyakit penting secara perubatan yang disebut secara bersamaan bakteria Gram-negatif diperhatikan untuk membran \"luar\" tambahan yang mengelilingi sel mereka. Protein yang terdapat dalam protein membran luar membran ini, atau OMP adalah kepentingan penyelidikan utama untuk reka bentuk ubat antibiotik dan vaksin kerana terdapat di permukaan bakteria dan begitu juga sasaran yang paling mudah diakses untuk mengembangkan ubat baru. Dengan perkembangan teknologi penjujukan genom dan bioinformatik, ahli biologi kini dapat menyimpulkan semua protein yang kemungkinan dihasilkan dalam bakteria tertentu dan telah berusaha untuk mengklasifikasikan di mana protein berada di dalam sel bakteria. Walau bagaimanapun, program penyetempatan protein seperti ini paling tidak tepat ketika meramalkan OMP, dan oleh itu terdapat keperluan semasa untuk pengembangan pengelasan OMP yang lebih baik. Penyelidikan perlombongan data menunjukkan bahawa penggunaan corak yang kerap mempunyai prestasi yang baik dalam membantu pengembangan algoritma klasifikasi yang tepat dan efisien. Dalam makalah ini, kami menyajikan dua kaedah untuk mengenal pasti OMP berdasarkan turutan yang kerap dan mengujinya pada semua protein bakteria Gram-negatif yang penyetempatannya telah ditentukan oleh eksperimen biologi. Satu pengkelas mengikuti pendekatan peraturan persatuan, sementara yang lain berdasarkan SVM mesin vektor sokongan. Kami membandingkan kaedah yang dicadangkan dengan kaedah canggih dalam domain biologi. Hasilnya menunjukkan bahawa kaedah kami lebih baik baik dari segi mengenal pasti OMP secara tepat dan memberikan pandangan biologi yang meningkatkan pemahaman kami mengenai struktur dan fungsi protein penting ini. [[EENNDD]] mesin vektor sokongan; pengelasan; penyetempatan subselular; aplikasi pangkalan data; protein membran luar; peraturan persatuan"], [{"string": "Automatic mining of fruit fly embryo images We present FEMine , an automatic system for image-based gene expression analysis . We perform experiments on the largest publicly available collection of Drosophila ISH in situ hybridization images , showing that our FEMine system achieves excellent performance in classification , clustering , and content-based image retrieval . The major innovation of FEMine is the use of automatically discovered latent spatial `` themes '' of gene expressions , LGEs , in the whole-embryo context , as opposed to patterns in nearly disjoint portions of an embryo proposed in previous methods .", "keywords": ["embryonic image analysis", "independent component analysis", "drosophila", "gene expression"], "combined": "Automatic mining of fruit fly embryo images We present FEMine , an automatic system for image-based gene expression analysis . We perform experiments on the largest publicly available collection of Drosophila ISH in situ hybridization images , showing that our FEMine system achieves excellent performance in classification , clustering , and content-based image retrieval . The major innovation of FEMine is the use of automatically discovered latent spatial `` themes '' of gene expressions , LGEs , in the whole-embryo context , as opposed to patterns in nearly disjoint portions of an embryo proposed in previous methods . [[EENNDD]] embryonic image analysis; independent component analysis; drosophila; gene expression"}, "Perlombongan automatik gambar embrio lalat buah Kami menyajikan FEMine, sistem automatik untuk analisis ekspresi gen berasaskan gambar. Kami melakukan eksperimen pada koleksi gambar hibridisasi inos situ Drosophila ISH yang tersedia untuk umum, menunjukkan bahawa sistem FEMine kami mencapai prestasi yang sangat baik dalam klasifikasi, pengelompokan, dan pengambilan gambar berdasarkan kandungan. Inovasi utama FEMine adalah penggunaan \"tema\" spatial laten yang dijumpai secara automatik dari ekspresi gen, LGE, dalam konteks keseluruhan embrio, berbanding dengan corak pada bahagian yang hampir tidak terpisah dari embrio yang dicadangkan dalam kaedah sebelumnya. [[EENNDD]] analisis gambar embrio; analisis komponen bebas; drosophila; ungkapan gen"], [{"string": "Rapid detection of significant spatial clusters Given an N x N grid of squares , where each square has a count cij and an underlying population pij , our goal is to find the rectangular region with the highest density , and to calculate its significance by randomization . An arbitrary density function D , dependent on a region 's total count C and total population P , can be used . For example , if each count represents the number of disease cases occurring in that square , we can use Kulldorff 's spatial scan statistic DK to find the most significant spatial disease cluster . A naive approach to finding the maximum density region requires O N4 time , and is generally computationally infeasible . We present a multiresolution algorithm which partitions the grid into overlapping regions using a novel overlap-kd tree data structure , bounds the maximum score of subregions contained in each region , and prunes regions which can not contain the maximum density region . For sufficiently dense regions , this method finds the maximum density region in O N log N 2 time , in practice resulting in significant 20-2000x speedups on both real and simulated datasets .", "keywords": ["spatial scan statistics", "cluster detection", "spatial data mining algorithms", "biosurveillance"], "combined": "Rapid detection of significant spatial clusters Given an N x N grid of squares , where each square has a count cij and an underlying population pij , our goal is to find the rectangular region with the highest density , and to calculate its significance by randomization . An arbitrary density function D , dependent on a region 's total count C and total population P , can be used . For example , if each count represents the number of disease cases occurring in that square , we can use Kulldorff 's spatial scan statistic DK to find the most significant spatial disease cluster . A naive approach to finding the maximum density region requires O N4 time , and is generally computationally infeasible . We present a multiresolution algorithm which partitions the grid into overlapping regions using a novel overlap-kd tree data structure , bounds the maximum score of subregions contained in each region , and prunes regions which can not contain the maximum density region . For sufficiently dense regions , this method finds the maximum density region in O N log N 2 time , in practice resulting in significant 20-2000x speedups on both real and simulated datasets . [[EENNDD]] spatial scan statistics; cluster detection; spatial data mining algorithms; biosurveillance"}, "Pengesanan yang cepat bagi kelompok spasial yang signifikan Mengingat grid segiempat sama N, N, di mana setiap petak mempunyai bilangan cij dan populasi yang mendasari, tujuan kami adalah untuk mencari kawasan segi empat tepat dengan ketumpatan tertinggi, dan untuk mengira kepentingannya secara rawak. Fungsi ketumpatan sewenang-wenangnya D, bergantung pada jumlah wilayah C dan jumlah populasi P, dapat digunakan. Sebagai contoh, jika setiap kiraan menunjukkan jumlah kes penyakit yang berlaku di dataran itu, kita dapat menggunakan statistik DK scan spatial Kulldorff untuk mencari kelompok penyakit spasial yang paling signifikan. Pendekatan naif untuk mencari kawasan kepadatan maksimum memerlukan masa O N4, dan secara umum tidak dapat dilaksanakan secara komputasi. Kami menyajikan algoritma multiresolusi yang membahagi grid menjadi kawasan tumpang tindih menggunakan struktur data pohon tumpang tindih-kd, mengikat skor maksimum subkawasan yang terdapat di setiap wilayah, dan memangkas wilayah yang tidak dapat mengandungi wilayah kepadatan maksimum. Untuk kawasan yang cukup padat, kaedah ini menemui kawasan kepadatan maksimum dalam masa O N log N 2, dalam praktiknya menghasilkan kelajuan 20-2000x yang signifikan pada kedua-dua set data sebenar dan simulasi. [[EENNDD]] statistik imbasan ruang; pengesanan kluster; algoritma perlombongan data spatial; pengawasan biosur"], [{"string": "Activity monitoring : noticing interesting changes in behavior", "keywords": ["security and protection", "database administration", "database applications"], "combined": "Activity monitoring : noticing interesting changes in behavior [[EENNDD]] security and protection; database administration; database applications"}, "Pemantauan aktiviti: memerhatikan perubahan tingkah laku yang menarik [[EENNDD]] keselamatan dan perlindungan; pentadbiran pangkalan data; aplikasi pangkalan data"], [{"string": "Cold start link prediction In the traditional link prediction problem , a snapshot of a social network is used as a starting point to predict , by means of graph-theoretic measures , the links that are likely to appear in the future . In this paper , we introduce cold start link prediction as the problem of predicting the structure of a social network when the network itself is totally missing while some other information regarding the nodes is available . We propose a two-phase method based on the bootstrap probabilistic graph . The first phase generates an implicit social network under the form of a probabilistic graph . The second phase applies probabilistic graph-based measures to produce the final prediction . We assess our method empirically over a large data collection obtained from Flickr , using interest groups as the initial information . The experiments confirm the effectiveness of our approach .", "keywords": ["probabilistic graph", "social networks", "link prediction"], "combined": "Cold start link prediction In the traditional link prediction problem , a snapshot of a social network is used as a starting point to predict , by means of graph-theoretic measures , the links that are likely to appear in the future . In this paper , we introduce cold start link prediction as the problem of predicting the structure of a social network when the network itself is totally missing while some other information regarding the nodes is available . We propose a two-phase method based on the bootstrap probabilistic graph . The first phase generates an implicit social network under the form of a probabilistic graph . The second phase applies probabilistic graph-based measures to produce the final prediction . We assess our method empirically over a large data collection obtained from Flickr , using interest groups as the initial information . The experiments confirm the effectiveness of our approach . [[EENNDD]] probabilistic graph; social networks; link prediction"}, "Ramalan pautan permulaan sejuk Dalam masalah ramalan pautan tradisional, gambaran ringkas rangkaian sosial digunakan sebagai titik permulaan untuk meramalkan, dengan kaedah grafik-teori, pautan yang kemungkinan akan muncul di masa depan. Dalam makalah ini, kami memperkenalkan ramalan pautan permulaan yang sejuk sebagai masalah untuk meramalkan struktur rangkaian sosial apabila rangkaian itu sendiri benar-benar hilang sementara beberapa maklumat lain mengenai nod tersedia. Kami mencadangkan kaedah dua fasa berdasarkan grafik probabilistik bootstrap. Fasa pertama menghasilkan rangkaian sosial yang tersirat di bawah bentuk graf probabilistik. Fasa kedua menggunakan ukuran berdasarkan grafik probabilistik untuk menghasilkan ramalan akhir. Kami menilai kaedah kami secara empirik terhadap pengumpulan data besar yang diperoleh dari Flickr, menggunakan kumpulan minat sebagai maklumat awal. Eksperimen mengesahkan keberkesanan pendekatan kami. [[EENNDD]] graf probabilistik; rangkaian sosial; ramalan pautan"], [{"string": "Mining unexpected rules by pushing user dynamics Unexpected rules are interesting because they are either previously unknown or deviate from what prior user knowledge would suggest . In this paper , we study three important issues that have been previously ignored in mining unexpected rules . First , the unexpectedness of a rule depends on how the user prefers to apply the prior knowledge to a given scenario , in addition to the knowledge itself . Second , the prior knowledge should be considered right from the start to focus the search on unexpected rules . Third , the unexpectedness of a rule depends on what other rules the user has seen so far . Thus , only rules that remain unexpected given what the user has seen should be considered interesting . We develop an approach that addresses all three problems above and evaluate it by means of experiments focusing on finding interesting rules .", "keywords": ["subjective interestingness", "unexpected rule", "association rule"], "combined": "Mining unexpected rules by pushing user dynamics Unexpected rules are interesting because they are either previously unknown or deviate from what prior user knowledge would suggest . In this paper , we study three important issues that have been previously ignored in mining unexpected rules . First , the unexpectedness of a rule depends on how the user prefers to apply the prior knowledge to a given scenario , in addition to the knowledge itself . Second , the prior knowledge should be considered right from the start to focus the search on unexpected rules . Third , the unexpectedness of a rule depends on what other rules the user has seen so far . Thus , only rules that remain unexpected given what the user has seen should be considered interesting . We develop an approach that addresses all three problems above and evaluate it by means of experiments focusing on finding interesting rules . [[EENNDD]] subjective interestingness; unexpected rule; association rule"}, "Melombong peraturan yang tidak dijangka dengan mendorong dinamika pengguna Peraturan yang tidak dijangka adalah menarik kerana sebelumnya tidak diketahui atau menyimpang dari apa yang dicadangkan oleh pengetahuan pengguna sebelumnya. Dalam makalah ini, kami mengkaji tiga isu penting yang sebelumnya telah diabaikan dalam perlombongan peraturan yang tidak dijangka. Pertama, ketidakpastian peraturan bergantung pada bagaimana pengguna lebih suka mengaplikasikan pengetahuan sebelumnya untuk senario tertentu, selain pengetahuan itu sendiri. Kedua, pengetahuan sebelumnya harus dipertimbangkan sejak awal untuk memfokuskan pencarian pada peraturan yang tidak dijangka. Ketiga, ketidakpastian peraturan bergantung pada peraturan lain yang dilihat pengguna selama ini. Oleh itu, hanya peraturan yang tetap tidak dijangka memandangkan apa yang dilihat pengguna harus dianggap menarik. Kami mengembangkan pendekatan yang menangani ketiga-tiga masalah di atas dan mengevaluasinya melalui eksperimen yang berfokus untuk mencari peraturan yang menarik. [[EENNDD]] minat subjektif; peraturan yang tidak dijangka; peraturan persatuan"], [{"string": "Mining intrusion detection alarms for actionable knowledge In response to attacks against enterprise networks , administrators increasingly deploy intrusion detection systems . These systems monitor hosts , networks , and other resources for signs of security violations . The use of intrusion detection has given rise to another difficult problem , namely the handling of a generally large number of alarms . In this paper , we mine historical alarms to learn how future alarms can be handled more efficiently . First , we investigate episode rules with respect to their suitability in this approach . We report the difficulties encountered and the unexpected insights gained . In addition , we introduce a new conceptual clustering technique , and use it in extensive experiments with real-world data to show that intrusion detection alarms can be handled efficiently by using previously mined knowledge .", "keywords": ["conceptual clustering", "intrusion detection", "episode rules", "alarm investigation"], "combined": "Mining intrusion detection alarms for actionable knowledge In response to attacks against enterprise networks , administrators increasingly deploy intrusion detection systems . These systems monitor hosts , networks , and other resources for signs of security violations . The use of intrusion detection has given rise to another difficult problem , namely the handling of a generally large number of alarms . In this paper , we mine historical alarms to learn how future alarms can be handled more efficiently . First , we investigate episode rules with respect to their suitability in this approach . We report the difficulties encountered and the unexpected insights gained . In addition , we introduce a new conceptual clustering technique , and use it in extensive experiments with real-world data to show that intrusion detection alarms can be handled efficiently by using previously mined knowledge . [[EENNDD]] conceptual clustering; intrusion detection; episode rules; alarm investigation"}, "Menambang penggera pengesanan pencerobohan untuk pengetahuan yang dapat ditindaklanjuti Sebagai tindak balas terhadap serangan terhadap rangkaian perusahaan, pentadbir semakin banyak menggunakan sistem pengesanan pencerobohan. Sistem ini memantau host, rangkaian, dan sumber lain untuk tanda-tanda pelanggaran keselamatan. Penggunaan pengesanan pencerobohan telah menimbulkan masalah lain yang sukar, iaitu pengendalian sejumlah besar penggera. Dalam makalah ini, kami menggunakan penggera sejarah untuk mengetahui bagaimana penggera masa depan dapat ditangani dengan lebih cekap. Pertama, kami menyiasat peraturan episod berkenaan dengan kesesuaiannya dalam pendekatan ini. Kami melaporkan kesukaran yang dihadapi dan pandangan yang tidak dijangka diperoleh. Selain itu, kami memperkenalkan teknik pengelompokan konseptual baru, dan menggunakannya dalam eksperimen ekstensif dengan data dunia nyata untuk menunjukkan bahawa penggera pengesanan pencerobohan dapat ditangani dengan efisien dengan menggunakan pengetahuan yang ditambang sebelumnya. [[EENNDD]] pengelompokan konsep; pengesanan pencerobohan; peraturan episod; penyiasatan penggera"], [{"string": "Identifying bridging rules between conceptual clusters A bridging rule in this paper has its antecedent and action from different conceptual clusters . We first design two algorithms for mining bridging rules between clusters in a database , and then propose two non-linear metrics for measuring the interestingness of bridging rules . Bridging rules can be distinct from association rules or frequent itemsets . This is because 1 bridging rules can be generated by infrequent itemsets that are pruned in association rule mining ; and 2 bridging rules are measured by the importance that includes the distance between two conceptual clusters , whereas frequent itemsets are measured by only the support .", "keywords": ["bridging rule", "learning", "association rule", "outlier", "entropy", "clustering"], "combined": "Identifying bridging rules between conceptual clusters A bridging rule in this paper has its antecedent and action from different conceptual clusters . We first design two algorithms for mining bridging rules between clusters in a database , and then propose two non-linear metrics for measuring the interestingness of bridging rules . Bridging rules can be distinct from association rules or frequent itemsets . This is because 1 bridging rules can be generated by infrequent itemsets that are pruned in association rule mining ; and 2 bridging rules are measured by the importance that includes the distance between two conceptual clusters , whereas frequent itemsets are measured by only the support . [[EENNDD]] bridging rule; learning; association rule; outlier; entropy; clustering"}, "Mengenal pasti peraturan merapatkan antara kelompok konseptual Satu peraturan merapatkan dalam makalah ini mempunyai anteseden dan tindakannya dari kelompok konsep yang berbeza. Mula-mula kami merancang dua algoritma untuk melekatkan peraturan merapatkan antara kluster dalam pangkalan data, dan kemudian mencadangkan dua metrik bukan linier untuk mengukur kepentingan peraturan merapatkan. Peraturan merapatkan boleh berbeza dengan peraturan pergaulan atau set item yang kerap. Ini kerana 1 peraturan merapatkan dapat dihasilkan oleh set item yang jarang dipangkas dalam perlombongan peraturan persatuan; dan 2 peraturan merapatkan diukur dengan kepentingan yang merangkumi jarak antara dua kelompok konseptual, sedangkan set item yang kerap diukur hanya dengan sokongan. [[EENNDD]] peraturan merapatkan; belajar; peraturan persatuan; luaran; entropi; pengelompokan"], [{"string": "Characterising the difference Characterising the differences between two databases is an often occurring problem in Data Mining . Detection of change over time is a prime example , comparing databases from two branches is another one . The key problem is to discover the patterns that describe the difference . Emerging patterns provide only a partial answer to this question . In previous work , we showed that the data distribution can be captured in a pattern-based model using compression 12 . Here , we extend this approach to define a generic dissimilarity measure on databases . Moreover , we show that this approach can identify those patterns that characterise the differences between two distributions . Experimental results show that our method provides a well-founded way to independently measure database dissimilarity that allows for thorough inspection of the actual differences . This illustrates the use of our approach in real world data mining .", "keywords": ["compression", "database dissimilarity", "applications", "temporal data mining"], "combined": "Characterising the difference Characterising the differences between two databases is an often occurring problem in Data Mining . Detection of change over time is a prime example , comparing databases from two branches is another one . The key problem is to discover the patterns that describe the difference . Emerging patterns provide only a partial answer to this question . In previous work , we showed that the data distribution can be captured in a pattern-based model using compression 12 . Here , we extend this approach to define a generic dissimilarity measure on databases . Moreover , we show that this approach can identify those patterns that characterise the differences between two distributions . Experimental results show that our method provides a well-founded way to independently measure database dissimilarity that allows for thorough inspection of the actual differences . This illustrates the use of our approach in real world data mining . [[EENNDD]] compression; database dissimilarity; applications; temporal data mining"}, "Mencirikan perbezaan Mencirikan perbezaan antara dua pangkalan data adalah masalah yang sering berlaku dalam Perlombongan Data. Pengesanan perubahan dari masa ke masa adalah contoh utama, membandingkan pangkalan data dari dua cabang adalah yang lain. Masalah utama adalah mencari corak yang menerangkan perbezaannya. Pola yang muncul hanya memberikan sebahagian jawapan untuk soalan ini. Dalam karya sebelumnya, kami menunjukkan bahawa pengedaran data dapat ditangkap dalam model berdasarkan pola menggunakan pemampatan 12. Di sini, kami memperluaskan pendekatan ini untuk menentukan ukuran perbezaan umum pada pangkalan data. Lebih-lebih lagi, kami menunjukkan bahawa pendekatan ini dapat mengenal pasti corak-corak yang menjadi ciri perbezaan antara dua taburan. Hasil eksperimen menunjukkan bahawa kaedah kami menyediakan cara yang baik untuk mengukur kesamaan pangkalan data secara bebas yang memungkinkan pemeriksaan menyeluruh mengenai perbezaan sebenar. Ini menggambarkan penggunaan pendekatan kami dalam perlombongan data dunia nyata. [[EENNDD]] pemampatan; ketidaksamaan pangkalan data; permohonan; perlombongan data temporal"], [{"string": "Attack detection in time series for recommender systems Recent research has identified significant vulnerabilities in recommender systems . Shilling attacks , in which attackers introduce biased ratings in order to influence future recommendations , have been shown to be effective against collaborative filtering algorithms . We postulate that the distribution of item ratings in time can reveal the presence of a wide range of shilling attacks given reasonable assumptions about their duration . To construct a time series of ratings for an item , we use a window size of k to group consecutive ratings for the item into disjoint windows and compute the sample average and sample entropy in each window . We derive a theoretically optimal window size to best detect an attack event if the number of attack profiles is known . For practical applications where this number is unknown , we propose a heuristic algorithm that adaptively changes the window size . Our experimental results demonstrate that monitoring rating distributions in time series is an effective approach for detecting shilling attacks .", "keywords": ["anomaly detection", "time series", "recommender systems", "shilling attacks"], "combined": "Attack detection in time series for recommender systems Recent research has identified significant vulnerabilities in recommender systems . Shilling attacks , in which attackers introduce biased ratings in order to influence future recommendations , have been shown to be effective against collaborative filtering algorithms . We postulate that the distribution of item ratings in time can reveal the presence of a wide range of shilling attacks given reasonable assumptions about their duration . To construct a time series of ratings for an item , we use a window size of k to group consecutive ratings for the item into disjoint windows and compute the sample average and sample entropy in each window . We derive a theoretically optimal window size to best detect an attack event if the number of attack profiles is known . For practical applications where this number is unknown , we propose a heuristic algorithm that adaptively changes the window size . Our experimental results demonstrate that monitoring rating distributions in time series is an effective approach for detecting shilling attacks . [[EENNDD]] anomaly detection; time series; recommender systems; shilling attacks"}, "Pengesanan serangan dalam siri masa untuk sistem pengesyoran Penyelidikan baru-baru ini telah mengenal pasti kelemahan ketara dalam sistem pengesyorkan. Serangan Shilling, di mana penyerang memperkenalkan penilaian yang berat sebelah untuk mempengaruhi cadangan masa depan, telah terbukti efektif terhadap algoritma penyaringan kolaboratif. Kami mendalilkan bahawa penyebaran penilaian item pada waktunya dapat memperlihatkan adanya berbagai serangan shilling yang diberikan andaian yang munasabah mengenai jangka masa mereka. Untuk membina siri peringkat untuk item, kami menggunakan ukuran tetingkap k untuk mengelompokkan penilaian berturut-turut untuk item tersebut ke dalam tetingkap tetapan dan menghitung rata-rata sampel dan sampel entropi di setiap tetingkap. Kami memperoleh ukuran tetingkap optimum secara optimum untuk mengesan peristiwa serangan jika jumlah profil serangan diketahui. Untuk aplikasi praktikal di mana nombor ini tidak diketahui, kami mencadangkan algoritma heuristik yang mengubah ukuran tetingkap secara adaptif. Hasil eksperimen kami menunjukkan bahawa memantau pengedaran penilaian dalam siri masa adalah pendekatan yang berkesan untuk mengesan serangan shilling. [[EENNDD]] pengesanan anomali; siri masa; sistem pengesyorkan; serangan shilling"], [{"string": "Discriminative topic modeling based on manifold learning Topic modeling has been popularly used for data analysis in various domains including text documents . Previous topic models , such as probabilistic Latent Semantic Analysis pLSA and Latent Dirichlet Allocation LDA , have shown impressive success in discovering low-rank hidden structures for modeling text documents . These models , however , do not take into account the manifold structure of data , which is generally informative for the non-linear dimensionality reduction mapping . More recent models , namely Laplacian PLSI LapPLSI and Locally-consistent Topic Model LTM , have incorporated the local manifold structure into topic models and have shown the resulting benefits . But these approaches fall short of the full discriminating power of manifold learning as they only enhance the proximity between the low-rank representations of neighboring pairs without any consideration for non-neighboring pairs . In this paper , we propose Discriminative Topic Model DTM that separates non-neighboring pairs from each other in addition to bringing neighboring pairs closer together , thereby preserving the global manifold structure as well as improving the local consistency . We also present a novel model fitting algorithm based on the generalized EM and the concept of Pareto improvement . As a result , DTM achieves higher classification performance in a semi-supervised setting by effectively exposing the manifold structure of data . We provide empirical evidence on text corpora to demonstrate the success of DTM in terms of classification accuracy and robustness to parameters compared to state-of-the-art techniques .", "keywords": ["general", "document classification", "dimensionality reduction", "topic modeling", "semi-supervised learning"], "combined": "Discriminative topic modeling based on manifold learning Topic modeling has been popularly used for data analysis in various domains including text documents . Previous topic models , such as probabilistic Latent Semantic Analysis pLSA and Latent Dirichlet Allocation LDA , have shown impressive success in discovering low-rank hidden structures for modeling text documents . These models , however , do not take into account the manifold structure of data , which is generally informative for the non-linear dimensionality reduction mapping . More recent models , namely Laplacian PLSI LapPLSI and Locally-consistent Topic Model LTM , have incorporated the local manifold structure into topic models and have shown the resulting benefits . But these approaches fall short of the full discriminating power of manifold learning as they only enhance the proximity between the low-rank representations of neighboring pairs without any consideration for non-neighboring pairs . In this paper , we propose Discriminative Topic Model DTM that separates non-neighboring pairs from each other in addition to bringing neighboring pairs closer together , thereby preserving the global manifold structure as well as improving the local consistency . We also present a novel model fitting algorithm based on the generalized EM and the concept of Pareto improvement . As a result , DTM achieves higher classification performance in a semi-supervised setting by effectively exposing the manifold structure of data . We provide empirical evidence on text corpora to demonstrate the success of DTM in terms of classification accuracy and robustness to parameters compared to state-of-the-art techniques . [[EENNDD]] general; document classification; dimensionality reduction; topic modeling; semi-supervised learning"}, "Pemodelan topik diskriminatif berdasarkan pembelajaran manifold Pemodelan topik telah popular digunakan untuk analisis data dalam pelbagai domain termasuk dokumen teks. Model topik sebelumnya, seperti Probabilistic Latent Semantic Analysis pLSA dan Latent Dirichlet Allocation LDA, telah menunjukkan kejayaan yang mengagumkan dalam menemui struktur tersembunyi peringkat rendah untuk memodelkan dokumen teks. Model-model ini, bagaimanapun, tidak memperhitungkan struktur data yang berlipat ganda, yang pada umumnya informatif untuk pemetaan pengurangan dimensi tidak linear. Model yang lebih baru, iaitu Laplacian PLSI LapPLSI dan Model Topik LTM yang konsisten secara tempatan, telah memasukkan struktur manifold tempatan ke dalam model topik dan telah menunjukkan faedah yang dihasilkan. Tetapi pendekatan ini kekurangan kekuatan diskriminasi penuh dari pembelajaran pelbagai kerana mereka hanya meningkatkan jarak antara perwakilan berpasangan rendah dari pasangan jiran tanpa pertimbangan untuk pasangan bukan jiran. Dalam makalah ini, kami mengusulkan Model Topik Diskriminatif DTM yang memisahkan pasangan bukan jiran antara satu sama lain di samping mendekatkan pasangan tetangga lebih dekat, sehingga memelihara struktur manifold global serta meningkatkan konsistensi tempatan. Kami juga menyajikan algoritma pemasangan model novel berdasarkan EM umum dan konsep peningkatan Pareto. Hasilnya, DTM mencapai prestasi klasifikasi yang lebih tinggi dalam keadaan separa diselia dengan mendedahkan struktur data yang berlipat ganda secara berkesan. Kami memberikan bukti empirikal pada corpora teks untuk menunjukkan kejayaan DTM dari segi ketepatan klasifikasi dan ketahanan terhadap parameter berbanding dengan teknik canggih. [[EENNDD]] umum; pengelasan dokumen; pengurangan dimensi; pemodelan topik; pembelajaran separa penyeliaan"], [{"string": "Assessment and pruning of hierarchical model based clustering The goal of clustering is to identify distinct groups in a dataset . The basic idea of model-based clustering is to approximate the data density by a mixture model , typically a mixture of Gaussians , and to estimate the parameters of the component densities , the mixing fractions , and the number of components from the data . The number of distinct groups in the data is then taken to be the number of mixture components , and the observations are partitioned into clusters estimates of the groups using Bayes ' rule . If the groups are well separated and look Gaussian , then the resulting clusters will indeed tend to be `` distinct '' in the most common sense of the word - contiguous , densely populated areas of feature space , separated by contiguous , relatively empty regions . If the groups are not Gaussian , however , this correspondence may break down ; an isolated group with a non-elliptical distribution , for example , may be modeled by not one , but several mixture components , and the corresponding clusters will no longer be well separated . We present methods for assessing the degree of separation between the components of a mixture model and between the corresponding clusters . We also propose a new clustering method that can be regarded as a hybrid between model-based and nonparametric clustering . The hybrid clustering algorithm prunes the cluster tree generated by hierarchical model-based clustering . Starting with the tree corresponding to the mixture model chosen by the Bayesian Information Criterion , it progressively merges clusters that do not appear to correspond to different modes of the data density .", "keywords": ["unimodality", "model-based clustering", "density estimation", "nonparametric clustering", "clustering"], "combined": "Assessment and pruning of hierarchical model based clustering The goal of clustering is to identify distinct groups in a dataset . The basic idea of model-based clustering is to approximate the data density by a mixture model , typically a mixture of Gaussians , and to estimate the parameters of the component densities , the mixing fractions , and the number of components from the data . The number of distinct groups in the data is then taken to be the number of mixture components , and the observations are partitioned into clusters estimates of the groups using Bayes ' rule . If the groups are well separated and look Gaussian , then the resulting clusters will indeed tend to be `` distinct '' in the most common sense of the word - contiguous , densely populated areas of feature space , separated by contiguous , relatively empty regions . If the groups are not Gaussian , however , this correspondence may break down ; an isolated group with a non-elliptical distribution , for example , may be modeled by not one , but several mixture components , and the corresponding clusters will no longer be well separated . We present methods for assessing the degree of separation between the components of a mixture model and between the corresponding clusters . We also propose a new clustering method that can be regarded as a hybrid between model-based and nonparametric clustering . The hybrid clustering algorithm prunes the cluster tree generated by hierarchical model-based clustering . Starting with the tree corresponding to the mixture model chosen by the Bayesian Information Criterion , it progressively merges clusters that do not appear to correspond to different modes of the data density . [[EENNDD]] unimodality; model-based clustering; density estimation; nonparametric clustering; clustering"}, "Penilaian dan pemangkasan pengelompokan berdasarkan model hierarki Matlamat pengelompokan adalah untuk mengenal pasti kumpulan yang berbeza dalam set data. Idea asas pengelompokan berdasarkan model adalah untuk menghampiri ketumpatan data dengan model campuran, biasanya campuran Gaussians, dan untuk memperkirakan parameter kepadatan komponen, pecahan pencampuran, dan jumlah komponen dari data. Bilangan kumpulan yang berbeza dalam data kemudian diambil sebagai jumlah komponen campuran, dan pengamatan dibahagikan kepada perkiraan kelompok kelompok yang menggunakan aturan Bayes. Sekiranya kumpulan dipisahkan dengan baik dan kelihatan Gaussian, maka kelompok yang dihasilkan sememangnya cenderung \"berbeza\" dalam pengertian yang paling umum dari kata tersebut - kawasan ciri yang berdekatan, padat dan padat, dipisahkan oleh wilayah yang berdekatan dan relatif kosong. Sekiranya kumpulan itu bukan orang Gauss, surat-menyurat ini boleh rosak; kumpulan terpencil dengan taburan bukan elips, misalnya, boleh dimodelkan oleh bukan satu, tetapi beberapa komponen campuran, dan kelompok yang sesuai tidak lagi dapat dipisahkan dengan baik. Kami mengemukakan kaedah untuk menilai tahap pemisahan antara komponen model campuran dan antara kelompok yang sesuai. Kami juga mencadangkan kaedah pengelompokan baru yang dapat dianggap sebagai kacukan antara pengelompokan berdasarkan model dan nonparametrik. Algoritma pengelompokan hibrid memangkas pokok kluster yang dihasilkan oleh pengelompokan berasaskan model hierarki. Bermula dengan pohon yang sesuai dengan model campuran yang dipilih oleh Bayesian Information Criterion, secara progresif menggabungkan kluster yang tampaknya tidak sesuai dengan mod kepadatan data yang berbeza. [[EENNDD]] unimodality; pengelompokan berasaskan model; anggaran ketumpatan; pengelompokan bukan parametrik; pengelompokan"], [{"string": "Fast coordinate descent methods with variable selection for non-negative matrix factorization Nonnegative Matrix Factorization NMF is an effective dimension reduction method for non-negative dyadic data , and has proven to be useful in many areas , such as text mining , bioinformatics and image processing . NMF is usually formulated as a constrained non-convex optimization problem , and many algorithms have been developed for solving it . Recently , a coordinate descent method , called FastHals , has been proposed to solve least squares NMF and is regarded as one of the state-of-the-art techniques for the problem . In this paper , we first show that FastHals has an inefficiency in that it uses a cyclic coordinate descent scheme and thus , performs unneeded descent steps on unimportant variables . We then present a variable selection scheme that uses the gradient of the objective function to arrive at a new coordinate descent method . Our new method is considerably faster in practice and we show that it has theoretical convergence guarantees . Moreover when the solution is sparse , as is often the case in real applications , our new method benefits by selecting important variables to update more often , thus resulting in higher speed . As an example , on a text dataset RCV1 , our method is 7 times faster than FastHals , and more than 15 times faster when the sparsity is increased by adding an L1 penalty . We also develop new coordinate descent methods when error in NMF is measured by KL-divergence by applying the Newton method to solve the one-variable sub-problems . Experiments indicate that our algorithm for minimizing the KL-divergence is faster than the Lee & Seung multiplicative rule by a factor of 10 on the CBCL image dataset .", "keywords": ["learning", "non-negative matrix factorization", "coordinate descent method", "convergence"], "combined": "Fast coordinate descent methods with variable selection for non-negative matrix factorization Nonnegative Matrix Factorization NMF is an effective dimension reduction method for non-negative dyadic data , and has proven to be useful in many areas , such as text mining , bioinformatics and image processing . NMF is usually formulated as a constrained non-convex optimization problem , and many algorithms have been developed for solving it . Recently , a coordinate descent method , called FastHals , has been proposed to solve least squares NMF and is regarded as one of the state-of-the-art techniques for the problem . In this paper , we first show that FastHals has an inefficiency in that it uses a cyclic coordinate descent scheme and thus , performs unneeded descent steps on unimportant variables . We then present a variable selection scheme that uses the gradient of the objective function to arrive at a new coordinate descent method . Our new method is considerably faster in practice and we show that it has theoretical convergence guarantees . Moreover when the solution is sparse , as is often the case in real applications , our new method benefits by selecting important variables to update more often , thus resulting in higher speed . As an example , on a text dataset RCV1 , our method is 7 times faster than FastHals , and more than 15 times faster when the sparsity is increased by adding an L1 penalty . We also develop new coordinate descent methods when error in NMF is measured by KL-divergence by applying the Newton method to solve the one-variable sub-problems . Experiments indicate that our algorithm for minimizing the KL-divergence is faster than the Lee & Seung multiplicative rule by a factor of 10 on the CBCL image dataset . [[EENNDD]] learning; non-negative matrix factorization; coordinate descent method; convergence"}, "Kaedah keturunan koordinat pantas dengan pemilihan pemboleh ubah untuk pemfaktoran matriks bukan negatif Pemfaktoran Matriks Nonnegatif NMF adalah kaedah pengurangan dimensi yang berkesan untuk data dyadik bukan negatif, dan telah terbukti berguna dalam banyak bidang, seperti perlombongan teks, bioinformatik dan pemprosesan gambar. NMF biasanya dirumuskan sebagai masalah pengoptimuman bukan cembung yang terkendali, dan banyak algoritma telah dikembangkan untuk menyelesaikannya. Baru-baru ini, kaedah keturunan koordinat, yang disebut FastHals, telah dicadangkan untuk menyelesaikan NMF kotak paling sedikit dan dianggap sebagai salah satu teknik canggih untuk masalah ini. Dalam makalah ini, pertama kami menunjukkan bahawa FastHals mempunyai ketidakcekapan kerana menggunakan skema keturunan koordinat siklik dan dengan itu, melakukan langkah keturunan yang tidak diperlukan pada pemboleh ubah yang tidak penting. Kami kemudian membentangkan skema pemilihan pemboleh ubah yang menggunakan kecerunan fungsi objektif untuk mencapai kaedah keturunan koordinat baru. Kaedah baru kami jauh lebih pantas dalam praktik dan kami menunjukkan bahawa kaedah ini mempunyai jaminan penumpuan teori. Lebih-lebih lagi apabila penyelesaiannya jarang, seperti yang sering terjadi dalam aplikasi nyata, kaedah baru kami mendapat keuntungan dengan memilih pemboleh ubah penting untuk dikemas kini lebih kerap, sehingga menghasilkan kecepatan yang lebih tinggi. Sebagai contoh, pada kumpulan data teks RCV1, kaedah kami adalah 7 kali lebih cepat daripada FastHals, dan lebih daripada 15 kali lebih cepat apabila jarak meningkat dengan menambahkan penalti L1. Kami juga mengembangkan kaedah penurunan koordinat baru apabila ralat di NMF diukur dengan KL-divergence dengan menerapkan kaedah Newton untuk menyelesaikan sub-masalah satu-pemboleh ubah. Eksperimen menunjukkan bahawa algoritma kami untuk meminimumkan perbezaan KL-lebih cepat daripada peraturan pendaraban Lee & Seung dengan faktor 10 pada set data gambar CBCL. [[EENNDD]] pembelajaran; pemfaktoran matriks bukan negatif; kaedah keturunan koordinat; penumpuan"], [{"string": "Fast discovery of unexpected patterns in data , relative to a Bayesian network We consider a model in which background knowledge on a given domain of interest is available in terms of a Bayesian network , in addition to a large database . The mining problem is to discover unexpected patterns : our goal is to find the strongest discrepancies between network and database . This problem is intrinsically difficult because it requires inference in a Bayesian network and processing the entire , potentially very large , database . A sampling-based method that we introduce is efficient and yet provably finds the approximately most interesting unexpected patterns . We give a rigorous proof of the method 's correctness . Experiments shed light on its efficiency and practicality for large-scale Bayesian networks and databases .", "keywords": ["sampling", "bayesian networks", "association rules"], "combined": "Fast discovery of unexpected patterns in data , relative to a Bayesian network We consider a model in which background knowledge on a given domain of interest is available in terms of a Bayesian network , in addition to a large database . The mining problem is to discover unexpected patterns : our goal is to find the strongest discrepancies between network and database . This problem is intrinsically difficult because it requires inference in a Bayesian network and processing the entire , potentially very large , database . A sampling-based method that we introduce is efficient and yet provably finds the approximately most interesting unexpected patterns . We give a rigorous proof of the method 's correctness . Experiments shed light on its efficiency and practicality for large-scale Bayesian networks and databases . [[EENNDD]] sampling; bayesian networks; association rules"}, "Penemuan pantas dalam bentuk data yang tidak dijangka, relatif terhadap jaringan Bayesian Kami mempertimbangkan model di mana pengetahuan latar belakang pada domain minat tertentu tersedia dari segi jaringan Bayesian, selain pangkalan data yang besar. Masalah perlombongan adalah mencari corak yang tidak dijangka: tujuan kami adalah untuk mencari perbezaan yang paling kuat antara rangkaian dan pangkalan data. Masalah ini sememangnya sukar kerana memerlukan kesimpulan dalam rangkaian Bayesian dan memproses keseluruhan pangkalan data yang berpotensi sangat besar. Kaedah berasaskan sampel yang kami perkenalkan adalah cekap dan terbukti dapat menjumpai corak yang tidak dijangka yang paling menarik. Kami memberikan bukti yang tepat mengenai kebenaran kaedah ini. Eksperimen menjelaskan kecekapan dan kepraktisannya untuk rangkaian dan pangkalan data Bayesian berskala besar. [[EENNDD]] persampelan; rangkaian bayesian; peraturan persatuan"], [{"string": "Information theoretic regularization for semi-supervised boosting We present novel semi-supervised boosting algorithms that incrementally build linear combinations of weak classifiers through generic functional gradient descent using both labeled and unlabeled training data . Our approach is based on extending information regularization framework to boosting , bearing loss functions that combine log loss on labeled data with the information-theoretic measures to encode unlabeled data . Even though the information-theoretic regularization terms make the optimization non-convex , we propose simple sequential gradient descent optimization algorithms , and obtain impressively improved results on synthetic , benchmark and real world tasks over supervised boosting algorithms which use the labeled data alone and a state-of-the-art semi-supervised boosting algorithm .", "keywords": ["ensemble", "learning", "semi-supervised learning"], "combined": "Information theoretic regularization for semi-supervised boosting We present novel semi-supervised boosting algorithms that incrementally build linear combinations of weak classifiers through generic functional gradient descent using both labeled and unlabeled training data . Our approach is based on extending information regularization framework to boosting , bearing loss functions that combine log loss on labeled data with the information-theoretic measures to encode unlabeled data . Even though the information-theoretic regularization terms make the optimization non-convex , we propose simple sequential gradient descent optimization algorithms , and obtain impressively improved results on synthetic , benchmark and real world tasks over supervised boosting algorithms which use the labeled data alone and a state-of-the-art semi-supervised boosting algorithm . [[EENNDD]] ensemble; learning; semi-supervised learning"}, "Regularisasi teori maklumat untuk penggalakan separa penyeliaan Kami menyajikan algoritma penggalak separa pengawasan baru yang secara bertahap membina kombinasi linear pengklasifikasi lemah melalui penurunan kecerunan fungsional generik menggunakan data latihan berlabel dan tidak berlabel. Pendekatan kami didasarkan pada memperluas kerangka regularisasi informasi untuk meningkatkan, menanggung fungsi kehilangan yang menggabungkan kehilangan log pada data berlabel dengan langkah-langkah teori-maklumat untuk menyandikan data yang tidak berlabel. Walaupun istilah pengaturcaraan maklumat-teori menjadikan pengoptimuman tidak cembung, kami mencadangkan algoritma pengoptimuman keturunan kecerunan berurutan yang sederhana, dan memperoleh hasil yang sangat baik pada tugas sintetik, penanda aras dan dunia nyata di atas algoritma peningkatan yang diawasi yang menggunakan data berlabel sahaja dan keadaan algoritma peningkatan separa penyeliaan yang canggih. [[EENNDD]] ensemble; belajar; pembelajaran separa diselia"], [{"string": "Catching the drift : learning broad matches from clickthrough data Identifying similar keywords , known as broad matches , is an important task in online advertising that has become a standard feature on all major keyword advertising platforms . Effective broad matching leads to improvements in both relevance and monetization , while increasing advertisers ' reach and making campaign management easier . In this paper , we present a learning-based approach to broad matching that is based on exploiting implicit feedback in the form of advertisement clickthrough logs . Our method can utilize arbitrary similarity functions by incorporating them as features . We present an online learning algorithm , Amnesiac Averaged Perceptron , that is highly efficient yet able to quickly adjust to the rapidly-changing distributions of bidded keywords , advertisements and user behavior . Experimental results obtained from 1 historical logs and 2 live trials on a large-scale advertising platform demonstrate the effectiveness of the proposed algorithm and the overall success of our approach in identifying high-quality broad match mappings .", "keywords": ["on-line information services", "keyword-based advertising", "information search and retrieval", "keyword similarity", "online learning"], "combined": "Catching the drift : learning broad matches from clickthrough data Identifying similar keywords , known as broad matches , is an important task in online advertising that has become a standard feature on all major keyword advertising platforms . Effective broad matching leads to improvements in both relevance and monetization , while increasing advertisers ' reach and making campaign management easier . In this paper , we present a learning-based approach to broad matching that is based on exploiting implicit feedback in the form of advertisement clickthrough logs . Our method can utilize arbitrary similarity functions by incorporating them as features . We present an online learning algorithm , Amnesiac Averaged Perceptron , that is highly efficient yet able to quickly adjust to the rapidly-changing distributions of bidded keywords , advertisements and user behavior . Experimental results obtained from 1 historical logs and 2 live trials on a large-scale advertising platform demonstrate the effectiveness of the proposed algorithm and the overall success of our approach in identifying high-quality broad match mappings . [[EENNDD]] on-line information services; keyword-based advertising; information search and retrieval; keyword similarity; online learning"}, "Mengatasi arus: belajar padanan luas dari data klik lalu Mengenal pasti kata kunci serupa, yang dikenali sebagai padanan luas, adalah tugas penting dalam iklan dalam talian yang telah menjadi ciri standard pada semua platform iklan kata kunci utama. Pencocokan luas yang berkesan membawa kepada peningkatan relevansi dan pengewangan, sambil meningkatkan jangkauan pengiklan dan menjadikan pengurusan kempen lebih mudah. Dalam makalah ini, kami menyajikan pendekatan berbasis pembelajaran untuk pencocokan luas yang didasarkan pada memanfaatkan umpan balik tersirat dalam bentuk log klik-tayang iklan. Kaedah kami dapat menggunakan fungsi kesamaan sewenang-wenang dengan memasukkannya sebagai ciri. Kami menyajikan algoritma pembelajaran dalam talian, Amnesiac Averaged Perceptron, yang sangat cekap tetapi dapat menyesuaikan diri dengan cepat dengan sebaran pantas kata kunci, iklan dan tingkah laku pengguna yang ditawar. Hasil eksperimen yang diperoleh dari 1 log sejarah dan 2 percubaan langsung pada platform iklan berskala besar menunjukkan keberkesanan algoritma yang dicadangkan dan kejayaan keseluruhan pendekatan kami dalam mengenal pasti pemetaan padanan am berkualiti tinggi. [[EENNDD]] perkhidmatan maklumat dalam talian; iklan berdasarkan kata kunci; carian dan pengambilan maklumat; persamaan kata kunci; pembelajaran dalam talian"], [{"string": "Extracting relevant named entities for automated expense reimbursement Expense reimbursement is a time-consuming and labor-intensive process across organizations . In this paper , we present a prototype expense reimbursement system that dramatically reduces the elapsed time and costs involved , by eliminating paper from the process life cycle . Our complete solution involves 1 an electronic submission infrastructure that provides multi - channel image capture , secure transport and centralized storage of paper documents ; 2 an unconstrained data mining approach to extracting relevant named entities from un-structured document images ; 3 automation of auditing procedures that enables automatic expense validation with minimum human interaction . Extracting relevant named entities robustly from document images with unconstrained layouts and diverse formatting is a fundamental technical challenge to image-based data mining , question answering , and other information retrieval tasks . In many applications that require such capability , applying traditional language modeling techniques to the stream of OCR text does not give satisfactory result due to the absence of linguistic context . We present an approach for extracting relevant named entities from document images by combining rich page layout features in the image space with language content in the OCR text using a discriminative conditional random field CRF framework . We integrate this named entity extraction engine into our expense reimbursement solution and evaluate the system performance on large collections of real-world receipt images provided by IBM World Wide Reimbursement Center .", "keywords": ["general", "document layout analysis", "conditional random fields", "learning", "named entity extraction", "optical character recognition"], "combined": "Extracting relevant named entities for automated expense reimbursement Expense reimbursement is a time-consuming and labor-intensive process across organizations . In this paper , we present a prototype expense reimbursement system that dramatically reduces the elapsed time and costs involved , by eliminating paper from the process life cycle . Our complete solution involves 1 an electronic submission infrastructure that provides multi - channel image capture , secure transport and centralized storage of paper documents ; 2 an unconstrained data mining approach to extracting relevant named entities from un-structured document images ; 3 automation of auditing procedures that enables automatic expense validation with minimum human interaction . Extracting relevant named entities robustly from document images with unconstrained layouts and diverse formatting is a fundamental technical challenge to image-based data mining , question answering , and other information retrieval tasks . In many applications that require such capability , applying traditional language modeling techniques to the stream of OCR text does not give satisfactory result due to the absence of linguistic context . We present an approach for extracting relevant named entities from document images by combining rich page layout features in the image space with language content in the OCR text using a discriminative conditional random field CRF framework . We integrate this named entity extraction engine into our expense reimbursement solution and evaluate the system performance on large collections of real-world receipt images provided by IBM World Wide Reimbursement Center . [[EENNDD]] general; document layout analysis; conditional random fields; learning; named entity extraction; optical character recognition"}, "Mengambil entiti bernama yang relevan untuk penggantian perbelanjaan automatik Penggantian perbelanjaan adalah proses yang memakan masa dan intensif tenaga kerja di seluruh organisasi. Dalam makalah ini, kami menyajikan sistem penggantian biaya prototaip yang secara dramatik mengurangkan masa dan kos yang berlalu yang terlibat, dengan menghilangkan kertas dari kitaran hidup proses. Penyelesaian lengkap kami melibatkan 1 infrastruktur penyerahan elektronik yang menyediakan tangkapan gambar berbilang saluran, pengangkutan selamat dan penyimpanan dokumen kertas terpusat; 2 pendekatan perlombongan data yang tidak terkawal untuk mengekstrak entiti bernama yang relevan dari gambar dokumen yang tidak berstruktur; 3 automasi prosedur audit yang membolehkan pengesahan perbelanjaan automatik dengan interaksi minimum manusia. Mengekstrak entiti bernama yang relevan dengan kuat dari gambar dokumen dengan susun atur yang tidak terkawal dan format yang pelbagai adalah cabaran teknikal asas untuk perlombongan data berasaskan gambar, menjawab soalan, dan tugas pengambilan maklumat lain. Dalam banyak aplikasi yang memerlukan kemampuan seperti itu, menerapkan teknik pemodelan bahasa tradisional ke aliran teks OCR tidak memberikan hasil yang memuaskan kerana tidak adanya konteks linguistik. Kami menyajikan pendekatan untuk mengekstrak entiti bernama yang relevan dari gambar dokumen dengan menggabungkan ciri susun atur halaman yang kaya di ruang gambar dengan kandungan bahasa dalam teks OCR menggunakan kerangka CRF bidang rawak bersyarat yang diskriminatif. Kami mengintegrasikan enjin pengekstrakan entiti bernama ini ke dalam penyelesaian penggantian perbelanjaan kami dan menilai prestasi sistem pada koleksi besar gambar penerimaan dunia nyata yang disediakan oleh IBM World Wide Reimbursement Center. [[EENNDD]] umum; analisis susun atur dokumen; medan rawak bersyarat; belajar; pengekstrakan entiti bernama; pengecaman aksara optik"], [{"string": "Probabilistic author-topic models for information discovery We propose a new unsupervised learning technique for extracting information from large text collections . We model documents as if they were generated by a two-stage stochastic process . Each author is represented by a probability distribution over topics , and each topic is represented as a probability distribution over words for that topic . The words in a multi-author paper are assumed to be the result of a mixture of each authors ' topic mixture . The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm . We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library , and learn a model with 300 topics . We discuss in detail the interpretation of the results discovered by the system including specific topic and author models , ranking of authors by topic and topics by author , significant trends in the computer science literature between 1990 and 2002 , parsing of abstracts by topics and authors and detection of unusual papers by specific authors . An online query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer .", "keywords": ["text modeling", "gibbs sampling", "learning", "unsupervised learning"], "combined": "Probabilistic author-topic models for information discovery We propose a new unsupervised learning technique for extracting information from large text collections . We model documents as if they were generated by a two-stage stochastic process . Each author is represented by a probability distribution over topics , and each topic is represented as a probability distribution over words for that topic . The words in a multi-author paper are assumed to be the result of a mixture of each authors ' topic mixture . The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm . We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library , and learn a model with 300 topics . We discuss in detail the interpretation of the results discovered by the system including specific topic and author models , ranking of authors by topic and topics by author , significant trends in the computer science literature between 1990 and 2002 , parsing of abstracts by topics and authors and detection of unusual papers by specific authors . An online query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer . [[EENNDD]] text modeling; gibbs sampling; learning; unsupervised learning"}, "Model topik pengarang probabilistik untuk penemuan maklumat Kami mencadangkan teknik pembelajaran tanpa pengawasan baru untuk mengekstrak maklumat dari koleksi teks yang besar. Kami memodelkan dokumen seolah-olah ia dihasilkan oleh proses stokastik dua peringkat. Setiap penulis diwakili oleh taburan kebarangkalian untuk topik, dan setiap topik diwakili sebagai taburan kebarangkalian untuk kata-kata untuk topik itu. Kata-kata dalam makalah berbilang penulis dianggap sebagai hasil gabungan campuran topik setiap pengarang. Pembahagian topik-kata dan pengarang-topik dipelajari dari data secara tidak diawasi menggunakan algoritma Markov chain Monte Carlo. Kami menerapkan metodologi untuk sejumlah besar 160,000 abstrak dan 85,000 pengarang dari perpustakaan digital CiteSeer yang terkenal, dan mempelajari model dengan 300 topik. Kami membincangkan secara terperinci penafsiran hasil yang ditemui oleh sistem termasuk topik dan model pengarang tertentu, peringkat pengarang mengikut topik dan topik oleh pengarang, trend yang signifikan dalam literatur sains komputer antara tahun 1990 dan 2002, penghuraian abstrak oleh topik dan pengarang dan pengesanan makalah yang tidak biasa oleh pengarang tertentu. Antara muka pertanyaan dalam talian untuk model juga dibincangkan yang membolehkan penerokaan interaktif model topik pengarang untuk syarikat seperti CiteSeer. [[EENNDD]] pemodelan teks; persampelan gibbs; belajar; pembelajaran tanpa pengawasan"], [{"string": "Detecting research topics via the correlation between graphs and texts In this paper we address the problem of detecting topics in large-scale linked document collections . Recently , topic detection has become a very active area of research due to its utility for information navigation , trend analysis , and high-level description of data . We present a unique approach that uses the correlation between the distribution of a term that represents a topic and the link distribution in the citation graph where the nodes are limited to the documents containing the term . This tight coupling between term and graph analysis is distinguished from other approaches such as those that focus on language models . We develop a topic score measure for each term , using the likelihood ratio of binary hypotheses based on a probabilistic description of graph connectivity . Our approach is based on the intuition that if a term is relevant to a topic , the documents containing the term have denser connectivity than a random selection of documents . We extend our algorithm to detect a topic represented by a set of terms , using the intuition that if the co-occurrence of terms represents a new topic , the citation pattern should exhibit the synergistic effect . We test our algorithm on two electronic research literature collections , arXiv and Citeseer . Our evaluation shows that the approach is effective and reveals some novel aspects of topic detection .", "keywords": ["topic detection", "content analysis and indexing", "probabilistic measure", "graph mining", "citation graphs", "correlation of text and links"], "combined": "Detecting research topics via the correlation between graphs and texts In this paper we address the problem of detecting topics in large-scale linked document collections . Recently , topic detection has become a very active area of research due to its utility for information navigation , trend analysis , and high-level description of data . We present a unique approach that uses the correlation between the distribution of a term that represents a topic and the link distribution in the citation graph where the nodes are limited to the documents containing the term . This tight coupling between term and graph analysis is distinguished from other approaches such as those that focus on language models . We develop a topic score measure for each term , using the likelihood ratio of binary hypotheses based on a probabilistic description of graph connectivity . Our approach is based on the intuition that if a term is relevant to a topic , the documents containing the term have denser connectivity than a random selection of documents . We extend our algorithm to detect a topic represented by a set of terms , using the intuition that if the co-occurrence of terms represents a new topic , the citation pattern should exhibit the synergistic effect . We test our algorithm on two electronic research literature collections , arXiv and Citeseer . Our evaluation shows that the approach is effective and reveals some novel aspects of topic detection . [[EENNDD]] topic detection; content analysis and indexing; probabilistic measure; graph mining; citation graphs; correlation of text and links"}, "Mengesan topik penyelidikan melalui korelasi antara grafik dan teks Dalam makalah ini kita menangani masalah mengesan topik dalam koleksi dokumen berskala besar. Baru-baru ini, pengesanan topik telah menjadi bidang penyelidikan yang sangat aktif kerana penggunaannya untuk navigasi maklumat, analisis trend, dan keterangan data tahap tinggi. Kami menyajikan pendekatan unik yang menggunakan korelasi antara pembahagian istilah yang mewakili topik dan pengedaran pautan dalam graf petikan di mana node terhad kepada dokumen yang mengandungi istilah tersebut. Gandingan erat antara analisis istilah dan grafik ini dibezakan daripada pendekatan lain seperti pendekatan yang memfokuskan pada model bahasa. Kami mengembangkan ukuran skor topik untuk setiap istilah, menggunakan nisbah kemungkinan hipotesis binari berdasarkan keterangan probabilistik penyambungan grafik. Pendekatan kami didasarkan pada intuisi bahawa jika istilah relevan dengan topik, dokumen yang mengandung istilah tersebut mempunyai hubungan yang lebih padat daripada pemilihan dokumen secara rawak. Kami memperluas algoritma kami untuk mengesan topik yang diwakili oleh sekumpulan istilah, dengan menggunakan intuisi bahawa jika wujudnya istilah mewakili topik baru, corak petikan harus menunjukkan kesan sinergi. Kami menguji algoritma kami pada dua koleksi literatur penyelidikan elektronik, arXiv dan Citeseer. Penilaian kami menunjukkan bahawa pendekatan itu berkesan dan mendedahkan beberapa aspek baru pengesanan topik. [[EENNDD]] pengesanan topik; analisis kandungan dan pengindeksan; ukuran kebarangkalian; perlombongan grafik; graf petikan; korelasi teks dan pautan"], [{"string": "The offset tree for learning with partial labels We present an algorithm , called the Offset Tree , for learning to make decisions in situations where the payoff of only one choice is observed , rather than all choices . The algorithm reduces this setting to binary classification , allowing one to reuse any existing , fully supervised binary classification algorithm in this partial information setting . We show that the Offset Tree is an optimal reduction to binary classification . In particular , it has regret at most k-1 times the regret of the binary classifier it uses where k is the number of choices , and no reduction to binary classification can do better . This reduction is also computationally optimal , both at training and test time , requiring just O log2 k work to train on an example or make a prediction . Experiments with the Offset Tree show that it generally performs better than several alternative approaches .", "keywords": ["learning", "associative reinforcement learning", "contextual bandits", "interactive learning"], "combined": "The offset tree for learning with partial labels We present an algorithm , called the Offset Tree , for learning to make decisions in situations where the payoff of only one choice is observed , rather than all choices . The algorithm reduces this setting to binary classification , allowing one to reuse any existing , fully supervised binary classification algorithm in this partial information setting . We show that the Offset Tree is an optimal reduction to binary classification . In particular , it has regret at most k-1 times the regret of the binary classifier it uses where k is the number of choices , and no reduction to binary classification can do better . This reduction is also computationally optimal , both at training and test time , requiring just O log2 k work to train on an example or make a prediction . Experiments with the Offset Tree show that it generally performs better than several alternative approaches . [[EENNDD]] learning; associative reinforcement learning; contextual bandits; interactive learning"}, "Pohon ofset untuk belajar dengan label separa Kami menyajikan algoritma, yang disebut Pohon Offset, untuk belajar membuat keputusan dalam situasi di mana pembayaran hanya satu pilihan diperhatikan, dan bukan semua pilihan. Algoritma mengurangkan tetapan ini kepada klasifikasi binari, yang memungkinkan seseorang untuk menggunakan semula mana-mana algoritma klasifikasi binari yang ada dan diawasi sepenuhnya dalam tetapan maklumat separa ini. Kami menunjukkan bahawa Pohon Offset adalah pengurangan optimum kepada klasifikasi binari. Khususnya, penyesalan paling banyak k-1 kali penyesalan pengkelasan binari yang digunakannya di mana k adalah bilangan pilihan, dan pengurangan klasifikasi binari tidak dapat dilakukan dengan lebih baik. Pengurangan ini juga optimum secara komputasi, baik pada waktu latihan dan ujian, hanya memerlukan kerja O log2 k untuk melatih contoh atau membuat ramalan. Eksperimen dengan Offset Tree menunjukkan bahawa ia biasanya lebih baik daripada beberapa pendekatan alternatif. [[EENNDD]] pembelajaran; pembelajaran peneguhan bersekutu; penyamun kontekstual; pembelajaran interaktif"], [{"string": "Turning down the noise in the blogosphere In recent years , the blogosphere has experienced a substantial increase in the number of posts published daily , forcing users to cope with information overload . The task of guiding users through this flood of information has thus become critical . To address this issue , we present a principled approach for picking a set of posts that best covers the important stories in the blogosphere . We define a simple and elegant notion of coverage and formalize it as a submodular optimization problem , for which we can efficiently compute a near-optimal solution . In addition , since people have varied interests , the ideal coverage algorithm should incorporate user preferences in order to tailor the selected posts to individual tastes . We define the problem of learning a personalized coverage function by providing an appropriate user-interaction model and formalizing an online learning framework for this task . We then provide a no-regret algorithm which can quickly learn a user 's preferences from limited feedback . We evaluate our coverage and personalization algorithms extensively over real blog data . Results from a user study show that our simple coverage algorithm does as well as most popular blog aggregation sites , including Google Blog Search , Yahoo ! Buzz , and Digg . Furthermore , we demonstrate empirically that our algorithm can successfully adapt to user preferences . We believe that our technique , especially with personalization , can dramatically reduce information overload .", "keywords": ["personalization", "blogs", "learning"], "combined": "Turning down the noise in the blogosphere In recent years , the blogosphere has experienced a substantial increase in the number of posts published daily , forcing users to cope with information overload . The task of guiding users through this flood of information has thus become critical . To address this issue , we present a principled approach for picking a set of posts that best covers the important stories in the blogosphere . We define a simple and elegant notion of coverage and formalize it as a submodular optimization problem , for which we can efficiently compute a near-optimal solution . In addition , since people have varied interests , the ideal coverage algorithm should incorporate user preferences in order to tailor the selected posts to individual tastes . We define the problem of learning a personalized coverage function by providing an appropriate user-interaction model and formalizing an online learning framework for this task . We then provide a no-regret algorithm which can quickly learn a user 's preferences from limited feedback . We evaluate our coverage and personalization algorithms extensively over real blog data . Results from a user study show that our simple coverage algorithm does as well as most popular blog aggregation sites , including Google Blog Search , Yahoo ! Buzz , and Digg . Furthermore , we demonstrate empirically that our algorithm can successfully adapt to user preferences . We believe that our technique , especially with personalization , can dramatically reduce information overload . [[EENNDD]] personalization; blogs; learning"}, "Menurunkan kebisingan di blogosphere Beberapa tahun kebelakangan ini, blogosphere telah mengalami peningkatan yang banyak dalam jumlah posting yang diterbitkan setiap hari, memaksa pengguna untuk mengatasi banyak maklumat. Oleh itu, tugas membimbing pengguna melalui banjir maklumat menjadi sangat penting. Untuk mengatasi masalah ini, kami menyajikan pendekatan berprinsip untuk memilih sekumpulan catatan yang paling merangkumi kisah-kisah penting di blogosphere. Kami menentukan konsep liputan yang sederhana dan elegan dan memformalkannya sebagai masalah pengoptimuman submodular, yang mana kami dapat menghitung penyelesaian yang hampir optimum dengan cekap. Di samping itu, kerana orang mempunyai minat yang berbeza-beza, algoritma liputan yang ideal harus menggabungkan pilihan pengguna untuk menyesuaikan catatan yang dipilih dengan selera individu. Kami menentukan masalah mempelajari fungsi liputan yang diperibadikan dengan menyediakan model interaksi pengguna yang sesuai dan memformalkan kerangka pembelajaran dalam talian untuk tugas ini. Kami kemudian memberikan algoritma tanpa penyesalan yang dapat dengan cepat mempelajari pilihan pengguna daripada maklum balas terhad. Kami menilai algoritma liputan dan pemperibadian kami secara meluas berbanding data blog sebenar. Hasil dari kajian pengguna menunjukkan bahawa algoritma liputan mudah kami juga seperti laman agregasi blog yang paling popular, termasuk Carian Blog Google, Yahoo! Buzz, dan Digg. Tambahan pula, kami menunjukkan secara empirik bahawa algoritma kami berjaya menyesuaikan diri dengan pilihan pengguna. Kami percaya bahawa teknik kami, terutama dengan pemperibadian, dapat mengurangkan maklumat secara berlebihan. [[EENNDD]] pemperibadian; blog; belajar"], [{"string": "A family of dissimilarity measures between nodes generalizing both the shortest-path and the commute-time distances This work introduces a new family of link-based dissimilarity measures between nodes of a weighted directed graph . This measure , called the randomized shortest-path RSP dissimilarity , depends on a parameter \u03b8 and has the interesting property of reducing , on one end , to the standard shortest-path distance when \u03b8 is large and , on the other end , to the commute-time or resistance distance when \u03b8 is small near zero . Intuitively , it corresponds to the expected cost incurred by a random walker in order to reach a destination node from a starting node while maintaining a constant entropy related to \u03b8 spread in the graph . The parameter \u03b8 is therefore biasing gradually the simple random walk on the graph towards the shortest-path policy . By adopting a statistical physics approach and computing a sum over all the possible paths discrete path integral , it is shown that the RSP dissimilarity from every node to a particular node of interest can be computed efficiently by solving two linear systems of n equations , where n is the number of nodes . On the other hand , the dissimilarity between every couple of nodes is obtained by inverting an n x n matrix . The proposed measure can be used for various graph mining tasks such as computing betweenness centrality , finding dense communities , etc , as shown in the experimental section .", "keywords": ["resistance distance", "commute-time distance", "graph mining", "shortest path", "kernel on a graph", "biased random walk"], "combined": "A family of dissimilarity measures between nodes generalizing both the shortest-path and the commute-time distances This work introduces a new family of link-based dissimilarity measures between nodes of a weighted directed graph . This measure , called the randomized shortest-path RSP dissimilarity , depends on a parameter \u03b8 and has the interesting property of reducing , on one end , to the standard shortest-path distance when \u03b8 is large and , on the other end , to the commute-time or resistance distance when \u03b8 is small near zero . Intuitively , it corresponds to the expected cost incurred by a random walker in order to reach a destination node from a starting node while maintaining a constant entropy related to \u03b8 spread in the graph . The parameter \u03b8 is therefore biasing gradually the simple random walk on the graph towards the shortest-path policy . By adopting a statistical physics approach and computing a sum over all the possible paths discrete path integral , it is shown that the RSP dissimilarity from every node to a particular node of interest can be computed efficiently by solving two linear systems of n equations , where n is the number of nodes . On the other hand , the dissimilarity between every couple of nodes is obtained by inverting an n x n matrix . The proposed measure can be used for various graph mining tasks such as computing betweenness centrality , finding dense communities , etc , as shown in the experimental section . [[EENNDD]] resistance distance; commute-time distance; graph mining; shortest path; kernel on a graph; biased random walk"}, "Sekelompok ukuran ketidaksamaan antara nod yang membuat generalisasi kedua-dua jalan terpendek dan jarak masa perjalanan Ukuran ini, yang disebut ketidaksamaan RSP jalan terpendek secara rawak, bergantung pada parameter \u03b8 dan mempunyai sifat menarik untuk mengurangkan, pada satu hujung, ke jarak jalan terpendek standard apabila \u03b8 besar dan, di hujung yang lain, ke perjalanan -jarak atau jarak rintangan apabila \u03b8 kecil berhampiran sifar Secara intuitif, ia sesuai dengan jangkaan biaya yang ditanggung oleh pejalan kaki rawak untuk mencapai simpul tujuan dari simpul permulaan sambil mengekalkan entropi berterusan yang berkaitan dengan penyebaran \u03b8 dalam grafik. Oleh itu, parameter \u03b8 adalah bias secara beransur-ansur jalan rawak mudah pada grafik ke arah jalan terpendek. Dengan mengadopsi pendekatan fizik statistik dan menghitung jumlah semua kemungkinan jalan yang tidak dapat dipisahkan, ditunjukkan bahawa perbezaan RSP dari setiap simpul ke simpul minat tertentu dapat dihitung dengan berkesan dengan menyelesaikan dua sistem linear persamaan n, di mana n ialah bilangan nod. Sebaliknya, perbezaan antara setiap nod diperoleh dengan membalikkan matriks n x n. Ukuran yang dicadangkan dapat digunakan untuk pelbagai tugas penambangan grafik seperti pengkomputeran antara pusat, mencari komuniti padat, dan lain-lain, seperti yang ditunjukkan dalam bahagian eksperimen. [[EENNDD]] jarak rintangan; jarak masa berulang-alik; perlombongan grafik; jalan terpendek; kernel pada graf; berjalan rawak berat sebelah"], [{"string": "Automatic multimedia cross-modal correlation discovery Given an image or video clip , or audio song , how do we automatically assign keywords to it ? The general problem is to find correlations across the media in a collection of multimedia objects like video clips , with colors , and\\/or motion , and\\/or audio , and\\/or text scripts . We propose a novel , graph-based approach , `` MMG '' , to discover such cross-modal correlations . Our `` MMG '' method requires no tuning , no clustering , no user-determined constants ; it can be applied to any multimedia collection , as long as we have a similarity function for each medium ; and it scales linearly with the database size . We report auto-captioning experiments on the `` standard '' Corel image database of 680 MB , where it outperforms domain specific , fine-tuned methods by up to 10 percentage points in captioning accuracy 50 % relative improvement .", "keywords": ["graph-based model", "automatic image captioning", "cross-modal correlation"], "combined": "Automatic multimedia cross-modal correlation discovery Given an image or video clip , or audio song , how do we automatically assign keywords to it ? The general problem is to find correlations across the media in a collection of multimedia objects like video clips , with colors , and\\/or motion , and\\/or audio , and\\/or text scripts . We propose a novel , graph-based approach , `` MMG '' , to discover such cross-modal correlations . Our `` MMG '' method requires no tuning , no clustering , no user-determined constants ; it can be applied to any multimedia collection , as long as we have a similarity function for each medium ; and it scales linearly with the database size . We report auto-captioning experiments on the `` standard '' Corel image database of 680 MB , where it outperforms domain specific , fine-tuned methods by up to 10 percentage points in captioning accuracy 50 % relative improvement . [[EENNDD]] graph-based model; automatic image captioning; cross-modal correlation"}, "Penemuan korelasi lintas mod multimedia automatik Mengingat gambar atau klip video, atau lagu audio, bagaimana kita secara automatik memberikan kata kunci kepadanya? Masalah umum adalah mencari korelasi di seluruh media dalam koleksi objek multimedia seperti klip video, dengan warna, dan \\ / atau gerakan, dan \\ / atau audio, dan \\ / atau skrip teks. Kami mengusulkan sebuah novel, pendekatan berbasis grafik, \"MMG\", untuk mengetahui korelasi lintas modal. Kaedah \"MMG\" kami tidak memerlukan penalaan, pengelompokan, pemalar yang ditentukan pengguna; ia dapat digunakan untuk koleksi multimedia, asalkan kita mempunyai fungsi kesamaan untuk setiap media; dan skala secara linear dengan ukuran pangkalan data. Kami melaporkan eksperimen kapsyen automatik pada pangkalan data gambar Corel \"standard\" 680 MB, di mana ia mengatasi kaedah khusus, domain yang disempurnakan sehingga 10 mata peratusan dalam ketepatan keterangan peningkatan 50% relatif. [[EENNDD]] model berasaskan grafik; kapsyen gambar automatik; korelasi mod-silang"], [{"string": "Clinical and financial outcomes analysis with existing hospital patient records Existing patient records are a valuable resource for automated outcomes analysis and knowledge discovery . However , key clinical data in these records is typically recorded in unstructured form as free text and images , and most structured clinical information is poorly organized . Time-consuming interpretation and analysis is required to convert these records into structured clinical data . Thus , only a tiny fraction of this resource is utilized . We present REMIND , a Bayesian Framework for Reliable Extraction and Meaningful Inference from Nonstructured Data . REMIND integrates and blends the structured and unstructured clinical data in patient records to automatically created high-quality structured clinical data . This structuring allows existing patient records to be mined for quality assurance , regulatory compliance , and to relate financial and clinical factors . We demonstrate REMIND on two medical applications : a Extract `` recurrence '' , the key outcome for measuring treatment effectiveness , for colon cancer patients ii Extract key diagnoses and complications for acute myocardial infarction heart attack patients , and demonstrate the impact of these clinical factors on financial outcomes .", "keywords": ["bayes nets", "data mining", "temporal reasoning", "database applications", "hmms"], "combined": "Clinical and financial outcomes analysis with existing hospital patient records Existing patient records are a valuable resource for automated outcomes analysis and knowledge discovery . However , key clinical data in these records is typically recorded in unstructured form as free text and images , and most structured clinical information is poorly organized . Time-consuming interpretation and analysis is required to convert these records into structured clinical data . Thus , only a tiny fraction of this resource is utilized . We present REMIND , a Bayesian Framework for Reliable Extraction and Meaningful Inference from Nonstructured Data . REMIND integrates and blends the structured and unstructured clinical data in patient records to automatically created high-quality structured clinical data . This structuring allows existing patient records to be mined for quality assurance , regulatory compliance , and to relate financial and clinical factors . We demonstrate REMIND on two medical applications : a Extract `` recurrence '' , the key outcome for measuring treatment effectiveness , for colon cancer patients ii Extract key diagnoses and complications for acute myocardial infarction heart attack patients , and demonstrate the impact of these clinical factors on financial outcomes . [[EENNDD]] bayes nets; data mining; temporal reasoning; database applications; hmms"}, "Analisis hasil klinikal dan kewangan dengan rekod pesakit hospital yang ada Rekod pesakit yang ada adalah sumber yang berharga untuk analisis hasil automatik dan penemuan pengetahuan. Walau bagaimanapun, data klinikal utama dalam catatan ini biasanya direkodkan dalam bentuk tidak berstruktur sebagai teks dan gambar percuma, dan kebanyakan maklumat klinikal berstruktur kurang tersusun. Tafsiran dan analisis yang memakan masa diperlukan untuk menukar rekod ini menjadi data klinikal berstruktur. Oleh itu, hanya sebahagian kecil sumber ini yang digunakan. Kami menyajikan REMIND, Rangka Kerja Bayesian untuk Pengekstrakan yang Boleh Dipercayai dan Inferensi Bererti dari Data Tidak Berstruktur. REMIND mengintegrasikan dan menggabungkan data klinikal berstruktur dan tidak berstruktur dalam rekod pesakit untuk membuat data klinikal berstruktur berkualiti tinggi secara automatik. Penstrukturan ini membolehkan rekod pesakit yang ada ditambang untuk jaminan kualiti, pematuhan peraturan, dan untuk menghubungkan faktor kewangan dan klinikal. Kami menunjukkan PERINGATAN pada dua aplikasi perubatan: Ekstrak \"berulang\", hasil utama untuk mengukur keberkesanan rawatan, untuk pesakit barah usus ii. Ekstrak diagnosis dan komplikasi utama bagi pesakit serangan jantung infark miokard akut, dan menunjukkan kesan faktor klinikal ini mengenai hasil kewangan. [[EENNDD]] jaring bayes; perlombongan data; penaakulan sementara; aplikasi pangkalan data; hmms"], [{"string": "Mining broad latent query aspects from search sessions Search queries are typically very short , which means they are often underspecified or have senses that the user did not think of . A broad latent query aspect is a set of keywords that succinctly represents one particular sense , or one particular information need , that can aid users in reformulating such queries . We extract such broad latent aspects from query reformulations found in historical search session logs . We propose a framework under which the problem of extracting such broad latent aspects reduces to that of optimizing a formal objective function under constraints on the total number of aspects the system can store , and the number of aspects that can be shown in response to any given query . We present algorithms to find a good set of aspects , and also to pick the best k aspects matching any query . Empirical results on real-world search engine logs show significant gains over a strong baseline that uses single-keyword reformulations : a gain of 14 % and 23 % in terms of human-judged accuracy and click-through data respectively , and around 20 % in terms of consistency among aspects predicted for `` similar '' queries . This demonstrates both the importance of broad query aspects , and the efficacy of our algorithms for extracting them .", "keywords": ["latent user intent", "query aspects", "search sessions"], "combined": "Mining broad latent query aspects from search sessions Search queries are typically very short , which means they are often underspecified or have senses that the user did not think of . A broad latent query aspect is a set of keywords that succinctly represents one particular sense , or one particular information need , that can aid users in reformulating such queries . We extract such broad latent aspects from query reformulations found in historical search session logs . We propose a framework under which the problem of extracting such broad latent aspects reduces to that of optimizing a formal objective function under constraints on the total number of aspects the system can store , and the number of aspects that can be shown in response to any given query . We present algorithms to find a good set of aspects , and also to pick the best k aspects matching any query . Empirical results on real-world search engine logs show significant gains over a strong baseline that uses single-keyword reformulations : a gain of 14 % and 23 % in terms of human-judged accuracy and click-through data respectively , and around 20 % in terms of consistency among aspects predicted for `` similar '' queries . This demonstrates both the importance of broad query aspects , and the efficacy of our algorithms for extracting them . [[EENNDD]] latent user intent; query aspects; search sessions"}, "Perlombongan aspek pertanyaan laten luas dari sesi carian Pertanyaan carian biasanya sangat pendek, yang bermaksud ia sering tidak ditentukan atau mempunyai deria yang tidak difikirkan oleh pengguna. Aspek pertanyaan laten yang luas adalah sekumpulan kata kunci yang secara ringkas mewakili satu pengertian tertentu, atau satu keperluan maklumat tertentu, yang dapat membantu pengguna dalam merumuskan kembali pertanyaan tersebut. Kami mengekstrak aspek laten yang luas dari penyusunan semula pertanyaan yang terdapat dalam log sesi carian sejarah. Kami mencadangkan kerangka kerja di mana masalah pengekstrakan aspek laten luas seperti itu untuk mengoptimumkan fungsi objektif formal di bawah batasan jumlah aspek yang dapat disimpan oleh sistem, dan jumlah aspek yang dapat ditunjukkan sebagai tindak balas terhadap setiap yang diberikan pertanyaan. Kami mengemukakan algoritma untuk mencari sekumpulan aspek yang baik, dan juga untuk memilih aspek k terbaik yang sesuai dengan sebarang pertanyaan. Hasil empirik pada log enjin carian dunia nyata menunjukkan keuntungan yang signifikan berbanding asas yang kuat yang menggunakan penyusunan semula kata kunci tunggal: keuntungan 14% dan 23% dari segi ketepatan yang dinilai oleh manusia dan data klik-tayang masing-masing, dan sekitar 20% di syarat konsistensi antara aspek yang diramalkan untuk pertanyaan \"serupa\". Ini menunjukkan pentingnya aspek pertanyaan yang luas, dan keberkesanan algoritma kami untuk mengekstraknya. [[EENNDD]] maksud pengguna pendam; aspek pertanyaan; sesi carian"], [{"string": "Mining progressive confident rules Many real world objects have states that change over time . By tracking the state sequences of these objects , we can study their behavior and take preventive measures before they reach some undesirable states . In this paper , we propose a new kind of pattern called progressive confident rules to describe sequences of states with an increasing confidence that lead to a particular end state . We give a formal definition of progressive confident rules and their concise set . We devise pruning strategies to reduce the enormous search space . Experiment result shows that the proposed algorithm is efficient and scalable . We also demonstrate the application of progressive confident rules in classification .", "keywords": ["sequence", "progressive confident", "classification"], "combined": "Mining progressive confident rules Many real world objects have states that change over time . By tracking the state sequences of these objects , we can study their behavior and take preventive measures before they reach some undesirable states . In this paper , we propose a new kind of pattern called progressive confident rules to describe sequences of states with an increasing confidence that lead to a particular end state . We give a formal definition of progressive confident rules and their concise set . We devise pruning strategies to reduce the enormous search space . Experiment result shows that the proposed algorithm is efficient and scalable . We also demonstrate the application of progressive confident rules in classification . [[EENNDD]] sequence; progressive confident; classification"}, "Melombong peraturan yakin progresif Banyak objek dunia nyata mempunyai keadaan yang berubah dari masa ke masa. Dengan mengesan urutan keadaan objek-objek ini, kita dapat mempelajari tingkah laku mereka dan mengambil langkah pencegahan sebelum mencapai beberapa keadaan yang tidak diingini. Dalam makalah ini, kami mengusulkan jenis pola baru yang disebut aturan yakin progresif untuk menggambarkan urutan keadaan dengan keyakinan yang meningkat yang menuju ke keadaan akhir tertentu. Kami memberikan definisi formal mengenai peraturan yakin progresif dan set ringkasnya. Kami merancang strategi pemangkasan untuk mengurangkan ruang carian yang sangat besar. Hasil eksperimen menunjukkan bahawa algoritma yang dicadangkan adalah cekap dan berskala. Kami juga menunjukkan penerapan peraturan yakin progresif dalam klasifikasi. [[EENNDD]] urutan; progresif yakin; pengelasan"], [{"string": "Mining risk patterns in medical data In this paper , we discuss a problem of finding risk patterns in medical data . We define risk patterns by a statistical metric , relative risk , which has been widely used in epidemiological research . We characterise the problem of mining risk patterns as an optimal rule discovery problem . We study an anti-monotone property for mining optimal risk pattern sets and present an algorithm to make use of the property in risk pattern discovery . The method has been applied to a real world data set to find patterns associated with an allergic event for ACE inhibitors . The algorithm has generated some useful results for medical researchers .", "keywords": ["relative risk", "rule", "medical application", "optimal risk pattern set"], "combined": "Mining risk patterns in medical data In this paper , we discuss a problem of finding risk patterns in medical data . We define risk patterns by a statistical metric , relative risk , which has been widely used in epidemiological research . We characterise the problem of mining risk patterns as an optimal rule discovery problem . We study an anti-monotone property for mining optimal risk pattern sets and present an algorithm to make use of the property in risk pattern discovery . The method has been applied to a real world data set to find patterns associated with an allergic event for ACE inhibitors . The algorithm has generated some useful results for medical researchers . [[EENNDD]] relative risk; rule; medical application; optimal risk pattern set"}, "Pola risiko penambangan dalam data perubatan Dalam makalah ini, kita membincangkan masalah mencari pola risiko dalam data perubatan. Kami menentukan corak risiko dengan metrik statistik, risiko relatif, yang telah banyak digunakan dalam penyelidikan epidemiologi. Kami mencirikan masalah pola risiko perlombongan sebagai masalah penemuan peraturan yang optimum. Kami mengkaji sifat anti-monoton untuk perlombongan set corak risiko yang optimum dan menyajikan algoritma untuk memanfaatkan harta tanah dalam penemuan corak risiko. Kaedah ini telah diterapkan pada kumpulan data dunia nyata untuk mencari corak yang berkaitan dengan kejadian alergi untuk penghambat ACE. Algoritma telah menghasilkan beberapa hasil yang berguna untuk penyelidik perubatan. [[EENNDD]] risiko relatif; peraturan; permohonan perubatan; set corak risiko optimum"], [{"string": "A two-way visualization method for clustered data We describe a novel approach to the visualization of hierarchical clustering that superimposes the classical dendrogram over a fully synchronized low-dimensional embedding , thereby gaining the benefits of both approaches . In a single image one can view all the clusters , examine the relations between them and study many of their properties . The method is based on an algorithm for low-dimensional embedding of clustered data , with the property that separation between all clusters is guaranteed , regardless of their nature . In particular , the algorithm was designed to produce embeddings that strictly adhere to a given hierarchical clustering of the data , so that every two disjoint clusters in the hierarchy are drawn separately .", "keywords": ["hierarchical clustering", "dendrogram", "information visualization"], "combined": "A two-way visualization method for clustered data We describe a novel approach to the visualization of hierarchical clustering that superimposes the classical dendrogram over a fully synchronized low-dimensional embedding , thereby gaining the benefits of both approaches . In a single image one can view all the clusters , examine the relations between them and study many of their properties . The method is based on an algorithm for low-dimensional embedding of clustered data , with the property that separation between all clusters is guaranteed , regardless of their nature . In particular , the algorithm was designed to produce embeddings that strictly adhere to a given hierarchical clustering of the data , so that every two disjoint clusters in the hierarchy are drawn separately . [[EENNDD]] hierarchical clustering; dendrogram; information visualization"}, "Kaedah visualisasi dua hala untuk data berkelompok Kami menerangkan pendekatan baru untuk visualisasi pengelompokan hierarki yang menumpangkan dendrogram klasik melalui penyisipan dimensi rendah yang diselaraskan sepenuhnya, sehingga memperoleh keuntungan dari kedua pendekatan tersebut. Dalam satu gambar seseorang dapat melihat semua kelompok, memeriksa hubungan antara mereka dan mengkaji banyak sifatnya. Kaedah ini didasarkan pada algoritma untuk penyisipan data berkelompok dimensi rendah, dengan sifat bahawa pemisahan antara semua kelompok dijamin, tanpa mengira sifatnya. Khususnya, algoritma ini dirancang untuk menghasilkan penyisipan yang benar-benar mematuhi pengelompokan data hierarki tertentu, sehingga setiap dua kelompok yang terpisah dalam hierarki dilukis secara terpisah. [[EENNDD]] pengelompokan hierarki; dendrogram; visualisasi maklumat"], [{"string": "Mining time-changing data streams Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a stationary distribution . Unfortunately , most of the large databases available for mining today violate this assumption . They were gathered over months or years , and the underlying processes generating them changed during this time , sometimes radically . Although a number of algorithms have been proposed for learning time-changing concepts , they generally do not scale well to very large databases . In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams , based on the ultra-fast VFDT decision tree learner . This algorithm , called CVFDT , stays current while making the most of old data by growing an alternative subtree whenever an old one becomes questionable , and replacing the old with the new when the new becomes more accurate . CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives , but with O 1 complexity per example , as opposed to O w , where w is the size of the window . Experiments on a set of large time-changing data streams demonstrate the utility of this approach .", "keywords": ["decision trees", "data streams", "subsampling", "concept drift", "incremental learning", "hoeffding bounds"], "combined": "Mining time-changing data streams Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a stationary distribution . Unfortunately , most of the large databases available for mining today violate this assumption . They were gathered over months or years , and the underlying processes generating them changed during this time , sometimes radically . Although a number of algorithms have been proposed for learning time-changing concepts , they generally do not scale well to very large databases . In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams , based on the ultra-fast VFDT decision tree learner . This algorithm , called CVFDT , stays current while making the most of old data by growing an alternative subtree whenever an old one becomes questionable , and replacing the old with the new when the new becomes more accurate . CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives , but with O 1 complexity per example , as opposed to O w , where w is the size of the window . Experiments on a set of large time-changing data streams demonstrate the utility of this approach . [[EENNDD]] decision trees; data streams; subsampling; concept drift; incremental learning; hoeffding bounds"}, "Menambang aliran data yang mengubah masa Sebilangan besar algoritma statistik dan pembelajaran mesin menganggap bahawa data adalah sampel rawak yang diambil dari taburan pegun. Malangnya, kebanyakan pangkalan data besar yang tersedia untuk perlombongan hari ini melanggar andaian ini. Mereka dikumpulkan selama berbulan-bulan atau bertahun-tahun, dan proses mendasarnya menghasilkannya berubah selama ini, kadang-kadang secara radikal. Walaupun sebilangan algoritma telah diusulkan untuk mempelajari konsep yang mengubah waktu, mereka umumnya tidak sesuai dengan pangkalan data yang sangat besar. Dalam makalah ini kami mencadangkan algoritma yang cekap untuk menambang pokok keputusan dari aliran data yang terus berubah, berdasarkan pelajar pohon keputusan VFDT yang sangat cepat. Algoritma ini, yang disebut CVFDT, tetap terkini sambil memanfaatkan sebagian besar data lama dengan mengembangkan subtree alternatif setiap kali yang lama menjadi dipersoalkan, dan menggantikan yang lama dengan yang baru apabila yang baru menjadi lebih tepat. CVFDT mempelajari model yang mirip ketepatannya dengan model yang akan dipelajari dengan mengaplikasikan kembali VFDT ke tetingkap contoh yang bergerak setiap kali contoh baru tiba, tetapi dengan kerumitan O 1 setiap contoh, berbanding dengan O w, di mana w adalah saiz tingkap. Eksperimen pada satu set aliran data yang mengubah masa yang besar menunjukkan kegunaan pendekatan ini. [[EENNDD]] pokok keputusan; aliran data; pengambilan sampel; drift konsep; pembelajaran tambahan; had cangkul"], [{"string": "Detecting outliers using transduction and statistical testing Outlier detection can uncover malicious behavior in fields like intrusion detection and fraud analysis . Although there has been a significant amount of work in outlier detection , most of the algorithms proposed in the literature are based on a particular definition of outliers e.g. , density-based , and use ad-hoc thresholds to detect them . In this paper we present a novel technique to detect outliers with respect to an existing clustering model . However , the test can also be successfully utilized to recognize outliers when the clustering information is not available . Our method is based on Transductive Confidence Machines , which have been previously proposed as a mechanism to provide individual confidence measures on classification decisions . The test uses hypothesis testing to prove or disprove whether a point is fit to be in each of the clusters of the model . We experimentally demonstrate that the test is highly robust , and produces very few misdiagnosed points , even when no clustering information is available . Furthermore , our experiments demonstrate the robustness of our method under the circumstances of data contaminated by outliers . We finally show that our technique can be successfully applied to identify outliers in a noisy data set for which no information is available e.g. , ground truth , clustering structure , etc. . As such our proposed methodology is capable of bootstrapping from a noisy data set a clean one that can be used to identify future outliers .", "keywords": ["clustering"], "combined": "Detecting outliers using transduction and statistical testing Outlier detection can uncover malicious behavior in fields like intrusion detection and fraud analysis . Although there has been a significant amount of work in outlier detection , most of the algorithms proposed in the literature are based on a particular definition of outliers e.g. , density-based , and use ad-hoc thresholds to detect them . In this paper we present a novel technique to detect outliers with respect to an existing clustering model . However , the test can also be successfully utilized to recognize outliers when the clustering information is not available . Our method is based on Transductive Confidence Machines , which have been previously proposed as a mechanism to provide individual confidence measures on classification decisions . The test uses hypothesis testing to prove or disprove whether a point is fit to be in each of the clusters of the model . We experimentally demonstrate that the test is highly robust , and produces very few misdiagnosed points , even when no clustering information is available . Furthermore , our experiments demonstrate the robustness of our method under the circumstances of data contaminated by outliers . We finally show that our technique can be successfully applied to identify outliers in a noisy data set for which no information is available e.g. , ground truth , clustering structure , etc. . As such our proposed methodology is capable of bootstrapping from a noisy data set a clean one that can be used to identify future outliers . [[EENNDD]] clustering"}, "Mengesan outliers menggunakan transduction dan ujian statistik Pengesanan outlier dapat mendedahkan tingkah laku jahat dalam bidang seperti pengesanan pencerobohan dan analisis penipuan. Walaupun terdapat sejumlah besar pekerjaan dalam pengesanan outlier, kebanyakan algoritma yang dicadangkan dalam literatur didasarkan pada definisi tertentu dari outliers mis. , berdasarkan kepadatan, dan menggunakan ambang ad-hoc untuk mengesannya. Dalam makalah ini kami menyajikan teknik baru untuk mengesan outlier berkaitan dengan model pengelompokan yang ada. Walau bagaimanapun, ujian ini juga dapat digunakan untuk mengenali outliers ketika maklumat pengelompokan tidak tersedia. Kaedah kami didasarkan pada Mesin Keyakinan Transduktif, yang sebelumnya telah diusulkan sebagai mekanisme untuk memberikan ukuran keyakinan individu terhadap keputusan klasifikasi. Ujian ini menggunakan pengujian hipotesis untuk membuktikan atau membantah sama ada titik sesuai untuk berada dalam setiap kelompok model. Kami secara eksperimen menunjukkan bahawa ujian ini sangat mantap, dan menghasilkan sedikit titik yang salah didiagnosis, walaupun tidak ada maklumat pengelompokan. Selanjutnya, eksperimen kami menunjukkan ketahanan kaedah kami dalam keadaan data yang tercemar oleh outliers. Kami akhirnya menunjukkan bahawa teknik kami dapat diterapkan dengan jayanya untuk mengenal pasti outliers dalam satu set data yang bising yang tidak ada maklumat seperti. , kebenaran tanah, struktur pengelompokan, dll. Oleh itu, metodologi yang dicadangkan kami mampu melakukan bootstrap dari set data yang bising dan bersih yang dapat digunakan untuk mengenal pasti outlier masa depan. [[EENNDD]] pengelompokan"], [{"string": "Privacy-preserving cox regression for survival analysis Privacy-preserving data mining PPDM is an emergent research area that addresses the incorporation of privacy preserving concerns to data mining techniques . In this paper we propose a privacy-preserving PP Cox model for survival analysis , and consider a real clinical setting where the data is horizontally distributed among different institutions . The proposed model is based on linearly projecting the data to a lower dimensional space through an optimal mapping obtained by solving a linear programming problem . Our approach differs from the commonly used random projection approach since it instead finds a projection that is optimal at preserving the properties of the data that are important for the specific problem at hand . Since our proposed approach produces an sparse mapping , it also generates a PP mapping that not only projects the data to a lower dimensional space but it also depends on a smaller subset of the original features it provides explicit feature selection . Real data from several European healthcare institutions are used to test our model for survival prediction of non-small-cell lung cancer patients . These results are also confirmed using publicly available benchmark datasets . Our experimental results show that we are able to achieve a near-optimal performance without directly sharing the data across different data sources . This model makes it possible to conduct large-scale multi-centric survival analysis without violating privacy-preserving requirements .", "keywords": ["cox regression", "survival analysis", "miscellaneous", "privacy-preserving data mining"], "combined": "Privacy-preserving cox regression for survival analysis Privacy-preserving data mining PPDM is an emergent research area that addresses the incorporation of privacy preserving concerns to data mining techniques . In this paper we propose a privacy-preserving PP Cox model for survival analysis , and consider a real clinical setting where the data is horizontally distributed among different institutions . The proposed model is based on linearly projecting the data to a lower dimensional space through an optimal mapping obtained by solving a linear programming problem . Our approach differs from the commonly used random projection approach since it instead finds a projection that is optimal at preserving the properties of the data that are important for the specific problem at hand . Since our proposed approach produces an sparse mapping , it also generates a PP mapping that not only projects the data to a lower dimensional space but it also depends on a smaller subset of the original features it provides explicit feature selection . Real data from several European healthcare institutions are used to test our model for survival prediction of non-small-cell lung cancer patients . These results are also confirmed using publicly available benchmark datasets . Our experimental results show that we are able to achieve a near-optimal performance without directly sharing the data across different data sources . This model makes it possible to conduct large-scale multi-centric survival analysis without violating privacy-preserving requirements . [[EENNDD]] cox regression; survival analysis; miscellaneous; privacy-preserving data mining"}, "Regresi cox pelestarian privasi untuk analisis kelangsungan hidup Perlombongan data pelestarian privasi PPDM adalah bidang penyelidikan yang muncul yang menangani penggabungan kebimbangan menjaga privasi terhadap teknik perlombongan data. Dalam makalah ini kami mencadangkan model PP Cox yang menjaga privasi untuk analisis kelangsungan hidup, dan mempertimbangkan pengaturan klinikal sebenar di mana data diedarkan secara mendatar di antara institusi yang berbeza. Model yang dicadangkan didasarkan pada memproyeksikan data secara linear ke ruang dimensi yang lebih rendah melalui pemetaan optimum yang diperoleh dengan menyelesaikan masalah pengaturcaraan linear. Pendekatan kami berbeza dengan pendekatan unjuran rawak yang biasa digunakan kerana sebaliknya mencari unjuran yang optimum untuk memelihara sifat data yang penting untuk masalah khusus yang dihadapi. Oleh kerana pendekatan yang dicadangkan kami menghasilkan pemetaan yang jarang, ini juga menghasilkan pemetaan PP yang tidak hanya memproyeksikan data ke ruang dimensi yang lebih rendah tetapi juga bergantung pada subkumpulan fitur asal yang lebih kecil, ia menyediakan pemilihan ciri eksplisit. Data sebenar dari beberapa institusi kesihatan Eropah digunakan untuk menguji model kami untuk ramalan survival pesakit kanser paru-paru bukan sel kecil. Hasil ini juga disahkan menggunakan set data penanda aras yang tersedia untuk umum. Hasil eksperimen kami menunjukkan bahawa kami dapat mencapai prestasi yang hampir optimum tanpa berkongsi data secara langsung di pelbagai sumber data. Model ini memungkinkan untuk melakukan analisis kelangsungan hidup multi-sentral berskala besar tanpa melanggar syarat menjaga privasi. [[EENNDD]] regresi cox; analisis survival; pelbagai; perlombongan data yang memelihara privasi"], [{"string": "User grouping behavior in online forums Online forums represent one type of social media that is particularly rich for studying human behavior in information seeking and diffusing . The way users join communities is a reflection of the changing and expanding of their interests toward information . In this paper , we study the patterns of user participation behavior , and the feature factors that influence such behavior on different forum datasets . We find that , despite the relative randomness and lesser commitment of structural relationships in online forums , users ' community joining behaviors display some strong regularities . One particularly interesting observation is that the very weak relationships between users defined by online replies have similar diffusion curves as those of real friendships or co-authorships . We build social selection models , Bipartite Markov Random Field BiMRF , to quantitatively evaluate the prediction performance of those feature factors and their relationships . Using these models , we show that some features carry supplementary information , and the effectiveness of different features vary in different types of forums . Moreover , the results of BiMRF with two-star configurations suggest that the feature of user similarity defined by frequency of communication or number of common friends is inadequate to predict grouping behavior , but adding node-level features can improve the fit of the model .", "keywords": ["online forums", "social selection model", "information diffusion", "social networks"], "combined": "User grouping behavior in online forums Online forums represent one type of social media that is particularly rich for studying human behavior in information seeking and diffusing . The way users join communities is a reflection of the changing and expanding of their interests toward information . In this paper , we study the patterns of user participation behavior , and the feature factors that influence such behavior on different forum datasets . We find that , despite the relative randomness and lesser commitment of structural relationships in online forums , users ' community joining behaviors display some strong regularities . One particularly interesting observation is that the very weak relationships between users defined by online replies have similar diffusion curves as those of real friendships or co-authorships . We build social selection models , Bipartite Markov Random Field BiMRF , to quantitatively evaluate the prediction performance of those feature factors and their relationships . Using these models , we show that some features carry supplementary information , and the effectiveness of different features vary in different types of forums . Moreover , the results of BiMRF with two-star configurations suggest that the feature of user similarity defined by frequency of communication or number of common friends is inadequate to predict grouping behavior , but adding node-level features can improve the fit of the model . [[EENNDD]] online forums; social selection model; information diffusion; social networks"}, "Tingkah laku pengelompokan pengguna dalam forum dalam talian Forum dalam talian mewakili satu jenis media sosial yang sangat kaya untuk mengkaji tingkah laku manusia dalam mencari dan menyebarkan maklumat. Cara pengguna bergabung dengan komuniti adalah cerminan perubahan dan pengembangan minat mereka terhadap maklumat. Dalam makalah ini, kami mengkaji corak tingkah laku penyertaan pengguna, dan faktor ciri yang mempengaruhi tingkah laku tersebut pada kumpulan data forum yang berbeza. Kami mendapati bahawa, walaupun terdapat kekacauan relatif dan komitmen hubungan struktur yang kurang dalam forum dalam talian, tingkah laku bergabung dengan komuniti pengguna menunjukkan beberapa keteraturan yang kuat. Satu pemerhatian yang sangat menarik adalah bahawa hubungan yang sangat lemah antara pengguna yang ditentukan oleh balasan dalam talian mempunyai keluk penyebaran yang serupa dengan hubungan persahabatan atau pengarang bersama. Kami membina model pemilihan sosial, Bipartite Markov Random Field BiMRF, untuk menilai prestasi ramalan faktor ciri dan hubungannya secara kuantitatif. Dengan menggunakan model-model ini, kami menunjukkan bahawa beberapa ciri membawa maklumat tambahan, dan keberkesanan ciri-ciri yang berbeza-beza berbeza dalam pelbagai jenis forum. Lebih-lebih lagi, hasil BiMRF dengan konfigurasi dua bintang menunjukkan bahawa ciri kesamaan pengguna yang ditentukan oleh kekerapan komunikasi atau bilangan rakan biasa tidak mencukupi untuk meramalkan tingkah laku pengelompokan, tetapi menambahkan ciri tahap nod dapat meningkatkan kesesuaian model. [[EENNDD]] forum dalam talian; model pemilihan sosial; penyebaran maklumat; rangkaian sosial"], [{"string": "Making holistic schema matching robust : an ensemble approach The Web has been rapidly `` deepened '' by myriad searchable databases online , where data are hidden behind query interfaces . As an essential task toward integrating these massive `` deep Web '' sources , large scale schema matching i.e. , discovering semantic correspondences of attributes across many query interfaces has been actively studied recently . In particular , many works have emerged to address this problem by `` holistically '' matching many schemas at the same time and thus pursuing `` mining '' approaches in nature . However , while holistic schema matching has built its promise upon the large quantity of input schemas , it also suffers the robustness problem caused by noisy data quality . Such noises often inevitably arise in the automatic extraction of schema data , which is mandatory in large scale integration . For holistic matching to be viable , it is thus essential to make it robust against noisy schemas . To tackle this challenge , we propose a data-ensemble framework with sampling and voting techniques , which is inspired by bagging predictors . Specifically , our approach creates an ensemble of matchers , by randomizing input schema data into many independently downsampled trials , executing the same matcher on each trial and then aggregating their ranked results by taking majority voting . As a principled basis , we provide analytic justification of the effectiveness of this data-ensemble framework . Further , empirically , our experiments on real Web data show that the `` ensemblization '' indeed significantly boosts the matching accuracy under noisy schema input , and thus maintains the desired robustness of a holistic matcher .", "keywords": ["ensemble", "bagging predictors", "data integration", "schema matching", "heterogeneous databases", "deep web"], "combined": "Making holistic schema matching robust : an ensemble approach The Web has been rapidly `` deepened '' by myriad searchable databases online , where data are hidden behind query interfaces . As an essential task toward integrating these massive `` deep Web '' sources , large scale schema matching i.e. , discovering semantic correspondences of attributes across many query interfaces has been actively studied recently . In particular , many works have emerged to address this problem by `` holistically '' matching many schemas at the same time and thus pursuing `` mining '' approaches in nature . However , while holistic schema matching has built its promise upon the large quantity of input schemas , it also suffers the robustness problem caused by noisy data quality . Such noises often inevitably arise in the automatic extraction of schema data , which is mandatory in large scale integration . For holistic matching to be viable , it is thus essential to make it robust against noisy schemas . To tackle this challenge , we propose a data-ensemble framework with sampling and voting techniques , which is inspired by bagging predictors . Specifically , our approach creates an ensemble of matchers , by randomizing input schema data into many independently downsampled trials , executing the same matcher on each trial and then aggregating their ranked results by taking majority voting . As a principled basis , we provide analytic justification of the effectiveness of this data-ensemble framework . Further , empirically , our experiments on real Web data show that the `` ensemblization '' indeed significantly boosts the matching accuracy under noisy schema input , and thus maintains the desired robustness of a holistic matcher . [[EENNDD]] ensemble; bagging predictors; data integration; schema matching; heterogeneous databases; deep web"}, "Menjadikan padanan skema holistik: pendekatan ensemble Web dengan cepat \"diperdalam\" oleh pelbagai pangkalan data yang dicari dalam talian, di mana data tersembunyi di sebalik antara muka pertanyaan. Sebagai tugas penting untuk mengintegrasikan sumber-sumber \"Web dalam\" yang besar ini, pencocokan skema skala besar iaitu, menemui korespondensi semantik atribut di banyak antara muka pertanyaan telah dikaji secara aktif baru-baru ini. Khususnya, banyak karya telah muncul untuk mengatasi masalah ini dengan \"secara holistik\" mencocokkan banyak skema pada masa yang sama dan dengan demikian menjalankan pendekatan \"perlombongan\" secara semula jadi. Namun, sementara pemadanan skema holistik telah memenuhi janjinya berdasarkan sejumlah besar skema input, ia juga mengalami masalah ketahanan yang disebabkan oleh kualiti data yang bising. Bunyi seperti itu sering kali timbul dalam pengekstrakan data skema secara automatik, yang wajib dilakukan dalam penyatuan skala besar. Agar pemadanan secara holistik dapat dilaksanakan, sangat penting untuk menjadikannya kuat daripada skema yang bising. Untuk mengatasi cabaran ini, kami mengusulkan kerangka data-ensemble dengan teknik pengambilan sampel dan pemungutan suara, yang diilhami oleh prediksi penumpukan. Secara khusus, pendekatan kami membuat kumpulan pencocokan, dengan mengacak data skema input ke dalam banyak percobaan yang tidak disampel secara bebas, melaksanakan pencocokan yang sama pada setiap percubaan dan kemudian mengumpulkan hasil peringkat mereka dengan mengambil suara mayoritas. Sebagai asas, kami memberikan justifikasi analitik mengenai keberkesanan kerangka data-ensemble ini. Selanjutnya, secara empirikal, eksperimen kami pada data Web nyata menunjukkan bahawa \"ensemblisasi\" sememangnya meningkatkan ketepatan pemadanan dengan input skema yang bising, dan dengan demikian mempertahankan ketahanan yang sesuai dari pemadan holistik. [[EENNDD]] ensemble; ramalan pengantungan; penyatuan data; pemadanan skema; pangkalan data heterogen; web dalam"], [{"string": "Expertise modeling for matching papers with reviewers An essential part of an expert-finding task , such as matching reviewers to submitted papers , is the ability to model the expertise of a person based on documents . We evaluate several measures of the association between an author in an existing collection of research papers and a previously unseen document . We compare two language model based approaches with a novel topic model , Author-Persona-Topic APT . In this model , each author can write under one or more `` personas , '' which are represented as independent distributions over hidden topics . Examples of previous papers written by prospective reviewers are gathered from the Rexa database , which extracts and disambiguates author mentions from documents gathered from the web . We evaluate the models using a reviewer matching task based on human relevance judgments determining how well the expertise of proposed reviewers matches a submission . We find that the APT topic model outperforms the other models .", "keywords": ["topic models", "reviewer finding"], "combined": "Expertise modeling for matching papers with reviewers An essential part of an expert-finding task , such as matching reviewers to submitted papers , is the ability to model the expertise of a person based on documents . We evaluate several measures of the association between an author in an existing collection of research papers and a previously unseen document . We compare two language model based approaches with a novel topic model , Author-Persona-Topic APT . In this model , each author can write under one or more `` personas , '' which are represented as independent distributions over hidden topics . Examples of previous papers written by prospective reviewers are gathered from the Rexa database , which extracts and disambiguates author mentions from documents gathered from the web . We evaluate the models using a reviewer matching task based on human relevance judgments determining how well the expertise of proposed reviewers matches a submission . We find that the APT topic model outperforms the other models . [[EENNDD]] topic models; reviewer finding"}, "Pemodelan kepakaran untuk mencocokkan makalah dengan pengulas Bahagian penting dari tugas mencari pakar, seperti mencocokkan pengulas dengan kertas yang diserahkan, adalah kemampuan untuk memodelkan kepakaran seseorang berdasarkan dokumen. Kami menilai beberapa ukuran hubungan antara pengarang dalam koleksi kertas penyelidikan yang ada dan dokumen yang sebelumnya tidak kelihatan. Kami membandingkan dua pendekatan berdasarkan model bahasa dengan model topik baru, APT Pengarang-Persona-Topik. Dalam model ini, setiap pengarang dapat menulis di bawah satu atau lebih \"personas,\" yang diwakili sebagai pengedaran bebas atas topik tersembunyi. Contoh makalah sebelumnya yang ditulis oleh calon pengulas dikumpulkan dari pangkalan data Rexa, yang mengekstrak dan menyingkirkan sebutan pengarang dari dokumen yang dikumpulkan dari web. Kami menilai model menggunakan tugas pencocokan pengulas berdasarkan penilaian relevan manusia yang menentukan sejauh mana kepakaran pengulas yang dicadangkan sesuai dengan penyerahan. Kami mendapati bahawa model topik APT mengatasi model lain. [[EENNDD]] model topik; penemuan pengulas"], [{"string": "On privacy preservation against adversarial data mining Privacy preserving data processing has become an important topic recently because of advances in hardware technology which have lead to widespread proliferation of demographic and sensitive data . A rudimentary way to preserve privacy is to simply hide the information in some of the sensitive fields picked by a user . However , such a method is far from satisfactory in its ability to prevent adversarial data mining . Real data records are not randomly distributed . As a result , some fields in the records may be correlated with one another . If the correlation is sufficiently high , it may be possible for an adversary to predict some of the sensitive fields using other fields . In this paper , we study the problem of privacy preservation against adversarial data mining , which is to hide a minimal set of entries so that the privacy of the sensitive fields are satisfactorily preserved . In other words , even by data mining , an adversary still can not accurately recover the hidden data entries . We model the problem concisely and develop an efficient heuristic algorithm which can find good solutions in practice . An extensive performance study is conducted on both synthetic and real data sets to examine the effectiveness of our approach .", "keywords": ["privacy preservation", "association rules"], "combined": "On privacy preservation against adversarial data mining Privacy preserving data processing has become an important topic recently because of advances in hardware technology which have lead to widespread proliferation of demographic and sensitive data . A rudimentary way to preserve privacy is to simply hide the information in some of the sensitive fields picked by a user . However , such a method is far from satisfactory in its ability to prevent adversarial data mining . Real data records are not randomly distributed . As a result , some fields in the records may be correlated with one another . If the correlation is sufficiently high , it may be possible for an adversary to predict some of the sensitive fields using other fields . In this paper , we study the problem of privacy preservation against adversarial data mining , which is to hide a minimal set of entries so that the privacy of the sensitive fields are satisfactorily preserved . In other words , even by data mining , an adversary still can not accurately recover the hidden data entries . We model the problem concisely and develop an efficient heuristic algorithm which can find good solutions in practice . An extensive performance study is conducted on both synthetic and real data sets to examine the effectiveness of our approach . [[EENNDD]] privacy preservation; association rules"}, "Mengenai pemeliharaan privasi terhadap perlombongan data musuh Privasi pemeliharaan data menjadi topik penting baru-baru ini kerana kemajuan teknologi perkakasan yang menyebabkan berlakunya penyebaran data demografi dan sensitif secara meluas. Cara asas untuk menjaga privasi adalah dengan menyembunyikan maklumat di beberapa bidang sensitif yang dipilih oleh pengguna. Walau bagaimanapun, kaedah seperti ini jauh dari memuaskan dalam kemampuannya untuk mencegah perlombongan data lawan. Rekod data sebenar tidak diedarkan secara rawak. Akibatnya, beberapa bidang dalam catatan mungkin saling berhubungan satu sama lain. Sekiranya korelasi cukup tinggi, mungkin lawan dapat meramalkan beberapa medan sensitif menggunakan medan lain. Dalam makalah ini, kami mengkaji masalah pemeliharaan privasi terhadap perlombongan data musuh, iaitu menyembunyikan sekumpulan entri minimum sehingga privasi bidang sensitif terpelihara dengan memuaskan. Dengan kata lain, walaupun dengan perlombongan data, musuh masih tidak dapat memulihkan entri data yang tersembunyi dengan tepat. Kami memodelkan masalah dengan ringkas dan mengembangkan algoritma heuristik yang cekap yang dapat mencari penyelesaian yang baik dalam praktiknya. Kajian prestasi yang luas dilakukan pada kedua-dua kumpulan data sintetik dan sebenar untuk mengkaji keberkesanan pendekatan kami. [[EENNDD]] pemeliharaan privasi; peraturan persatuan"], [{"string": "GPCA : an efficient dimension reduction scheme for image compression and retrieval Recent years have witnessed a dramatic increase in the quantity of image data collected , due to advances in fields such as medical imaging , reconnaissance , surveillance , astronomy , multimedia etc. . With this increase has come the need to be able to store , transmit , and query large volumes of image data efficiently . A common operation on image databases is the retrieval of all images that are similar to a query image . For this , the images in the database are often represented as vectors in a high-dimensional space and a query is answered by retrieving all image vectors that are proximal to the query image in this space , under a suitable similarity metric . To overcome problems associated with high dimensionality , such as high storage and retrieval times , a dimension reduction step is usually applied to the vectors to concentrate relevant information in a small number of dimensions . Principal Component Analysis PCA is a well-known dimension reduction scheme . However , since it works with vectorized representations of images , PCA does not take into account the spatial locality of pixels in images . In this paper , a new dimension reduction scheme , called Generalized Principal Component Analysis GPCA , is presented . This scheme works directly with images in their native state , as two-dimensional matrices , by projecting the images to a vector space that is the tensor product of two lower-dimensional vector spaces . Experiments on databases of face images show that , for the same amount of storage , GPCA is superior to PCA in terms of quality of the compressed images , query precision , and computational cost .", "keywords": ["image compression", "singular value decomposition", "information search and retrieval", "principal component analysis", "tensor product", "dimension reduction", "vector space"], "combined": "GPCA : an efficient dimension reduction scheme for image compression and retrieval Recent years have witnessed a dramatic increase in the quantity of image data collected , due to advances in fields such as medical imaging , reconnaissance , surveillance , astronomy , multimedia etc. . With this increase has come the need to be able to store , transmit , and query large volumes of image data efficiently . A common operation on image databases is the retrieval of all images that are similar to a query image . For this , the images in the database are often represented as vectors in a high-dimensional space and a query is answered by retrieving all image vectors that are proximal to the query image in this space , under a suitable similarity metric . To overcome problems associated with high dimensionality , such as high storage and retrieval times , a dimension reduction step is usually applied to the vectors to concentrate relevant information in a small number of dimensions . Principal Component Analysis PCA is a well-known dimension reduction scheme . However , since it works with vectorized representations of images , PCA does not take into account the spatial locality of pixels in images . In this paper , a new dimension reduction scheme , called Generalized Principal Component Analysis GPCA , is presented . This scheme works directly with images in their native state , as two-dimensional matrices , by projecting the images to a vector space that is the tensor product of two lower-dimensional vector spaces . Experiments on databases of face images show that , for the same amount of storage , GPCA is superior to PCA in terms of quality of the compressed images , query precision , and computational cost . [[EENNDD]] image compression; singular value decomposition; information search and retrieval; principal component analysis; tensor product; dimension reduction; vector space"}, "GPCA: skema pengurangan dimensi yang cekap untuk pemampatan dan pengambilan gambar Beberapa tahun kebelakangan ini menyaksikan peningkatan dramatik dalam kuantiti data gambar yang dikumpulkan, kerana kemajuan dalam bidang seperti pengimejan perubatan, pengintaian, pengawasan, astronomi, multimedia dll. Dengan peningkatan ini, keperluan untuk dapat menyimpan, mengirim, dan menanyakan sejumlah besar data gambar dengan cekap. Operasi biasa pada pangkalan data gambar adalah pengambilan semua gambar yang serupa dengan gambar pertanyaan. Untuk ini, gambar dalam pangkalan data sering ditunjukkan sebagai vektor dalam ruang dimensi tinggi dan pertanyaan dijawab dengan mengambil semua vektor gambar yang berdekatan dengan gambar pertanyaan di ruang ini, di bawah metrik kesamaan yang sesuai. Untuk mengatasi masalah yang berkaitan dengan dimensi tinggi, seperti masa penyimpanan dan pengambilan yang tinggi, langkah pengurangan dimensi biasanya diterapkan pada vektor untuk memusatkan maklumat yang relevan dalam sebilangan kecil dimensi. Analisis Komponen Utama PCA adalah skema pengurangan dimensi yang terkenal. Walau bagaimanapun, kerana ia berfungsi dengan representasi gambar yang vektor, PCA tidak mengambil kira kedudukan ruang piksel dalam gambar. Dalam makalah ini, skema pengurangan dimensi baru, yang disebut Generalized Principal Component Analysis GPCA, disajikan. Skema ini berfungsi secara langsung dengan gambar dalam keadaan asalnya, sebagai matriks dua dimensi, dengan memproyeksikan gambar ke ruang vektor yang merupakan produk tensor dari dua ruang vektor dimensi bawah. Eksperimen pada pangkalan data gambar wajah menunjukkan bahawa, untuk jumlah simpanan yang sama, GPCA lebih unggul daripada PCA dari segi kualiti gambar yang dimampatkan, ketepatan pertanyaan, dan kos pengiraan. [[EENNDD]] pemampatan gambar; penguraian nilai tunggal; carian dan pengambilan maklumat; analisis komponen utama; produk tensor; pengurangan dimensi; ruang vektor"], [{"string": "An association analysis approach to biclustering The discovery of biclusters , which denote groups of items that show coherent values across a subset of all the transactions in a data set , is an important type of analysis performed on real-valued data sets in various domains , such as biology . Several algorithms have been proposed to find different types of biclusters in such data sets . However , these algorithms are unable to search the space of all possible biclusters exhaustively . Pattern mining algorithms in association analysis also essentially produce biclusters as their result , since the patterns consist of items that are supported by a subset of all the transactions . However , a major limitation of the numerous techniques developed in association analysis is that they are only able to analyze data sets with binary and\\/or categorical variables , and their application to real-valued data sets often involves some lossy transformation such as discretization or binarization of the attributes . In this paper , we propose a novel association analysis framework for exhaustively and efficiently mining `` range support '' patterns from such a data set . On one hand , this framework reduces the loss of information incurred by the binarization - and discretization-based approaches , and on the other , it enables the exhaustive discovery of coherent biclusters . We compared the performance of our framework with two standard biclustering algorithms through the evaluation of the similarity of the cellular functions of the genes constituting the patterns\\/biclusters derived by these algorithms from microarray data . These experiments show that the real-valued patterns discovered by our framework are better enriched by small biologically interesting functional classes . Also , through specific examples , we demonstrate the ability of the RAP framework to discover functionally enriched patterns that are not found by the commonly used biclustering algorithm ISA . The source code and data sets used in this paper , as well as the supplementary material , are available at http:\\/\\/www.cs.umn.edu\\/vk\\/gaurav\\/rap .", "keywords": ["functional modules", "microarray data", "real-valued data", "association analysis", "biclustering", "range support"], "combined": "An association analysis approach to biclustering The discovery of biclusters , which denote groups of items that show coherent values across a subset of all the transactions in a data set , is an important type of analysis performed on real-valued data sets in various domains , such as biology . Several algorithms have been proposed to find different types of biclusters in such data sets . However , these algorithms are unable to search the space of all possible biclusters exhaustively . Pattern mining algorithms in association analysis also essentially produce biclusters as their result , since the patterns consist of items that are supported by a subset of all the transactions . However , a major limitation of the numerous techniques developed in association analysis is that they are only able to analyze data sets with binary and\\/or categorical variables , and their application to real-valued data sets often involves some lossy transformation such as discretization or binarization of the attributes . In this paper , we propose a novel association analysis framework for exhaustively and efficiently mining `` range support '' patterns from such a data set . On one hand , this framework reduces the loss of information incurred by the binarization - and discretization-based approaches , and on the other , it enables the exhaustive discovery of coherent biclusters . We compared the performance of our framework with two standard biclustering algorithms through the evaluation of the similarity of the cellular functions of the genes constituting the patterns\\/biclusters derived by these algorithms from microarray data . These experiments show that the real-valued patterns discovered by our framework are better enriched by small biologically interesting functional classes . Also , through specific examples , we demonstrate the ability of the RAP framework to discover functionally enriched patterns that are not found by the commonly used biclustering algorithm ISA . The source code and data sets used in this paper , as well as the supplementary material , are available at http:\\/\\/www.cs.umn.edu\\/vk\\/gaurav\\/rap . [[EENNDD]] functional modules; microarray data; real-valued data; association analysis; biclustering; range support"}, "Pendekatan analisis persatuan untuk biclustering Penemuan biclusters, yang menunjukkan kumpulan item yang menunjukkan nilai koheren pada subset dari semua transaksi dalam satu set data, adalah jenis analisis penting yang dilakukan pada set data bernilai nyata dalam pelbagai domain, seperti sebagai biologi. Beberapa algoritma telah dicadangkan untuk mencari pelbagai jenis biclusters dalam set data tersebut. Walau bagaimanapun, algoritma ini tidak dapat mencari ruang semua kemungkinan biclusters secara menyeluruh. Algoritma perlombongan corak dalam analisis persatuan juga pada dasarnya menghasilkan bislusters sebagai hasilnya, kerana corak terdiri daripada item yang disokong oleh subset dari semua transaksi. Walau bagaimanapun, batasan utama dari banyak teknik yang dikembangkan dalam analisis pergaulan adalah bahawa mereka hanya dapat menganalisis set data dengan pemboleh ubah binari dan \\ / atau kategorik, dan penerapannya ke set data bernilai nyata sering kali melibatkan beberapa transformasi kerugian seperti diskretisasi atau binarizasi atribut. Dalam makalah ini, kami mengusulkan kerangka analisis persatuan baru untuk melengkapkan corak \"jangkauan sokongan\" secara menyeluruh dan cekap dari set data tersebut. Di satu pihak, kerangka ini mengurangkan kehilangan maklumat yang disebabkan oleh pendekatan binarizasi - dan berdasarkan diskritisasi, dan di sisi lain, ini memungkinkan penemuan menyeluruh mengenai biclusters yang koheren. Kami membandingkan prestasi kerangka kerja kami dengan dua algoritma biclustering standard melalui penilaian kesamaan fungsi sel dari gen yang membentuk corak \\ / biclusters yang dihasilkan oleh algoritma ini dari data microarray. Eksperimen ini menunjukkan bahawa corak bernilai sebenar yang ditemui oleh kerangka kerja kami lebih baik diperkaya oleh kelas fungsi kecil yang menarik secara biologi. Juga, melalui contoh khusus, kami menunjukkan kemampuan kerangka RAP untuk menemukan corak yang diperkaya secara fungsional yang tidak dijumpai oleh algoritma biclustering ISA yang biasa digunakan. Kod sumber dan set data yang digunakan dalam makalah ini, serta bahan tambahan, boleh didapati di http: \\ / \\ / www.cs.umn.edu \\ / vk \\ / gaurav \\ / rap. [[EENNDD]] modul berfungsi; data microarray; data bernilai sebenar; analisis persatuan; berbual-bual; sokongan pelbagai"], [{"string": "On the need for time series data mining benchmarks : a survey and empirical demonstration In the last decade there has been an explosion of interest in mining time series data . Literally hundreds of papers have introduced new algorithms to index , classify , cluster and segment time series . In this work we make the following claim . Much of this work has very little utility because the contribution made speed in the case of indexing , accuracy in the case of classification and clustering , model accuracy in the case of segmentation offer an amount of `` improvement '' that would have been completely dwarfed by the variance that would have been observed by testing on many real world datasets , or the variance that would have been observed by changing minor unstated implementation details . To illustrate our point , we have undertaken the most exhaustive set of time series experiments ever attempted , re-implementing the contribution of more than two dozen papers , and testing them on 50 real world , highly diverse datasets . Our empirical results strongly support our assertion , and suggest the need for a set of time series benchmarks and more careful empirical evaluation in the data mining community .", "keywords": ["time series", "experimental evaluation"], "combined": "On the need for time series data mining benchmarks : a survey and empirical demonstration In the last decade there has been an explosion of interest in mining time series data . Literally hundreds of papers have introduced new algorithms to index , classify , cluster and segment time series . In this work we make the following claim . Much of this work has very little utility because the contribution made speed in the case of indexing , accuracy in the case of classification and clustering , model accuracy in the case of segmentation offer an amount of `` improvement '' that would have been completely dwarfed by the variance that would have been observed by testing on many real world datasets , or the variance that would have been observed by changing minor unstated implementation details . To illustrate our point , we have undertaken the most exhaustive set of time series experiments ever attempted , re-implementing the contribution of more than two dozen papers , and testing them on 50 real world , highly diverse datasets . Our empirical results strongly support our assertion , and suggest the need for a set of time series benchmarks and more careful empirical evaluation in the data mining community . [[EENNDD]] time series; experimental evaluation"}, "Mengenai keperluan penanda aras perlombongan data siri masa: tinjauan dan demonstrasi empirikal Dalam dekad terakhir telah terjadi ledakan minat terhadap data siri masa perlombongan. Beratus-ratus kertas telah memperkenalkan algoritma baru untuk mengindeks, mengklasifikasikan, mengelompokkan dan segmen siri masa. Dalam karya ini kami membuat tuntutan berikut. Sebilangan besar karya ini memiliki utiliti yang sangat sedikit kerana kontribusi yang dibuat cepat dalam hal pengindeksan, ketepatan dalam hal klasifikasi dan pengelompokan, ketepatan model dalam hal segmentasi menawarkan sejumlah \"peningkatan\" yang akan benar-benar kerdil oleh varians yang akan diperhatikan dengan menguji pada banyak kumpulan data dunia nyata, atau varians yang akan diperhatikan dengan mengubah perincian pelaksanaan kecil yang tidak dinyatakan. Untuk menggambarkan maksud kami, kami telah melakukan satu set eksperimen siri masa yang paling lengkap yang pernah dicuba, melaksanakan kembali sumbangan lebih dari dua dozen makalah, dan mengujinya di 50 kumpulan data dunia nyata, yang sangat pelbagai. Hasil empirikal kami sangat menyokong pernyataan kami, dan menunjukkan perlunya satu set penanda aras siri masa dan penilaian empirik yang lebih teliti dalam komuniti perlombongan data. [[EENNDD]] siri masa; penilaian eksperimen"], [{"string": "Web usage mining based on probabilistic latent semantic analysis The primary goal of Web usage mining is the discovery of patterns in the navigational behavior of Web users . Standard approaches , such as clustering of user sessions and discovering association rules or frequent navigational paths , do not generally provide the ability to automatically characterize or quantify the unobservable factors that lead to common navigational patterns . It is , therefore , necessary to develop techniques that can automatically discover hidden semantic relationships among users as well as between users and Web objects . Probabilistic Latent Semantic Analysis PLSA is particularly useful in this context , since it can uncover latent semantic associations among users and pages based on the co-occurrence patterns of these pages in user sessions . In this paper , we develop a unified framework for the discovery and analysis of Web navigational patterns based on PLSA . We show the flexibility of this framework in characterizing various relationships among users and Web objects . Since these relationships are measured in terms of probabilities , we are able to use probabilistic inference to perform a variety of analysis tasks such as user segmentation , page classification , as well as predictive tasks such as collaborative recommendations . We demonstrate the effectiveness of our approach through experiments performed on real-world data sets .", "keywords": ["plsa", "web usage mining", "user profiling"], "combined": "Web usage mining based on probabilistic latent semantic analysis The primary goal of Web usage mining is the discovery of patterns in the navigational behavior of Web users . Standard approaches , such as clustering of user sessions and discovering association rules or frequent navigational paths , do not generally provide the ability to automatically characterize or quantify the unobservable factors that lead to common navigational patterns . It is , therefore , necessary to develop techniques that can automatically discover hidden semantic relationships among users as well as between users and Web objects . Probabilistic Latent Semantic Analysis PLSA is particularly useful in this context , since it can uncover latent semantic associations among users and pages based on the co-occurrence patterns of these pages in user sessions . In this paper , we develop a unified framework for the discovery and analysis of Web navigational patterns based on PLSA . We show the flexibility of this framework in characterizing various relationships among users and Web objects . Since these relationships are measured in terms of probabilities , we are able to use probabilistic inference to perform a variety of analysis tasks such as user segmentation , page classification , as well as predictive tasks such as collaborative recommendations . We demonstrate the effectiveness of our approach through experiments performed on real-world data sets . [[EENNDD]] plsa; web usage mining; user profiling"}, "Perlombongan penggunaan web berdasarkan analisis semantik probabilistik Matlamat utama perlombongan penggunaan Web adalah penemuan corak dalam tingkah laku navigasi pengguna Web. Pendekatan standard, seperti pengelompokan sesi pengguna dan penemuan peraturan pergaulan atau jalan navigasi yang kerap, secara amnya tidak memberikan kemampuan untuk secara automatik mencirikan atau mengukur faktor-faktor yang tidak dapat diperhatikan yang membawa kepada pola navigasi umum. Oleh itu, adalah perlu untuk mengembangkan teknik yang dapat secara automatik menemui hubungan semantik tersembunyi di antara pengguna dan juga antara pengguna dan objek Web. Probabilistic Latent Semantic Analysis PLSA sangat berguna dalam konteks ini, kerana ia dapat mengungkap hubungan semantik laten di antara pengguna dan halaman berdasarkan corak kejadian halaman ini dalam sesi pengguna. Dalam makalah ini, kami mengembangkan kerangka kerja terpadu untuk penemuan dan analisis pola navigasi Web berdasarkan PLSA. Kami menunjukkan fleksibiliti kerangka ini dalam mencirikan pelbagai hubungan antara pengguna dan objek Web. Oleh kerana hubungan ini diukur dari segi kebarangkalian, kita dapat menggunakan inferensi probabilistik untuk melakukan berbagai tugas analisis seperti segmentasi pengguna, klasifikasi halaman, dan juga tugas-tugas ramalan seperti cadangan kolaboratif. Kami menunjukkan keberkesanan pendekatan kami melalui eksperimen yang dilakukan pada set data dunia nyata. [[EENNDD]] plsa; perlombongan penggunaan web; profil pengguna"], [{"string": "Knowledge transfer via multiple model local structure mapping The effectiveness of knowledge transfer using classification algorithms depends on the difference between the distribution that generates the training examples and the one from which test examples are to be drawn . The task can be especially difficult when the training examples are from one or several domains different from the test domain . In this paper , we propose a locally weighted ensemble framework to combine multiple models for transfer learning , where the weights are dynamically assigned according to a model 's predictive power on each test example . It can integrate the advantages of various learning algorithms and the labeled information from multiple training domains into one unified classification model , which can then be applied on a different domain . Importantly , different from many previously proposed methods , none of the base learning method is required to be specifically designed for transfer learning . We show the optimality of a locally weighted ensemble framework as a general approach to combine multiple models for domain transfer . We then propose an implementation of the local weight assignments by mapping the structures of a model onto the structures of the test domain , and then weighting each model locally according to its consistency with the neighborhood structure around the test example . Experimental results on text classification , spam filtering and intrusion detection data sets demonstrate significant improvements in classification accuracy gained by the framework . On a transfer learning task of newsgroup message categorization , the proposed locally weighted ensemble framework achieves 97 % accuracy when the best single model predicts correctly only on 73 % of the test examples . In summary , the improvement in accuracy is over 10 % and up to 30 % across different problems .", "keywords": ["transfer learning", "ensemble", "classification", "semi-supervised learning"], "combined": "Knowledge transfer via multiple model local structure mapping The effectiveness of knowledge transfer using classification algorithms depends on the difference between the distribution that generates the training examples and the one from which test examples are to be drawn . The task can be especially difficult when the training examples are from one or several domains different from the test domain . In this paper , we propose a locally weighted ensemble framework to combine multiple models for transfer learning , where the weights are dynamically assigned according to a model 's predictive power on each test example . It can integrate the advantages of various learning algorithms and the labeled information from multiple training domains into one unified classification model , which can then be applied on a different domain . Importantly , different from many previously proposed methods , none of the base learning method is required to be specifically designed for transfer learning . We show the optimality of a locally weighted ensemble framework as a general approach to combine multiple models for domain transfer . We then propose an implementation of the local weight assignments by mapping the structures of a model onto the structures of the test domain , and then weighting each model locally according to its consistency with the neighborhood structure around the test example . Experimental results on text classification , spam filtering and intrusion detection data sets demonstrate significant improvements in classification accuracy gained by the framework . On a transfer learning task of newsgroup message categorization , the proposed locally weighted ensemble framework achieves 97 % accuracy when the best single model predicts correctly only on 73 % of the test examples . In summary , the improvement in accuracy is over 10 % and up to 30 % across different problems . [[EENNDD]] transfer learning; ensemble; classification; semi-supervised learning"}, "Pemindahan pengetahuan melalui pemetaan struktur tempatan pelbagai model Keberkesanan pemindahan pengetahuan menggunakan algoritma klasifikasi bergantung pada perbezaan antara taburan yang menghasilkan contoh latihan dan yang dari mana contoh ujian akan diambil. Tugas boleh menjadi sangat sukar apabila contoh latihan adalah dari satu atau beberapa domain yang berbeza dari domain ujian. Dalam makalah ini, kami mencadangkan kerangka ensemble berwajaran tempatan untuk menggabungkan beberapa model untuk pembelajaran pemindahan, di mana bobot ditugaskan secara dinamis sesuai dengan kekuatan ramalan model pada setiap contoh ujian. Ia dapat mengintegrasikan kelebihan pelbagai algoritma pembelajaran dan maklumat berlabel dari beberapa domain latihan menjadi satu model klasifikasi terpadu, yang kemudian dapat diterapkan pada domain yang berbeda. Yang penting, berbeza dari banyak kaedah yang dicadangkan sebelumnya, tidak ada kaedah asas pembelajaran yang diperlukan untuk dirancang secara khusus untuk pemindahan pembelajaran. Kami menunjukkan optimum kerangka ensemble berwajaran tempatan sebagai pendekatan umum untuk menggabungkan beberapa model untuk pemindahan domain. Kami kemudian mengusulkan pelaksanaan penugasan berat lokal dengan memetakan struktur model ke struktur domain pengujian, dan kemudian menimbang setiap model secara lokal sesuai dengan konsistensinya dengan struktur lingkungan sekitar contoh ujian. Hasil eksperimen pada klasifikasi teks, penyaringan spam dan set data pengesanan pencerobohan menunjukkan peningkatan yang signifikan dalam ketepatan klasifikasi yang diperoleh oleh kerangka kerja. Pada tugas pembelajaran pemindahan pengkategorian mesej kumpulan berita, kerangka ensembel berwajaran tempatan yang dicadangkan mencapai ketepatan 97% apabila model tunggal terbaik meramalkan dengan tepat hanya pada 73% contoh ujian. Ringkasnya, peningkatan ketepatan melebihi 10% dan hingga 30% untuk pelbagai masalah. [[EENNDD]] memindahkan pembelajaran; ensembel; pengelasan; pembelajaran separa penyeliaan"], [{"string": "A framework for simultaneous co-clustering and learning from complex data For difficult classification or regression problems , practitioners often segment the data into relatively homogenous groups and then build a model for each group . This two-step procedure usually results in simpler , more interpretable and actionable models without any lossin accuracy . We consider problems such as predicting customer behavior across products , where the independent variables can be naturally partitioned into two groups . A pivoting operation can now result in the dependent variable showing up as entries in a `` customer by product '' data matrix . We present a model-based co-clustering meta - algorithm that interleaves clustering and construction of prediction models to iteratively improve both cluster assignment and fit of the models . This algorithm provably converges to a local minimum of a suitable cost function . The framework not only generalizes co-clustering and collaborative filtering to model-basedco-clustering , but can also be viewed as simultaneous co-segmentation and classification or regression , which is better than independently clustering the data first and then building models . Moreover , it applies to a wide range of bi-modal or multimodal data , and can be easily specialized to address classification and regression problems . We demonstrate the effectiveness of our approach on both these problems through experimentation on real and synthetic data .", "keywords": ["co-clustering", "classification", "multimodal data", "regression", "prediction models"], "combined": "A framework for simultaneous co-clustering and learning from complex data For difficult classification or regression problems , practitioners often segment the data into relatively homogenous groups and then build a model for each group . This two-step procedure usually results in simpler , more interpretable and actionable models without any lossin accuracy . We consider problems such as predicting customer behavior across products , where the independent variables can be naturally partitioned into two groups . A pivoting operation can now result in the dependent variable showing up as entries in a `` customer by product '' data matrix . We present a model-based co-clustering meta - algorithm that interleaves clustering and construction of prediction models to iteratively improve both cluster assignment and fit of the models . This algorithm provably converges to a local minimum of a suitable cost function . The framework not only generalizes co-clustering and collaborative filtering to model-basedco-clustering , but can also be viewed as simultaneous co-segmentation and classification or regression , which is better than independently clustering the data first and then building models . Moreover , it applies to a wide range of bi-modal or multimodal data , and can be easily specialized to address classification and regression problems . We demonstrate the effectiveness of our approach on both these problems through experimentation on real and synthetic data . [[EENNDD]] co-clustering; classification; multimodal data; regression; prediction models"}, "Kerangka kerja untuk pengelompokan bersama dan pembelajaran dari data yang kompleks Untuk masalah klasifikasi atau regresi yang sukar, pengamal sering membahagikan data ke dalam kumpulan yang relatif homogen dan kemudian membina model untuk setiap kumpulan. Prosedur dua langkah ini biasanya menghasilkan model yang lebih mudah, lebih mudah ditafsirkan dan dapat ditindaklanjuti tanpa ketepatan yang hilang. Kami mempertimbangkan masalah seperti meramalkan tingkah laku pelanggan di seluruh produk, di mana pemboleh ubah bebas secara semula jadi boleh dibahagikan kepada dua kumpulan. Operasi berpusing kini dapat menghasilkan pemboleh ubah bersandar yang muncul sebagai entri dalam matriks data \"pelanggan oleh produk\". Kami menyajikan meta - algoritma co-clustering berasaskan model yang menggabungkan clustering dan pembinaan model ramalan untuk memperbaiki secara berulang kedua tugas cluster dan kesesuaian model. Algoritma ini terbukti dapat disatukan ke minimum tempatan dengan fungsi kos yang sesuai. Kerangka ini tidak hanya menggeneralisasi penyatuan bersama dan penyaringan kolaboratif untuk pengelompokan berbasis model, tetapi juga dapat dilihat sebagai pembagian bersama dan klasifikasi atau regresi, yang lebih baik daripada pengelompokan data secara pertama dan kemudian membangun model. Lebih-lebih lagi, ini berlaku untuk berbagai data bi-modal atau multimodal, dan dapat dikhususkan dengan mudah untuk mengatasi masalah klasifikasi dan regresi. Kami menunjukkan keberkesanan pendekatan kami terhadap kedua-dua masalah ini melalui percubaan pada data sebenar dan sintetik. [[EENNDD]] penggabungan bersama; pengelasan; data multimodal; regresi; model ramalan"], [{"string": "Unweaving a web of documents We develop an algorithmic framework to decompose a collection of time-stamped text documents into semantically coherent threads . Our formulation leads to a graph decomposition problem on directed acyclic graphs , for which we obtain three algorithms -- an exact algorithm that is based on minimum cost flow and two more efficient algorithms based on maximum matching and dynamic programming that solve specific versions of the graph decomposition problem . Applications of our algorithms include superior summarization of news search results , improved browsing paradigms for large collections of text-intensive corpora , and integration of time-stamped documents from a variety of sources . Experimental results based on over 250,000 news articles from a major newspaper over a period of four years demonstrate that our algorithms efficiently identify robust threads of varying lengths and time-spans .", "keywords": ["information search and retrieval", "news threads", "graph decomposition", "graph algorithms"], "combined": "Unweaving a web of documents We develop an algorithmic framework to decompose a collection of time-stamped text documents into semantically coherent threads . Our formulation leads to a graph decomposition problem on directed acyclic graphs , for which we obtain three algorithms -- an exact algorithm that is based on minimum cost flow and two more efficient algorithms based on maximum matching and dynamic programming that solve specific versions of the graph decomposition problem . Applications of our algorithms include superior summarization of news search results , improved browsing paradigms for large collections of text-intensive corpora , and integration of time-stamped documents from a variety of sources . Experimental results based on over 250,000 news articles from a major newspaper over a period of four years demonstrate that our algorithms efficiently identify robust threads of varying lengths and time-spans . [[EENNDD]] information search and retrieval; news threads; graph decomposition; graph algorithms"}, "Membongkar web dokumen Kami mengembangkan kerangka algoritma untuk menguraikan koleksi dokumen teks yang dicap waktu menjadi utas yang semantik koheren. Perumusan kami membawa kepada masalah penguraian grafik pada grafik asiklik yang diarahkan, di mana kami memperoleh tiga algoritma - algoritma tepat yang berdasarkan aliran kos minimum dan dua algoritma yang lebih cekap berdasarkan padanan maksimum dan pengaturcaraan dinamik yang menyelesaikan versi grafik tertentu masalah penguraian. Aplikasi algoritma kami merangkumi ringkasan hasil carian berita yang unggul, paradigma penyemakan imbas yang lebih baik untuk koleksi besar korporat berintensifkan teks, dan penyatuan dokumen yang dicap waktu dari pelbagai sumber. Hasil eksperimen berdasarkan lebih dari 250,000 artikel berita dari sebuah akhbar utama dalam jangka masa empat tahun menunjukkan bahawa algoritma kami dengan berkesan mengenal pasti utas yang kuat dengan pelbagai panjang dan jangka masa. [[EENNDD]] carian dan pengambilan maklumat; utas berita; penguraian grafik; algoritma graf"], [{"string": "A quickstart in frequent structure mining can make a difference Given a database , structure mining algorithms search for substructures that satisfy constraints such as minimum frequency , minimum confidence , minimum interest and maximum frequency . Examples of substructures include graphs , trees and paths . For these substructures many mining algorithms have been proposed . In order to make graph mining more efficient , we investigate the use of the `` quickstart principle '' , which is based on the fact that these classes of structures are contained in each other , thus allowing for the development of structure mining algorithms that split the search into steps of increasing complexity . We introduce the GrAph\\/Sequence\\/Tree extractiON Gaston algorithm that implements this idea by searching first for frequent paths , then frequent free trees and finally cyclic graphs . We investigate two alternatives for computing the frequency of structures and present experimental results to relate these alternatives .", "keywords": ["semi-structures", "frequent item sets", "graphs", "structures"], "combined": "A quickstart in frequent structure mining can make a difference Given a database , structure mining algorithms search for substructures that satisfy constraints such as minimum frequency , minimum confidence , minimum interest and maximum frequency . Examples of substructures include graphs , trees and paths . For these substructures many mining algorithms have been proposed . In order to make graph mining more efficient , we investigate the use of the `` quickstart principle '' , which is based on the fact that these classes of structures are contained in each other , thus allowing for the development of structure mining algorithms that split the search into steps of increasing complexity . We introduce the GrAph\\/Sequence\\/Tree extractiON Gaston algorithm that implements this idea by searching first for frequent paths , then frequent free trees and finally cyclic graphs . We investigate two alternatives for computing the frequency of structures and present experimental results to relate these alternatives . [[EENNDD]] semi-structures; frequent item sets; graphs; structures"}, "Permulaan yang cepat dalam penambangan struktur yang kerap dapat membuat perbezaan Mengingat pangkalan data, algoritma perlombongan struktur mencari substruktur yang memenuhi batasan seperti frekuensi minimum, keyakinan minimum, minat minimum dan frekuensi maksimum. Contoh substruktur merangkumi grafik, pokok dan laluan. Untuk substruktur ini banyak algoritma perlombongan telah dicadangkan. Untuk menjadikan perlombongan grafik lebih cekap, kami menyiasat penggunaan \"prinsip permulaan cepat\", yang berdasarkan pada fakta bahawa kelas struktur ini terkandung satu sama lain, sehingga memungkinkan pengembangan algoritma perlombongan struktur yang berpecah pencarian menjadi langkah-langkah peningkatan kerumitan. Kami memperkenalkan algoritma GrAph \\ / Sequence \\ / Tree extractiON Gaston yang menerapkan idea ini dengan mencari terlebih dahulu jalan yang kerap, kemudian pokok bebas yang kerap dan akhirnya graf siklik. Kami menyiasat dua alternatif untuk mengira kekerapan struktur dan membentangkan hasil eksperimen untuk mengaitkan alternatif ini. [[EENNDD]] struktur separa; set barang yang kerap; grafik; struktur"], [{"string": "Frequent term-based text clustering Text clustering methods can be used to structure large sets of text or hypertext documents . The well-known methods of text clustering , however , do not really address the special problems of text clustering : very high dimensionality of the data , very large size of the databases and understandability of the cluster description . In this paper , we introduce a novel approach which uses frequent item term sets for text clustering . Such frequent sets can be efficiently discovered using algorithms for association rule mining . To cluster based on frequent term sets , we measure the mutual overlap of frequent sets with respect to the sets of supporting documents . We present two algorithms for frequent term-based text clustering , FTC which creates flat clusterings and HFTC for hierarchical clustering . An experimental evaluation on classical text documents as well as on web documents demonstrates that the proposed algorithms obtain clusterings of comparable quality significantly more efficiently than state-of-the - art text clustering algorithms . Furthermore , our methods provide an understandable description of the discovered clusters by their frequent term sets .", "keywords": ["frequent item sets", "text documents"], "combined": "Frequent term-based text clustering Text clustering methods can be used to structure large sets of text or hypertext documents . The well-known methods of text clustering , however , do not really address the special problems of text clustering : very high dimensionality of the data , very large size of the databases and understandability of the cluster description . In this paper , we introduce a novel approach which uses frequent item term sets for text clustering . Such frequent sets can be efficiently discovered using algorithms for association rule mining . To cluster based on frequent term sets , we measure the mutual overlap of frequent sets with respect to the sets of supporting documents . We present two algorithms for frequent term-based text clustering , FTC which creates flat clusterings and HFTC for hierarchical clustering . An experimental evaluation on classical text documents as well as on web documents demonstrates that the proposed algorithms obtain clusterings of comparable quality significantly more efficiently than state-of-the - art text clustering algorithms . Furthermore , our methods provide an understandable description of the discovered clusters by their frequent term sets . [[EENNDD]] frequent item sets; text documents"}, "Penggabungan teks berdasarkan istilah yang kerap Kaedah pengelompokan teks dapat digunakan untuk menyusun set teks atau teks hiperteks yang besar. Kaedah pengelompokan teks yang terkenal, bagaimanapun, tidak benar-benar mengatasi masalah khas pengelompokan teks: dimensi data yang sangat tinggi, ukuran pangkalan data yang sangat besar dan kefahaman mengenai deskripsi kluster. Dalam makalah ini, kami memperkenalkan pendekatan baru yang menggunakan set istilah item yang kerap untuk pengelompokan teks. Set kerap seperti itu dapat dijumpai dengan berkesan menggunakan algoritma untuk perlombongan peraturan persatuan. Untuk kluster berdasarkan set jangka kerap, kami mengukur pertindihan timbal balik bersama dengan set dokumen sokongan. Kami menyajikan dua algoritma untuk pengelompokan teks berdasarkan istilah yang kerap, FTC yang membuat pengelompokan rata dan HFTC untuk pengelompokan hierarki. Penilaian eksperimen terhadap dokumen teks klasik dan juga dokumen web menunjukkan bahawa algoritma yang dicadangkan memperoleh pengelompokan kualiti setanding dengan lebih berkesan daripada algoritma pengelompokan teks canggih. Selanjutnya, kaedah kami memberikan penerangan yang dapat difahami mengenai kelompok yang ditemui dengan kumpulan istilah mereka yang kerap. [[EENNDD]] set item yang kerap; dokumen teks"], [{"string": "Discrimination-aware data mining In the context of civil rights law , discrimination refers to unfair or unequal treatment of people based on membership to a category or a minority , without regard to individual merit . Rules extracted from databases by data mining techniques , such as classification or association rules , when used for decision tasks such as benefit or credit approval , can be discriminatory in the above sense . In this paper , the notion of discriminatory classification rules is introduced and studied . Providing a guarantee of non-discrimination is shown to be a non trivial task . A naive approach , like taking away all discriminatory attributes , is shown to be not enough when other background knowledge is available . Our approach leads to a precise formulation of the redlining problem along with a formal result relating discriminatory rules with apparently safe ones by means of background knowledge . An empirical assessment of the results on the German credit dataset is also provided .", "keywords": ["classification rules", "discrimination"], "combined": "Discrimination-aware data mining In the context of civil rights law , discrimination refers to unfair or unequal treatment of people based on membership to a category or a minority , without regard to individual merit . Rules extracted from databases by data mining techniques , such as classification or association rules , when used for decision tasks such as benefit or credit approval , can be discriminatory in the above sense . In this paper , the notion of discriminatory classification rules is introduced and studied . Providing a guarantee of non-discrimination is shown to be a non trivial task . A naive approach , like taking away all discriminatory attributes , is shown to be not enough when other background knowledge is available . Our approach leads to a precise formulation of the redlining problem along with a formal result relating discriminatory rules with apparently safe ones by means of background knowledge . An empirical assessment of the results on the German credit dataset is also provided . [[EENNDD]] classification rules; discrimination"}, "Perlombongan data yang menyedari diskriminasi Dalam konteks undang-undang hak sivil, diskriminasi merujuk kepada perlakuan tidak adil atau tidak adil terhadap orang berdasarkan keanggotaan ke dalam kategori atau minoriti, tanpa memperhatikan prestasi individu. Peraturan yang diekstrak dari pangkalan data dengan teknik penambangan data, seperti klasifikasi atau peraturan persatuan, ketika digunakan untuk tugas keputusan seperti persetujuan manfaat atau kredit, dapat diskriminatif dalam pengertian di atas. Dalam makalah ini, konsep peraturan klasifikasi diskriminatif diperkenalkan dan dikaji. Memberi jaminan untuk tidak melakukan diskriminasi ditunjukkan sebagai tugas yang tidak remeh. Pendekatan naif, seperti menghilangkan semua sifat diskriminasi, terbukti tidak cukup ketika pengetahuan latar belakang lain tersedia. Pendekatan kami membawa kepada perumusan yang tepat mengenai masalah redlining bersama dengan hasil formal yang menghubungkan peraturan diskriminasi dengan yang nampaknya selamat dengan pengetahuan latar belakang. Penilaian empirik hasil pada set data kredit Jerman juga diberikan. [[EENNDD]] peraturan pengelasan; diskriminasi"], [{"string": "Suggesting friends using the implicit social graph Although users of online communication tools rarely categorize their contacts into groups such as `` family '' , `` co-workers '' , or `` jogging buddies '' , they nonetheless implicitly cluster contacts , by virtue of their interactions with them , forming implicit groups . In this paper , we describe the implicit social graph which is formed by users ' interactions with contacts and groups of contacts , and which is distinct from explicit social graphs in which users explicitly add other individuals as their `` friends '' . We introduce an interaction-based metric for estimating a user 's affinity to his contacts and groups . We then describe a novel friend suggestion algorithm that uses a user 's implicit social graph to generate a friend group , given a small seed set of contacts which the user has already labeled as friends . We show experimental results that demonstrate the importance of both implicit group relationships and interaction-based affinity ranking in suggesting friends . Finally , we discuss two applications of the Friend Suggest algorithm that have been released as Gmail Labs features .", "keywords": ["contact group clustering", "group and organization interfaces", "implicit social graph", "tie strength", "clustering"], "combined": "Suggesting friends using the implicit social graph Although users of online communication tools rarely categorize their contacts into groups such as `` family '' , `` co-workers '' , or `` jogging buddies '' , they nonetheless implicitly cluster contacts , by virtue of their interactions with them , forming implicit groups . In this paper , we describe the implicit social graph which is formed by users ' interactions with contacts and groups of contacts , and which is distinct from explicit social graphs in which users explicitly add other individuals as their `` friends '' . We introduce an interaction-based metric for estimating a user 's affinity to his contacts and groups . We then describe a novel friend suggestion algorithm that uses a user 's implicit social graph to generate a friend group , given a small seed set of contacts which the user has already labeled as friends . We show experimental results that demonstrate the importance of both implicit group relationships and interaction-based affinity ranking in suggesting friends . Finally , we discuss two applications of the Friend Suggest algorithm that have been released as Gmail Labs features . [[EENNDD]] contact group clustering; group and organization interfaces; implicit social graph; tie strength; clustering"}, "Mencadangkan rakan menggunakan grafik sosial yang tersirat Walaupun pengguna alat komunikasi dalam talian jarang mengkategorikan kenalan mereka ke dalam kumpulan seperti \"keluarga\", \"rakan sekerja\", atau \"teman joging\", mereka secara tidak langsung menyatukan kenalan, oleh kebaikan interaksi mereka dengan mereka, membentuk kumpulan tersirat. Dalam makalah ini, kami menerangkan grafik sosial tersirat yang dibentuk oleh interaksi pengguna dengan kenalan dan kumpulan kenalan, dan yang berbeza dari grafik sosial yang eksplisit di mana pengguna secara eksplisit menambahkan individu lain sebagai \"rakan\" mereka. Kami memperkenalkan metrik berasaskan interaksi untuk mengira hubungan pengguna dengan kenalan dan kumpulannya. Kami kemudian menerangkan algoritma cadangan rakan baru yang menggunakan grafik sosial tersirat pengguna untuk menghasilkan kumpulan rakan, dengan sekumpulan kenalan kecil yang pengguna telah labelkan sebagai rakan. Kami menunjukkan hasil eksperimen yang menunjukkan pentingnya hubungan kumpulan tersirat dan peringkat hubungan berdasarkan interaksi dalam memberi cadangan kepada rakan. Akhirnya, kami membincangkan dua aplikasi algoritma Friend Suggest yang telah dikeluarkan sebagai ciri Gmail Labs. [[EENNDD]] kumpulan kumpulan kenalan; antara muka kumpulan dan organisasi; grafik sosial yang tersirat; kekuatan tali leher; pengelompokan"], [{"string": "Partial example acquisition in cost-sensitive learning It is often expensive to acquire data in real-world data mining applications . Most previous data mining and machine learning research , however , assumes that a fixed set of training examples is given . In this paper , we propose an online cost-sensitive framework that allows a learner to dynamically acquire examples as it learns , and to decide the ideal number of examples needed to minimize the total cost . We also propose a new strategy for Partial Example Acquisition PAS , in which the learner can acquire examples with a subset of attribute values to reduce the data acquisition cost . Experiments on UCI datasets show that the new PAS strategy is an effective method in reducing the total cost for data acquisition .", "keywords": ["cost-sensitive learning", "data mining", "data acquisition", "machine learning", "active learning", "active cost-sensitive learning", "interactive and online data mining"], "combined": "Partial example acquisition in cost-sensitive learning It is often expensive to acquire data in real-world data mining applications . Most previous data mining and machine learning research , however , assumes that a fixed set of training examples is given . In this paper , we propose an online cost-sensitive framework that allows a learner to dynamically acquire examples as it learns , and to decide the ideal number of examples needed to minimize the total cost . We also propose a new strategy for Partial Example Acquisition PAS , in which the learner can acquire examples with a subset of attribute values to reduce the data acquisition cost . Experiments on UCI datasets show that the new PAS strategy is an effective method in reducing the total cost for data acquisition . [[EENNDD]] cost-sensitive learning; data mining; data acquisition; machine learning; active learning; active cost-sensitive learning; interactive and online data mining"}, "Sebilangan contoh pemerolehan dalam pembelajaran sensitif kos Selalunya mahal untuk memperoleh data dalam aplikasi perlombongan data dunia nyata. Sebilangan besar penyelidikan perlombongan data dan pembelajaran mesin sebelumnya, bagaimanapun, menganggap bahawa satu set contoh latihan yang tetap diberikan. Dalam makalah ini, kami mencadangkan rangka kerja sensitif kos dalam talian yang membolehkan pelajar memperoleh contoh secara dinamis semasa belajar, dan memutuskan jumlah contoh ideal yang diperlukan untuk meminimumkan jumlah kos. Kami juga mengusulkan strategi baru untuk Partial Contoh Acquisition PAS, di mana pelajar dapat memperoleh contoh dengan subset nilai atribut untuk mengurangkan kos pemerolehan data. Eksperimen pada dataset UCI menunjukkan bahawa strategi PAS baru adalah kaedah yang berkesan dalam mengurangkan jumlah kos pemerolehan data. [[EENNDD]] pembelajaran sensitif kos; perlombongan data; perolehan data; pembelajaran mesin; pembelajaran aktif; pembelajaran sensitif kos aktif; perlombongan data interaktif dan dalam talian"], [{"string": "Outlier detection by sampling with accuracy guarantees An effective approach to detecting anomalous points in a data set is distance-based outlier detection . This paper describes a simple sampling algorithm to effciently detect distance-based outliers in domains where each and every distance computation is very expensive . Unlike any existing algorithms , the sampling algorithm requires a xed number of distance computations and can return good results with accuracy guarantees . The most computationally expensive aspect of estimating the accuracy of the result is sorting all of the distances computed by the sampling algorithm . The experimental study on two expensive domains as well as ten additional real-life datasets demonstrates both the effciency and effectiveness of the sampling algorithm in comparison with the state-of-the-art algorithm and there liability of the accuracy guarantees .", "keywords": ["ensemble method", "active learning", "outlier detection"], "combined": "Outlier detection by sampling with accuracy guarantees An effective approach to detecting anomalous points in a data set is distance-based outlier detection . This paper describes a simple sampling algorithm to effciently detect distance-based outliers in domains where each and every distance computation is very expensive . Unlike any existing algorithms , the sampling algorithm requires a xed number of distance computations and can return good results with accuracy guarantees . The most computationally expensive aspect of estimating the accuracy of the result is sorting all of the distances computed by the sampling algorithm . The experimental study on two expensive domains as well as ten additional real-life datasets demonstrates both the effciency and effectiveness of the sampling algorithm in comparison with the state-of-the-art algorithm and there liability of the accuracy guarantees . [[EENNDD]] ensemble method; active learning; outlier detection"}, "Pengesanan luar dengan pensampelan dengan jaminan ketepatan Pendekatan yang berkesan untuk mengesan titik anomali dalam kumpulan data adalah pengesanan outlier berdasarkan jarak. Makalah ini menerangkan algoritma pensampelan mudah untuk mengesan outliers jarak jauh dengan berkesan dalam domain di mana setiap pengiraan jarak sangat mahal. Tidak seperti algoritma yang ada, algoritma pensampelan memerlukan pengiraan jarak xed dan dapat memberikan hasil yang baik dengan jaminan ketepatan. Aspek yang paling mahal untuk mengira ketepatan hasil adalah menyusun semua jarak yang dikira oleh algoritma pensampelan. Kajian eksperimental pada dua domain mahal dan juga sepuluh kumpulan data kehidupan nyata menunjukkan kecekapan dan keberkesanan algoritma pensampelan berbanding dengan algoritma canggih dan terdapat tanggungjawab jaminan ketepatan. [[EENNDD]] kaedah ensemble; pembelajaran aktif; pengesanan luar"], [{"string": "Pattern discovery in sequences under a Markov assumption In this paper we investigate the general problem of discovering recurrent patterns that are embedded in categorical sequences . An important real-world problem of this nature is motif discovery in DNA sequences . We investigate the fundamental aspects of this data mining problem that can make discovery `` easy '' or `` hard . '' We present a general framework for characterizing learning in this context by deriving the Bayes error rate for this problem under a Markov assumption . The Bayes error framework demonstrates why certain patterns are much harder to discover than others . It also explains the role of different parameters such as pattern length and pattern frequency in sequential discovery . We demonstrate how the Bayes error can be used to calibrate existing discovery algorithms , providing a lower bound on achievable performance . We discuss a number of fundamental issues that characterize sequential pattern discovery in this context , present a variety of empirical results to complement and verify the theoretical analysis , and apply our methodology to real-world motif-discovery problems in computational biology .", "keywords": ["probabilistic algorithms"], "combined": "Pattern discovery in sequences under a Markov assumption In this paper we investigate the general problem of discovering recurrent patterns that are embedded in categorical sequences . An important real-world problem of this nature is motif discovery in DNA sequences . We investigate the fundamental aspects of this data mining problem that can make discovery `` easy '' or `` hard . '' We present a general framework for characterizing learning in this context by deriving the Bayes error rate for this problem under a Markov assumption . The Bayes error framework demonstrates why certain patterns are much harder to discover than others . It also explains the role of different parameters such as pattern length and pattern frequency in sequential discovery . We demonstrate how the Bayes error can be used to calibrate existing discovery algorithms , providing a lower bound on achievable performance . We discuss a number of fundamental issues that characterize sequential pattern discovery in this context , present a variety of empirical results to complement and verify the theoretical analysis , and apply our methodology to real-world motif-discovery problems in computational biology . [[EENNDD]] probabilistic algorithms"}, "Penemuan corak dalam urutan di bawah anggapan Markov Dalam makalah ini kami menyelidiki masalah umum untuk menemui corak berulang yang tertanam dalam urutan kategori. Masalah dunia nyata yang penting seperti ini adalah penemuan motif dalam urutan DNA. Kami menyiasat aspek asas masalah perlombongan data ini yang dapat menjadikan penemuan \"mudah\" atau \"sukar. '' Kami menyajikan kerangka umum untuk mencirikan pembelajaran dalam konteks ini dengan memperoleh kadar kesalahan Bayes untuk masalah ini berdasarkan anggapan Markov. Kerangka ralat Bayes menunjukkan mengapa corak tertentu jauh lebih sukar dijumpai daripada yang lain. Ia juga menjelaskan peranan parameter yang berbeza seperti panjang corak dan frekuensi corak dalam penemuan berurutan. Kami menunjukkan bagaimana kesalahan Bayes dapat digunakan untuk mengkalibrasi algoritma penemuan yang ada, memberikan batas yang lebih rendah pada prestasi yang dapat dicapai. Kami membincangkan sejumlah masalah mendasar yang mencirikan penemuan pola berurutan dalam konteks ini, menyajikan pelbagai hasil empirik untuk melengkapkan dan mengesahkan analisis teoritis, dan menerapkan metodologi kami untuk masalah penemuan motif dunia nyata dalam biologi komputasi. [[EENNDD]] algoritma probabilistik"], [{"string": "Ensemble-index : a new approach to indexing large databases The problem of similarity search query-by-content has attracted much research interest . It is a difficult problem because of the inherently high dimensionality of the data . The most promising solutions involve performing dimensionality reduction on the data , then indexing the reduced data with a multidimensional index structure . Many dimensionality reduction techniques have been proposed , including Singular Value Decomposition SVD , the Discrete Fourier Transform DFT , the Discrete Wavelet Transform DWT and Piecewise Polynomial Approximation . In this work , we introduce a novel framework for using ensembles of two or more representations for more efficient indexing . The basic idea is that instead of committing to a single representation for an entire dataset , different representations are chosen for indexing different parts of the database . The representations are chosen based upon a local view of the database . For example , sections of the data that can achieve a high fidelity representation with wavelets are indexed as wavelets , but highly spectral sections of the data are indexed using the Fourier transform . At query time , it is necessary to search several small heterogeneous indices , rather than one large homogeneous index . As we will theoretically and empirically demonstrate this results in much faster query response times .", "keywords": ["data mining", "content analysis and indexing", "time series", "dimensionality reduction", "indexing and retrieval", "similarity search"], "combined": "Ensemble-index : a new approach to indexing large databases The problem of similarity search query-by-content has attracted much research interest . It is a difficult problem because of the inherently high dimensionality of the data . The most promising solutions involve performing dimensionality reduction on the data , then indexing the reduced data with a multidimensional index structure . Many dimensionality reduction techniques have been proposed , including Singular Value Decomposition SVD , the Discrete Fourier Transform DFT , the Discrete Wavelet Transform DWT and Piecewise Polynomial Approximation . In this work , we introduce a novel framework for using ensembles of two or more representations for more efficient indexing . The basic idea is that instead of committing to a single representation for an entire dataset , different representations are chosen for indexing different parts of the database . The representations are chosen based upon a local view of the database . For example , sections of the data that can achieve a high fidelity representation with wavelets are indexed as wavelets , but highly spectral sections of the data are indexed using the Fourier transform . At query time , it is necessary to search several small heterogeneous indices , rather than one large homogeneous index . As we will theoretically and empirically demonstrate this results in much faster query response times . [[EENNDD]] data mining; content analysis and indexing; time series; dimensionality reduction; indexing and retrieval; similarity search"}, "Ensemble-index: pendekatan baru untuk mengindeks pangkalan data yang besar Masalah kesamaan carian dengan kandungan telah menarik banyak minat penyelidikan. Ini adalah masalah yang sukar kerana dimensi data yang tinggi. Penyelesaian yang paling menjanjikan melibatkan pengurangan dimensi pada data, kemudian mengindeks data yang dikurangkan dengan struktur indeks multidimensi. Banyak teknik pengurangan dimensi telah diusulkan, termasuk SVD Penguraian Nilai Singular, Transformasi Fourier Discrete DFT, Transformasi Wavelet Discrete DWT dan Pendekatan Polinomial Piecewise. Dalam karya ini, kami memperkenalkan kerangka novel untuk menggunakan ensembel dua atau lebih representasi untuk pengindeksan yang lebih efisien. Idea asasnya adalah bahawa bukannya melakukan satu perwakilan untuk keseluruhan kumpulan data, representasi yang berbeza dipilih untuk mengindeks bahagian-bahagian yang berbeza dari pangkalan data. Perwakilan dipilih berdasarkan pandangan tempatan pangkalan data. Contohnya, bahagian data yang dapat mencapai perwakilan kesetiaan tinggi dengan gelombang diindeks sebagai gelombang, tetapi bahagian data yang sangat spektrum diindeks menggunakan transformasi Fourier. Pada masa pertanyaan, perlu mencari beberapa indeks heterogen kecil, dan bukannya satu indeks homogen yang besar. Oleh kerana secara teori dan empirik kita akan menunjukkan hasil ini pada masa tindak balas pertanyaan yang lebih cepat. [[EENNDD]] perlombongan data; analisis kandungan dan pengindeksan; siri masa; pengurangan dimensi; pengindeksan dan pengambilan; carian kesamaan"], [{"string": "Center-piece subgraphs : problem definition and fast solutions Given Q nodes in a social network say , authorship network , how can we find the node\\/author that is the center-piece , and has direct or indirect connections to all , or most of them ? For example , this node could be the common advisor , or someone who started the research area that the Q nodes belong to . Isomorphic scenarios appear in law enforcement find the master-mind criminal , connected to all current suspects , gene regulatory networks find the protein that participates in pathways with all or most of the given Q proteins , viral marketing and many more . Connection subgraphs is an important first step , handling the case of Q = 2 query nodes . Then , the connection subgraph algorithm finds the b intermediate nodes , that provide a good connection between the two original query nodes . Here we generalize the challenge in multiple dimensions : First , we allow more than two query nodes . Second , we allow a whole family of queries , ranging from ` OR ' to ` AND ' , with ` softAND ' in-between . Finally , we design and compare a fast approximation , and study the quality\\/speed trade-off . We also present experiments on the DBLP dataset . The experiments confirm that our proposed method naturally deals with multi-source queries and that the resulting subgraphs agree with our intuition . Wall-clock timing results on the DBLP dataset show that our proposed approximation achieve good accuracy for about 6:1 speedup .", "keywords": ["goodness score", "kand", "center-piece subgraph"], "combined": "Center-piece subgraphs : problem definition and fast solutions Given Q nodes in a social network say , authorship network , how can we find the node\\/author that is the center-piece , and has direct or indirect connections to all , or most of them ? For example , this node could be the common advisor , or someone who started the research area that the Q nodes belong to . Isomorphic scenarios appear in law enforcement find the master-mind criminal , connected to all current suspects , gene regulatory networks find the protein that participates in pathways with all or most of the given Q proteins , viral marketing and many more . Connection subgraphs is an important first step , handling the case of Q = 2 query nodes . Then , the connection subgraph algorithm finds the b intermediate nodes , that provide a good connection between the two original query nodes . Here we generalize the challenge in multiple dimensions : First , we allow more than two query nodes . Second , we allow a whole family of queries , ranging from ` OR ' to ` AND ' , with ` softAND ' in-between . Finally , we design and compare a fast approximation , and study the quality\\/speed trade-off . We also present experiments on the DBLP dataset . The experiments confirm that our proposed method naturally deals with multi-source queries and that the resulting subgraphs agree with our intuition . Wall-clock timing results on the DBLP dataset show that our proposed approximation achieve good accuracy for about 6:1 speedup . [[EENNDD]] goodness score; kand; center-piece subgraph"}, "Subgraf bahagian tengah: definisi masalah dan penyelesaian pantas Mengingat node Q dalam rangkaian sosial mengatakan, rangkaian kepengarangan, bagaimana kita dapat mencari node \\ / pengarang yang merupakan bahagian tengah, dan mempunyai hubungan langsung atau tidak langsung ke semua, atau sebahagian besar mereka ? Sebagai contoh, simpul ini boleh menjadi penasihat biasa, atau seseorang yang memulakan bidang penyelidikan yang menjadi node Q. Senario isomorfik yang muncul dalam penguatkuasaan undang-undang mendapati penjenayah mind-master, terhubung dengan semua suspek semasa, rangkaian pengatur gen menemukan protein yang mengambil bahagian dalam jalur dengan semua atau sebahagian besar protein Q yang diberikan, pemasaran virus dan banyak lagi. Subgraf sambungan adalah langkah pertama yang penting, menangani kes Q = 2 nod pertanyaan. Kemudian, algoritma subgraf sambungan menemui nod perantara b, yang memberikan hubungan yang baik antara kedua-dua nod pertanyaan yang asal. Di sini kita umumkan cabaran dalam pelbagai dimensi: Pertama, kita membenarkan lebih daripada dua nod pertanyaan. Kedua, kami membenarkan sekumpulan pertanyaan, mulai dari \"ATAU\" hingga \"DAN\", dengan \"softAND\" di antara. Akhirnya, kami merancang dan membandingkan pendekatan yang cepat, dan mengkaji kualiti \\ / speed-off. Kami juga membentangkan eksperimen pada set data DBLP. Eksperimen mengesahkan bahawa kaedah yang dicadangkan kami secara semula jadi berurusan dengan pertanyaan pelbagai sumber dan bahawa subgraf yang dihasilkan bersetuju dengan intuisi kami. Hasil pemasaan jam dinding pada set data DBLP menunjukkan bahawa penghampiran yang dicadangkan kami mencapai ketepatan yang baik untuk peningkatan 6: 1. [[EENNDD]] skor kebaikan; kand; subgraf bahagian tengah"], [{"string": "Machine learning for online query relaxation In this paper we provide a fast , data-driven solution to the failing query problem : given a query that returns an empty answer , how can one relax the query 's constraints so that it returns a non-empty set of tuples ? We introduce a novel algorithm , loqr , which is designed to relax queries that are in the disjunctive normal form and contain a mixture of discrete and continuous attributes . loqr discovers the implicit relationships that exist among the various domain attributes and then uses this knowledge to relax the constraints from the failing query . In a first step , loqr uses a small , randomly-chosen subset of the target database to learn a set of decision rules that predict whether an attribute 's value satisfies the constraints in the failing query ; this query-driven operation is performed online for each failing query . In the second step , loqr uses nearest-neighbor techniques to find the learned rule that is the most similar to the failing query ; then it uses the attributes ' values from this rule to relax the failing query 's constraints . Our experiments on six application domains show that loqr is both robust and fast : it successfully relaxes more than 95 % of the failing queries , and it takes under a second for processing queries that consist of up to 20 attributes larger queries of up to 93 attributes are processed in several seconds .", "keywords": ["nearest neighbor", "web-based information sources", "learning", "online query relaxation", "failing query", "rule learning"], "combined": "Machine learning for online query relaxation In this paper we provide a fast , data-driven solution to the failing query problem : given a query that returns an empty answer , how can one relax the query 's constraints so that it returns a non-empty set of tuples ? We introduce a novel algorithm , loqr , which is designed to relax queries that are in the disjunctive normal form and contain a mixture of discrete and continuous attributes . loqr discovers the implicit relationships that exist among the various domain attributes and then uses this knowledge to relax the constraints from the failing query . In a first step , loqr uses a small , randomly-chosen subset of the target database to learn a set of decision rules that predict whether an attribute 's value satisfies the constraints in the failing query ; this query-driven operation is performed online for each failing query . In the second step , loqr uses nearest-neighbor techniques to find the learned rule that is the most similar to the failing query ; then it uses the attributes ' values from this rule to relax the failing query 's constraints . Our experiments on six application domains show that loqr is both robust and fast : it successfully relaxes more than 95 % of the failing queries , and it takes under a second for processing queries that consist of up to 20 attributes larger queries of up to 93 attributes are processed in several seconds . [[EENNDD]] nearest neighbor; web-based information sources; learning; online query relaxation; failing query; rule learning"}, "Pembelajaran mesin untuk kelonggaran pertanyaan dalam talian Dalam makalah ini, kami menyediakan penyelesaian cepat dan berdasarkan data untuk masalah pertanyaan yang gagal: diberikan pertanyaan yang mengembalikan jawapan kosong, bagaimana seseorang dapat melonggarkan kekangan pertanyaan sehingga mengembalikan yang tidak kosong set tupel? Kami memperkenalkan algoritma novel, loqr, yang dirancang untuk menenangkan pertanyaan yang dalam bentuk normal yang tidak berfungsi dan mengandungi campuran atribut diskrit dan berterusan. loqr menemui hubungan tersirat yang wujud di antara pelbagai atribut domain dan kemudian menggunakan pengetahuan ini untuk melonggarkan kekangan dari pertanyaan yang gagal. Pada langkah pertama, loqr menggunakan subset pangkalan data sasaran kecil yang dipilih secara rawak untuk mempelajari sekumpulan peraturan keputusan yang meramalkan sama ada nilai atribut memenuhi kekangan dalam pertanyaan yang gagal; operasi yang didorong oleh pertanyaan ini dilakukan dalam talian untuk setiap pertanyaan yang gagal. Pada langkah kedua, loqr menggunakan teknik tetangga terdekat untuk mencari peraturan yang dipelajari yang paling serupa dengan pertanyaan yang gagal; maka ia menggunakan nilai atribut dari peraturan ini untuk melonggarkan kekangan pertanyaan yang gagal. Eksperimen kami pada enam domain aplikasi menunjukkan bahawa loqr kuat dan pantas: ia berjaya melonggarkan lebih dari 95% daripada pertanyaan yang gagal, dan memerlukan sedikit masa sesaat untuk memproses pertanyaan yang terdiri daripada 20 atribut pertanyaan yang lebih besar hingga 93 atribut diproses dalam beberapa saat. [[EENNDD]] jiran terdekat; sumber maklumat berasaskan web; belajar; kelonggaran pertanyaan dalam talian; pertanyaan gagal; pembelajaran peraturan"], [{"string": "Suppressing model overfitting in mining concept-drifting data streams Mining data streams of changing class distributions is important for real-time business decision support . The stream classifier must evolve to reflect the current class distribution . This poses a serious challenge . On the one hand , relying on historical data may increase the chances of learning obsolete models . On the other hand , learning only from the latest data may lead to biased classifiers , as the latest data is often an unrepresentative sample of the current class distribution . The problem is particularly acute in classifying rare events , when , for example , instances of the rare class do not even show up in the most recent training data . In this paper , we use a stochastic model to describe the concept shifting patterns and formulate this problem as an optimization one : from the historical and the current training data that we have observed , find the most-likely current distribution , and learn a classifier based on the most-likely distribution . We derive an analytic solution and approximate this solution with an efficient algorithm , which calibrates the influence of historical data carefully to create an accurate classifier . We evaluate our algorithm with both synthetic and real-world datasets . Our results show that our algorithm produces accurate and efficient classification .", "keywords": ["data streams", "classifier", "classifier ensemble", "concept drift"], "combined": "Suppressing model overfitting in mining concept-drifting data streams Mining data streams of changing class distributions is important for real-time business decision support . The stream classifier must evolve to reflect the current class distribution . This poses a serious challenge . On the one hand , relying on historical data may increase the chances of learning obsolete models . On the other hand , learning only from the latest data may lead to biased classifiers , as the latest data is often an unrepresentative sample of the current class distribution . The problem is particularly acute in classifying rare events , when , for example , instances of the rare class do not even show up in the most recent training data . In this paper , we use a stochastic model to describe the concept shifting patterns and formulate this problem as an optimization one : from the historical and the current training data that we have observed , find the most-likely current distribution , and learn a classifier based on the most-likely distribution . We derive an analytic solution and approximate this solution with an efficient algorithm , which calibrates the influence of historical data carefully to create an accurate classifier . We evaluate our algorithm with both synthetic and real-world datasets . Our results show that our algorithm produces accurate and efficient classification . [[EENNDD]] data streams; classifier; classifier ensemble; concept drift"}, "Menindas kelebihan model dalam aliran data yang mengalir konsep konsep melombong Aliran data perlombongan pengedaran kelas yang berubah adalah penting untuk sokongan keputusan perniagaan masa nyata. Pengelas aliran mesti berkembang untuk mencerminkan taburan kelas semasa. Ini menimbulkan cabaran serius. Di satu pihak, bergantung pada data sejarah dapat meningkatkan peluang belajar model usang. Sebaliknya, belajar hanya dari data terbaru boleh menyebabkan pengklasifikasi berat sebelah, kerana data terbaru sering menjadi contoh yang tidak representatif dari pengedaran kelas semasa. Masalahnya amat serius dalam mengklasifikasikan kejadian yang jarang berlaku, ketika, misalnya, contoh kelas yang jarang berlaku bahkan tidak muncul dalam data latihan terbaru. Dalam makalah ini, kami menggunakan model stokastik untuk menerangkan konsep perubahan pola dan merumuskan masalah ini sebagai pengoptimuman: dari data latihan sejarah dan semasa yang telah kami amati, cari pengedaran semasa yang paling mungkin, dan pelajari berdasarkan pengkelasan pada pengedaran yang paling mungkin. Kami memperoleh penyelesaian analitik dan menghampiri penyelesaian ini dengan algoritma yang cekap, yang mengkalibrasi pengaruh data sejarah dengan teliti untuk membuat pengkelasan yang tepat. Kami menilai algoritma kami dengan kumpulan data sintetik dan dunia nyata. Hasil kajian kami menunjukkan bahawa algoritma kami menghasilkan klasifikasi yang tepat dan cekap. [[EENNDD]] aliran data; pengelas; ensembel pengkelasan; drift konsep"], [{"string": "Local decomposition for rare class analysis Given its importance , the problem of predicting rare classes in large-scale multi-labeled data sets has attracted great attentions in the literature . However , the rare-class problem remains a critical challenge , because there is no natural way developed for handling imbalanced class distributions . This paper thus fills this crucial void by developing a method for Classification using lOcal clusterinG COG . Specifically , for a data set with an imbalanced class distribution , we perform clustering within each large class and produce sub-classes with relatively balanced sizes . Then , we apply traditional supervised learning algorithms , such as Support Vector Machines SVMs , for classification . Indeed , our experimental results on various real-world data sets show that our method produces significantly higher prediction accuracies on rare classes than state-of-the-art methods . Furthermore , we show that COG can also improve the performance of traditional supervised learning algorithms on data sets with balanced class distributions .", "keywords": ["rare class analysis", "local clustering", "k-means clustering support vector machines"], "combined": "Local decomposition for rare class analysis Given its importance , the problem of predicting rare classes in large-scale multi-labeled data sets has attracted great attentions in the literature . However , the rare-class problem remains a critical challenge , because there is no natural way developed for handling imbalanced class distributions . This paper thus fills this crucial void by developing a method for Classification using lOcal clusterinG COG . Specifically , for a data set with an imbalanced class distribution , we perform clustering within each large class and produce sub-classes with relatively balanced sizes . Then , we apply traditional supervised learning algorithms , such as Support Vector Machines SVMs , for classification . Indeed , our experimental results on various real-world data sets show that our method produces significantly higher prediction accuracies on rare classes than state-of-the-art methods . Furthermore , we show that COG can also improve the performance of traditional supervised learning algorithms on data sets with balanced class distributions . [[EENNDD]] rare class analysis; local clustering; k-means clustering support vector machines"}, "Penguraian tempatan untuk analisis kelas yang jarang berlaku Memandangkan kepentingannya, masalah untuk meramalkan kelas yang jarang berlaku dalam kumpulan data berlabel berskala besar telah menarik perhatian besar dalam literatur. Namun, masalah kelas langka tetap menjadi cabaran kritikal, kerana tidak ada cara semula jadi yang dikembangkan untuk menangani pembahagian kelas yang tidak seimbang. Makalah ini mengisi kekosongan penting ini dengan mengembangkan kaedah untuk Klasifikasi menggunakan lOcal clusterinG COG. Khususnya, untuk kumpulan data dengan distribusi kelas yang tidak seimbang, kami melakukan pengelompokan dalam setiap kelas besar dan menghasilkan subkelas dengan ukuran yang relatif seimbang. Kemudian, kami menggunakan algoritma pembelajaran yang diselia tradisional, seperti SVM Mesin Vektor Sokongan, untuk klasifikasi. Sesungguhnya, hasil eksperimen kami pada pelbagai set data dunia nyata menunjukkan bahawa kaedah kami menghasilkan ketepatan ramalan yang lebih tinggi secara signifikan pada kelas yang jarang berlaku berbanding kaedah canggih. Selanjutnya, kami menunjukkan bahawa COG juga dapat meningkatkan prestasi algoritma pembelajaran diawasi tradisional pada set data dengan pengagihan kelas yang seimbang. [[EENNDD]] analisis kelas yang jarang berlaku; pengelompokan tempatan; k-bermaksud mesin vektor sokongan pengelompokan"], [{"string": "Cleaning disguised missing data : a heuristic approach In some applications such as filling in a customer information form on the web , some missing values may not be explicitly represented as such , but instead appear as potentially valid data values . Such missing values are known as disguised missing data , which may impair the quality of data analysis severely , such as causing significant biases and misleading results in hypothesis tests , correlation analysis and regressions . The very limited previous studies on cleaning disguised missing data use outlier mining and distribution anomaly detection . They highly rely on domain background knowledge in specific applications and may not work well for the cases where the disguise values are inliers . To tackle the problem of cleaning disguised missing data , in this paper , we first model the distribution of disguised missing data , and propose the embedded unbiased sample heuristic . Then , we develop an effective and efficient method to identify the frequently used disguise values which capture the major body of the disguised missing data . Our method does not require any domain background knowledge to find the suspicious disguise values . We report an empirical evaluation using real data sets , which shows that our method is effective - the frequently used disguise values found by our method match the values identified by the domain experts nicely . Our method is also efficient and scalable for processing large data sets .", "keywords": ["disguised missing data", "data cleaning", "data quality"], "combined": "Cleaning disguised missing data : a heuristic approach In some applications such as filling in a customer information form on the web , some missing values may not be explicitly represented as such , but instead appear as potentially valid data values . Such missing values are known as disguised missing data , which may impair the quality of data analysis severely , such as causing significant biases and misleading results in hypothesis tests , correlation analysis and regressions . The very limited previous studies on cleaning disguised missing data use outlier mining and distribution anomaly detection . They highly rely on domain background knowledge in specific applications and may not work well for the cases where the disguise values are inliers . To tackle the problem of cleaning disguised missing data , in this paper , we first model the distribution of disguised missing data , and propose the embedded unbiased sample heuristic . Then , we develop an effective and efficient method to identify the frequently used disguise values which capture the major body of the disguised missing data . Our method does not require any domain background knowledge to find the suspicious disguise values . We report an empirical evaluation using real data sets , which shows that our method is effective - the frequently used disguise values found by our method match the values identified by the domain experts nicely . Our method is also efficient and scalable for processing large data sets . [[EENNDD]] disguised missing data; data cleaning; data quality"}, "Membersihkan data yang tersembunyi: pendekatan heuristik Dalam beberapa aplikasi seperti mengisi borang maklumat pelanggan di web, beberapa nilai yang hilang mungkin tidak dinyatakan secara eksplisit seperti itu, tetapi muncul sebagai nilai data yang berpotensi berlaku. Nilai hilang tersebut dikenali sebagai data hilang yang tersembunyi, yang boleh merosakkan kualiti analisis data dengan teruk, seperti menyebabkan bias yang signifikan dan hasil yang menyesatkan dalam ujian hipotesis, analisis korelasi dan regresi. Kajian terdahulu yang sangat terhad mengenai pembersihan data tersembunyi yang tersembunyi menggunakan pengesanan anomali perlombongan dan pengedaran. Mereka sangat bergantung pada pengetahuan latar belakang domain dalam aplikasi tertentu dan mungkin tidak berfungsi dengan baik untuk kes-kes di mana nilai penyamaran adalah penyisipan. Untuk mengatasi masalah membersihkan data hilang yang tersembunyi, dalam makalah ini, kami pertama kali memodelkan penyebaran data yang tersembunyi yang hilang, dan mengusulkan heuristik sampel yang tidak berat sebelah yang disertakan. Kemudian, kami mengembangkan kaedah yang berkesan dan cekap untuk mengenal pasti nilai penyamaran yang sering digunakan yang menangkap sebahagian besar data yang tersembunyi yang hilang. Kaedah kami tidak memerlukan pengetahuan latar belakang domain untuk mencari nilai penyamaran yang mencurigakan. Kami melaporkan penilaian empirikal menggunakan set data sebenar, yang menunjukkan bahawa kaedah kami berkesan - nilai penyamaran yang sering digunakan oleh kaedah kami sesuai dengan nilai yang dikenal pasti oleh pakar domain dengan baik. Kaedah kami juga cekap dan berskala untuk memproses set data yang besar. [[EENNDD]] menyamar data yang hilang; pembersihan data; kualiti data"], [{"string": "Boosting with structure information in the functional space : an application to graph classification Boosting is a very successful classification algorithm that produces a linear combination of `` weak '' classifiers a.k.a. base learners to obtain high quality classification models . In this paper we propose a new boosting algorithm where base learners have structure relationships in the functional space . Though such relationships are generic , our work is particularly motivated by the emerging topic of pattern based classification for semi-structured data including graphs . Towards an efficient incorporation of the structure information , we have designed a general model where we use an undirected graph to capture the relationship of subgraph-based base learners . In our method , we combine both L1 norm and Laplacian based L2 norm penalty with Logit loss function of Logit Boost . In this approach , we enforce model sparsity and smoothness in the functional space spanned by the basis functions . We have derived efficient optimization algorithms based on coordinate decent for the new boosting formulation and theoretically prove that it exhibits a natural grouping effect for nearby spatial or overlapping features . Using comprehensive experimental study , we have demonstrated the effectiveness of the proposed learning methods .", "keywords": ["l1 regularization", "semi-structured data", "graph classification", "boosting", "feature selection"], "combined": "Boosting with structure information in the functional space : an application to graph classification Boosting is a very successful classification algorithm that produces a linear combination of `` weak '' classifiers a.k.a. base learners to obtain high quality classification models . In this paper we propose a new boosting algorithm where base learners have structure relationships in the functional space . Though such relationships are generic , our work is particularly motivated by the emerging topic of pattern based classification for semi-structured data including graphs . Towards an efficient incorporation of the structure information , we have designed a general model where we use an undirected graph to capture the relationship of subgraph-based base learners . In our method , we combine both L1 norm and Laplacian based L2 norm penalty with Logit loss function of Logit Boost . In this approach , we enforce model sparsity and smoothness in the functional space spanned by the basis functions . We have derived efficient optimization algorithms based on coordinate decent for the new boosting formulation and theoretically prove that it exhibits a natural grouping effect for nearby spatial or overlapping features . Using comprehensive experimental study , we have demonstrated the effectiveness of the proposed learning methods . [[EENNDD]] l1 regularization; semi-structured data; graph classification; boosting; feature selection"}, "Meningkatkan dengan maklumat struktur di ruang fungsional: aplikasi untuk klasifikasi grafik Boosting adalah algoritma klasifikasi yang sangat berjaya yang menghasilkan gabungan linear pengklasifikasi asas \"lemah\" antara pelajar asas untuk memperoleh model klasifikasi berkualiti tinggi. Dalam makalah ini kami mencadangkan algoritma penambah baru di mana pelajar asas mempunyai hubungan struktur di ruang fungsional. Walaupun hubungan semacam itu bersifat generik, pekerjaan kami terutama dimotivasi oleh topik klasifikasi berdasarkan pola yang muncul untuk data separa berstruktur termasuk grafik. Ke arah penggabungan maklumat struktur yang cekap, kami telah merancang model umum di mana kami menggunakan grafik yang tidak diarahkan untuk menangkap hubungan pelajar asas berasaskan subgraf. Dalam kaedah kami, kami menggabungkan kedua-dua norma L1 dan hukuman norma L2 berdasarkan Laplacian dengan fungsi kehilangan Logit Boost. Dalam pendekatan ini, kami menerapkan kelangkaan dan kelancaran model di ruang fungsional yang dibentangkan oleh fungsi dasar. Kami telah memperoleh algoritma pengoptimuman yang cekap berdasarkan koordinat yang tepat untuk perumusan penggalak baru dan secara teorinya membuktikan bahawa ia menunjukkan kesan pengelompokan semula jadi untuk ciri ruang atau pertindihan yang berdekatan. Dengan menggunakan kajian eksperimen yang komprehensif, kami telah menunjukkan keberkesanan kaedah pembelajaran yang dicadangkan. [[EENNDD]] l1 regularisasi; data separa berstruktur; klasifikasi grafik; meningkatkan; pemilihan ciri"], [{"string": "COA : finding novel patents through text analysis In recent years , the number of patents filed by the business enterprises in the technology industry are growing rapidly , thus providing unprecedented opportunities for knowledge discovery in patent data . One important task in this regard is to employ data mining techniques to rank patents in terms of their potential to earn money through licensing . Availability of such ranking can substantially reduce enterprise IP Intellectual Property management costs . Unfortunately , the existing software systems in the IP domain do not address this task directly . Through our research , we build a patent ranking software , named COA Claim Originality Analysis that rates a patent based on its value by measuring the recency and the impact of the important phrases that appear in the `` claims '' section of a patent . Experiments show that COA produces meaningful ranking when comparing it with other indirect patent evaluation metrics -- citation count , patent status , and attorney 's rating . In reallife settings , this tool was used by beta-testers in the IBM IP department . Lawyers found it very useful in patent rating , specifically , in highlighting potentially valuable patents in a patent cluster . In this article , we describe the ranking techniques and system architecture of COA . We also present the results that validate its effectiveness .", "keywords": ["information retrieval", "patent processing", "patent visualization", "document ranking"], "combined": "COA : finding novel patents through text analysis In recent years , the number of patents filed by the business enterprises in the technology industry are growing rapidly , thus providing unprecedented opportunities for knowledge discovery in patent data . One important task in this regard is to employ data mining techniques to rank patents in terms of their potential to earn money through licensing . Availability of such ranking can substantially reduce enterprise IP Intellectual Property management costs . Unfortunately , the existing software systems in the IP domain do not address this task directly . Through our research , we build a patent ranking software , named COA Claim Originality Analysis that rates a patent based on its value by measuring the recency and the impact of the important phrases that appear in the `` claims '' section of a patent . Experiments show that COA produces meaningful ranking when comparing it with other indirect patent evaluation metrics -- citation count , patent status , and attorney 's rating . In reallife settings , this tool was used by beta-testers in the IBM IP department . Lawyers found it very useful in patent rating , specifically , in highlighting potentially valuable patents in a patent cluster . In this article , we describe the ranking techniques and system architecture of COA . We also present the results that validate its effectiveness . [[EENNDD]] information retrieval; patent processing; patent visualization; document ranking"}, "COA: mencari paten baru melalui analisis teks Dalam beberapa tahun terakhir, jumlah paten yang diajukan oleh perusahaan perniagaan dalam industri teknologi berkembang pesat, sehingga memberikan peluang yang belum pernah terjadi sebelumnya untuk penemuan pengetahuan dalam data paten. Satu tugas penting dalam hal ini adalah menggunakan teknik perlombongan data untuk memberi peringkat kepada paten dari segi potensi mereka untuk mendapatkan wang melalui perlesenan. Ketersediaan peringkat tersebut dapat mengurangkan kos pengurusan Harta Intelek IP perusahaan dengan ketara. Malangnya, sistem perisian yang ada dalam domain IP tidak menangani tugas ini secara langsung. Melalui penyelidikan kami, kami membina perisian peringkat paten, bernama COA Claim Originality Analysis yang menilai paten berdasarkan nilainya dengan mengukur kebelakangan dan kesan frasa penting yang muncul di bahagian \"tuntutan\" paten. Eksperimen menunjukkan bahawa COA menghasilkan peringkat yang bermakna ketika membandingkannya dengan metrik penilaian paten tidak langsung yang lain - jumlah kutipan, status paten, dan penilaian peguam. Dalam tetapan reallife, alat ini digunakan oleh penguji beta di jabatan IP IBM. Peguam menganggapnya sangat berguna dalam penilaian paten, khususnya, dalam menonjolkan hak paten yang berpotensi berharga dalam kelompok paten. Dalam artikel ini, kami menerangkan teknik peringkat dan seni bina sistem COA. Kami juga membentangkan hasil yang mengesahkan keberkesanannya. [[EENNDD]] pengambilan maklumat; pemprosesan paten; visualisasi paten; kedudukan dokumen"], [{"string": "Privacy-preserving Bayesian network structure computation on distributed heterogeneous data As more and more activities are carried out using computers and computer networks , the amount of potentially sensitive data stored by business , governments , and other parties increases . Different parties may wish to benefit from cooperative use of their data , but privacy regulations and other privacy concerns may prevent the parties from sharing their data . Privacy-preserving data mining provides a solution by creating distributed data mining algorithms in which the underlying data is not revealed . In this paper , we present a privacy-preserving protocol for a particular data mining task : learning the Bayesian network structure for distributed heterogeneous data . In this setting , two parties owning confidential databases wish to learn the structure of Bayesian network on the combination of their databases without revealing anything about their data to each other . We give an efficient and privacy-preserving version of the K2 algorithm to construct the structure of a Bayesian network for the parties ' joint data .", "keywords": ["privacy-preserving data mining", "distributed databases", "bayesian network"], "combined": "Privacy-preserving Bayesian network structure computation on distributed heterogeneous data As more and more activities are carried out using computers and computer networks , the amount of potentially sensitive data stored by business , governments , and other parties increases . Different parties may wish to benefit from cooperative use of their data , but privacy regulations and other privacy concerns may prevent the parties from sharing their data . Privacy-preserving data mining provides a solution by creating distributed data mining algorithms in which the underlying data is not revealed . In this paper , we present a privacy-preserving protocol for a particular data mining task : learning the Bayesian network structure for distributed heterogeneous data . In this setting , two parties owning confidential databases wish to learn the structure of Bayesian network on the combination of their databases without revealing anything about their data to each other . We give an efficient and privacy-preserving version of the K2 algorithm to construct the structure of a Bayesian network for the parties ' joint data . [[EENNDD]] privacy-preserving data mining; distributed databases; bayesian network"}, "Pengiraan struktur jaringan Bayesian yang menjaga privasi pada data heterogen yang diedarkan Oleh kerana semakin banyak aktiviti dijalankan menggunakan komputer dan rangkaian komputer, jumlah data yang berpotensi sensitif yang disimpan oleh perniagaan, pemerintah, dan pihak lain meningkat. Pihak yang berlainan mungkin ingin memanfaatkan penggunaan data mereka secara koperasi, tetapi peraturan privasi dan kebimbangan privasi lain dapat mencegah pihak tersebut untuk berkongsi data mereka. Perlombongan data yang memelihara privasi memberikan penyelesaian dengan membuat algoritma perlombongan data yang diedarkan di mana data yang mendasari tidak didedahkan. Dalam makalah ini, kami menyajikan protokol pelestarian privasi untuk tugas perlombongan data tertentu: mempelajari struktur jaringan Bayesian untuk data heterogen yang diedarkan. Dalam keadaan ini, dua pihak yang memiliki pangkalan data sulit ingin mempelajari struktur rangkaian Bayesian pada gabungan pangkalan data mereka tanpa mengungkapkan apa-apa mengenai data mereka satu sama lain. Kami memberikan versi algoritma K2 yang cekap dan memelihara privasi untuk membina struktur rangkaian Bayesian untuk data bersama pihak tersebut. [[EENNDD]] perlombongan data yang memelihara privasi; pangkalan data yang diedarkan; rangkaian bayesian"], [{"string": "Trajectory pattern mining The increasing pervasiveness of location-acquisition technologies GPS , GSM networks , etc. is leading to the collection of large spatio-temporal datasets and to the opportunity of discovering usable knowledge about movement behavior , which fosters novel applications and services . In this paper , we move towards this direction and develop an extension of the sequential pattern mining paradigm that analyzes the trajectories of moving objects . We introduce trajectory patterns as concise descriptions of frequent behaviors , in terms of both space i.e. , the regions of space visited during movements and time i.e. , the duration of movements . In this setting , we provide a general formal statement of the novel mining problem and then study several different instantiations of different complexity . The various approaches are then empirically evaluated over real data and synthetic benchmarks , comparing their strengths and weaknesses .", "keywords": ["spatio-temporal data mining", "trajectory patterns"], "combined": "Trajectory pattern mining The increasing pervasiveness of location-acquisition technologies GPS , GSM networks , etc. is leading to the collection of large spatio-temporal datasets and to the opportunity of discovering usable knowledge about movement behavior , which fosters novel applications and services . In this paper , we move towards this direction and develop an extension of the sequential pattern mining paradigm that analyzes the trajectories of moving objects . We introduce trajectory patterns as concise descriptions of frequent behaviors , in terms of both space i.e. , the regions of space visited during movements and time i.e. , the duration of movements . In this setting , we provide a general formal statement of the novel mining problem and then study several different instantiations of different complexity . The various approaches are then empirically evaluated over real data and synthetic benchmarks , comparing their strengths and weaknesses . [[EENNDD]] spatio-temporal data mining; trajectory patterns"}, "Perlombongan corak lintasan Peningkatan daya tarikan teknologi pemerolehan lokasi GPS, rangkaian GSM, dan lain-lain membawa kepada pengumpulan kumpulan data spatio-temporal yang besar dan peluang untuk menemukan pengetahuan yang berguna mengenai tingkah laku pergerakan, yang memupuk aplikasi dan perkhidmatan baru. Dalam makalah ini, kita bergerak ke arah ini dan mengembangkan lanjutan dari paradigma perlombongan pola berurutan yang menganalisis lintasan objek bergerak. Kami memperkenalkan corak lintasan sebagai gambaran ringkas mengenai tingkah laku yang kerap, dari segi kedua-dua ruang, iaitu kawasan ruang yang dikunjungi semasa pergerakan dan waktu, iaitu, jangka masa pergerakan. Dalam keadaan ini, kami memberikan pernyataan formal umum mengenai masalah penambangan novel dan kemudian mengkaji beberapa contoh kerumitan yang berbeza. Pelbagai pendekatan tersebut kemudiannya dinilai secara empirik terhadap data sebenar dan penanda aras sintetik, membandingkan kekuatan dan kelemahan mereka. [[EENNDD]] perlombongan data spatio-temporal; corak lintasan"], [{"string": "Simple and effective visual models for gene expression cancer diagnostics In the paper we show that diagnostic classes in cancer gene expression data sets , which most often include thousands of features genes , may be effectively separated with simple two-dimensional plots such as scatterplot and radviz graph . The principal innovation proposed in the paper is a method called VizRank , which is able to score and identify the best among possibly millions of candidate projections for visualizations . Compared to recently much applied techniques in the field of cancer genomics that include neural networks , support vector machines and various ensemble-based approaches , VizRank is fast and finds visualization models that can be easily examined and interpreted by domain experts . Our experiments on a number of gene expression data sets show that VizRank was always able to find data visualizations with a small number of two to seven genes and excellent class separation . In addition to providing grounds for gene expression cancer diagnosis , VizRank and its visualizations also identify small sets of relevant genes , uncover interesting gene interactions and point to outliers and potential misclassifications in cancer data sets .", "keywords": ["data mining", "gene expression analysis", "data visualization", "cancer diagnosis", "machine learning"], "combined": "Simple and effective visual models for gene expression cancer diagnostics In the paper we show that diagnostic classes in cancer gene expression data sets , which most often include thousands of features genes , may be effectively separated with simple two-dimensional plots such as scatterplot and radviz graph . The principal innovation proposed in the paper is a method called VizRank , which is able to score and identify the best among possibly millions of candidate projections for visualizations . Compared to recently much applied techniques in the field of cancer genomics that include neural networks , support vector machines and various ensemble-based approaches , VizRank is fast and finds visualization models that can be easily examined and interpreted by domain experts . Our experiments on a number of gene expression data sets show that VizRank was always able to find data visualizations with a small number of two to seven genes and excellent class separation . In addition to providing grounds for gene expression cancer diagnosis , VizRank and its visualizations also identify small sets of relevant genes , uncover interesting gene interactions and point to outliers and potential misclassifications in cancer data sets . [[EENNDD]] data mining; gene expression analysis; data visualization; cancer diagnosis; machine learning"}, "Model visual yang ringkas dan berkesan untuk diagnostik kanser ekspresi gen Dalam makalah ini kami menunjukkan bahawa kelas diagnostik dalam kumpulan data ekspresi gen barah, yang paling sering merangkumi ribuan gen ciri, dapat dipisahkan secara efektif dengan plot dua dimensi yang mudah seperti scatterplot dan grafik radviz . Inovasi utama yang dicadangkan dalam makalah ini adalah kaedah yang disebut VizRank, yang mampu menjaringkan dan mengenal pasti yang terbaik di antara jutaan calon unjuran untuk visualisasi. Berbanding dengan teknik yang baru diterapkan dalam bidang genomik barah yang merangkumi rangkaian saraf, mesin vektor sokongan dan pelbagai pendekatan berasaskan ensemble, VizRank cepat dan menemukan model visualisasi yang dapat dengan mudah diperiksa dan ditafsirkan oleh pakar domain. Eksperimen kami pada sejumlah set data ekspresi gen menunjukkan bahawa VizRank selalu dapat mencari visualisasi data dengan sebilangan kecil dua hingga tujuh gen dan pemisahan kelas yang sangat baik. Selain memberikan alasan untuk diagnosis kanser ekspresi gen, VizRank dan visualisasinya juga mengenal pasti sekumpulan kecil gen yang relevan, mengungkap interaksi gen yang menarik dan menunjukkan kepada penyimpangan dan kemungkinan salah klasifikasi dalam kumpulan data kanser. [[EENNDD]] perlombongan data; analisis ekspresi gen; visualisasi data; diagnosis kanser; pembelajaran mesin"], [{"string": "Exploiting dictionaries in named entity extraction : combining semi-Markov extraction processes and data integration methods We consider the problem of improving named entity recognition NER systems by using external dictionaries -- more specifically , the problem of extending state-of-the-art NER systems by incorporating information about the similarity of extracted entities to entities in an external dictionary . This is difficult because most high-performance named entity recognition systems operate by sequentially classifying words as to whether or not they participate in an entity name ; however , the most useful similarity measures score entire candidate names . To correct this mismatch we formalize a semi-Markov extraction process , which is based on sequentially classifying segments of several adjacent words , rather than single words . In addition to allowing a natural way of coupling high-performance NER methods and high-performance similarity functions , this formalism also allows the direct use of other useful entity-level features , and provides a more natural formulation of the NER problem than sequential word classification . Experiments in multiple domains show that the new model can substantially improve extraction performance over previous methods for using external dictionaries in NER .", "keywords": ["named entity recognition", "information extraction", "learning", "data integration", "sequential learning"], "combined": "Exploiting dictionaries in named entity extraction : combining semi-Markov extraction processes and data integration methods We consider the problem of improving named entity recognition NER systems by using external dictionaries -- more specifically , the problem of extending state-of-the-art NER systems by incorporating information about the similarity of extracted entities to entities in an external dictionary . This is difficult because most high-performance named entity recognition systems operate by sequentially classifying words as to whether or not they participate in an entity name ; however , the most useful similarity measures score entire candidate names . To correct this mismatch we formalize a semi-Markov extraction process , which is based on sequentially classifying segments of several adjacent words , rather than single words . In addition to allowing a natural way of coupling high-performance NER methods and high-performance similarity functions , this formalism also allows the direct use of other useful entity-level features , and provides a more natural formulation of the NER problem than sequential word classification . Experiments in multiple domains show that the new model can substantially improve extraction performance over previous methods for using external dictionaries in NER . [[EENNDD]] named entity recognition; information extraction; learning; data integration; sequential learning"}, "Mengeksploitasi kamus dalam pengekstrakan entiti bernama: menggabungkan proses pengekstrakan semi-Markov dan kaedah penyatuan data Kami mempertimbangkan masalah peningkatan sistem NER pengenalan entiti bernama dengan menggunakan kamus luaran - lebih khusus lagi, masalah pengembangan sistem NER yang canggih dengan memasukkan maklumat mengenai persamaan entiti yang diekstrak dengan entiti dalam kamus luaran. Ini sukar kerana kebanyakan sistem pengiktirafan entiti berprestasi tinggi beroperasi dengan mengklasifikasikan secara berurutan kata-kata sama ada mereka mengambil bahagian dalam nama entiti atau tidak; namun, ukuran kesamaan yang paling berguna mencetak keseluruhan nama calon. Untuk membetulkan ketidakcocokan ini, kami memformalkan proses pengekstrakan semi-Markov, yang berdasarkan pada mengklasifikasikan segmen beberapa kata bersebelahan, bukan satu kata. Selain membenarkan kaedah semula jadi menggabungkan kaedah NER berprestasi tinggi dan fungsi kesamaan prestasi tinggi, formalisme ini juga membenarkan penggunaan langsung ciri-ciri tahap entiti lain yang berguna, dan memberikan rumusan yang lebih semula jadi dari masalah NER daripada klasifikasi kata berurutan . Eksperimen dalam beberapa domain menunjukkan bahawa model baru dapat meningkatkan prestasi pengekstrakan berbanding kaedah sebelumnya untuk menggunakan kamus luaran di NER. [[EENNDD]] bernama pengiktirafan entiti; pengekstrakan maklumat; belajar; penyatuan data; pembelajaran berurutan"], [{"string": "A probabilistic framework for semi-supervised clustering Unsupervised clustering can be significantly improved using supervision in the form of pairwise constraints , i.e. , pairs of instances labeled as belonging to same or different clusters . In recent years , a number of algorithms have been proposed for enhancing clustering quality by employing such supervision . Such methods use the constraints to either modify the objective function , or to learn the distance measure . We propose a probabilistic model for semi-supervised clustering based on Hidden Markov Random Fields HMRFs that provides a principled framework for incorporating supervision into prototype-based clustering . The model generalizes a previous approach that combines constraints and Euclidean distance learning , and allows the use of a broad range of clustering distortion measures , including Bregman divergences e.g. , Euclidean distance and I-divergence and directional similarity measures e.g. , cosine similarity . We present an algorithm that performs partitional semi-supervised clustering of data by minimizing an objective function derived from the posterior energy of the HMRF model . Experimental results on several text data sets demonstrate the advantages of the proposed framework .", "keywords": ["hidden markov random fields", "learning", "semi-supervised clustering", "distance metric learning"], "combined": "A probabilistic framework for semi-supervised clustering Unsupervised clustering can be significantly improved using supervision in the form of pairwise constraints , i.e. , pairs of instances labeled as belonging to same or different clusters . In recent years , a number of algorithms have been proposed for enhancing clustering quality by employing such supervision . Such methods use the constraints to either modify the objective function , or to learn the distance measure . We propose a probabilistic model for semi-supervised clustering based on Hidden Markov Random Fields HMRFs that provides a principled framework for incorporating supervision into prototype-based clustering . The model generalizes a previous approach that combines constraints and Euclidean distance learning , and allows the use of a broad range of clustering distortion measures , including Bregman divergences e.g. , Euclidean distance and I-divergence and directional similarity measures e.g. , cosine similarity . We present an algorithm that performs partitional semi-supervised clustering of data by minimizing an objective function derived from the posterior energy of the HMRF model . Experimental results on several text data sets demonstrate the advantages of the proposed framework . [[EENNDD]] hidden markov random fields; learning; semi-supervised clustering; distance metric learning"}, "Kerangka probabilistik untuk pengelompokan separa yang diawasi Pengelompokan yang tidak diawasi dapat ditingkatkan secara signifikan dengan menggunakan pengawasan dalam bentuk batasan berpasangan, yaitu, pasangan contoh yang dilabel sebagai milik kelompok yang sama atau berbeza. Dalam beberapa tahun terakhir, sejumlah algoritma telah diusulkan untuk meningkatkan kualitas pengelompokan dengan menggunakan pengawasan tersebut. Kaedah sedemikian menggunakan batasan untuk mengubah fungsi objektif, atau untuk mengetahui ukuran jarak. Kami mencadangkan model probabilistik untuk pengelompokan separa diselia berdasarkan HMRFs Hidden Markov Random Fields yang menyediakan kerangka berprinsip untuk memasukkan pengawasan ke dalam pengelompokan berdasarkan prototaip. Model ini menggeneralisasi pendekatan sebelumnya yang menggabungkan kekangan dan pembelajaran jarak jauh Euclidean, dan memungkinkan penggunaan pelbagai langkah penyimpangan pengelompokan, termasuk perbezaan Bregman, mis. , Jarak Euclidean dan perbezaan-I dan kesamaan arah, mis. , kesamaan kosinus. Kami menyajikan algoritma yang melakukan pengelompokan separa pengawasan separa data dengan meminimumkan fungsi objektif yang berasal dari tenaga posterior model HMRF. Hasil eksperimen pada beberapa set data teks menunjukkan kelebihan kerangka yang dicadangkan. [[EENNDD]] medan rawak markov tersembunyi; belajar; pengelompokan separa diselia; pembelajaran metrik jarak jauh"], [{"string": "ArnetMiner : extraction and mining of academic social networks This paper addresses several key issues in the ArnetMiner system , which aims at extracting and mining academic social networks . Specifically , the system focuses on : 1 Extracting researcher profiles automatically from the Web ; 2 Integrating the publication data into the network from existing digital libraries ; 3 Modeling the entire academic network ; and 4 Providing search services for the academic network . So far , 448,470 researcher profiles have been extracted using a unified tagging approach . We integrate publications from online Web databases and propose a probabilistic framework to deal with the name ambiguity problem . Furthermore , we propose a unified modeling approach to simultaneously model topical aspects of papers , authors , and publication venues . Search services such as expertise search and people association search have been provided based on the modeling results . In this paper , we describe the architecture and main features of the system . We also present the empirical evaluation of the proposed methods .", "keywords": ["expertise search", "association search", "information extraction", "information search and retrieval", "topic modeling", "social network", "name disambiguation"], "combined": "ArnetMiner : extraction and mining of academic social networks This paper addresses several key issues in the ArnetMiner system , which aims at extracting and mining academic social networks . Specifically , the system focuses on : 1 Extracting researcher profiles automatically from the Web ; 2 Integrating the publication data into the network from existing digital libraries ; 3 Modeling the entire academic network ; and 4 Providing search services for the academic network . So far , 448,470 researcher profiles have been extracted using a unified tagging approach . We integrate publications from online Web databases and propose a probabilistic framework to deal with the name ambiguity problem . Furthermore , we propose a unified modeling approach to simultaneously model topical aspects of papers , authors , and publication venues . Search services such as expertise search and people association search have been provided based on the modeling results . In this paper , we describe the architecture and main features of the system . We also present the empirical evaluation of the proposed methods . [[EENNDD]] expertise search; association search; information extraction; information search and retrieval; topic modeling; social network; name disambiguation"}, "ArnetMiner: pengekstrakan dan penambangan rangkaian sosial akademik Makalah ini membahas beberapa isu utama dalam sistem ArnetMiner, yang bertujuan untuk mengekstrak dan melombong rangkaian sosial akademik. Secara khusus, sistem ini memberi tumpuan kepada: 1 Mengekstrak profil penyelidik secara automatik dari Web; 2 Mengintegrasikan data penerbitan ke dalam rangkaian dari perpustakaan digital yang ada; 3 Memodelkan keseluruhan rangkaian akademik; dan 4 Menyediakan perkhidmatan carian untuk rangkaian akademik. Setakat ini, 448,470 profil penyelidik telah diekstrak menggunakan pendekatan penandaan bersatu. Kami mengintegrasikan penerbitan dari pangkalan data Web dalam talian dan mencadangkan kerangka probabilistik untuk menangani masalah kesamaran nama. Selanjutnya, kami mencadangkan pendekatan pemodelan terpadu untuk memodelkan aspek topikal makalah, pengarang, dan tempat penerbitan secara serentak. Perkhidmatan carian seperti carian kepakaran dan pencarian persatuan orang telah disediakan berdasarkan hasil pemodelan. Dalam makalah ini, kami menerangkan seni bina dan ciri utama sistem. Kami juga membentangkan penilaian empirik kaedah yang dicadangkan. [[EENNDD]] carian kepakaran; carian persatuan; pengekstrakan maklumat; carian dan pengambilan maklumat; pemodelan topik; rangkaian sosial; disambiguasi nama"], [{"string": "Support feature machine for classification of abnormal brain activity In this study , a novel multidimensional time series classification technique , namely support feature machine SFM , is proposed . SFM is inspired by the optimization model of support vector machine and the nearest neighbor rule to incorporate both spatial and temporal of the multi-dimensional time series data . This paper also describes an application of SFM for detecting abnormal brain activity . Epilepsy is a case in point in this study . In epilepsy studies , electroencephalograms EEGs , acquired in multidimensional time series format , have been traditionally used as a gold-standard tool for capturing the electrical changes in the brain . From multi-dimensional EEG time series data , SFM was used to identify seizure pre-cursors and detect seizure susceptibility pre-seizure periods . The empirical results showed that SFM achieved over 80 % correct classification of per-seizure EEG on average in 10 patients using 5-fold cross validation . The proposed optimization model of SFM is very compact and scalable , and can be implemented as an online algorithm . The outcome of this study suggests that it is possible to construct a computerized algorithm used to detect seizure pre-cursors and warn of impending seizures through EEG classification .", "keywords": ["epilepsy", "nearest neighbor", "classification", "optimization", "multi-dimensional time series"], "combined": "Support feature machine for classification of abnormal brain activity In this study , a novel multidimensional time series classification technique , namely support feature machine SFM , is proposed . SFM is inspired by the optimization model of support vector machine and the nearest neighbor rule to incorporate both spatial and temporal of the multi-dimensional time series data . This paper also describes an application of SFM for detecting abnormal brain activity . Epilepsy is a case in point in this study . In epilepsy studies , electroencephalograms EEGs , acquired in multidimensional time series format , have been traditionally used as a gold-standard tool for capturing the electrical changes in the brain . From multi-dimensional EEG time series data , SFM was used to identify seizure pre-cursors and detect seizure susceptibility pre-seizure periods . The empirical results showed that SFM achieved over 80 % correct classification of per-seizure EEG on average in 10 patients using 5-fold cross validation . The proposed optimization model of SFM is very compact and scalable , and can be implemented as an online algorithm . The outcome of this study suggests that it is possible to construct a computerized algorithm used to detect seizure pre-cursors and warn of impending seizures through EEG classification . [[EENNDD]] epilepsy; nearest neighbor; classification; optimization; multi-dimensional time series"}, "Mesin ciri sokongan untuk klasifikasi aktiviti otak yang tidak normal Dalam kajian ini, teknik klasifikasi siri masa multidimensi baru, iaitu mesin ciri sokongan SFM, diusulkan. SFM diilhamkan oleh model pengoptimuman mesin vektor sokongan dan peraturan tetangga terdekat untuk menggabungkan data spasial dan temporal data siri masa pelbagai dimensi. Makalah ini juga menerangkan aplikasi SFM untuk mengesan aktiviti otak yang tidak normal. Epilepsi adalah kes dalam kajian ini. Dalam kajian epilepsi, EEG elektroensefalogram, yang diperoleh dalam format siri masa multidimensi, secara tradisional telah digunakan sebagai alat standard emas untuk menangkap perubahan elektrik di otak. Dari data siri masa EEG pelbagai dimensi, SFM digunakan untuk mengenal pasti pra-kursor kejang dan mengesan tempoh pra-penyitaan kerentanan kejang. Hasil empirikal menunjukkan bahawa SFM mencapai lebih dari 80% klasifikasi EEG per-kejang yang betul secara purata pada 10 pesakit menggunakan pengesahan silang 5 kali ganda. Model pengoptimuman SFM yang dicadangkan sangat padat dan boleh diskalakan, dan dapat dilaksanakan sebagai algoritma dalam talian. Hasil kajian ini menunjukkan bahawa mungkin untuk membuat algoritma berkomputer yang digunakan untuk mengesan pra-kursor kejang dan memberi amaran mengenai kejang yang akan berlaku melalui klasifikasi EEG. [[EENNDD]] epilepsi; jiran terdekat; pengelasan; pengoptimuman; siri masa pelbagai dimensi"], [{"string": "Efficient incremental constrained clustering Clustering with constraints is an emerging area of data mining research . However , most work assumes that the constraints are given as one large batch . In this paper we explore the situation where the constraints are incrementally given . In this way the user after seeing a clustering can provide positive and negative feedback via constraints to critique a clustering solution . We consider the problem of efficiently updating a clustering to satisfy the new and old constraints rather than reclustering the entire data set . We show that the problem of incremental clustering under constraints is NP-hard in general , but identify several sufficient conditions which lead to efficiently solvable versions . These translate into a set of rules on the types of constraints thatcan be added and constraint set properties that must be maintained . We demonstrate that this approach is more efficient than re-clustering the entire data set and has several other advantages .", "keywords": ["constraints", "complexity", "clustering"], "combined": "Efficient incremental constrained clustering Clustering with constraints is an emerging area of data mining research . However , most work assumes that the constraints are given as one large batch . In this paper we explore the situation where the constraints are incrementally given . In this way the user after seeing a clustering can provide positive and negative feedback via constraints to critique a clustering solution . We consider the problem of efficiently updating a clustering to satisfy the new and old constraints rather than reclustering the entire data set . We show that the problem of incremental clustering under constraints is NP-hard in general , but identify several sufficient conditions which lead to efficiently solvable versions . These translate into a set of rules on the types of constraints thatcan be added and constraint set properties that must be maintained . We demonstrate that this approach is more efficient than re-clustering the entire data set and has several other advantages . [[EENNDD]] constraints; complexity; clustering"}, "Penggabungan berkelompok tambahan yang cekap Pengelompokan dengan batasan adalah bidang penyelidikan perlombongan data yang muncul. Walau bagaimanapun, kebanyakan kerja menganggap bahawa kekangan diberikan sebagai satu kumpulan besar. Dalam makalah ini kita meneroka keadaan di mana kekangan diberikan secara bertahap. Dengan cara ini pengguna setelah melihat pengelompokan dapat memberikan maklum balas positif dan negatif melalui batasan untuk mengkritik penyelesaian pengelompokan. Kami menganggap masalah mengemas kini pengelompokan dengan cekap untuk memenuhi kekangan baru dan lama daripada mengumpulkan semula keseluruhan kumpulan data. Kami menunjukkan bahawa masalah pengelompokan tambahan di bawah kekangan adalah NP-keras pada umumnya, tetapi mengenal pasti beberapa keadaan yang mencukupi yang menyebabkan versi yang dapat diselesaikan dengan cekap. Ini diterjemahkan ke dalam sekumpulan peraturan mengenai jenis kekangan yang dapat ditambahkan dan sifat sekatan yang mesti dijaga. Kami menunjukkan bahawa pendekatan ini lebih cekap daripada menyusun semula keseluruhan kumpulan data dan mempunyai beberapa kelebihan lain. [[EENNDD]] kekangan; kerumitan; pengelompokan"], [{"string": "Application of neural networks to biological data mining : a case study in protein sequence classification", "keywords": ["sequence alignment", "bioinformatics", "neural networks", "feature extraction from protein data", "biological data mining", "machine learning"], "combined": "Application of neural networks to biological data mining : a case study in protein sequence classification [[EENNDD]] sequence alignment; bioinformatics; neural networks; feature extraction from protein data; biological data mining; machine learning"}, "Penerapan rangkaian saraf untuk perlombongan data biologi: kajian kes dalam klasifikasi urutan protein [[EENNDD]] penjajaran urutan; bioinformatik; rangkaian saraf; pengekstrakan ciri dari data protein; perlombongan data biologi; pembelajaran mesin"], [{"string": "New EM derived from Kullback-Leibler divergence We introduce a new EM framework in which it is possible not only to optimize the model parameters but also the number of model components . A key feature of our approach is that we use nonparametric density estimation to improve parametric density estimation in the EM framework . While the classical EM algorithm estimates model parameters empirically using the data points themselves , we estimate them using nonparametric density estimates . There exist many possible applications that require optimal adjustment of model components . We present experimental results in two domains . One is polygonal approximation of laser range data , which is an active research topic in robot navigation . The other is grouping of edge pixels to contour boundaries , which still belongs to unsolved problems in computer vision .", "keywords": ["general", "em", "expectation maximization", "kullback-leibler divergence"], "combined": "New EM derived from Kullback-Leibler divergence We introduce a new EM framework in which it is possible not only to optimize the model parameters but also the number of model components . A key feature of our approach is that we use nonparametric density estimation to improve parametric density estimation in the EM framework . While the classical EM algorithm estimates model parameters empirically using the data points themselves , we estimate them using nonparametric density estimates . There exist many possible applications that require optimal adjustment of model components . We present experimental results in two domains . One is polygonal approximation of laser range data , which is an active research topic in robot navigation . The other is grouping of edge pixels to contour boundaries , which still belongs to unsolved problems in computer vision . [[EENNDD]] general; em; expectation maximization; kullback-leibler divergence"}, "EM baru berasal dari perbezaan Kullback-Leibler Kami memperkenalkan kerangka EM baru di mana mungkin bukan sahaja untuk mengoptimumkan parameter model tetapi juga jumlah komponen model. Ciri utama pendekatan kami adalah bahawa kami menggunakan anggaran ketumpatan bukan parametrik untuk meningkatkan anggaran ketumpatan parametrik dalam kerangka EM. Walaupun algoritma EM klasik menganggarkan parameter model secara empirik menggunakan titik data itu sendiri, kami menganggarkannya menggunakan anggaran ketumpatan bukan parametrik. Terdapat banyak kemungkinan aplikasi yang memerlukan penyesuaian komponen model secara optimum. Kami membentangkan hasil eksperimen dalam dua domain. Salah satunya ialah pendekatan data jarak jauh poligonal, yang merupakan topik penyelidikan aktif dalam navigasi robot. Yang lain adalah pengelompokan piksel tepi ke batas kontur, yang masih tergolong dalam masalah yang tidak dapat diselesaikan dalam penglihatan komputer. [[EENNDD]] umum; em; memaksimumkan jangkaan; perbezaan kullback-leibler"], [{"string": "Frequent regular itemset mining Concise representations of frequent itemsets sacrifice readability and direct interpretability by a data analyst of the concise patterns extracted . In this paper , we introduce an extension of itemsets , called regular , with an immediate semantics and interpretability , and a conciseness comparable to closed itemsets . Regular itemsets allow for specifying that an item may or may not be present ; that any subset of an itemset may be present ; and that any non-empty subset of an itemset may be present . We devise a procedure , called RegularMine , for mining a set of regular itemsets that is a concise representation of frequent itemsets . The procedure computes a covering , in terms of regular itemsets , of the frequent itemsets in the class of equivalence of a closed one . We report experimental results on several standard dense and sparse datasets that validate the proposed approach .", "keywords": ["closed and free itemsets", "concise representations"], "combined": "Frequent regular itemset mining Concise representations of frequent itemsets sacrifice readability and direct interpretability by a data analyst of the concise patterns extracted . In this paper , we introduce an extension of itemsets , called regular , with an immediate semantics and interpretability , and a conciseness comparable to closed itemsets . Regular itemsets allow for specifying that an item may or may not be present ; that any subset of an itemset may be present ; and that any non-empty subset of an itemset may be present . We devise a procedure , called RegularMine , for mining a set of regular itemsets that is a concise representation of frequent itemsets . The procedure computes a covering , in terms of regular itemsets , of the frequent itemsets in the class of equivalence of a closed one . We report experimental results on several standard dense and sparse datasets that validate the proposed approach . [[EENNDD]] closed and free itemsets; concise representations"}, "Perlombongan itemet biasa yang kerap Perwakilan ringkas dari itemets yang kerap mengorbankan kebolehbacaan dan kebolehtafsiran langsung oleh penganalisis data mengenai corak ringkas yang diekstrak. Dalam makalah ini, kami memperkenalkan perluasan kumpulan item, disebut biasa, dengan semantik dan tafsiran langsung, dan kesimpulan yang setanding dengan set barang tertutup. Set item biasa memungkinkan untuk menentukan bahawa item mungkin ada atau tidak; bahawa mana-mana subset dari set item mungkin ada; dan bahawa subset itemet yang tidak kosong mungkin ada. Kami merancang prosedur, yang disebut RegularMine, untuk melombong satu set itemets biasa yang merupakan representasi ringkas dari item yang kerap. Prosedur ini merangkumi penutup, dari segi set barang biasa, dari set barang yang kerap dalam kelas kesetaraan yang tertutup. Kami melaporkan hasil eksperimen pada beberapa set data padat dan jarang standard yang mengesahkan pendekatan yang dicadangkan. [[EENNDD]] set item tertutup dan percuma; perwakilan ringkas"], [{"string": "Assessing data mining results via swap randomization The problem of assessing the significance of data mining results on high-dimensional 0-1 data sets has been studied extensively in the literature . For problems such as mining frequent sets and finding correlations , significance testing can be done by , e.g. , chi-square tests , or many other methods . However , the results of such tests depend only on the specific attributes and not on the dataset as a whole . Moreover , the tests are more difficult to apply to sets of patterns or other complex results of data mining . In this paper , we consider a simple randomization technique that deals with this shortcoming . The approach consists of producing random datasets that have the same row and column margins with the given dataset , computing the results of interest on the randomized instances , and comparing them against the results on the actual data . This randomization technique can be used to assess the results of many different types of data mining algorithms , such as frequent sets , clustering , and rankings . To generate random datasets with given margins , we use variations of a Markov chain approach , which is based on a simple swap operation . We give theoretical results on the efficiency of different randomization methods , and apply the swap randomization method to several well-known datasets . Our results indicate that for some datasets the structure discovered by the data mining algorithms is a random artifact , while for other datasets the discovered structure conveys meaningful information .", "keywords": ["swaps", "significance testing", "randomization tests", "0-1 data"], "combined": "Assessing data mining results via swap randomization The problem of assessing the significance of data mining results on high-dimensional 0-1 data sets has been studied extensively in the literature . For problems such as mining frequent sets and finding correlations , significance testing can be done by , e.g. , chi-square tests , or many other methods . However , the results of such tests depend only on the specific attributes and not on the dataset as a whole . Moreover , the tests are more difficult to apply to sets of patterns or other complex results of data mining . In this paper , we consider a simple randomization technique that deals with this shortcoming . The approach consists of producing random datasets that have the same row and column margins with the given dataset , computing the results of interest on the randomized instances , and comparing them against the results on the actual data . This randomization technique can be used to assess the results of many different types of data mining algorithms , such as frequent sets , clustering , and rankings . To generate random datasets with given margins , we use variations of a Markov chain approach , which is based on a simple swap operation . We give theoretical results on the efficiency of different randomization methods , and apply the swap randomization method to several well-known datasets . Our results indicate that for some datasets the structure discovered by the data mining algorithms is a random artifact , while for other datasets the discovered structure conveys meaningful information . [[EENNDD]] swaps; significance testing; randomization tests; 0-1 data"}, "Menilai hasil perlombongan data melalui rawak swap Masalah menilai kepentingan hasil perlombongan data pada set data 0-1 dimensi tinggi telah dikaji secara meluas dalam literatur. Untuk masalah seperti perlombongan yang kerap berlaku dan mencari korelasi, ujian kepentingan boleh dilakukan dengan, mis. , ujian chi-square, atau banyak kaedah lain. Walau bagaimanapun, hasil ujian tersebut hanya bergantung pada atribut tertentu dan bukan pada set data secara keseluruhan. Lebih-lebih lagi, ujian lebih sukar diterapkan pada set corak atau hasil pelombongan data yang kompleks. Dalam makalah ini, kami mempertimbangkan teknik pengacakan sederhana yang menangani kekurangan ini. Pendekatan ini terdiri daripada menghasilkan kumpulan data rawak yang mempunyai margin baris dan lajur yang sama dengan set data yang diberikan, menghitung hasil minat pada kejadian rawak, dan membandingkannya dengan hasil pada data sebenar. Teknik pengacakan ini dapat digunakan untuk menilai hasil dari berbagai jenis algoritma perlombongan data, seperti set kerap, pengelompokan, dan peringkat. Untuk menghasilkan set data rawak dengan margin yang diberikan, kami menggunakan variasi pendekatan rantai Markov, yang berdasarkan operasi pertukaran sederhana. Kami memberikan hasil teori mengenai kecekapan kaedah pengacakan yang berbeza, dan menerapkan kaedah rawak pertukaran ke beberapa set data yang terkenal. Hasil kajian kami menunjukkan bahawa untuk beberapa kumpulan data, struktur yang ditemui oleh algoritma perlombongan data adalah artifak rawak, sementara untuk set data lain, struktur yang ditemui menyampaikan maklumat yang bermakna. [[EENNDD]] pertukaran; ujian kepentingan; ujian rawak; Data 0-1"], [{"string": "Data mining criteria for tree-based regression and classification This paper is concerned with the construction of regression and classification trees that are more adapted to data mining applications than conventional trees . To this end , we propose new splitting criteria for growing trees . Conventional splitting criteria attempt to perform well on both sides of a split by attempting a compromise in the quality of fit between the left and the right side . By contrast , we adopt a data mining point of view by proposing criteria that search for interesting subsets of the data , as opposed to modeling all of the data equally well . The new criteria do not split based on a compromise between the left and the right bucket ; they effectively pick the more interesting bucket and ignore the other . As expected , the result is often a simpler characterization of interesting subsets of the data . Less expected is that the new criteria often yield whole trees that provide more interpretable data descriptions . Surprisingly , it is a `` flaw '' that works to their advantage : The new criteria have an increased tendency to accept splits near the boundaries of the predictor ranges . This so-called `` end-cut problem '' leads to the repeated peeling of small layers of data and results in very unbalanced but highly expressive and interpretable trees .", "keywords": ["pima indians diabetes data", "cart", "boston housing data", "splitting criteria"], "combined": "Data mining criteria for tree-based regression and classification This paper is concerned with the construction of regression and classification trees that are more adapted to data mining applications than conventional trees . To this end , we propose new splitting criteria for growing trees . Conventional splitting criteria attempt to perform well on both sides of a split by attempting a compromise in the quality of fit between the left and the right side . By contrast , we adopt a data mining point of view by proposing criteria that search for interesting subsets of the data , as opposed to modeling all of the data equally well . The new criteria do not split based on a compromise between the left and the right bucket ; they effectively pick the more interesting bucket and ignore the other . As expected , the result is often a simpler characterization of interesting subsets of the data . Less expected is that the new criteria often yield whole trees that provide more interpretable data descriptions . Surprisingly , it is a `` flaw '' that works to their advantage : The new criteria have an increased tendency to accept splits near the boundaries of the predictor ranges . This so-called `` end-cut problem '' leads to the repeated peeling of small layers of data and results in very unbalanced but highly expressive and interpretable trees . [[EENNDD]] pima indians diabetes data; cart; boston housing data; splitting criteria"}, "Kriteria perlombongan data untuk regresi dan klasifikasi berasaskan pokok Makalah ini berkaitan dengan pembinaan pohon regresi dan klasifikasi yang lebih disesuaikan dengan aplikasi perlombongan data daripada pokok konvensional. Untuk tujuan ini, kami mencadangkan kriteria pemisahan baru untuk menanam pokok. Kriteria pemisahan konvensional berupaya menunjukkan prestasi yang baik di kedua-dua belah perpecahan dengan mencuba kompromi dalam kualiti muat antara sebelah kiri dan kanan. Sebaliknya, kami mengadopsi sudut pandang perlombongan data dengan mengusulkan kriteria yang mencari subset data yang menarik, dan bukannya memodelkan semua data dengan baik. Kriteria baru tidak berpecah berdasarkan kompromi antara baldi kiri dan kanan; mereka dengan berkesan memilih baldi yang lebih menarik dan mengabaikan yang lain. Seperti yang dijangkakan, hasilnya sering kali merupakan ciri yang lebih sederhana bagi subset data yang menarik. Kurang diharapkan adalah bahawa kriteria baru sering menghasilkan keseluruhan pokok yang memberikan keterangan data yang lebih dapat ditafsirkan. Anehnya, ini adalah \"cacat\" yang berfungsi untuk keuntungan mereka: Kriteria baru mempunyai kecenderungan yang meningkat untuk menerima perpecahan di dekat batas rentang ramalan. Masalah yang disebut \"end-cut\" ini menyebabkan pengelupasan berulang lapisan data kecil dan menghasilkan pokok yang sangat tidak seimbang tetapi sangat ekspresif dan dapat ditafsirkan. [[EENNDD]] data diabetes pima indians; troli; data perumahan boston; kriteria pemisahan"], [{"string": "Ranking-based classification of heterogeneous information networks It has been recently recognized that heterogeneous information networks composed of multiple types of nodes and links are prevalent in the real world . Both classification and ranking of the nodes or data objects in such networks are essential for network analysis . However , so far these approaches have generally been performed separately . In this paper , we combine ranking and classification in order to perform more accurate analysis of a heterogeneous information network . Our intuition is that highly ranked objects within a class should play more important roles in classification . On the other hand , class membership information is important for determining a quality ranking over a dataset . We believe it is therefore beneficial to integrate classification and ranking in a simultaneous , mutually enhancing process , and to this end , propose a novel ranking-based iterative classification framework , called RankClass . Specifically , we build a graph-based ranking model to iteratively compute the ranking distribution of the objects within each class . At each iteration , according to the current ranking results , the graph structure used in the ranking algorithm is adjusted so that the sub-network corresponding to the specific class is emphasized , while the rest of the network is weakened . As our experiments show , integrating ranking with classification not only generates more accurate classes than the state-of-art classification methods on networked data , but also provides meaningful ranking of objects within each class , serving as a more informative view of the data than traditional classification .", "keywords": ["ranking", "heterogeneous information network", "classification"], "combined": "Ranking-based classification of heterogeneous information networks It has been recently recognized that heterogeneous information networks composed of multiple types of nodes and links are prevalent in the real world . Both classification and ranking of the nodes or data objects in such networks are essential for network analysis . However , so far these approaches have generally been performed separately . In this paper , we combine ranking and classification in order to perform more accurate analysis of a heterogeneous information network . Our intuition is that highly ranked objects within a class should play more important roles in classification . On the other hand , class membership information is important for determining a quality ranking over a dataset . We believe it is therefore beneficial to integrate classification and ranking in a simultaneous , mutually enhancing process , and to this end , propose a novel ranking-based iterative classification framework , called RankClass . Specifically , we build a graph-based ranking model to iteratively compute the ranking distribution of the objects within each class . At each iteration , according to the current ranking results , the graph structure used in the ranking algorithm is adjusted so that the sub-network corresponding to the specific class is emphasized , while the rest of the network is weakened . As our experiments show , integrating ranking with classification not only generates more accurate classes than the state-of-art classification methods on networked data , but also provides meaningful ranking of objects within each class , serving as a more informative view of the data than traditional classification . [[EENNDD]] ranking; heterogeneous information network; classification"}, "Pengelasan berdasarkan rangkaian rangkaian maklumat heterogen Baru-baru ini diakui bahawa rangkaian maklumat heterogen yang terdiri daripada pelbagai jenis nod dan pautan terdapat di dunia nyata. Klasifikasi dan peringkat nod atau objek data dalam rangkaian sedemikian penting untuk analisis rangkaian. Namun, setakat ini pendekatan ini secara amnya dilakukan secara berasingan. Dalam makalah ini, kami menggabungkan peringkat dan klasifikasi untuk melakukan analisis yang lebih tepat mengenai rangkaian maklumat yang heterogen. Intuisi kami adalah bahawa objek yang diberi peringkat tinggi dalam kelas harus memainkan peranan yang lebih penting dalam klasifikasi. Sebaliknya, maklumat keahlian kelas adalah penting untuk menentukan kedudukan kualiti daripada set data. Oleh itu, kami percaya adalah menguntungkan untuk menggabungkan klasifikasi dan peringkat dalam proses serentak, saling meningkatkan, dan untuk tujuan ini, mencadangkan kerangka klasifikasi berulang berdasarkan peringkat baru, yang disebut RankClass. Secara khusus, kami membangun model peringkat berdasarkan grafik untuk mengira secara berulang pengagihan kedudukan objek dalam setiap kelas. Pada setiap iterasi, menurut hasil pemeringkatan terkini, struktur grafik yang digunakan dalam algoritma peringkat disesuaikan sehingga sub-jaringan yang sesuai dengan kelas tertentu ditekankan, sementara rangkaian yang lain lemah. Seperti yang ditunjukkan oleh eksperimen kami, mengintegrasikan peringkat dengan klasifikasi tidak hanya menghasilkan kelas yang lebih tepat daripada kaedah klasifikasi canggih pada data rangkaian, tetapi juga memberikan peringkat objek yang bermakna dalam setiap kelas, berfungsi sebagai pandangan data yang lebih informatif daripada tradisional pengelasan. [[EENNDD]] kedudukan; rangkaian maklumat heterogen; pengelasan"], [{"string": "Beyond heuristics : learning to classify vulnerabilities and predict exploits The security demands on modern system administration are enormous and getting worse . Chief among these demands , administrators must monitor the continual ongoing disclosure of software vulnerabilities that have the potential to compromise their systems in some way . Such vulnerabilities include buffer overflow errors , improperly validated inputs , and other unanticipated attack modalities . In 2008 , over 7,400 new vulnerabilities were disclosed -- well over 100 per week . While no enterprise is affected by all of these disclosures , administrators commonly face many outstanding vulnerabilities across the software systems they manage . Vulnerabilities can be addressed by patches , reconfigurations , and other workarounds ; however , these actions may incur down-time or unforeseen side-effects . Thus , a key question for systems administrators is which vulnerabilities to prioritize . From publicly available databases that document past vulnerabilities , we show how to train classifiers that predict whether and how soon a vulnerability is likely to be exploited . As input , our classifiers operate on high dimensional feature vectors that we extract from the text fields , time stamps , cross references , and other entries in existing vulnerability disclosure reports . Compared to current industry-standard heuristics based on expert knowledge and static formulas , our classifiers predict much more accurately whether and how soon individual vulnerabilities are likely to be exploited .", "keywords": ["exploits", "supervised learning", "svm", "security and protection", "vulnerabilities"], "combined": "Beyond heuristics : learning to classify vulnerabilities and predict exploits The security demands on modern system administration are enormous and getting worse . Chief among these demands , administrators must monitor the continual ongoing disclosure of software vulnerabilities that have the potential to compromise their systems in some way . Such vulnerabilities include buffer overflow errors , improperly validated inputs , and other unanticipated attack modalities . In 2008 , over 7,400 new vulnerabilities were disclosed -- well over 100 per week . While no enterprise is affected by all of these disclosures , administrators commonly face many outstanding vulnerabilities across the software systems they manage . Vulnerabilities can be addressed by patches , reconfigurations , and other workarounds ; however , these actions may incur down-time or unforeseen side-effects . Thus , a key question for systems administrators is which vulnerabilities to prioritize . From publicly available databases that document past vulnerabilities , we show how to train classifiers that predict whether and how soon a vulnerability is likely to be exploited . As input , our classifiers operate on high dimensional feature vectors that we extract from the text fields , time stamps , cross references , and other entries in existing vulnerability disclosure reports . Compared to current industry-standard heuristics based on expert knowledge and static formulas , our classifiers predict much more accurately whether and how soon individual vulnerabilities are likely to be exploited . [[EENNDD]] exploits; supervised learning; svm; security and protection; vulnerabilities"}, "Di luar heuristik: belajar untuk mengklasifikasikan kelemahan dan meramalkan eksploitasi Tuntutan keselamatan terhadap pentadbiran sistem moden sangat besar dan semakin buruk. Di antara tuntutan ini, pentadbir mesti memantau pendedahan berterusan mengenai kelemahan perisian yang berpotensi untuk menjejaskan sistem mereka dalam beberapa cara. Kerentanan seperti itu merangkumi ralat buffer overflow, input yang disahkan tidak betul, dan modaliti serangan lain yang tidak dijangka. Pada tahun 2008, lebih daripada 7.400 kelemahan baru diungkapkan - lebih dari 100 setiap minggu. Walaupun tidak ada perusahaan yang terpengaruh oleh semua pendedahan ini, pentadbir biasanya menghadapi banyak kelemahan yang luar biasa di seluruh sistem perisian yang mereka kelola. Kerentanan dapat diatasi dengan tambalan, konfigurasi ulang, dan penyelesaian lain; namun, tindakan ini boleh menyebabkan kesan sampingan atau kesan sampingan yang tidak dijangka. Oleh itu, persoalan utama bagi pentadbir sistem adalah kelemahan mana yang harus diutamakan. Dari pangkalan data yang tersedia untuk umum yang mendokumentasikan kerentanan masa lalu, kami menunjukkan cara melatih pengklasifikasi yang meramalkan apakah dan seberapa cepat kerentanan kemungkinan akan dieksploitasi. Sebagai input, pengklasifikasi kami beroperasi pada vektor ciri dimensi tinggi yang kami ekstrak dari medan teks, cap waktu, rujukan silang, dan entri lain dalam laporan pendedahan kerentanan yang ada. Berbanding dengan heuristik standard industri semasa berdasarkan pengetahuan pakar dan formula statik, pengklasifikasi kami meramalkan dengan lebih tepat sama ada dan seberapa cepat kelemahan individu mungkin dieksploitasi. [[EENNDD]] eksploitasi; pembelajaran yang diselia; svm; keselamatan dan perlindungan; kelemahan"], [{"string": "Bayesian networks for lossless dataset compression", "keywords": ["uncertainty, fuzzy, and probabilistic reasoning"], "combined": "Bayesian networks for lossless dataset compression [[EENNDD]] uncertainty, fuzzy, and probabilistic reasoning"}, "Rangkaian Bayesian untuk pemadatan set data tanpa kerugian [[EENNDD]] ketidakpastian, kabur, dan penalaran probabilistik"], [{"string": "Relational data pre-processing techniques for improved securities fraud detection Commercial datasets are often large , relational , and dynamic . They contain many records of people , places , things , events and their interactions over time . Such datasets are rarely structured appropriately for knowledge discovery , and they often contain variables whose meanings change across different subsets of the data . We describe how these challenges were addressed in a collaborative analysis project undertaken by the University of Massachusetts Amherst and the National Association of Securities Dealers NASD . We describe several methods for data pre-processing that we applied to transform a large , dynamic , and relational dataset describing nearly the entirety of the U.S. securities industry , and we show how these methods made the dataset suitable for learning statistical relational models . To better utilize social structure , we first applied known consolidation and link formation techniques to associate individuals with branch office locations . In addition , we developed an innovative technique to infer professional associations by exploiting dynamic employment histories . Finally , we applied normalization techniques to create a suitable class label that adjusts for spatial , temporal , and other heterogeneity within the data . We show how these pre-processing techniques combine to provide the necessary foundation for learning high-performing statistical models of fraudulent activity .", "keywords": ["normalization", "data pre-processing", "fraud detection", "relational probability trees", "statistical relational learning"], "combined": "Relational data pre-processing techniques for improved securities fraud detection Commercial datasets are often large , relational , and dynamic . They contain many records of people , places , things , events and their interactions over time . Such datasets are rarely structured appropriately for knowledge discovery , and they often contain variables whose meanings change across different subsets of the data . We describe how these challenges were addressed in a collaborative analysis project undertaken by the University of Massachusetts Amherst and the National Association of Securities Dealers NASD . We describe several methods for data pre-processing that we applied to transform a large , dynamic , and relational dataset describing nearly the entirety of the U.S. securities industry , and we show how these methods made the dataset suitable for learning statistical relational models . To better utilize social structure , we first applied known consolidation and link formation techniques to associate individuals with branch office locations . In addition , we developed an innovative technique to infer professional associations by exploiting dynamic employment histories . Finally , we applied normalization techniques to create a suitable class label that adjusts for spatial , temporal , and other heterogeneity within the data . We show how these pre-processing techniques combine to provide the necessary foundation for learning high-performing statistical models of fraudulent activity . [[EENNDD]] normalization; data pre-processing; fraud detection; relational probability trees; statistical relational learning"}, "Teknik pemprosesan data hubungan untuk pengesanan penipuan sekuriti yang lebih baik Set data komersial sering kali besar, relasional, dan dinamik. Mereka mengandungi banyak catatan orang, tempat, perkara, peristiwa dan interaksi mereka dari masa ke masa. Set data semacam itu jarang disusun dengan tepat untuk penemuan pengetahuan, dan seringkali mengandungi pemboleh ubah yang maknanya berubah merentasi subset data yang berbeza. Kami menerangkan bagaimana cabaran ini ditangani dalam projek analisis kolaboratif yang dijalankan oleh University of Massachusetts Amherst dan Persatuan Nasional Penjual Sekuriti NASD. Kami menerangkan beberapa kaedah untuk pra-pemprosesan data yang kami gunakan untuk mengubah kumpulan data yang besar, dinamik, dan relasional yang menggambarkan hampir keseluruhan industri sekuriti A.S., dan kami menunjukkan bagaimana kaedah ini menjadikan set data sesuai untuk mempelajari model hubungan statistik. Untuk memanfaatkan struktur sosial dengan lebih baik, kami mula-mula menggunakan teknik penyatuan dan pembentukan pautan yang diketahui untuk mengaitkan individu dengan lokasi pejabat cawangan. Di samping itu, kami mengembangkan teknik inovatif untuk menyimpulkan persatuan profesional dengan memanfaatkan sejarah pekerjaan yang dinamik. Akhirnya, kami mengaplikasikan teknik normalisasi untuk membuat label kelas yang sesuai yang menyesuaikan dengan spasial, temporal, dan heterogenitas lain dalam data. Kami menunjukkan bagaimana teknik pra-pemprosesan ini bergabung untuk menyediakan asas yang diperlukan untuk mempelajari model statistik berprestasi tinggi aktiviti penipuan. [[EENNDD]] normalisasi; pra-pemprosesan data; pengesanan penipuan; pokok kebarangkalian hubungan; pembelajaran hubungan statistik"], [{"string": "Quantification and semi-supervised classification methods for handling changes in class distribution In realistic settings the prevalence of a class may change after a classifier is induced and this will degrade the performance of the classifier . Further complicating this scenario is the fact that labeled data is often scarce and expensive . In this paper we address the problem where the class distribution changes and only unlabeled examples are available from the new distribution . We design and evaluate a number of methods for coping with this problem and compare the performance of these methods . Our quantification-based methods estimate the class distribution of the unlabeled data from the changed distribution and adjust the original classifier accordingly , while our semi-supervised methods build a new classifier using the examples from the new unlabeled distribution which are supplemented with predicted class values . We also introduce a hybrid method that utilizes both quantification and semi-supervised learning . All methods are evaluated using accuracy and F-measure on a set of benchmark data sets . Our results demonstrate that our methods yield substantial improvements in accuracy and F-measure .", "keywords": ["classification", "quantification", "concept drift", "class distribution", "semi-supervised learning"], "combined": "Quantification and semi-supervised classification methods for handling changes in class distribution In realistic settings the prevalence of a class may change after a classifier is induced and this will degrade the performance of the classifier . Further complicating this scenario is the fact that labeled data is often scarce and expensive . In this paper we address the problem where the class distribution changes and only unlabeled examples are available from the new distribution . We design and evaluate a number of methods for coping with this problem and compare the performance of these methods . Our quantification-based methods estimate the class distribution of the unlabeled data from the changed distribution and adjust the original classifier accordingly , while our semi-supervised methods build a new classifier using the examples from the new unlabeled distribution which are supplemented with predicted class values . We also introduce a hybrid method that utilizes both quantification and semi-supervised learning . All methods are evaluated using accuracy and F-measure on a set of benchmark data sets . Our results demonstrate that our methods yield substantial improvements in accuracy and F-measure . [[EENNDD]] classification; quantification; concept drift; class distribution; semi-supervised learning"}, "Kaedah pengkelasan dan pengkelasan separa penyeliaan untuk menangani perubahan dalam pembahagian kelas. Dalam tetapan yang realistik, kelaziman kelas boleh berubah setelah pengelas diinduksi dan ini akan menurunkan prestasi pengklasifikasi. Lebih rumit lagi senario ini adalah hakikat bahawa data berlabel jarang dan mahal. Dalam makalah ini kita mengatasi masalah di mana pengedaran kelas berubah dan hanya contoh yang tidak berlabel yang tersedia dari pengedaran baru. Kami merancang dan menilai sebilangan kaedah untuk mengatasi masalah ini dan membandingkan prestasi kaedah ini. Kaedah berdasarkan kuantifikasi kami menganggarkan pengedaran kelas data tanpa label dari taburan yang berubah dan menyesuaikan pengklasifikasi asal dengan sewajarnya, sementara kaedah separa penyeliaan kami membina pengklasifikasi baru menggunakan contoh dari sebaran tidak berlabel baru yang ditambah dengan nilai kelas yang diramalkan. Kami juga memperkenalkan kaedah hibrid yang menggunakan pembelajaran kuantifikasi dan separa penyeliaan. Semua kaedah dinilai menggunakan ketepatan dan ukuran-F pada satu set set data penanda aras. Hasil kajian kami menunjukkan bahawa kaedah kami memberikan peningkatan yang besar dalam ketepatan dan ukuran-F. [[EENNDD]] klasifikasi; pengukuran; drift konsep; pembahagian kelas; pembelajaran separa penyeliaan"], [{"string": "Structure and evolution of online social networks In this paper , we consider the evolution of structure within large online social networks . We present a series of measurements of two such networks , together comprising in excess of five million people and ten million friendship links , annotated with metadata capturing the time of every event in the life of the network . Our measurements expose a surprising segmentation of these networks into three regions : singletons who do not participate in the network ; isolated communities which overwhelmingly display star structure ; and a giant component anchored by a well-connected core region which persists even in the absence of stars . We present a simple model of network growth which captures these aspects of component structure . The model follows our experimental results , characterizing users as either passive members of the network ; inviters who encourage offline friends and acquaintances to migrate online ; and linkers who fully participate in the social evolution of the network .", "keywords": ["graph evolution", "graph mining", "social networks", "small-world phenomenon", "stars"], "combined": "Structure and evolution of online social networks In this paper , we consider the evolution of structure within large online social networks . We present a series of measurements of two such networks , together comprising in excess of five million people and ten million friendship links , annotated with metadata capturing the time of every event in the life of the network . Our measurements expose a surprising segmentation of these networks into three regions : singletons who do not participate in the network ; isolated communities which overwhelmingly display star structure ; and a giant component anchored by a well-connected core region which persists even in the absence of stars . We present a simple model of network growth which captures these aspects of component structure . The model follows our experimental results , characterizing users as either passive members of the network ; inviters who encourage offline friends and acquaintances to migrate online ; and linkers who fully participate in the social evolution of the network . [[EENNDD]] graph evolution; graph mining; social networks; small-world phenomenon; stars"}, "Struktur dan evolusi rangkaian sosial dalam talian Dalam makalah ini, kita mempertimbangkan evolusi struktur dalam rangkaian sosial dalam talian yang besar. Kami menyajikan satu siri pengukuran dua rangkaian tersebut, bersama-sama terdiri dari lebih dari lima juta orang dan sepuluh juta tautan persahabatan, dijelaskan dengan metadata yang mencatat waktu setiap peristiwa dalam kehidupan rangkaian. Pengukuran kami memperlihatkan pemisahan rangkaian yang mengejutkan ini menjadi tiga wilayah: orang tunggal yang tidak mengambil bahagian dalam rangkaian; komuniti terpencil yang sangat memaparkan struktur bintang; dan komponen gergasi berlabuh oleh rantau teras yang bersambung dengan baik yang berterusan walaupun tanpa bintang. Kami menyajikan model pertumbuhan rangkaian yang ringkas yang merangkumi aspek struktur komponen ini. Model ini mengikuti hasil eksperimen kami, yang mencirikan pengguna sebagai anggota rangkaian pasif; jemputan yang mendorong rakan dan kenalan luar talian untuk berhijrah dalam talian; dan penghubung yang mengambil bahagian sepenuhnya dalam evolusi sosial rangkaian. [[EENNDD]] evolusi grafik; perlombongan grafik; rangkaian sosial; fenomena dunia kecil; bintang"], [{"string": "Mining viewpoint patterns in image databases The increasing number of image repositories has made image mining an important task because of its potential in discovering useful image patterns from a large set of images . In this paper , we introduce the notion of viewpoint patterns for image databases . Viewpoint patterns refer to patterns that capture the invariant relationships of one object from the point of view of another object . These patterns are unique and significant in images because the absolute positional information of objects for most images is not important , but rather , it is the relative distance and orientation of the objects from each other that is meaningful . We design a scalable and efficient algorithm to discover such viewpoint patterns . Experiments results on various image sets demonstrate that viewpoint patterns are meaningful and interesting to human users .", "keywords": ["image database", "image mining", "database applications", "spatial relationship"], "combined": "Mining viewpoint patterns in image databases The increasing number of image repositories has made image mining an important task because of its potential in discovering useful image patterns from a large set of images . In this paper , we introduce the notion of viewpoint patterns for image databases . Viewpoint patterns refer to patterns that capture the invariant relationships of one object from the point of view of another object . These patterns are unique and significant in images because the absolute positional information of objects for most images is not important , but rather , it is the relative distance and orientation of the objects from each other that is meaningful . We design a scalable and efficient algorithm to discover such viewpoint patterns . Experiments results on various image sets demonstrate that viewpoint patterns are meaningful and interesting to human users . [[EENNDD]] image database; image mining; database applications; spatial relationship"}, "Melombong corak sudut pandang dalam pangkalan data gambar Peningkatan jumlah repositori gambar telah menjadikan perlombongan gambar menjadi tugas penting kerana potensinya dalam mencari corak gambar yang berguna dari sekumpulan besar gambar. Dalam makalah ini, kami memperkenalkan konsep pola sudut pandang untuk pangkalan data gambar. Corak sudut pandang merujuk kepada corak yang menangkap hubungan invarian satu objek dari sudut pandang objek lain. Corak ini unik dan signifikan dalam gambar kerana maklumat kedudukan objek yang mutlak bagi kebanyakan gambar tidak penting, tetapi sebaliknya, jarak relatif dan orientasi objek antara satu sama lain adalah bermakna. Kami merancang algoritma yang berskala dan efisien untuk menemui corak sudut pandang seperti itu. Hasil eksperimen pada pelbagai set gambar menunjukkan bahawa corak sudut pandang bermakna dan menarik bagi pengguna manusia. [[EENNDD]] pangkalan data gambar; perlombongan imej; aplikasi pangkalan data; hubungan spatial"], [{"string": "Privacy-preservation for gradient descent methods Gradient descent is a widely used paradigm for solving many optimization problems . Stochastic gradient descent performs a series of iterations to minimize a target function in order to reach a local minimum . In machine learning or data mining , this function corresponds to a decision model that is to be discovered . The gradient descent paradigm underlies many commonly used techniques in data mining and machine learning , such as neural networks , Bayesian networks , genetic algorithms , and simulated annealing . To the best of our knowledge , there has not been any work that extends the notion of privacy preservation or secure multi-party computation to gradient-descent-based techniques . In this paper , we propose a preliminary approach to enable privacy preservation in gradient descent methods in general and demonstrate its feasibility in specific gradient descent methods .", "keywords": ["secure multi-party computation", "gradient descent method", "privacy preservation", "regression"], "combined": "Privacy-preservation for gradient descent methods Gradient descent is a widely used paradigm for solving many optimization problems . Stochastic gradient descent performs a series of iterations to minimize a target function in order to reach a local minimum . In machine learning or data mining , this function corresponds to a decision model that is to be discovered . The gradient descent paradigm underlies many commonly used techniques in data mining and machine learning , such as neural networks , Bayesian networks , genetic algorithms , and simulated annealing . To the best of our knowledge , there has not been any work that extends the notion of privacy preservation or secure multi-party computation to gradient-descent-based techniques . In this paper , we propose a preliminary approach to enable privacy preservation in gradient descent methods in general and demonstrate its feasibility in specific gradient descent methods . [[EENNDD]] secure multi-party computation; gradient descent method; privacy preservation; regression"}, "Pemeliharaan privasi untuk kaedah penurunan gradien Keturunan gradien adalah paradigma yang banyak digunakan untuk menyelesaikan banyak masalah pengoptimuman. Keturunan kecerunan stokastik melakukan serangkaian iterasi untuk meminimumkan fungsi sasaran agar dapat mencapai minimum setempat. Dalam pembelajaran mesin atau perlombongan data, fungsi ini sesuai dengan model keputusan yang akan ditemui. Paradigma penurunan gradien mendasari banyak teknik yang biasa digunakan dalam perlombongan data dan pembelajaran mesin, seperti rangkaian neural, jaringan Bayesian, algoritma genetik, dan penyepuhlindapan simulasi. Sepengetahuan kami, belum ada karya yang memperluas konsep pemeliharaan privasi atau pengiraan multi-pihak yang selamat hingga teknik berdasarkan kecerunan. Dalam makalah ini, kami mengusulkan pendekatan awal untuk memungkinkan pemeliharaan privasi dalam metode keturunan gradien secara umum dan menunjukkan kelayakannya dalam metode keturunan gradien tertentu. [[EENNDD]] pengiraan pelbagai pihak yang selamat; kaedah keturunan kecerunan; pemeliharaan privasi; regresi"], [{"string": "Learning to rank networked entities Several algorithms have been proposed to learn to rank entities modeled as feature vectors , based on relevance feedback . However , these algorithms do not model network connections or relations between entities . Meanwhile , Pagerank and variants find the stationary distribution of a reasonable but arbitrary Markov walk over a network , but do not learn from relevance feedback . We present a framework for ranking networked entities based on Markov walks with parameterized conductance values associated with the network edges . We propose two flavors of conductance learning problems in our framework . In the first setting , relevance feedback comparing node-pairs hints that the user has one or more hidden preferred communities with large edge conductance , and the algorithm must discover these communities . We present a constrained maximum entropy network flow formulation whose dual can be solved efficiently using a cutting-plane approach and a quasi-Newton optimizer . In the second setting , edges have types , and relevance feedback hints that each edge type has a potentially different conductance , but this is fixed across the whole network . Our algorithm learns the conductances using an approximate Newton method .", "keywords": ["conductance matrix", "network flow", "pagerank", "maximum entropy"], "combined": "Learning to rank networked entities Several algorithms have been proposed to learn to rank entities modeled as feature vectors , based on relevance feedback . However , these algorithms do not model network connections or relations between entities . Meanwhile , Pagerank and variants find the stationary distribution of a reasonable but arbitrary Markov walk over a network , but do not learn from relevance feedback . We present a framework for ranking networked entities based on Markov walks with parameterized conductance values associated with the network edges . We propose two flavors of conductance learning problems in our framework . In the first setting , relevance feedback comparing node-pairs hints that the user has one or more hidden preferred communities with large edge conductance , and the algorithm must discover these communities . We present a constrained maximum entropy network flow formulation whose dual can be solved efficiently using a cutting-plane approach and a quasi-Newton optimizer . In the second setting , edges have types , and relevance feedback hints that each edge type has a potentially different conductance , but this is fixed across the whole network . Our algorithm learns the conductances using an approximate Newton method . [[EENNDD]] conductance matrix; network flow; pagerank; maximum entropy"}, "Belajar memberi peringkat entiti rangkaian Beberapa algoritma telah dicadangkan untuk belajar memberi peringkat entiti yang dimodelkan sebagai vektor ciri, berdasarkan maklum balas yang relevan. Walau bagaimanapun, algoritma ini tidak memodelkan hubungan rangkaian atau hubungan antara entiti. Sementara itu, Pagerank dan varian mendapati penyebaran pegun dari Markov yang wajar tetapi sewenang-wenangnya berjalan melalui rangkaian, tetapi tidak belajar dari maklum balas yang relevan. Kami menyajikan kerangka kerja untuk peringkat entiti jaringan berdasarkan Markov berjalan dengan nilai konduktor parameter yang berkaitan dengan tepi rangkaian. Kami mencadangkan dua rasa masalah pembelajaran konduktansi dalam kerangka kerja kami. Pada tetapan pertama, maklum balas relevan yang membandingkan pasangan simpul mengisyaratkan bahawa pengguna mempunyai satu atau lebih komuniti pilihan tersembunyi dengan kekonduksian tepi besar, dan algoritma mesti menemui komuniti ini. Kami menyajikan formulasi aliran jaringan entropi maksimum yang terkendali yang dualnya dapat diselesaikan secara efisien dengan menggunakan pendekatan pemotongan dan pengoptimasi kuasi-Newton. Dalam tetapan kedua, tepi mempunyai jenis, dan petunjuk maklum balas relevansi bahawa setiap jenis tepi mempunyai kekonduksian yang berpotensi berbeza, tetapi ini diperbaiki di seluruh rangkaian. Algoritma kami mempelajari kekonduksian menggunakan kaedah Newton perkiraan. [[EENNDD]] matriks konduktansi; aliran rangkaian; pagerank; entropi maksimum"], [{"string": "Mining concept-drifting data streams using ensemble classifiers Recently , mining data streams with concept drifts for actionable insights has become an important and challenging task for a wide range of applications including credit card fraud protection , target marketing , network intrusion detection , etc. . Conventional knowledge discovery tools are facing two challenges , the overwhelming volume of the streaming data , and the concept drifts . In this paper , we propose a general framework for mining concept-drifting data streams using weighted ensemble classifiers . We train an ensemble of classification models , such as C4 .5 , RIPPER , naive Beyesian , etc. , from sequential chunks of the data stream . The classifiers in the ensemble are judiciously weighted based on their expected classification accuracy on the test data under the time-evolving environment . Thus , the ensemble approach improves both the efficiency in learning the model and the accuracy in performing classification . Our empirical study shows that the proposed methods have substantial advantage over single-classifier approaches in prediction accuracy , and the ensemble framework is effective for a variety of classification models .", "keywords": ["data streams", "classifier", "classifier ensemble", "concept drift"], "combined": "Mining concept-drifting data streams using ensemble classifiers Recently , mining data streams with concept drifts for actionable insights has become an important and challenging task for a wide range of applications including credit card fraud protection , target marketing , network intrusion detection , etc. . Conventional knowledge discovery tools are facing two challenges , the overwhelming volume of the streaming data , and the concept drifts . In this paper , we propose a general framework for mining concept-drifting data streams using weighted ensemble classifiers . We train an ensemble of classification models , such as C4 .5 , RIPPER , naive Beyesian , etc. , from sequential chunks of the data stream . The classifiers in the ensemble are judiciously weighted based on their expected classification accuracy on the test data under the time-evolving environment . Thus , the ensemble approach improves both the efficiency in learning the model and the accuracy in performing classification . Our empirical study shows that the proposed methods have substantial advantage over single-classifier approaches in prediction accuracy , and the ensemble framework is effective for a variety of classification models . [[EENNDD]] data streams; classifier; classifier ensemble; concept drift"}, "Perlombongan aliran data aliran konsep menggunakan pengkelasan ensemble Baru-baru ini, aliran data perlombongan dengan drift konsep untuk pandangan yang dapat ditindaklanjuti menjadi tugas penting dan mencabar untuk pelbagai aplikasi termasuk perlindungan penipuan kad kredit, pemasaran sasaran, pengesanan pencerobohan rangkaian, dll. Alat penemuan pengetahuan konvensional menghadapi dua cabaran, jumlah aliran data yang banyak, dan konsepnya bergerak. Dalam makalah ini, kami mencadangkan kerangka umum untuk aliran data yang mengalihkan konsep perlombongan menggunakan pengklasifikasi ensemble berwajaran. Kami melatih kumpulan model klasifikasi, seperti C4 .5, RIPPER, Beyesian naif, dan lain-lain, dari potongan aliran data yang berurutan. Pengelasan dalam ensemble diberi wajaran berdasarkan ketepatan klasifikasi yang diharapkan pada data ujian di bawah persekitaran yang berkembang mengikut masa. Oleh itu, pendekatan ensemble meningkatkan kecekapan dalam mempelajari model dan ketepatan dalam melakukan klasifikasi. Kajian empirikal kami menunjukkan bahawa kaedah yang dicadangkan mempunyai kelebihan yang besar berbanding pendekatan pengkelasan tunggal dalam ketepatan ramalan, dan kerangka ensemble berkesan untuk pelbagai model klasifikasi. [[EENNDD]] aliran data; pengelas; ensembel pengkelasan; drift konsep"], [{"string": "k-NN as an implementation of situation testing for discrimination discovery and prevention With the support of the legally-grounded methodology of situation testing , we tackle the problems of discrimination discovery and prevention from a dataset of historical decisions by adopting a variant of k-NN classification . A tuple is labeled as discriminated if we can observe a significant difference of treatment among its neighbors belonging to a protected-by-law group and its neighbors not belonging to it . Discrimination discovery boils down to extracting a classification model from the labeled tuples . Discrimination prevention is tackled by changing the decision value for tuples labeled as discriminated before training a classifier . The approach of this paper overcomes legal weaknesses and technical limitations of existing proposals .", "keywords": ["discrimination discovery and prevention", "k-nn classification"], "combined": "k-NN as an implementation of situation testing for discrimination discovery and prevention With the support of the legally-grounded methodology of situation testing , we tackle the problems of discrimination discovery and prevention from a dataset of historical decisions by adopting a variant of k-NN classification . A tuple is labeled as discriminated if we can observe a significant difference of treatment among its neighbors belonging to a protected-by-law group and its neighbors not belonging to it . Discrimination discovery boils down to extracting a classification model from the labeled tuples . Discrimination prevention is tackled by changing the decision value for tuples labeled as discriminated before training a classifier . The approach of this paper overcomes legal weaknesses and technical limitations of existing proposals . [[EENNDD]] discrimination discovery and prevention; k-nn classification"}, "k-NN sebagai implementasi pengujian situasi untuk penemuan dan pencegahan diskriminasi Dengan dukungan metodologi pengujian situasi berdasarkan hukum, kami menangani masalah penemuan diskriminasi dan pencegahan dari kumpulan data keputusan sejarah dengan mengadopsi varian k-NN pengelasan. Tuple dilabel sebagai diskriminasi jika kita dapat melihat perbezaan perlakuan yang ketara di antara jirannya yang tergolong dalam kumpulan undang-undang yang dilindungi dan jirannya yang bukan miliknya. Penemuan diskriminasi bermula dengan mengekstrak model klasifikasi dari tupel berlabel. Pencegahan diskriminasi ditangani dengan mengubah nilai keputusan untuk tupel yang dilabel sebagai diskriminasi sebelum melatih pengkelas. Pendekatan makalah ini mengatasi kelemahan undang-undang dan batasan teknikal dari cadangan yang ada. [[EENNDD]] penemuan dan pencegahan diskriminasi; klasifikasi k-nn"], [{"string": "Named entity mining from click-through data using weakly supervised latent dirichlet allocation This paper addresses Named Entity Mining NEM , in which we mine knowledge about named entities such as movies , games , and books from a huge amount of data . NEM is potentially useful in many applications including web search , online advertisement , and recommender system . There are three challenges for the task : finding suitable data source , coping with the ambiguities of named entity classes , and incorporating necessary human supervision into the mining process . This paper proposes conducting NEM by using click-through data collected at a web search engine , employing a topic model that generates the click-through data , and learning the topic model by weak supervision from humans . Specifically , it characterizes each named entity by its associated queries and URLs in the click-through data . It uses the topic model to resolve ambiguities of named entity classes by representing the classes as topics . It employs a method , referred to as Weakly Supervised Latent Dirichlet Allocation WS-LDA , to accurately learn the topic model with partially labeled named entities . Experiments on a large scale click-through data containing over 1.5 billion query-URL pairs show that the proposed approach can conduct very accurate NEM and significantly outperforms the baseline .", "keywords": ["topic model", "web mining", "search log mining", "named entity recognition"], "combined": "Named entity mining from click-through data using weakly supervised latent dirichlet allocation This paper addresses Named Entity Mining NEM , in which we mine knowledge about named entities such as movies , games , and books from a huge amount of data . NEM is potentially useful in many applications including web search , online advertisement , and recommender system . There are three challenges for the task : finding suitable data source , coping with the ambiguities of named entity classes , and incorporating necessary human supervision into the mining process . This paper proposes conducting NEM by using click-through data collected at a web search engine , employing a topic model that generates the click-through data , and learning the topic model by weak supervision from humans . Specifically , it characterizes each named entity by its associated queries and URLs in the click-through data . It uses the topic model to resolve ambiguities of named entity classes by representing the classes as topics . It employs a method , referred to as Weakly Supervised Latent Dirichlet Allocation WS-LDA , to accurately learn the topic model with partially labeled named entities . Experiments on a large scale click-through data containing over 1.5 billion query-URL pairs show that the proposed approach can conduct very accurate NEM and significantly outperforms the baseline . [[EENNDD]] topic model; web mining; search log mining; named entity recognition"}, "Perlombongan entiti bernama dari data klik-tayang dengan menggunakan peruntukan laten laten yang diawasi dengan lemah. Makalah ini membahas NEM Entiti Perlombongan Dinamakan, di mana kami menimba pengetahuan mengenai entiti bernama seperti filem, permainan, dan buku dari sejumlah besar data. NEM berpotensi berguna dalam banyak aplikasi termasuk carian web, iklan dalam talian, dan sistem cadangan. Terdapat tiga cabaran untuk tugas tersebut: mencari sumber data yang sesuai, mengatasi kekaburan kelas entiti bernama, dan memasukkan pengawasan manusia yang diperlukan ke dalam proses penambangan. Makalah ini mencadangkan melakukan NEM dengan menggunakan data klik-tayang yang dikumpulkan di mesin carian web, menggunakan model topik yang menghasilkan data klik-tayang, dan mempelajari model topik dengan pengawasan yang lemah dari manusia. Secara khusus, ia mencirikan setiap entiti bernama dengan pertanyaan dan URL yang berkaitan dalam data klik-tayang. Ia menggunakan model topik untuk menyelesaikan kesamaran kelas entiti bernama dengan mewakili kelas sebagai topik. Ini menggunakan metode, yang disebut sebagai Alokasi Dirichlet Allent WS-LDA yang Dilindungi Lemah, untuk mempelajari model topik dengan tepat dengan entitas bernama yang dilabelkan. Eksperimen pada data klik-tayang berskala besar yang mengandungi lebih daripada 1.5 bilion pasangan pertanyaan-URL menunjukkan bahawa pendekatan yang dicadangkan dapat melakukan NEM yang sangat tepat dan jauh melebihi tahap awal. [[EENNDD]] model topik; perlombongan web; perlombongan log carian; bernama pengiktirafan entiti"], [{"string": "Event summarization for system management In system management applications , an overwhelming amount of data are generated and collected in the form of temporal events . While mining temporal event data to discover interesting and frequent patterns has obtained rapidly increasing research efforts , users of the applications are overwhelmed by the mining results . The extracted patterns are generally of large volume and hard to interpret , they may be of no emphasis , intricate and meaningless to non-experts , even to domain experts . While traditional research efforts focus on finding interesting patterns , in this paper , we take a novel approach called event summarization towards the understanding of the seemingly chaotic temporal data . Event summarization aims at providing a concise interpretation of the seemingly chaotic data , so that domain experts may take actions upon the summarized models . Event summarization decomposes the temporal information into many independent subsets and finds well fitted models to describe each subset .", "keywords": ["ern", "temporal dependency", "log", "event summarization"], "combined": "Event summarization for system management In system management applications , an overwhelming amount of data are generated and collected in the form of temporal events . While mining temporal event data to discover interesting and frequent patterns has obtained rapidly increasing research efforts , users of the applications are overwhelmed by the mining results . The extracted patterns are generally of large volume and hard to interpret , they may be of no emphasis , intricate and meaningless to non-experts , even to domain experts . While traditional research efforts focus on finding interesting patterns , in this paper , we take a novel approach called event summarization towards the understanding of the seemingly chaotic temporal data . Event summarization aims at providing a concise interpretation of the seemingly chaotic data , so that domain experts may take actions upon the summarized models . Event summarization decomposes the temporal information into many independent subsets and finds well fitted models to describe each subset . [[EENNDD]] ern; temporal dependency; log; event summarization"}, "Ringkasan peristiwa untuk pengurusan sistem Dalam aplikasi pengurusan sistem, sejumlah besar data dihasilkan dan dikumpulkan dalam bentuk peristiwa temporal. Walaupun data peristiwa temporal perlombongan untuk mencari corak menarik dan kerap telah memperoleh usaha penyelidikan yang semakin meningkat, pengguna aplikasi terbeban dengan hasil perlombongan. Corak yang diekstrak umumnya bervolume besar dan sukar ditafsirkan, mereka mungkin tidak memberi penekanan, rumit dan tidak bermakna kepada bukan pakar, bahkan untuk pakar domain. Walaupun usaha penyelidikan tradisional menumpukan pada mencari corak yang menarik, dalam makalah ini, kami mengambil pendekatan baru yang disebut ringkasan peristiwa ke arah pemahaman data temporal yang kelihatan kacau. Ringkasan acara bertujuan memberikan tafsiran ringkas mengenai data yang nampaknya huru-hara, sehingga pakar domain dapat mengambil tindakan berdasarkan model yang diringkaskan. Ringkasan peristiwa menguraikan maklumat temporal menjadi banyak subset bebas dan menemui model yang sesuai untuk menerangkan setiap subset. [[EENNDD]] ern; pergantungan sementara; log; ringkasan acara"], [{"string": "B-EM : a classifier incorporating bootstrap with EM approach for data mining This paper investigates the problem of augmenting labeled data with unlabeled data to improve classification accuracy . This is significant for many applications such as image classification where obtaining classification labels is expensive , while large unlabeled examples are easily available . We investigate an Expectation Maximization EM algorithm for learning from labeled and unlabeled data . The reason why unlabeled data boosts learning accuracy is because it provides the information about the joint probability distribution . A theoretical argument shows that the more unlabeled examples are combined in learning , the more accurate the result . We then introduce B-EM algorithm , based on the combination of EM with bootstrap method , to exploit the large unlabeled data while avoiding prohibitive I\\/O cost . Experimental results over both synthetic and real data sets that the proposed approach has a satisfactory performance .", "keywords": ["classification", "supervised and unsupervised learning", "miscellaneous", "bootstrap method", "expectation maximization"], "combined": "B-EM : a classifier incorporating bootstrap with EM approach for data mining This paper investigates the problem of augmenting labeled data with unlabeled data to improve classification accuracy . This is significant for many applications such as image classification where obtaining classification labels is expensive , while large unlabeled examples are easily available . We investigate an Expectation Maximization EM algorithm for learning from labeled and unlabeled data . The reason why unlabeled data boosts learning accuracy is because it provides the information about the joint probability distribution . A theoretical argument shows that the more unlabeled examples are combined in learning , the more accurate the result . We then introduce B-EM algorithm , based on the combination of EM with bootstrap method , to exploit the large unlabeled data while avoiding prohibitive I\\/O cost . Experimental results over both synthetic and real data sets that the proposed approach has a satisfactory performance . [[EENNDD]] classification; supervised and unsupervised learning; miscellaneous; bootstrap method; expectation maximization"}, "B-EM: pengkelasan yang menggabungkan bootstrap dengan pendekatan EM untuk perlombongan data Kertas ini menyiasat masalah peningkatan data berlabel dengan data tidak berlabel untuk meningkatkan ketepatan klasifikasi. Ini penting bagi banyak aplikasi seperti klasifikasi gambar di mana mendapatkan label klasifikasi mahal, sementara contoh yang tidak berlabel besar mudah didapati. Kami menyiasat algoritma EM Pengharapan Harapan untuk belajar dari data berlabel dan tidak berlabel. Sebab mengapa data tidak berlabel meningkatkan ketepatan pembelajaran adalah kerana ia memberikan maklumat mengenai taburan kebarangkalian bersama. Hujah teori menunjukkan bahawa semakin banyak contoh yang tidak berlabel digabungkan dalam pembelajaran, semakin tepat hasilnya. Kami kemudian memperkenalkan algoritma B-EM, berdasarkan gabungan EM dengan kaedah bootstrap, untuk mengeksploitasi data tanpa label yang besar sambil mengelakkan kos I \\ / O yang melarang. Hasil eksperimen ke atas kedua-dua kumpulan data sintetik dan nyata bahawa pendekatan yang dicadangkan mempunyai prestasi yang memuaskan. [[EENNDD]] klasifikasi; pembelajaran yang diselia dan tidak diawasi; pelbagai; kaedah bootstrap; memaksimumkan jangkaan"], [{"string": "Online novelty detection on temporal sequences In this paper , we present a new framework for online novelty detection on temporal sequences . This framework include a mechanism for associating each detection result with a confidence value . Based on this framework , we develop a concrete online detection algorithm , by modeling the temporal sequence using an online support vector regression algorithm . Experiments on both synthetic and real world data are performed to demonstrate the promising performance of our proposed detection algorithm .", "keywords": ["online algorithm", "anomaly detection", "design methodology", "support vector regression", "novelty detection"], "combined": "Online novelty detection on temporal sequences In this paper , we present a new framework for online novelty detection on temporal sequences . This framework include a mechanism for associating each detection result with a confidence value . Based on this framework , we develop a concrete online detection algorithm , by modeling the temporal sequence using an online support vector regression algorithm . Experiments on both synthetic and real world data are performed to demonstrate the promising performance of our proposed detection algorithm . [[EENNDD]] online algorithm; anomaly detection; design methodology; support vector regression; novelty detection"}, "Pengesanan kebaruan dalam talian pada urutan temporal Dalam makalah ini, kami memaparkan kerangka baru untuk pengesanan kebaruan dalam talian mengenai urutan temporal. Kerangka ini merangkumi mekanisme untuk mengaitkan setiap hasil pengesanan dengan nilai keyakinan. Berdasarkan kerangka ini, kami mengembangkan algoritma pengesanan dalam talian konkrit, dengan memodelkan urutan temporal menggunakan algoritma regresi vektor sokongan dalam talian. Eksperimen pada data sintetik dan dunia nyata dilakukan untuk menunjukkan prestasi yang menjanjikan dari algoritma pengesanan yang kami cadangkan. [[EENNDD]] algoritma dalam talian; pengesanan anomali; metodologi reka bentuk; menyokong regresi vektor; pengesanan kebaruan"], [{"string": "Frequent subgraph mining in outerplanar graphs In recent years there has been an increased interest in algorithms that can perform frequent pattern discovery in large databases of graph structured objects . While the frequent connected subgraph mining problem for tree datasets can be solved in incremental polynomial time , it becomes intractable for arbitrary graph databases . Existing approaches have therefore resorted to various heuristic strategies and restrictions of the search space , but have not identified a practically relevant tractable graph class beyond trees . In this paper , we define the class of so called tenuous outerplanar graphs , a strict generalization of trees , develop a frequent subgraph mining algorithm for tenuous outerplanar graphs that works in incremental polynomial time , and evaluate the algorithm empirically on the NCI molecular graph dataset .", "keywords": ["frequent pattern discovery", "computational chemistry", "graph mining"], "combined": "Frequent subgraph mining in outerplanar graphs In recent years there has been an increased interest in algorithms that can perform frequent pattern discovery in large databases of graph structured objects . While the frequent connected subgraph mining problem for tree datasets can be solved in incremental polynomial time , it becomes intractable for arbitrary graph databases . Existing approaches have therefore resorted to various heuristic strategies and restrictions of the search space , but have not identified a practically relevant tractable graph class beyond trees . In this paper , we define the class of so called tenuous outerplanar graphs , a strict generalization of trees , develop a frequent subgraph mining algorithm for tenuous outerplanar graphs that works in incremental polynomial time , and evaluate the algorithm empirically on the NCI molecular graph dataset . [[EENNDD]] frequent pattern discovery; computational chemistry; graph mining"}, "Perlombongan subgraf yang kerap berlaku dalam grafik luar rencana Dalam beberapa tahun kebelakangan ini terdapat peningkatan minat terhadap algoritma yang dapat melakukan penemuan corak yang kerap di pangkalan data besar objek berstruktur grafik. Walaupun masalah penambangan subgraf yang sering dihubungkan untuk set data pokok dapat diselesaikan dalam masa tambahan polinomial, ia menjadi sukar untuk pangkalan data grafik sewenang-wenangnya. Oleh itu, pendekatan yang ada telah menggunakan pelbagai strategi heuristik dan sekatan ruang carian, tetapi belum mengenal pasti kelas grafik yang praktikal relevan di luar pokok. Dalam makalah ini, kami mendefinisikan kelas grafik luar yang lemah, generalisasi pokok yang ketat, mengembangkan algoritma perlombongan subgraf yang kerap untuk graf luar yang lemah yang berfungsi dalam masa tambahan polinomial, dan menilai algoritma secara empirik pada dataset grafik molekul NCI. [[EENNDD]] penemuan corak yang kerap; kimia pengiraan; perlombongan grafik"], [{"string": "IncSpan : incremental mining of sequential patterns in large database Many real life sequence databases grow incrementally . It is undesirable to mine sequential patterns from scratch each time when a small set of sequences grow , or when some new sequences are added into the database . Incremental algorithm should be developed for sequential pattern mining so that mining can be adapted to incremental database updates . However , it is nontrivial to mine sequential patterns incrementally , especially when the existing sequences grow incrementally because such growth may lead to the generation of many new patterns due to the interactions of the growing subsequences with the original ones . In this study , we develop an efficient algorithm , IncSpan , for incremental mining of sequential patterns , by exploring some interesting properties . Our performance study shows that IncSpan outperforms some previously proposed incremental algorithms as well as a non-incremental one with a wide margin .", "keywords": ["incremental mining", "buffering pattern", "shared projection", "reverse pattern matching"], "combined": "IncSpan : incremental mining of sequential patterns in large database Many real life sequence databases grow incrementally . It is undesirable to mine sequential patterns from scratch each time when a small set of sequences grow , or when some new sequences are added into the database . Incremental algorithm should be developed for sequential pattern mining so that mining can be adapted to incremental database updates . However , it is nontrivial to mine sequential patterns incrementally , especially when the existing sequences grow incrementally because such growth may lead to the generation of many new patterns due to the interactions of the growing subsequences with the original ones . In this study , we develop an efficient algorithm , IncSpan , for incremental mining of sequential patterns , by exploring some interesting properties . Our performance study shows that IncSpan outperforms some previously proposed incremental algorithms as well as a non-incremental one with a wide margin . [[EENNDD]] incremental mining; buffering pattern; shared projection; reverse pattern matching"}, "IncSpan: perlombongan penambahan corak jujukan dalam pangkalan data besar Banyak pangkalan data urutan kehidupan sebenar bertambah secara bertahap. Adalah tidak diinginkan untuk menambang corak urutan dari awal setiap kali sekumpulan urutan kecil tumbuh, atau ketika beberapa urutan baru ditambahkan ke dalam pangkalan data. Algoritma inkremental harus dikembangkan untuk perlombongan pola berurutan supaya perlombongan dapat disesuaikan dengan kemas kini pangkalan data tambahan. Walau bagaimanapun, adalah tidak wajar untuk menambang corak urutan secara bertahap, terutamanya apabila urutan yang ada tumbuh secara bertahap kerana pertumbuhan tersebut dapat menyebabkan generasi banyak corak baru kerana interaksi pertumbuhan yang seterusnya dengan yang asli. Dalam kajian ini, kami mengembangkan algoritma yang cekap, IncSpan, untuk penambangan bertahap corak urutan, dengan meneroka beberapa sifat menarik. Kajian prestasi kami menunjukkan bahawa IncSpan mengungguli beberapa algoritma kenaikan yang dicadangkan sebelumnya dan juga yang bukan kenaikan dengan margin yang luas. [[EENNDD]] perlombongan tambahan; corak penyangga; unjuran bersama; padanan corak terbalik"], [{"string": "Experiments with random projections for machine learning Dimensionality reduction via Random Projections has attracted considerable attention in recent years . The approach has interesting theoretical underpinnings and offers computational advantages . In this paper we report a number of experiments to evaluate Random Projections in the context of inductive supervised learning . In particular , we compare Random Projections and PCA on a number of different datasets and using different machine learning methods . While we find that the random projection approach predictively underperforms PCA , its computational advantages may make it attractive for certain applications .", "keywords": ["pattern recognition", "dimensionality reduction", "random projection", "probability and statistics"], "combined": "Experiments with random projections for machine learning Dimensionality reduction via Random Projections has attracted considerable attention in recent years . The approach has interesting theoretical underpinnings and offers computational advantages . In this paper we report a number of experiments to evaluate Random Projections in the context of inductive supervised learning . In particular , we compare Random Projections and PCA on a number of different datasets and using different machine learning methods . While we find that the random projection approach predictively underperforms PCA , its computational advantages may make it attractive for certain applications . [[EENNDD]] pattern recognition; dimensionality reduction; random projection; probability and statistics"}, "Eksperimen dengan unjuran rawak untuk pembelajaran mesin Pengurangan dimensi melalui Unjuran Rawak telah menarik perhatian besar dalam beberapa tahun terakhir. Pendekatan ini mempunyai asas teori yang menarik dan menawarkan kelebihan komputasi. Dalam makalah ini kami melaporkan sejumlah eksperimen untuk menilai Unjuran Rawak dalam konteks pembelajaran diawasi induktif. Secara khusus, kami membandingkan Unjuran Rawak dan PCA pada sejumlah set data yang berbeza dan menggunakan kaedah pembelajaran mesin yang berbeza. Walaupun kami mendapati bahawa pendekatan unjuran rawak secara prediktif kurang menghasilkan PCA, kelebihan komputasinya mungkin menjadikannya menarik untuk aplikasi tertentu. [[EENNDD]] pengecaman corak; pengurangan dimensi; unjuran rawak; kebarangkalian dan statistik"], [{"string": "Playing hide-and-seek with correlations We present a method for very high-dimensional correlation analysis . The method relies equally on rigorous search strategies and on human interaction . At each step , the method conservatively `` shaves off '' a fraction of the database tuples and attributes , so that most of the correlations present in the data are not affected by the decomposition . Instead , the correlations become more obvious to the user , because they are hidden in a much smaller portion of the database . This process can be repeated iteratively and interactively , until only the most important correlations remain . The main technical difficulty of the approach is figuring out how to `` shave off '' part of the database so as to preserve most correlations . We develop an algorithm for this problem that has a polynomial running time and guarantees result quality .", "keywords": ["data mining", "association rules", "database applications", "minimum cut", "correlations"], "combined": "Playing hide-and-seek with correlations We present a method for very high-dimensional correlation analysis . The method relies equally on rigorous search strategies and on human interaction . At each step , the method conservatively `` shaves off '' a fraction of the database tuples and attributes , so that most of the correlations present in the data are not affected by the decomposition . Instead , the correlations become more obvious to the user , because they are hidden in a much smaller portion of the database . This process can be repeated iteratively and interactively , until only the most important correlations remain . The main technical difficulty of the approach is figuring out how to `` shave off '' part of the database so as to preserve most correlations . We develop an algorithm for this problem that has a polynomial running time and guarantees result quality . [[EENNDD]] data mining; association rules; database applications; minimum cut; correlations"}, "Bermain persembunyian dengan korelasi Kami menyajikan kaedah untuk analisis korelasi dimensi yang sangat tinggi. Kaedah ini bergantung pada strategi pencarian yang ketat dan interaksi manusia. Pada setiap langkah, metode ini secara konservatif \"menghilangkan\" sebagian kecil tupel dan atribut pangkalan data, sehingga sebagian besar korelasi yang ada dalam data tidak terpengaruh oleh penguraian. Sebaliknya, korelasi menjadi lebih jelas bagi pengguna, kerana ia tersembunyi di bahagian pangkalan data yang jauh lebih kecil. Proses ini dapat diulang secara berulang dan interaktif, sehingga hanya terdapat korelasi yang paling penting. Kesukaran teknikal utama pendekatan adalah mencari cara untuk \"mencukur\" sebahagian pangkalan data sehingga dapat mengekalkan kebanyakan korelasi. Kami mengembangkan algoritma untuk masalah ini yang mempunyai masa berjalan polinomial dan menjamin kualiti hasil. [[EENNDD]] perlombongan data; peraturan persatuan; aplikasi pangkalan data; pemotongan minimum; korelasi"], [{"string": "Modeling relationships at multiple scales to improve accuracy of large recommender systems The collaborative filtering approach to recommender systems predicts user preferences for products or services by learning past user-item relationships . In this work , we propose novel algorithms for predicting user ratings of items by integrating complementary models that focus on patterns at different scales . At a local scale , we use a neighborhood-based technique that infers ratings from observed ratings by similar users or of similar items . Unlike previous local approaches , our method is based on a formal model that accounts for interactions within the neighborhood , leading to improved estimation quality . At a higher , regional , scale , we use SVD-like matrix factorization for recovering the major structural patterns in the user-item rating matrix . Unlike previous approaches that require imputations in order to fill in the unknown matrix entries , our new iterative algorithm avoids imputation . Because the models involve estimation of millions , or even billions , of parameters , shrinkage of estimated values to account for sampling variability proves crucial to prevent overfitting . Both the local and the regional approaches , and in particular their combination through a unifying model , compare favorably with other approaches and deliver substantially better results than the commercial Netflix Cinematch recommender system on a large publicly available data set .", "keywords": ["collaborative filtering", "netflix prize", "recommender systems"], "combined": "Modeling relationships at multiple scales to improve accuracy of large recommender systems The collaborative filtering approach to recommender systems predicts user preferences for products or services by learning past user-item relationships . In this work , we propose novel algorithms for predicting user ratings of items by integrating complementary models that focus on patterns at different scales . At a local scale , we use a neighborhood-based technique that infers ratings from observed ratings by similar users or of similar items . Unlike previous local approaches , our method is based on a formal model that accounts for interactions within the neighborhood , leading to improved estimation quality . At a higher , regional , scale , we use SVD-like matrix factorization for recovering the major structural patterns in the user-item rating matrix . Unlike previous approaches that require imputations in order to fill in the unknown matrix entries , our new iterative algorithm avoids imputation . Because the models involve estimation of millions , or even billions , of parameters , shrinkage of estimated values to account for sampling variability proves crucial to prevent overfitting . Both the local and the regional approaches , and in particular their combination through a unifying model , compare favorably with other approaches and deliver substantially better results than the commercial Netflix Cinematch recommender system on a large publicly available data set . [[EENNDD]] collaborative filtering; netflix prize; recommender systems"}, "Memodelkan hubungan pada pelbagai skala untuk meningkatkan ketepatan sistem pengesyorkan yang besar Pendekatan penapisan kolaboratif untuk sistem pengesyorkan meramalkan pilihan pengguna untuk produk atau perkhidmatan dengan mempelajari hubungan item pengguna lalu. Dalam karya ini, kami mencadangkan algoritma baru untuk meramalkan penilaian pengguna item dengan mengintegrasikan model pelengkap yang memfokuskan pada corak pada skala yang berbeza. Pada skala tempatan, kami menggunakan teknik berasaskan kejiranan yang mengganggu penilaian dari pemerhatian yang diperhatikan oleh pengguna serupa atau item serupa. Tidak seperti pendekatan tempatan sebelumnya, kaedah kami didasarkan pada model formal yang memperhitungkan interaksi dalam lingkungan, yang membawa kepada peningkatan kualiti anggaran. Pada skala yang lebih tinggi, serantau, kami menggunakan faktorisasi matriks seperti SVD untuk memulihkan corak struktur utama dalam matriks penilaian item pengguna. Tidak seperti pendekatan sebelumnya yang memerlukan imputasi untuk mengisi entri matriks yang tidak diketahui, algoritma berulang baru kami mengelakkan penyangkalan. Kerana model melibatkan anggaran berjuta-juta, atau bahkan berbilion-bilion, parameter, penyusutan nilai-nilai yang dianggarkan untuk memperhitungkan kebolehubahan sampel terbukti penting untuk mencegah overfitting. Kedua-dua pendekatan tempatan dan serantau, dan khususnya kombinasi mereka melalui model penyatuan, membandingkan dengan pendekatan lain dan memberikan hasil yang jauh lebih baik daripada sistem pengesyorkan Netflix Cinematch komersial pada satu set data yang tersedia untuk umum. [[EENNDD]] penapisan kolaboratif; hadiah netflix; sistem cadangan"], [{"string": "Consistent bipartite graph co-partitioning for star-structured high-order heterogeneous data co-clustering Heterogeneous data co-clustering has attracted more and more attention in recent years due to its high impact on various applications . While the co-clustering algorithms for two types of heterogeneous data denoted by pair-wise co-clustering , such as documents and terms , have been well studied in the literature , the work on more types of heterogeneous data denoted by high-order co-clustering is still very limited . As an attempt in this direction , in this paper , we worked on a specific case of high-order co-clustering in which there is a central type of objects that connects the other types so as to form a star structure of the inter-relationships . Actually , this case could be a very good abstract for many real-world applications , such as the co-clustering of categories , documents and terms in text mining . In our philosophy , we treated such kind of problems as the fusion of multiple pair-wise co-clustering sub-problems with the constraint of the star structure . Accordingly , we proposed the concept of consistent bipartite graph co-partitioning , and developed an algorithm based on semi-definite programming SDP for efficient computation of the clustering results . Experiments on toy problems and real data both verified the effectiveness of our proposed method .", "keywords": ["high-order heterogeneous data", "co-clustering", "spectral graph", "consistency"], "combined": "Consistent bipartite graph co-partitioning for star-structured high-order heterogeneous data co-clustering Heterogeneous data co-clustering has attracted more and more attention in recent years due to its high impact on various applications . While the co-clustering algorithms for two types of heterogeneous data denoted by pair-wise co-clustering , such as documents and terms , have been well studied in the literature , the work on more types of heterogeneous data denoted by high-order co-clustering is still very limited . As an attempt in this direction , in this paper , we worked on a specific case of high-order co-clustering in which there is a central type of objects that connects the other types so as to form a star structure of the inter-relationships . Actually , this case could be a very good abstract for many real-world applications , such as the co-clustering of categories , documents and terms in text mining . In our philosophy , we treated such kind of problems as the fusion of multiple pair-wise co-clustering sub-problems with the constraint of the star structure . Accordingly , we proposed the concept of consistent bipartite graph co-partitioning , and developed an algorithm based on semi-definite programming SDP for efficient computation of the clustering results . Experiments on toy problems and real data both verified the effectiveness of our proposed method . [[EENNDD]] high-order heterogeneous data; co-clustering; spectral graph; consistency"}, "Grafik bipartit yang konsisten bersama untuk pengkomputeran data heterogen pesanan tinggi berstruktur bintang yang disatukan bersama Pengumpulan data heterogen telah menarik perhatian semakin banyak dalam beberapa tahun kebelakangan ini kerana kesannya yang tinggi terhadap pelbagai aplikasi. Walaupun algoritma kluster bersama untuk dua jenis data heterogen yang dilambangkan oleh kluster gabungan pasangan, seperti dokumen dan istilah, telah dipelajari dengan baik dalam literatur, karya mengenai lebih banyak jenis data heterogen yang dilambangkan oleh koordinasi pesanan tinggi pengelompokan masih sangat terhad. Sebagai percubaan ke arah ini, dalam makalah ini, kami mengusahakan satu kes kluster pesanan tinggi yang spesifik di mana terdapat jenis objek pusat yang menghubungkan jenis-jenis lain sehingga dapat membentuk struktur bintang antara hubungan . Sebenarnya, kes ini boleh menjadi abstrak yang sangat baik untuk banyak aplikasi dunia nyata, seperti penggabungan kategori, dokumen dan istilah dalam penambangan teks. Dalam falsafah kami, kami menangani masalah seperti penyatuan sub-masalah penggabungan bersama pasangan dengan kekangan struktur bintang. Oleh itu, kami mencadangkan konsep pembahagian graf bipartit yang konsisten, dan mengembangkan algoritma berdasarkan SDP pengaturcaraan separa pasti untuk pengiraan hasil pengelompokan yang cekap. Eksperimen mengenai masalah mainan dan data sebenar mengesahkan keberkesanan kaedah cadangan kami. [[EENNDD]] data heterogen pesanan tinggi; penggabungan bersama; graf spektrum; ketekalan"], [{"string": "Time series shapelets : a new primitive for data mining Classification of time series has been attracting great interest over the past decade . Recent empirical evidence has strongly suggested that the simple nearest neighbor algorithm is very difficult to beat for most time series problems . While this may be considered good news , given the simplicity of implementing the nearest neighbor algorithm , there are some negative consequences of this . First , the nearest neighbor algorithm requires storing and searching the entire dataset , resulting in a time and space complexity that limits its applicability , especially on resource-limited sensors . Second , beyond mere classification accuracy , we often wish to gain some insight into the data . In this work we introduce a new time series primitive , time series shapelets , which addresses these limitations . Informally , shapelets are time series subsequences which are in some sense maximally representative of a class . As we shall show with extensive empirical evaluations in diverse domains , algorithms based on the time series shapelet primitives can be interpretable , more accurate and significantly faster than state-of-the-art classifiers .", "keywords": ["classification", "pattern extraction"], "combined": "Time series shapelets : a new primitive for data mining Classification of time series has been attracting great interest over the past decade . Recent empirical evidence has strongly suggested that the simple nearest neighbor algorithm is very difficult to beat for most time series problems . While this may be considered good news , given the simplicity of implementing the nearest neighbor algorithm , there are some negative consequences of this . First , the nearest neighbor algorithm requires storing and searching the entire dataset , resulting in a time and space complexity that limits its applicability , especially on resource-limited sensors . Second , beyond mere classification accuracy , we often wish to gain some insight into the data . In this work we introduce a new time series primitive , time series shapelets , which addresses these limitations . Informally , shapelets are time series subsequences which are in some sense maximally representative of a class . As we shall show with extensive empirical evaluations in diverse domains , algorithms based on the time series shapelet primitives can be interpretable , more accurate and significantly faster than state-of-the-art classifiers . [[EENNDD]] classification; pattern extraction"}, "Shapelet siri masa: primitif baru untuk perlombongan data Klasifikasi siri masa telah menarik minat yang besar sejak dekad yang lalu. Bukti empirikal baru-baru ini menunjukkan bahawa algoritma jiran terdekat mudah sangat sukar untuk dikalahkan untuk kebanyakan masalah siri masa. Walaupun ini boleh dianggap berita baik, memandangkan kesederhanaan dalam menerapkan algoritma jiran terdekat, ada beberapa akibat negatif dari ini. Pertama, algoritma jiran terdekat memerlukan menyimpan dan mencari keseluruhan set data, menghasilkan kerumitan masa dan ruang yang membatasi penerapannya, terutama pada sensor terhad sumber. Kedua, di luar ketepatan klasifikasi semata-mata, kami sering ingin mendapatkan gambaran mengenai data. Dalam karya ini kami memperkenalkan kapel siri masa baru, siri masa, yang menangani batasan ini. Secara tidak rasmi, shapelets adalah susunan siri masa yang dalam arti paling maksimum mewakili kelas. Seperti yang akan kita tunjukkan dengan penilaian empirikal yang luas dalam pelbagai domain, algoritma berdasarkan primitif shapelet siri masa dapat ditafsirkan, lebih tepat dan jauh lebih cepat daripada pengklasifikasi canggih. [[EENNDD]] klasifikasi; pengekstrakan corak"], [{"string": "Get another label ? improving data quality and data mining using multiple , noisy labelers This paper addresses the repeated acquisition of labels for data items when the labeling is imperfect . We examine the improvement or lack thereof in data quality via repeated labeling , and focus especially on the improvement of training labels for supervised induction . With the outsourcing of small tasks becoming easier , for example via Rent-A-Coder or Amazon 's Mechanical Turk , it often is possible to obtain less-than-expert labeling at low cost . With low-cost labeling , preparing the unlabeled part of the data can become considerably more expensive than labeling . We present repeated-labeling strategies of increasing complexity , and show several main results . i Repeated-labeling can improve label quality and model quality , but not always . ii When labels are noisy , repeated labeling can be preferable to single labeling even in the traditional setting where labels are not particularly cheap . iii As soon as the cost of processing the unlabeled data is not free , even the simple strategy of labeling everything multiple times can give considerable advantage . iv Repeatedly labeling a carefully chosen set of points is generally preferable , and we present a robust technique that combines different notions of uncertainty to select data points for which quality should be improved . The bottom line : the results show clearly that when labeling is not perfect , selective acquisition of multiple labels is a strategy that data miners should have in their repertoire ; for certain label-quality\\/cost regimes , the benefit is substantial .", "keywords": ["data preprocessing", "data selection"], "combined": "Get another label ? improving data quality and data mining using multiple , noisy labelers This paper addresses the repeated acquisition of labels for data items when the labeling is imperfect . We examine the improvement or lack thereof in data quality via repeated labeling , and focus especially on the improvement of training labels for supervised induction . With the outsourcing of small tasks becoming easier , for example via Rent-A-Coder or Amazon 's Mechanical Turk , it often is possible to obtain less-than-expert labeling at low cost . With low-cost labeling , preparing the unlabeled part of the data can become considerably more expensive than labeling . We present repeated-labeling strategies of increasing complexity , and show several main results . i Repeated-labeling can improve label quality and model quality , but not always . ii When labels are noisy , repeated labeling can be preferable to single labeling even in the traditional setting where labels are not particularly cheap . iii As soon as the cost of processing the unlabeled data is not free , even the simple strategy of labeling everything multiple times can give considerable advantage . iv Repeatedly labeling a carefully chosen set of points is generally preferable , and we present a robust technique that combines different notions of uncertainty to select data points for which quality should be improved . The bottom line : the results show clearly that when labeling is not perfect , selective acquisition of multiple labels is a strategy that data miners should have in their repertoire ; for certain label-quality\\/cost regimes , the benefit is substantial . [[EENNDD]] data preprocessing; data selection"}, "Dapatkan label lain? meningkatkan kualiti data dan perlombongan data menggunakan pelabel berganda, bising Kertas ini membahas pemerolehan berulang label untuk item data apabila pelabelan tidak sempurna. Kami mengkaji peningkatan atau kekurangannya dalam kualiti data melalui pelabelan berulang, dan fokus terutama pada peningkatan label latihan untuk induksi yang diawasi. Dengan pengalihan keluar tugas-tugas kecil menjadi lebih mudah, misalnya melalui Rent-A-Coder atau Amazon's Mechanical Turk, seringkali mungkin untuk mendapatkan pelabelan yang kurang dari ahli dengan kos rendah. Dengan pelabelan kos rendah, penyediaan bahagian data yang tidak berlabel boleh menjadi jauh lebih mahal daripada pelabelan. Kami menyajikan strategi pelabelan berulang yang meningkatkan kerumitan, dan menunjukkan beberapa hasil utama. i Pelabelan berulang dapat meningkatkan kualiti label dan kualiti model, tetapi tidak selalu. ii Apabila label berisik, pelabelan berulang lebih baik daripada label tunggal walaupun dalam keadaan tradisional di mana label tidak terlalu murah. iii Sebaik sahaja kos memproses data yang tidak dilabel tidak percuma, strategi mudah melabelkan semuanya berkali-kali dapat memberikan kelebihan yang besar. iv Melabel berulang-ulang sekumpulan titik yang dipilih dengan teliti pada umumnya lebih disukai, dan kami menyajikan teknik yang kuat yang menggabungkan pengertian ketidakpastian yang berbeza untuk memilih titik data yang kualiti harus ditingkatkan. Intinya: hasilnya menunjukkan dengan jelas bahawa apabila pelabelan tidak sempurna, pemerolehan beberapa label selektif adalah strategi yang harus dimiliki pelombong data dalam repertoar mereka; untuk rejim kualiti label / kos tertentu, faedahnya sangat besar. [[EENNDD]] pemprosesan data; pemilihan data"], [{"string": "A distributed learning framework for heterogeneous data sources We present a probabilistic model-based framework for distributed learning that takes into account privacy restrictions and is applicable to scenarios where the different sites have diverse , possibly overlapping subsets of features . Our framework decouples data privacy issues from knowledge integration issues by requiring the individual sites to share only privacy-safe probabilistic models of the local data , which are then integrated to obtain a global probabilistic model based on the union of the features available at all the sites . We provide a mathematical formulation of the model integration problem using the maximum likelihood and maximum entropy principles and describe iterative algorithms that are guaranteed to converge to the optimal solution . For certain commonly occurring special cases involving hierarchically ordered feature sets or conditional independence , we obtain closed form solutions and use these to propose an efficient alternative scheme by recursive decomposition of the model integration problem . To address interpretability concerns , we also present a modified formulation where the global model is assumed to belong to a specified parametric family . Finally , to highlight the generality of our framework , we provide empirical results for various learning tasks such as clustering and classification on different kinds of datasets consisting of continuous vector , categorical and directional attributes . The results show that high quality global models can be obtained without much loss of privacy .", "keywords": ["probabilistic models", "privacy", "distributed learning", "learning", "heterogeneous data sources"], "combined": "A distributed learning framework for heterogeneous data sources We present a probabilistic model-based framework for distributed learning that takes into account privacy restrictions and is applicable to scenarios where the different sites have diverse , possibly overlapping subsets of features . Our framework decouples data privacy issues from knowledge integration issues by requiring the individual sites to share only privacy-safe probabilistic models of the local data , which are then integrated to obtain a global probabilistic model based on the union of the features available at all the sites . We provide a mathematical formulation of the model integration problem using the maximum likelihood and maximum entropy principles and describe iterative algorithms that are guaranteed to converge to the optimal solution . For certain commonly occurring special cases involving hierarchically ordered feature sets or conditional independence , we obtain closed form solutions and use these to propose an efficient alternative scheme by recursive decomposition of the model integration problem . To address interpretability concerns , we also present a modified formulation where the global model is assumed to belong to a specified parametric family . Finally , to highlight the generality of our framework , we provide empirical results for various learning tasks such as clustering and classification on different kinds of datasets consisting of continuous vector , categorical and directional attributes . The results show that high quality global models can be obtained without much loss of privacy . [[EENNDD]] probabilistic models; privacy; distributed learning; learning; heterogeneous data sources"}, "Rangka kerja pembelajaran yang diedarkan untuk sumber data yang heterogen Kami menyajikan kerangka berdasarkan model probabilistik untuk pembelajaran yang diedarkan yang mengambil kira sekatan privasi dan berlaku untuk senario di mana laman web yang berlainan mempunyai sekumpulan ciri yang mungkin beragam, mungkin tumpang tindih. Rangka kerja kami menguraikan masalah privasi data dari masalah integrasi pengetahuan dengan mewajibkan setiap laman web hanya berkongsi model probabilistik yang selamat dari privasi data tempatan, yang kemudian disatukan untuk mendapatkan model probabilistik global berdasarkan penyatuan ciri yang tersedia di semua laman web . Kami memberikan rumusan matematik mengenai masalah integrasi model menggunakan prinsip kemungkinan besar dan prinsip entropi maksimum dan menerangkan algoritma berulang yang dijamin dapat bergabung ke penyelesaian yang optimum. Untuk kes khas yang sering berlaku yang melibatkan set ciri yang diperintahkan secara hierarki atau kebebasan bersyarat, kami memperoleh penyelesaian bentuk tertutup dan menggunakannya untuk mencadangkan skema alternatif yang efisien dengan penguraian berulang dari masalah integrasi model. Untuk mengatasi masalah kebolehtafsiran, kami juga menyajikan formulasi yang diubah suai di mana model global dianggap milik keluarga parametrik yang ditentukan. Akhirnya, untuk menyoroti kegunaan kerangka kerja kami, kami memberikan hasil empirikal untuk pelbagai tugas pembelajaran seperti pengelompokan dan klasifikasi pada berbagai jenis set data yang terdiri daripada atribut vektor, kategori dan arah yang berterusan. Hasilnya menunjukkan bahawa model global berkualiti tinggi dapat diperoleh tanpa kehilangan privasi. [[EENNDD]] model probabilistik; privasi; pembelajaran yang diedarkan; belajar; sumber data yang heterogen"], [{"string": "From run-time behavior to usage scenarios : an interaction-pattern mining approach A key challenge facing IT organizations today is their evolution towards adopting e-business practices that gives rise to the need for reengineering their underlying software systems . Any reengineering effort has to be aware of the functional requirements of the subject system , in order not to violate the integrity of its intended uses . However , as software systems get regularly maintained throughout their lifecycle , the documentation of their requirements often become obsolete or get lost . To address this problem of `` software requirements loss '' , we have developed an interaction-pattern mining method for the recovery of functional requirements as usage scenarios . Our method analyzes traces of the run-time system-user interaction to discover frequently recurring patterns ; these patterns correspond to the functionality currently exercised by the system users , represented as usage scenarios . The discovered scenarios provide the basis for reengineering the software system into web-accessible components , each one supporting one of the discovered scenarios . In this paper , we describe IPM2 , our interaction-pattern discovery algorithm , we illustrate it with a case study from a real application and we give an overview of the reengineering process in the context of which it is employed .", "keywords": ["interaction reengineering", "software engineering", "usage scenarios", "sequential pattern mining", "software requirements recovery", "run-time behavior analysis"], "combined": "From run-time behavior to usage scenarios : an interaction-pattern mining approach A key challenge facing IT organizations today is their evolution towards adopting e-business practices that gives rise to the need for reengineering their underlying software systems . Any reengineering effort has to be aware of the functional requirements of the subject system , in order not to violate the integrity of its intended uses . However , as software systems get regularly maintained throughout their lifecycle , the documentation of their requirements often become obsolete or get lost . To address this problem of `` software requirements loss '' , we have developed an interaction-pattern mining method for the recovery of functional requirements as usage scenarios . Our method analyzes traces of the run-time system-user interaction to discover frequently recurring patterns ; these patterns correspond to the functionality currently exercised by the system users , represented as usage scenarios . The discovered scenarios provide the basis for reengineering the software system into web-accessible components , each one supporting one of the discovered scenarios . In this paper , we describe IPM2 , our interaction-pattern discovery algorithm , we illustrate it with a case study from a real application and we give an overview of the reengineering process in the context of which it is employed . [[EENNDD]] interaction reengineering; software engineering; usage scenarios; sequential pattern mining; software requirements recovery; run-time behavior analysis"}, "Dari tingkah laku jangka masa hingga senario penggunaan: pendekatan perlombongan corak interaksi Cabaran utama yang dihadapi organisasi IT saat ini adalah evolusi mereka ke arah menerapkan amalan e-perniagaan yang menimbulkan keperluan untuk merekayasa semula sistem perisian yang mendasarinya. Segala usaha rekayasa semula harus mengetahui persyaratan fungsional sistem subjek, agar tidak melanggar integritas penggunaan yang dimaksudkan. Namun, apabila sistem perisian dikekalkan secara berkala sepanjang kitaran hayatnya, dokumentasi keperluannya sering menjadi usang atau hilang. Untuk mengatasi masalah \"kehilangan keperluan perisian\" ini, kami telah mengembangkan kaedah perlombongan pola interaksi untuk pemulihan keperluan fungsional sebagai senario penggunaan. Kaedah kami menganalisis jejak interaksi pengguna-sistem jangka masa untuk mengetahui corak yang sering berulang; corak ini sesuai dengan fungsi yang sedang dilakukan oleh pengguna sistem, yang dinyatakan sebagai senario penggunaan. Senario yang ditemui menyediakan asas untuk merekayasa semula sistem perisian menjadi komponen yang dapat diakses di web, masing-masing menyokong salah satu senario yang ditemui. Dalam makalah ini, kami menerangkan IPM2, algoritma penemuan corak interaksi kami, kami menggambarkannya dengan kajian kes dari aplikasi sebenar dan kami memberikan gambaran keseluruhan mengenai proses rekayasa semula dalam konteks yang digunakan. [[EENNDD]] penjanaan semula interaksi; Kejuruteraan perisian; senario penggunaan; perlombongan corak jujukan; pemulihan keperluan perisian; analisis tingkah laku jangka masa"], [{"string": "Finding partial orders from unordered 0-1 data In applications such as paleontology and medical genetics the 0-1 data has an underlying unknown order the ages of the fossil sites , the locations of markers in the genome . The order might be total or partial : for example , two sites in different parts of the globe might be ecologically incomparable , or the ordering of certain markers might be different in different subgroups of the data . We consider the following problem . Given a table over a set of 0-1 variables , find a partial order for the rows minimizing a score function and being as specific as possible . The score function can be , e.g. , the number of changes from 1 to 0 in a column for paleontology or the likelihood of the marker sequence for genomic data . Our solution for this task first constructs small totally ordered fragments of the partial order , then finds good orientations for the fragments , and finally uses a simple and efficient heuristic method for finding a partial order that corresponds well with the collection of fragments . We describe the method , discuss its properties , and give empirical results on paleontological data demonstrating the usefulness of the method . In the application the use of the method highlighted some previously unknown properties of the data and pointed out probable errors in the data .", "keywords": ["partial order", "consecutive ones property", "hidden ordering"], "combined": "Finding partial orders from unordered 0-1 data In applications such as paleontology and medical genetics the 0-1 data has an underlying unknown order the ages of the fossil sites , the locations of markers in the genome . The order might be total or partial : for example , two sites in different parts of the globe might be ecologically incomparable , or the ordering of certain markers might be different in different subgroups of the data . We consider the following problem . Given a table over a set of 0-1 variables , find a partial order for the rows minimizing a score function and being as specific as possible . The score function can be , e.g. , the number of changes from 1 to 0 in a column for paleontology or the likelihood of the marker sequence for genomic data . Our solution for this task first constructs small totally ordered fragments of the partial order , then finds good orientations for the fragments , and finally uses a simple and efficient heuristic method for finding a partial order that corresponds well with the collection of fragments . We describe the method , discuss its properties , and give empirical results on paleontological data demonstrating the usefulness of the method . In the application the use of the method highlighted some previously unknown properties of the data and pointed out probable errors in the data . [[EENNDD]] partial order; consecutive ones property; hidden ordering"}, "Mencari pesanan separa dari data 0-1 yang tidak tersusun Dalam aplikasi seperti paleontologi dan genetik perubatan, data 0-1 mempunyai susunan yang tidak diketahui yang mendasari usia laman fosil, lokasi penanda dalam genom. Urutannya mungkin total atau separa: misalnya, dua laman web di berbagai belahan dunia mungkin tidak dapat dibandingkan secara ekologi, atau susunan penanda tertentu mungkin berbeza pada subkumpulan data yang berbeza. Kami mempertimbangkan masalah berikut. Diberikan jadual untuk sekumpulan pemboleh ubah 0-1, cari susunan parsial untuk baris yang meminimumkan fungsi skor dan seinci mungkin. Fungsi skor boleh, mis. , jumlah perubahan dari 1 hingga 0 dalam lajur untuk paleontologi atau kemungkinan urutan penanda untuk data genom. Penyelesaian kami untuk tugas ini terlebih dahulu membina fragmen kecil yang tertib lengkap dari susunan separa, kemudian mencari orientasi yang baik untuk serpihan, dan akhirnya menggunakan kaedah heuristik yang mudah dan cekap untuk mencari susunan separa yang sesuai dengan koleksi serpihan. Kami menerangkan kaedahnya, membincangkan sifatnya, dan memberikan hasil empirikal pada data paleontologi yang menunjukkan kegunaan kaedah tersebut. Dalam aplikasi tersebut, penggunaan metode tersebut menyoroti beberapa sifat data yang sebelumnya tidak diketahui dan menunjukkan kemungkinan kesalahan dalam data tersebut. [[EENNDD]] pesanan separa; harta yang berturut-turut; pesanan tersembunyi"], [{"string": "Feedback effects between similarity and social influence in online communities A fundamental open question in the analysis of social networks is to understand the interplay between similarity and social ties . People are similar to their neighbors in a social network for two distinct reasons : first , they grow to resemble their current friends due to social influence ; and second , they tend to form new links to others who are already like them , a process often termed selection by sociologists . While both factors are present in everyday social processes , they are in tension : social influence can push systems toward uniformity of behavior , while selection can lead to fragmentation . As such , it is important to understand the relative effects of these forces , and this has been a challenge due to the difficulty of isolating and quantifying them in real settings . We develop techniques for identifying and modeling the interactions between social influence and selection , using data from online communities where both social interaction and changes in behavior over time can be measured . We find clear feedback effects between the two factors , with rising similarity between two individuals serving , in aggregate , as an indicator of future interaction -- but with similarity then continuing to increase steadily , although at a slower rate , for long periods after initial interactions . We also consider the relative value of similarity and social influence in modeling future behavior . For instance , to predict the activities that an individual is likely to do next , is it more useful to know the current activities of their friends , or of the people most similar to them ?", "keywords": ["social networks", "social influence", "online communities"], "combined": "Feedback effects between similarity and social influence in online communities A fundamental open question in the analysis of social networks is to understand the interplay between similarity and social ties . People are similar to their neighbors in a social network for two distinct reasons : first , they grow to resemble their current friends due to social influence ; and second , they tend to form new links to others who are already like them , a process often termed selection by sociologists . While both factors are present in everyday social processes , they are in tension : social influence can push systems toward uniformity of behavior , while selection can lead to fragmentation . As such , it is important to understand the relative effects of these forces , and this has been a challenge due to the difficulty of isolating and quantifying them in real settings . We develop techniques for identifying and modeling the interactions between social influence and selection , using data from online communities where both social interaction and changes in behavior over time can be measured . We find clear feedback effects between the two factors , with rising similarity between two individuals serving , in aggregate , as an indicator of future interaction -- but with similarity then continuing to increase steadily , although at a slower rate , for long periods after initial interactions . We also consider the relative value of similarity and social influence in modeling future behavior . For instance , to predict the activities that an individual is likely to do next , is it more useful to know the current activities of their friends , or of the people most similar to them ? [[EENNDD]] social networks; social influence; online communities"}, "Kesan maklum balas antara kesamaan dan pengaruh sosial dalam komuniti dalam talian Satu persoalan terbuka yang mendasar dalam analisis rangkaian sosial adalah memahami interaksi antara kesamaan dan hubungan sosial. Orang-orang serupa dengan jiran mereka dalam rangkaian sosial kerana dua sebab yang berbeza: pertama, mereka tumbuh menyerupai rakan mereka sekarang kerana pengaruh sosial; dan kedua, mereka cenderung membentuk pautan baru kepada orang lain yang sudah seperti mereka, suatu proses yang sering disebut pemilihan oleh ahli sosiologi. Walaupun kedua-dua faktor tersebut terdapat dalam proses sosial sehari-hari, mereka berada dalam ketegangan: pengaruh sosial dapat mendorong sistem menuju keseragaman tingkah laku, sementara pemilihan dapat menyebabkan perpecahan. Oleh itu, penting untuk memahami kesan relatif dari kekuatan-kekuatan ini, dan ini telah menjadi cabaran kerana kesukaran untuk mengasingkan dan mengukurnya dalam keadaan sebenar. Kami mengembangkan teknik untuk mengenal pasti dan memodelkan interaksi antara pengaruh sosial dan pemilihan, menggunakan data dari komuniti dalam talian di mana interaksi sosial dan perubahan tingkah laku dari masa ke masa dapat diukur. Kami mendapati kesan maklum balas yang jelas antara kedua-dua faktor tersebut, dengan peningkatan kesamaan antara dua individu yang berkhidmat, secara agregat, sebagai petunjuk interaksi masa depan - tetapi dengan kesamaan kemudian terus meningkat dengan stabil, walaupun pada kadar yang lebih perlahan, untuk jangka masa yang lama setelah interaksi awal . Kami juga mempertimbangkan nilai relatif kesamaan dan pengaruh sosial dalam memodelkan tingkah laku masa depan. Sebagai contoh, untuk meramalkan aktiviti yang mungkin dilakukan oleh individu seterusnya, adakah lebih berguna untuk mengetahui aktiviti semasa rakan mereka, atau orang yang paling serupa dengan mereka? [[EENNDD]] rangkaian sosial; pengaruh sosial; komuniti dalam talian"], [{"string": "Statistical change detection for multi-dimensional data This paper deals with detecting change of distribution in multi-dimensional data sets . For a given baseline data set and a set of newly observed data points , we define a statistical test called the density test for deciding if the observed data points are sampled from the underlying distribution that produced the baseline data set . We define a test statistic that is strictly distribution-free under the null hypothesis . Our experimental results show that the density test has substantially more power than the two existing methods for multi-dimensional change detection .", "keywords": ["density test", "kernel density estimation", "change detection", "database applications"], "combined": "Statistical change detection for multi-dimensional data This paper deals with detecting change of distribution in multi-dimensional data sets . For a given baseline data set and a set of newly observed data points , we define a statistical test called the density test for deciding if the observed data points are sampled from the underlying distribution that produced the baseline data set . We define a test statistic that is strictly distribution-free under the null hypothesis . Our experimental results show that the density test has substantially more power than the two existing methods for multi-dimensional change detection . [[EENNDD]] density test; kernel density estimation; change detection; database applications"}, "Pengesanan perubahan statistik untuk data pelbagai dimensi Makalah ini berkaitan dengan mengesan perubahan taburan dalam set data pelbagai dimensi. Untuk satu set data asas dan satu set titik data yang baru diperhatikan, kami menentukan ujian statistik yang disebut ujian kepadatan untuk menentukan sama ada titik data yang diperhatikan diambil sampel dari taburan asas yang menghasilkan kumpulan data asas. Kami menentukan statistik ujian yang bebas pengedaran berdasarkan hipotesis nol. Hasil eksperimen kami menunjukkan bahawa ujian ketumpatan mempunyai kekuatan yang jauh lebih besar daripada dua kaedah yang ada untuk pengesanan perubahan multi-dimensi. [[EENNDD]] ujian ketumpatan; anggaran ketumpatan kernel; pengesanan perubahan; aplikasi pangkalan data"], [{"string": "Transforming classifier scores into accurate multiclass probability estimates Class membership probability estimates are important for many applications of data mining in which classification outputs are combined with other sources of information for decision-making , such as example-dependent misclassification costs , the outputs of other classifiers , or domain knowledge . Previous calibration methods apply only to two-class problems . Here , we show how to obtain accurate probability estimates for multiclass problems by combining calibrated binary probability estimates . We also propose a new method for obtaining calibrated two-class probability estimates that can be applied to any classifier that produces a ranking of examples . Using naive Bayes and support vector machine classifiers , we give experimental results from a variety of two-class and multiclass domains , including direct marketing , text categorization and digit recognition .", "keywords": ["probabilistic algorithms"], "combined": "Transforming classifier scores into accurate multiclass probability estimates Class membership probability estimates are important for many applications of data mining in which classification outputs are combined with other sources of information for decision-making , such as example-dependent misclassification costs , the outputs of other classifiers , or domain knowledge . Previous calibration methods apply only to two-class problems . Here , we show how to obtain accurate probability estimates for multiclass problems by combining calibrated binary probability estimates . We also propose a new method for obtaining calibrated two-class probability estimates that can be applied to any classifier that produces a ranking of examples . Using naive Bayes and support vector machine classifiers , we give experimental results from a variety of two-class and multiclass domains , including direct marketing , text categorization and digit recognition . [[EENNDD]] probabilistic algorithms"}, "Mengubah skor pengkelasan menjadi anggaran kebarangkalian multikelas yang tepat Anggaran kebarangkalian keahlian kelas adalah penting untuk banyak aplikasi perlombongan data di mana output klasifikasi digabungkan dengan sumber maklumat lain untuk membuat keputusan, seperti kos misklasifikasi bergantung pada contoh, output dari pengkelasan lain, atau pengetahuan domain. Kaedah penentukuran sebelumnya hanya berlaku untuk masalah dua kelas. Di sini, kami menunjukkan cara mendapatkan anggaran kebarangkalian yang tepat untuk masalah multikelas dengan menggabungkan anggaran kebarangkalian binari yang dikalibrasi. Kami juga mencadangkan kaedah baru untuk mendapatkan taksiran kebarangkalian dua kelas yang dapat dikalibrasi yang dapat digunakan untuk mana-mana pengklasifikasi yang menghasilkan pemeringkatan contoh. Dengan menggunakan Bayes yang naif dan menyokong pengelasan mesin vektor, kami memberikan hasil eksperimen dari pelbagai domain kelas dua dan multikelas, termasuk pemasaran langsung, pengkategorian teks dan pengenalan digit. [[EENNDD]] algoritma probabilistik"], [{"string": "On community outliers and their efficient detection in information networks Linked or networked data are ubiquitous in many applications . Examples include web data or hypertext documents connected via hyperlinks , social networks or user profiles connected via friend links , co-authorship and citation information , blog data , movie reviews and so on . In these datasets called `` information networks '' , closely related objects that share the same properties or interests form a community . For example , a community in blogsphere could be users mostly interested in cell phone reviews and news . Outlier detection in information networks can reveal important anomalous and interesting behaviors that are not obvious if community information is ignored . An example could be a low-income person being friends with many rich people even though his income is not anomalously low when considered over the entire population . This paper first introduces the concept of community outliers interesting points or rising stars for a more positive sense , and then shows that well-known baseline approaches without considering links or community information can not find these community outliers . We propose an efficient solution by modeling networked data as a mixture model composed of multiple normal communities and a set of randomly generated outliers . The probabilistic model characterizes both data and links simultaneously by defining their joint distribution based on hidden Markov random fields HMRF . Maximizing the data likelihood and the posterior of the model gives the solution to the outlier inference problem . We apply the model on both synthetic data and DBLP data sets , and the results demonstrate importance of this concept , as well as the effectiveness and efficiency of the proposed approach .", "keywords": ["outlier detection", "information networks", "community discovery"], "combined": "On community outliers and their efficient detection in information networks Linked or networked data are ubiquitous in many applications . Examples include web data or hypertext documents connected via hyperlinks , social networks or user profiles connected via friend links , co-authorship and citation information , blog data , movie reviews and so on . In these datasets called `` information networks '' , closely related objects that share the same properties or interests form a community . For example , a community in blogsphere could be users mostly interested in cell phone reviews and news . Outlier detection in information networks can reveal important anomalous and interesting behaviors that are not obvious if community information is ignored . An example could be a low-income person being friends with many rich people even though his income is not anomalously low when considered over the entire population . This paper first introduces the concept of community outliers interesting points or rising stars for a more positive sense , and then shows that well-known baseline approaches without considering links or community information can not find these community outliers . We propose an efficient solution by modeling networked data as a mixture model composed of multiple normal communities and a set of randomly generated outliers . The probabilistic model characterizes both data and links simultaneously by defining their joint distribution based on hidden Markov random fields HMRF . Maximizing the data likelihood and the posterior of the model gives the solution to the outlier inference problem . We apply the model on both synthetic data and DBLP data sets , and the results demonstrate importance of this concept , as well as the effectiveness and efficiency of the proposed approach . [[EENNDD]] outlier detection; information networks; community discovery"}, "Mengenai outlier komuniti dan pengesanannya yang cekap dalam rangkaian maklumat Data yang dipautkan atau berangkaian terdapat di banyak aplikasi. Contohnya termasuk data web atau dokumen hiperteks yang disambungkan melalui pautan hiper, rangkaian sosial atau profil pengguna yang disambungkan melalui pautan rakan, pengarang bersama dan maklumat petikan, data blog, ulasan filem dan sebagainya. Dalam kumpulan data ini disebut \"rangkaian maklumat\", objek yang berkait rapat yang mempunyai sifat atau minat yang sama membentuk sebuah komuniti. Sebagai contoh, komuniti di blogsphere mungkin pengguna yang kebanyakannya berminat dengan ulasan dan berita telefon bimbit. Pengesanan luar dalam rangkaian maklumat dapat mendedahkan tingkah laku anomali dan menarik yang penting yang tidak jelas jika maklumat masyarakat diabaikan. Contohnya ialah orang berpendapatan rendah berkawan dengan banyak orang kaya walaupun penghasilannya tidak rendah jika dianggap keseluruhan penduduk. Makalah ini pertama kali memperkenalkan konsep outlier komuniti poin menarik atau bintang yang meningkat untuk pengertian yang lebih positif, dan kemudian menunjukkan bahawa pendekatan dasar yang terkenal tanpa mempertimbangkan pautan atau maklumat komuniti tidak dapat mencari outliers komuniti ini. Kami mencadangkan penyelesaian yang berkesan dengan memodelkan data jaringan sebagai model campuran yang terdiri daripada beberapa komuniti normal dan sekumpulan outlier yang dihasilkan secara rawak. Model probabilistik mencirikan kedua-dua data dan pautan secara serentak dengan menentukan pengedaran bersama berdasarkan medan rawak Markov tersembunyi HMRF. Memaksimumkan kemungkinan data dan posterior model memberikan penyelesaian untuk masalah inferensi luar. Kami menerapkan model tersebut pada kedua-dua kumpulan data sintetik dan DBLP, dan hasilnya menunjukkan kepentingan konsep ini, serta keberkesanan dan kecekapan pendekatan yang dicadangkan. [[EENNDD]] pengesanan luar; rangkaian maklumat; penemuan masyarakat"], [{"string": "Meme-tracking and the dynamics of the news cycle Tracking new topics , ideas , and `` memes '' across the Web has been an issue of considerable interest . Recent work has developed methods for tracking topic shifts over long time scales , as well as abrupt spikes in the appearance of particular named entities . However , these approaches are less well suited to the identification of content that spreads widely and then fades over time scales on the order of days - the time scale at which we perceive news and events . We develop a framework for tracking short , distinctive phrases that travel relatively intact through on-line text ; developing scalable algorithms for clustering textual variants of such phrases , we identify a broad class of memes that exhibit wide spread and rich variation on a daily basis . As our principal domain of study , we show how such a meme-tracking approach can provide a coherent representation of the news cycle - the daily rhythms in the news media that have long been the subject of qualitative interpretation but have never been captured accurately enough to permit actual quantitative analysis . We tracked 1.6 million mainstream media sites and blogs over a period of three months with the total of 90 million articles and we find a set of novel and persistent temporal patterns in the news cycle . In particular , we observe a typical lag of 2.5 hours between the peaks of attention to a phrase in the news media and in blogs respectively , with divergent behavior around the overall peak and a `` heartbeat '' - like pattern in the handoff between news and blogs . We also develop and analyze a mathematical model for the kinds of temporal variation that the system exhibits .", "keywords": ["information networks", "news media", "information cascades", "news cycle", "social networks", "meme-tracking", "blogs"], "combined": "Meme-tracking and the dynamics of the news cycle Tracking new topics , ideas , and `` memes '' across the Web has been an issue of considerable interest . Recent work has developed methods for tracking topic shifts over long time scales , as well as abrupt spikes in the appearance of particular named entities . However , these approaches are less well suited to the identification of content that spreads widely and then fades over time scales on the order of days - the time scale at which we perceive news and events . We develop a framework for tracking short , distinctive phrases that travel relatively intact through on-line text ; developing scalable algorithms for clustering textual variants of such phrases , we identify a broad class of memes that exhibit wide spread and rich variation on a daily basis . As our principal domain of study , we show how such a meme-tracking approach can provide a coherent representation of the news cycle - the daily rhythms in the news media that have long been the subject of qualitative interpretation but have never been captured accurately enough to permit actual quantitative analysis . We tracked 1.6 million mainstream media sites and blogs over a period of three months with the total of 90 million articles and we find a set of novel and persistent temporal patterns in the news cycle . In particular , we observe a typical lag of 2.5 hours between the peaks of attention to a phrase in the news media and in blogs respectively , with divergent behavior around the overall peak and a `` heartbeat '' - like pattern in the handoff between news and blogs . We also develop and analyze a mathematical model for the kinds of temporal variation that the system exhibits . [[EENNDD]] information networks; news media; information cascades; news cycle; social networks; meme-tracking; blogs"}, "Pelacakan Meme dan dinamika kitaran berita Menjejaki topik, idea, dan \"meme\" baru di seluruh Web telah menjadi isu yang sangat menarik. Kerja terbaru telah mengembangkan kaedah untuk mengesan pergeseran topik dalam skala waktu yang lama, serta lonjakan mendadak dalam penampilan entiti bernama tertentu. Walau bagaimanapun, pendekatan ini kurang sesuai dengan pengenalpastian kandungan yang tersebar secara meluas dan kemudian memudar mengikut skala masa mengikut urutan hari - skala waktu di mana kita melihat berita dan peristiwa. Kami mengembangkan rangka untuk mengesan frasa pendek dan khas yang bergerak secara utuh melalui teks dalam talian; mengembangkan algoritma berskala untuk mengumpulkan varian teks frasa seperti itu, kami mengenal pasti kelas meme yang luas yang menunjukkan penyebaran luas dan variasi kaya setiap hari. Sebagai domain kajian utama kami, kami menunjukkan bagaimana pendekatan meme-tracking dapat memberikan gambaran yang koheren dari kitaran berita - irama harian di media berita yang telah lama menjadi subjek tafsiran kualitatif tetapi tidak pernah ditangkap dengan cukup tepat untuk membenarkan analisis kuantitatif sebenar. Kami mengesan 1.6 juta laman media dan blog arus perdana dalam jangka masa tiga bulan dengan jumlah keseluruhan 90 juta artikel dan kami menemui satu set corak temporal baru dan berterusan dalam kitaran berita. Khususnya, kita melihat ketinggian tipikal selama 2.5 jam antara puncak perhatian terhadap frasa di media berita dan di blog masing-masing, dengan tingkah laku yang berbeza di sekitar puncak keseluruhan dan corak seperti \"degupan jantung\" dalam pemberhentian antara berita dan blog. Kami juga mengembangkan dan menganalisis model matematik untuk jenis variasi temporal yang ditunjukkan oleh sistem. [[EENNDD]] rangkaian maklumat; berita media; lata maklumat; kitaran berita; rangkaian sosial; meme-tracking; blog"], [{"string": "Fast approximate spectral clustering Spectral clustering refers to a flexible class of clustering procedures that can produce high-quality clusterings on small data sets but which has limited applicability to large-scale problems due to its computational complexity of O n3 in general , with n the number of data points . We extend the range of spectral clustering by developing a general framework for fast approximate spectral clustering in which a distortion-minimizing local transformation is first applied to the data . This framework is based on a theoretical analysis that provides a statistical characterization of the effect of local distortion on the mis-clustering rate . We develop two concrete instances of our general framework , one based on local k-means clustering KASP and one based on random projection trees RASP . Extensive experiments show that these algorithms can achieve significant speedups with little degradation in clustering accuracy . Specifically , our algorithms outperform k-means by a large margin in terms of accuracy , and run several times faster than approximate spectral clustering based on the Nystrom method , with comparable accuracy and significantly smaller memory footprint . Remarkably , our algorithms make it possible for a single machine to spectral cluster data sets with a million observations within several minutes .", "keywords": ["spectral clustering", "data quantization", "learning", "unsupervised learning"], "combined": "Fast approximate spectral clustering Spectral clustering refers to a flexible class of clustering procedures that can produce high-quality clusterings on small data sets but which has limited applicability to large-scale problems due to its computational complexity of O n3 in general , with n the number of data points . We extend the range of spectral clustering by developing a general framework for fast approximate spectral clustering in which a distortion-minimizing local transformation is first applied to the data . This framework is based on a theoretical analysis that provides a statistical characterization of the effect of local distortion on the mis-clustering rate . We develop two concrete instances of our general framework , one based on local k-means clustering KASP and one based on random projection trees RASP . Extensive experiments show that these algorithms can achieve significant speedups with little degradation in clustering accuracy . Specifically , our algorithms outperform k-means by a large margin in terms of accuracy , and run several times faster than approximate spectral clustering based on the Nystrom method , with comparable accuracy and significantly smaller memory footprint . Remarkably , our algorithms make it possible for a single machine to spectral cluster data sets with a million observations within several minutes . [[EENNDD]] spectral clustering; data quantization; learning; unsupervised learning"}, "Penggabungan spektrum anggaran cepat Penggabungan spektral merujuk kepada kelas prosedur pengelompokan yang fleksibel yang dapat menghasilkan pengelompokan berkualiti tinggi pada set data kecil tetapi yang terbatas untuk diterapkan pada masalah berskala besar kerana kerumitan komputasi O n3 secara umum, dengan bilangan n titik data. Kami memperluas jangkauan pengelompokan spektrum dengan mengembangkan kerangka umum untuk pengelompokan spektrum perkiraan cepat di mana transformasi tempatan yang meminimumkan distorsi pertama kali diterapkan pada data. Kerangka ini didasarkan pada analisis teoritis yang memberikan ciri statistik pengaruh distorsi tempatan terhadap kadar mis-clustering. Kami mengembangkan dua contoh konkrit dari kerangka umum kami, satu berdasarkan k-tempatan pengelompokan KASP dan satu berdasarkan pokok unjuran RASP. Eksperimen yang luas menunjukkan bahawa algoritma ini dapat mencapai peningkatan yang ketara dengan sedikit penurunan dalam ketepatan pengelompokan. Secara khusus, algoritma kami mengungguli k-berarti dengan margin yang besar dari segi ketepatan, dan berjalan beberapa kali lebih cepat daripada perkiraan spektrum berdasarkan kaedah Nystrom, dengan ketepatan yang setanding dan jejak memori yang jauh lebih kecil. Hebatnya, algoritma kami memungkinkan satu mesin untuk set data kelompok spektrum dengan sejuta pemerhatian dalam beberapa minit. [[EENNDD]] pengelompokan spektrum; pengkuantuman data; belajar; pembelajaran tanpa pengawasan"], [{"string": "IMMC : incremental maximum margin criterion Subspace learning approaches have attracted much attention in academia recently . However , the classical batch algorithms no longer satisfy the applications on streaming data or large-scale data . To meet this desirability , Incremental Principal Component Analysis IPCA algorithm has been well established , but it is an unsupervised subspace learning approach and is not optimal for general classification tasks , such as face recognition and Web document categorization . In this paper , we propose an incremental supervised subspace learning algorithm , called Incremental Maximum Margin Criterion IMMC , to infer an adaptive subspace by optimizing the Maximum Margin Criterion . We also present the proof for convergence of the proposed algorithm . Experimental results on both synthetic dataset and real world datasets show that IMMC converges to the similar subspace as that of batch approach .", "keywords": ["optimization", "maximum margin criterion", "learning", "linear discriminant analysis", "principal component analysis"], "combined": "IMMC : incremental maximum margin criterion Subspace learning approaches have attracted much attention in academia recently . However , the classical batch algorithms no longer satisfy the applications on streaming data or large-scale data . To meet this desirability , Incremental Principal Component Analysis IPCA algorithm has been well established , but it is an unsupervised subspace learning approach and is not optimal for general classification tasks , such as face recognition and Web document categorization . In this paper , we propose an incremental supervised subspace learning algorithm , called Incremental Maximum Margin Criterion IMMC , to infer an adaptive subspace by optimizing the Maximum Margin Criterion . We also present the proof for convergence of the proposed algorithm . Experimental results on both synthetic dataset and real world datasets show that IMMC converges to the similar subspace as that of batch approach . [[EENNDD]] optimization; maximum margin criterion; learning; linear discriminant analysis; principal component analysis"}, "IMMC: kriteria margin maksimum tambahan Pendekatan pembelajaran ruang bawah telah menarik banyak perhatian di akademik baru-baru ini. Walau bagaimanapun, algoritma kumpulan klasik tidak lagi memuaskan aplikasi pada streaming data atau data berskala besar. Untuk memenuhi kehendak ini, algoritma IPCA Analisis Komponen Prinsipal yang baik telah dibentuk dengan baik, tetapi ini adalah pendekatan pembelajaran ruang bawah tanpa pengawasan dan tidak optimum untuk tugas klasifikasi umum, seperti pengecaman wajah dan pengkategorian dokumen Web. Dalam makalah ini, kami mengusulkan algoritma pembelajaran ruang bawah seliaan tambahan, yang disebut Incremental Maximum Margin Criterion IMMC, untuk menyimpulkan subspace adaptif dengan mengoptimumkan Kriteria Margin Maksimum. Kami juga mengemukakan bukti penumpuan algoritma yang dicadangkan. Hasil eksperimen pada kumpulan data sintetik dan dataset dunia nyata menunjukkan bahawa IMMC beralih ke ruang bawah yang serupa dengan pendekatan kumpulan. [[EENNDD]] pengoptimuman; kriteria margin maksimum; belajar; analisis diskriminasi linear; analisis komponen utama"], [{"string": "A theoretical framework for learning from a pool of disparate data sources Many enterprises incorporate information gathered from a variety of data sources into an integrated input for some learning task . For example , aiming towards the design of an automated diagnostic tool for some disease , one may wish to integrate data gathered in many different hospitals . A major obstacle to such endeavors is that different data sources may vary considerably in the way they choose to represent related data . In practice , the problem is usually solved by a manual construction of semantic mappings and translations between the different sources . Recently there have been attempts to introduce automated algorithms based on machine learning tools for the construction of such translations . In this work we propose a theoretical framework for making classification predictions from a collection of different data sources , without creating explicit translations between them . Our framework allows a precise mathematical analysis of the complexity of such tasks , and it provides a tool for the development and comparison of different learning algorithms . Our main objective , at this stage , is to demonstrate the usefulness of computational learning theory to this practically important area and to stimulate further theoretical and experimental research of questions related to this framework .", "keywords": ["probabilistic algorithms"], "combined": "A theoretical framework for learning from a pool of disparate data sources Many enterprises incorporate information gathered from a variety of data sources into an integrated input for some learning task . For example , aiming towards the design of an automated diagnostic tool for some disease , one may wish to integrate data gathered in many different hospitals . A major obstacle to such endeavors is that different data sources may vary considerably in the way they choose to represent related data . In practice , the problem is usually solved by a manual construction of semantic mappings and translations between the different sources . Recently there have been attempts to introduce automated algorithms based on machine learning tools for the construction of such translations . In this work we propose a theoretical framework for making classification predictions from a collection of different data sources , without creating explicit translations between them . Our framework allows a precise mathematical analysis of the complexity of such tasks , and it provides a tool for the development and comparison of different learning algorithms . Our main objective , at this stage , is to demonstrate the usefulness of computational learning theory to this practically important area and to stimulate further theoretical and experimental research of questions related to this framework . [[EENNDD]] probabilistic algorithms"}, "Kerangka teori untuk belajar dari kumpulan sumber data yang berbeza-beza Banyak perusahaan memasukkan maklumat yang dikumpulkan dari pelbagai sumber data ke dalam input bersepadu untuk beberapa tugas pembelajaran. Sebagai contoh, bertujuan untuk merancang alat diagnostik automatik untuk beberapa penyakit, seseorang mungkin ingin mengintegrasikan data yang dikumpulkan di banyak hospital yang berbeza. Halangan utama untuk usaha tersebut adalah bahawa sumber data yang berbeza mungkin sangat berbeza dalam cara mereka memilih untuk mewakili data yang berkaitan. Dalam praktiknya, masalah ini biasanya diselesaikan dengan pembinaan manual pemetaan dan terjemahan semantik antara sumber yang berlainan. Baru-baru ini terdapat percubaan untuk memperkenalkan algoritma automatik berdasarkan alat pembelajaran mesin untuk pembinaan terjemahan tersebut. Dalam karya ini, kami mencadangkan kerangka teori untuk membuat ramalan klasifikasi dari kumpulan sumber data yang berbeza, tanpa membuat terjemahan eksplisit di antara mereka. Kerangka kerja kami memungkinkan analisis matematik yang tepat mengenai kerumitan tugas-tugas tersebut, dan ini menyediakan alat untuk pengembangan dan perbandingan algoritma pembelajaran yang berbeza. Objektif utama kami, pada tahap ini, adalah untuk menunjukkan kegunaan teori pembelajaran komputasi untuk bidang yang sangat praktikal ini dan untuk merangsang penyelidikan teori dan eksperimen lebih lanjut mengenai persoalan yang berkaitan dengan kerangka ini. [[EENNDD]] algoritma probabilistik"], [{"string": "Learning to match and cluster large high-dimensional data sets for data integration Part of the process of data integration is determining which sets of identifiers refer to the same real-world entities . In integrating databases found on the Web or obtained by using information extraction methods , it is often possible to solve this problem by exploiting similarities in the textual names used for objects in different databases . In this paper we describe techniques for clustering and matching identifier names that are both scalable and adaptive , in the sense that they can be trained to obtain better performance in a particular domain . An experimental evaluation on a number of sample datasets shows that the adaptive method sometimes performs much better than either of two non-adaptive baseline systems , and is nearly always competitive with the best baseline system .", "keywords": ["text mining", "learning", "large datasets"], "combined": "Learning to match and cluster large high-dimensional data sets for data integration Part of the process of data integration is determining which sets of identifiers refer to the same real-world entities . In integrating databases found on the Web or obtained by using information extraction methods , it is often possible to solve this problem by exploiting similarities in the textual names used for objects in different databases . In this paper we describe techniques for clustering and matching identifier names that are both scalable and adaptive , in the sense that they can be trained to obtain better performance in a particular domain . An experimental evaluation on a number of sample datasets shows that the adaptive method sometimes performs much better than either of two non-adaptive baseline systems , and is nearly always competitive with the best baseline system . [[EENNDD]] text mining; learning; large datasets"}, "Belajar memadankan dan mengumpulkan kumpulan data dimensi tinggi yang besar untuk penyatuan data Sebahagian daripada proses integrasi data adalah menentukan set pengecam mana yang merujuk kepada entiti dunia nyata yang sama. Dalam mengintegrasikan pangkalan data yang terdapat di Web atau diperoleh dengan menggunakan kaedah pengekstrakan maklumat, sering kali dapat menyelesaikan masalah ini dengan memanfaatkan persamaan pada nama teks yang digunakan untuk objek dalam pangkalan data yang berlainan. Dalam makalah ini kami menerangkan teknik pengelompokan dan pemadanan nama pengenal yang dapat disesuaikan dan disesuaikan, dalam arti mereka dapat dilatih untuk memperoleh prestasi yang lebih baik dalam domain tertentu. Penilaian eksperimental pada sejumlah kumpulan data sampel menunjukkan bahawa kaedah penyesuaian kadangkala berkinerja jauh lebih baik daripada salah satu daripada dua sistem asas yang tidak adaptif, dan hampir selalu bersaing dengan sistem asas terbaik. [[EENNDD]] perlombongan teks; belajar; set data besar"], [{"string": "Style mining of electronic messages for multiple authorship discrimination : first results This paper considers the use of computational stylistics for performing authorship attribution of electronic messages , addressing categorization problems with as many as 20 different classes authors . Effective stylistic characterization of text is potentially useful for a variety of tasks , as language style contains cues regarding the authorship , purpose , and mood of the text , all of which would be useful adjuncts to information retrieval or knowledge-management tasks . We focus here on the problem of determining the author of an anonymous message , based only on the message text . Several multiclass variants of the Winnow algorithm were applied to a vector representation of the message texts to learn models for discriminating different authors . We present results comparing the classification accuracy of the different approaches . The results show that stylistic models can be accurately learned to determine an author 's identity .", "keywords": ["text mining", "information search and retrieval", "learning", "text categorization", "computational stylistics", "authorship attribution", "electronic communication"], "combined": "Style mining of electronic messages for multiple authorship discrimination : first results This paper considers the use of computational stylistics for performing authorship attribution of electronic messages , addressing categorization problems with as many as 20 different classes authors . Effective stylistic characterization of text is potentially useful for a variety of tasks , as language style contains cues regarding the authorship , purpose , and mood of the text , all of which would be useful adjuncts to information retrieval or knowledge-management tasks . We focus here on the problem of determining the author of an anonymous message , based only on the message text . Several multiclass variants of the Winnow algorithm were applied to a vector representation of the message texts to learn models for discriminating different authors . We present results comparing the classification accuracy of the different approaches . The results show that stylistic models can be accurately learned to determine an author 's identity . [[EENNDD]] text mining; information search and retrieval; learning; text categorization; computational stylistics; authorship attribution; electronic communication"}, "Perlombongan gaya pesanan elektronik untuk pelbagai diskriminasi kepengarangan: hasil pertama Makalah ini mempertimbangkan penggunaan gaya komputasi untuk melakukan atribusi kepengarangan terhadap mesej elektronik, mengatasi masalah pengkategorian dengan sebanyak 20 pengarang kelas yang berbeza. Pencirian gaya teks yang berkesan berpotensi berguna untuk berbagai tugas, kerana gaya bahasa mengandung petunjuk mengenai kepengarangan, tujuan, dan suasana teks, yang semuanya berguna sebagai tambahan kepada pencarian maklumat atau tugas pengurusan pengetahuan. Kami memberi tumpuan di sini pada masalah menentukan pengarang mesej tanpa nama, hanya berdasarkan teks mesej. Beberapa varian multikelas algoritma Winnow diterapkan pada representasi vektor teks mesej untuk mempelajari model untuk membeza-bezakan pengarang yang berbeza. Kami membentangkan hasil membandingkan ketepatan klasifikasi pendekatan yang berbeza. Hasil kajian menunjukkan bahawa model gaya dapat dipelajari dengan tepat untuk menentukan identiti pengarang. [[EENNDD]] perlombongan teks; carian dan pengambilan maklumat; belajar; pengkategorian teks; stilistik pengiraan; atribusi kepengarangan; komunikasi elektronik"], [{"string": "Information extraction from Wikipedia : moving down the long tail Not only is Wikipedia a comprehensive source of quality information , it has several kinds of internal structure e.g. , relational summaries known as infoboxes , which enable self-supervised information extraction . While previous efforts at extraction from Wikipedia achieve high precision and recall on well-populated classes of articles , they fail in a larger number of cases , largely because incomplete articles and infrequent use of infoboxes lead to insufficient training data . This paper presents three novel techniques for increasing recall from Wikipedia 's long tail of sparse classes : 1 shrinkage over an automatically-learned subsumption taxonomy , 2 a retraining technique for improving the training data , and 3 supplementing results by extracting from the broader Web . Our experiments compare design variations and show that , used in concert , these techniques increase recall by a factor of 1.76 to 8.71 while maintaining or increasing precision .", "keywords": ["information extraction", "wikipedia", "semantic web", "information systems applications"], "combined": "Information extraction from Wikipedia : moving down the long tail Not only is Wikipedia a comprehensive source of quality information , it has several kinds of internal structure e.g. , relational summaries known as infoboxes , which enable self-supervised information extraction . While previous efforts at extraction from Wikipedia achieve high precision and recall on well-populated classes of articles , they fail in a larger number of cases , largely because incomplete articles and infrequent use of infoboxes lead to insufficient training data . This paper presents three novel techniques for increasing recall from Wikipedia 's long tail of sparse classes : 1 shrinkage over an automatically-learned subsumption taxonomy , 2 a retraining technique for improving the training data , and 3 supplementing results by extracting from the broader Web . Our experiments compare design variations and show that , used in concert , these techniques increase recall by a factor of 1.76 to 8.71 while maintaining or increasing precision . [[EENNDD]] information extraction; wikipedia; semantic web; information systems applications"}, "Pengekstrakan maklumat dari Wikipedia: bergerak ke ekor panjang Bukan hanya Wikipedia sumber maklumat berkualiti yang komprehensif, tetapi mempunyai beberapa jenis struktur dalaman, mis. , ringkasan hubungan yang dikenali sebagai infobox, yang memungkinkan pengekstrakan maklumat yang diselia sendiri. Walaupun usaha sebelumnya dari pengekstrakan dari Wikipedia mencapai ketepatan tinggi dan mengingat kembali kelas artikel yang dihuni dengan baik, mereka gagal dalam sebilangan besar kes, sebahagian besarnya kerana artikel yang tidak lengkap dan penggunaan infobox yang jarang menyebabkan data latihan tidak mencukupi. Makalah ini menyajikan tiga teknik baru untuk meningkatkan penarikan dari ekor panjang kelas jarang Wikipedia: 1 penyusutan atas taksonomi penggabungan yang dipelajari secara automatik, 2 teknik latihan semula untuk meningkatkan data latihan, dan 3 hasil tambahan dengan mengekstrak dari Web yang lebih luas. Eksperimen kami membandingkan variasi reka bentuk dan menunjukkan bahawa, yang digunakan dalam konsert, teknik ini meningkatkan penarikan balik dengan faktor 1.76 hingga 8.71 sambil mengekalkan atau meningkatkan ketepatan. [[EENNDD]] pengekstrakan maklumat; wikipedia; web semantik; aplikasi sistem maklumat"], [{"string": "The IGrid index : reversing the dimensionality curse for similarity indexing in high dimensional space", "keywords": ["indexing", "dimensionality curse", "database applications"], "combined": "The IGrid index : reversing the dimensionality curse for similarity indexing in high dimensional space [[EENNDD]] indexing; dimensionality curse; database applications"}, "Indeks IGrid: membalikkan kutukan dimensi untuk pengindeksan kesamaan dalam pengindeksan ruang dimensi tinggi [[EENNDD]]; sumpahan dimensi; aplikasi pangkalan data"], [{"string": "A generative probabilistic approach to visualizing sets of symbolic sequences There is a notable interest in extending probabilistic generative modeling principles to accommodate for more complex structured data types . In this paper we develop a generative probabilistic model for visualizing sets of discrete symbolic sequences . The model , a constrained mixture of discrete hidden Markov models , is a generalization of density-based visualization methods previously developed for static data sets . We illustrate our approach on sequences representing web-log data and chorals by J.S. Bach .", "keywords": ["latent space models", "em algorithm", "information search and retrieval", "topographic mapping", "hidden markov model"], "combined": "A generative probabilistic approach to visualizing sets of symbolic sequences There is a notable interest in extending probabilistic generative modeling principles to accommodate for more complex structured data types . In this paper we develop a generative probabilistic model for visualizing sets of discrete symbolic sequences . The model , a constrained mixture of discrete hidden Markov models , is a generalization of density-based visualization methods previously developed for static data sets . We illustrate our approach on sequences representing web-log data and chorals by J.S. Bach . [[EENNDD]] latent space models; em algorithm; information search and retrieval; topographic mapping; hidden markov model"}, "Pendekatan probabilistik generatif untuk memvisualisasikan sekumpulan urutan simbolik Terdapat minat penting dalam memperluas prinsip pemodelan generatif probabilistik untuk menampung jenis data berstruktur yang lebih kompleks. Dalam makalah ini kami mengembangkan model probabilistik generatif untuk memvisualisasikan set urutan simbolik diskrit. Model, campuran model Markov tersembunyi tersekat, adalah generalisasi kaedah visualisasi berdasarkan kepadatan yang sebelumnya dikembangkan untuk set data statik. Kami menggambarkan pendekatan kami pada urutan yang mewakili data log web dan paduan suara oleh J.S. Bach. [[EENNDD]] model ruang pendam; algoritma em; pencarian dan pengambilan maklumat; pemetaan topografi; model markov tersembunyi"], [{"string": "Structured correspondence topic models for mining captioned figures in biological literature A major source of information often the most crucial and informative part in scholarly articles from scientific journals , proceedings and books are the figures that directly provide images and other graphical illustrations of key experimental results and other scientific contents . In biological articles , a typical figure often comprises multiple panels , accompanied by either scoped or global captioned text . Moreover , the text in the caption contains important semantic entities such as protein names , gene ontology , tissues labels , etc. , relevant to the images in the figure . Due to the avalanche of biological literature in recent years , and increasing popularity of various bio-imaging techniques , automatic retrieval and summarization of biological information from literature figures has emerged as a major unsolved challenge in computational knowledge extraction and management in the life science . We present a new structured probabilistic topic model built on a realistic figure generation scheme to model the structurally annotated biological figures , and we derive an efficient inference algorithm based on collapsed Gibbs sampling for information retrieval and visualization . The resulting program constitutes one of the key IR engines in our SLIF system that has recently entered the final round 4 out 70 competing systems of the Elsevier Grand Challenge on Knowledge Enhancement in the Life Science . Here we present various evaluations on a number of data mining tasks to illustrate our method .", "keywords": ["information retrieval", "bioinformatics", "gibbs sampling", "learning", "multimodal data", "topic models"], "combined": "Structured correspondence topic models for mining captioned figures in biological literature A major source of information often the most crucial and informative part in scholarly articles from scientific journals , proceedings and books are the figures that directly provide images and other graphical illustrations of key experimental results and other scientific contents . In biological articles , a typical figure often comprises multiple panels , accompanied by either scoped or global captioned text . Moreover , the text in the caption contains important semantic entities such as protein names , gene ontology , tissues labels , etc. , relevant to the images in the figure . Due to the avalanche of biological literature in recent years , and increasing popularity of various bio-imaging techniques , automatic retrieval and summarization of biological information from literature figures has emerged as a major unsolved challenge in computational knowledge extraction and management in the life science . We present a new structured probabilistic topic model built on a realistic figure generation scheme to model the structurally annotated biological figures , and we derive an efficient inference algorithm based on collapsed Gibbs sampling for information retrieval and visualization . The resulting program constitutes one of the key IR engines in our SLIF system that has recently entered the final round 4 out 70 competing systems of the Elsevier Grand Challenge on Knowledge Enhancement in the Life Science . Here we present various evaluations on a number of data mining tasks to illustrate our method . [[EENNDD]] information retrieval; bioinformatics; gibbs sampling; learning; multimodal data; topic models"}, "Model topik korespondensi berstruktur untuk tokoh-tokoh kapsyen perlombongan dalam sastera biologi Sumber maklumat utama selalunya bahagian yang paling penting dan berinformasi dalam artikel ilmiah dari jurnal, prosiding dan buku ilmiah adalah angka yang secara langsung memberikan gambar dan gambaran grafik lain mengenai hasil eksperimen utama dan lain-lain kandungan ilmiah. Dalam artikel biologi, tokoh khas sering kali terdiri daripada beberapa panel, disertai dengan teks teks atau kapsyen global. Lebih-lebih lagi, teks dalam kapsyen mengandungi entiti semantik penting seperti nama protein, ontologi gen, label tisu, dan lain-lain, yang relevan dengan gambar dalam gambar. Oleh kerana longsoran sastera biologi dalam beberapa tahun kebelakangan ini, dan peningkatan populariti pelbagai teknik pencitraan bio, pengambilan dan ringkasan maklumat biologi secara automatik dari tokoh sastera telah muncul sebagai cabaran utama yang belum dapat diselesaikan dalam pengekstrakan dan pengurusan pengetahuan komputasi dalam sains kehidupan. Kami menyajikan model topik probabilistik berstruktur baru yang dibina berdasarkan skema penjanaan angka yang realistik untuk memodelkan angka biologi yang diberi penjelasan secara struktural, dan kami memperoleh algoritma inferensi yang cekap berdasarkan pensampelan Gibbs yang runtuh untuk pengambilan maklumat dan visualisasi. Program yang dihasilkan merupakan salah satu enjin utama IR dalam sistem SLIF kami yang baru memasuki pusingan akhir 4 daripada 70 sistem pesaing Elsevier Grand Challenge on Knowledge Enhancement in the Life Science. Di sini kami menyajikan pelbagai penilaian mengenai sejumlah tugas perlombongan data untuk menggambarkan kaedah kami. [[EENNDD]] pengambilan maklumat; bioinformatik; persampelan gibbs; belajar; data multimodal; model topik"], [{"string": "Classification with partial labels In this paper , we address the problem of learning when some cases are fully labeled while other cases are only partially labeled , in the form of partial labels . Partial labels are represented as a set of possible labels for each training example , one of which is the correct label . We introduce a discriminative learning approach that incorporates partial label information into the conventional margin-based learning framework . The partial label learning problem is formulated as a convex quadratic optimization minimizing the L2-norm regularized empirical risk using hinge loss . We also present an efficient algorithm for classification in the presence of partial labels . Experiments with different data sets show that partial label information improves the performance of classification when there is traditional fully-labeled data , and also yields reasonable performance in the absence of any fully labeled data .", "keywords": ["support vectors", "partial labels"], "combined": "Classification with partial labels In this paper , we address the problem of learning when some cases are fully labeled while other cases are only partially labeled , in the form of partial labels . Partial labels are represented as a set of possible labels for each training example , one of which is the correct label . We introduce a discriminative learning approach that incorporates partial label information into the conventional margin-based learning framework . The partial label learning problem is formulated as a convex quadratic optimization minimizing the L2-norm regularized empirical risk using hinge loss . We also present an efficient algorithm for classification in the presence of partial labels . Experiments with different data sets show that partial label information improves the performance of classification when there is traditional fully-labeled data , and also yields reasonable performance in the absence of any fully labeled data . [[EENNDD]] support vectors; partial labels"}, "Klasifikasi dengan label separa Dalam makalah ini, kita menangani masalah pembelajaran apabila beberapa kes dilabel sepenuhnya sementara kes lain hanya dilabelkan sebahagian, dalam bentuk label separa. Sebahagian label ditunjukkan sebagai satu set label yang mungkin untuk setiap contoh latihan, salah satunya adalah label yang betul. Kami memperkenalkan pendekatan pembelajaran diskriminatif yang menggabungkan maklumat label separa ke dalam kerangka pembelajaran berasaskan margin konvensional. Masalah pembelajaran label separa dirumuskan sebagai pengoptimuman kuadratik cembung yang meminimumkan risiko empirikal norma L2 yang menggunakan kerugian engsel. Kami juga mengemukakan algoritma yang cekap untuk klasifikasi dengan adanya label separa. Eksperimen dengan set data yang berbeza menunjukkan bahawa maklumat label separa meningkatkan prestasi klasifikasi apabila terdapat data berlabel sepenuhnya tradisional, dan juga memberikan prestasi yang wajar sekiranya tidak ada data yang dilabel sepenuhnya. [[EENNDD]] vektor sokongan; label separa"], [{"string": "Sleeved coclustering A coCluster of a m x n matrix X is a submatrix determined by a subset of the rows and a subset of the columns . The problem of finding coClusters with specific properties is of interest , in particular , in the analysis of microarray experiments . In that case the entries of the matrix X are the expression levels of $ m $ genes in each of $ n $ tissue samples . One goal of the analysis is to extract a subset of the samples and a subset of the genes , such that the expression levels of the chosen genes behave similarly across the subset of the samples , presumably reflecting an underlying regulatory mechanism governing the expression level of the genes . We propose to base the similarity of the genes in a coCluster on a simple biological model , in which the strength of the regulatory mechanism in sample j is Hj , and the response strength of gene i to the regulatory mechanism is Gi . In other words , every two genes participating in a good coCluster should have expression values in each of the participating samples , whose ratio is a constant depending only on the two genes . Noise in the expression levels of genes is taken into account by allowing a deviation from the model , measured by a relative error criterion . The sleeve-width of the coCluster reflects the extent to which entry i , j in the coCluster is allowed to deviate , relatively , from being expressed as the product GiHj . We present a polynomial-time Monte-Carlo algorithm which outputs a list of coClusters whose sleeve-widths do not exceed a prespecified value . Moreover , we prove that the list includes , with fixed probability , a coCluster which is near-optimal in its dimensions . Extensive experimentation with synthetic data shows that the algorithm performs well .", "keywords": ["co-regulation", "coclustering", "clustering", "gene expression data"], "combined": "Sleeved coclustering A coCluster of a m x n matrix X is a submatrix determined by a subset of the rows and a subset of the columns . The problem of finding coClusters with specific properties is of interest , in particular , in the analysis of microarray experiments . In that case the entries of the matrix X are the expression levels of $ m $ genes in each of $ n $ tissue samples . One goal of the analysis is to extract a subset of the samples and a subset of the genes , such that the expression levels of the chosen genes behave similarly across the subset of the samples , presumably reflecting an underlying regulatory mechanism governing the expression level of the genes . We propose to base the similarity of the genes in a coCluster on a simple biological model , in which the strength of the regulatory mechanism in sample j is Hj , and the response strength of gene i to the regulatory mechanism is Gi . In other words , every two genes participating in a good coCluster should have expression values in each of the participating samples , whose ratio is a constant depending only on the two genes . Noise in the expression levels of genes is taken into account by allowing a deviation from the model , measured by a relative error criterion . The sleeve-width of the coCluster reflects the extent to which entry i , j in the coCluster is allowed to deviate , relatively , from being expressed as the product GiHj . We present a polynomial-time Monte-Carlo algorithm which outputs a list of coClusters whose sleeve-widths do not exceed a prespecified value . Moreover , we prove that the list includes , with fixed probability , a coCluster which is near-optimal in its dimensions . Extensive experimentation with synthetic data shows that the algorithm performs well . [[EENNDD]] co-regulation; coclustering; clustering; gene expression data"}, "Sleeveed coclustering CoCluster dari m x n matriks X adalah submatrix yang ditentukan oleh subset baris dan subset lajur. Masalah mencari kluster dengan sifat tertentu menarik, khususnya, dalam analisis eksperimen microarray. Dalam kes itu, entri matriks X adalah tahap ekspresi gen $ m $ dalam setiap sampel tisu $ n $. Salah satu tujuan analisis adalah untuk mengekstrak subset sampel dan subset gen, sehingga tahap ekspresi gen yang dipilih berkelakuan serupa di subset sampel, mungkin mencerminkan mekanisme peraturan yang mendasari tahap ekspresi gen. Kami mencadangkan untuk mendasarkan kesamaan gen dalam coCluster pada model biologi sederhana, di mana kekuatan mekanisme pengawalseliaan dalam sampel j adalah Hj, dan kekuatan tindak balas gen i terhadap mekanisme pengawalseliaan adalah Gi. Dengan kata lain, setiap dua gen yang berpartisipasi dalam coCluster yang baik harus mempunyai nilai ekspresi pada setiap sampel yang mengambil bahagian, yang nisbahnya adalah pemalar yang bergantung hanya pada dua gen. Kebisingan dalam tahap ekspresi gen dipertimbangkan dengan membiarkan penyimpangan dari model, diukur dengan kriteria kesalahan relatif. Lebar lengan coCluster mencerminkan sejauh mana kemasukan i, j dalam coCluster dibenarkan untuk menyimpang, relatif, daripada dinyatakan sebagai produk GiHj. Kami menyajikan algoritma Monte-Carlo polinomial-waktu yang mengeluarkan senarai coClusters yang lebar lengannya tidak melebihi nilai yang ditentukan. Lebih-lebih lagi, kami membuktikan bahawa senarai itu merangkumi, dengan kebarangkalian tetap, coCluster yang hampir optimum dalam dimensinya. Eksperimen yang meluas dengan data sintetik menunjukkan bahawa algoritma berprestasi dengan baik. [[EENNDD]] peraturan bersama; kepompong; pengelompokan; data ekspresi gen"], [{"string": "A Web page prediction model based on click-stream tree representation of user behavior Predicting the next request of a user as she visits Web pages has gained importance as Web-based activity increases . Markov models and their variations , or models based on sequence mining have been found well suited for this problem . However , higher order Markov models are extremely complicated due to their large number of states whereas lower order Markov models do not capture the entire behavior of a user in a session . The models that are based on sequential pattern mining only consider the frequent sequences in the data set , making it difficult to predict the next request following a page that is not in the sequential pattern . Furthermore , it is hard to find models for mining two different kinds of information of a user session . We propose a new model that considers both the order information of pages in a session and the time spent on them . We cluster user sessions based on their pair-wise similarity and represent the resulting clusters by a click-stream tree . The new user session is then assigned to a cluster based on a similarity measure . The click-stream tree of that cluster is used to generate the recommendation set . The model can be used as part of a cache prefetching system as well as a recommendation model .", "keywords": ["design methodology", "graph based clustering", "two dimensional sequential model", "web usage mining"], "combined": "A Web page prediction model based on click-stream tree representation of user behavior Predicting the next request of a user as she visits Web pages has gained importance as Web-based activity increases . Markov models and their variations , or models based on sequence mining have been found well suited for this problem . However , higher order Markov models are extremely complicated due to their large number of states whereas lower order Markov models do not capture the entire behavior of a user in a session . The models that are based on sequential pattern mining only consider the frequent sequences in the data set , making it difficult to predict the next request following a page that is not in the sequential pattern . Furthermore , it is hard to find models for mining two different kinds of information of a user session . We propose a new model that considers both the order information of pages in a session and the time spent on them . We cluster user sessions based on their pair-wise similarity and represent the resulting clusters by a click-stream tree . The new user session is then assigned to a cluster based on a similarity measure . The click-stream tree of that cluster is used to generate the recommendation set . The model can be used as part of a cache prefetching system as well as a recommendation model . [[EENNDD]] design methodology; graph based clustering; two dimensional sequential model; web usage mining"}, "Model ramalan halaman web berdasarkan perwakilan pohon klik-aliran terhadap tingkah laku pengguna Meramalkan permintaan pengguna seterusnya semasa dia mengunjungi halaman Web telah mendapat kepentingan seiring dengan peningkatan aktiviti berasaskan Web. Model Markov dan variasinya, atau model berdasarkan urutan perlombongan didapati sangat sesuai untuk masalah ini. Walau bagaimanapun, model Markov yang lebih tinggi sangat rumit kerana jumlah keadaannya yang besar sedangkan model Markov yang lebih rendah tidak menangkap keseluruhan tingkah laku pengguna dalam satu sesi. Model-model yang didasarkan pada perlombongan pola berurutan hanya mempertimbangkan urutan yang sering berlaku dalam set data, sehingga sukar untuk memprediksi permintaan berikutnya setelah halaman yang tidak berada dalam pola urutan. Selain itu, sukar untuk mencari model untuk melombong dua jenis maklumat sesi pengguna. Kami mencadangkan model baru yang mempertimbangkan maklumat pesanan halaman dalam satu sesi dan masa yang dihabiskan untuknya. Kami mengumpulkan sesi pengguna berdasarkan kesamaan pasangan mereka dan mewakili kelompok yang dihasilkan oleh pohon aliran klik. Sesi pengguna baru kemudian ditugaskan ke kluster berdasarkan ukuran kesamaan. Pokok aliran klik kluster itu digunakan untuk menghasilkan set cadangan. Model ini dapat digunakan sebagai bagian dari sistem pengambilan awal cache dan juga model cadangan. [[EENNDD]] metodologi reka bentuk; pengelompokan berdasarkan grafik; model berurutan dua dimensi; perlombongan penggunaan web"], [{"string": "Mining heterogeneous gene expression data with time lagged recurrent neural networks Heterogeneous types of gene expressions may provide a better insight into the biological role of gene interaction with the environment , disease development and drug effect at the molecular level . In this paper for both exploring and prediction purposes a Time Lagged Recurrent Neural Network with trajectory learning is proposed for identifying and classifying the gene functional patterns from the heterogeneous nonlinear time series microarray experiments . The proposed procedures identify gene functional patterns from the dynamics of a state-trajectory learned in the heterogeneous time series and the gradient information over time . Also , the trajectory learning with Back-propagation through time algorithm can recognize gene expression patterns vary over time . This may reveal much more information about the regulatory network underlying gene expressions . The analyzed data were extracted from spotted DNA microarrays in the budding yeast expression measurements , produced by Eisen et al. . The gene matrix contained 79 experiments over a variety of heterogeneous experiment conditions . The number of recognized gene patterns in our study ranged from two to ten and were divided into three cases . Optimal network architectures with different memory structures were selected based on Akaike and Bayesian information statistical criteria using two-way factorial design . The optimal model performance was compared to other popular gene classification algorithms such as Nearest Neighbor , Support Vector Machine , and Self-Organized Map . The reliability of the performance was verified with multiple iterated runs .", "keywords": ["heterogeneous", "backpropagation through time", "probabilistic algorithms", "trajectory learning", "self-modifying machines", "time lagged neural network", "gene expression"], "combined": "Mining heterogeneous gene expression data with time lagged recurrent neural networks Heterogeneous types of gene expressions may provide a better insight into the biological role of gene interaction with the environment , disease development and drug effect at the molecular level . In this paper for both exploring and prediction purposes a Time Lagged Recurrent Neural Network with trajectory learning is proposed for identifying and classifying the gene functional patterns from the heterogeneous nonlinear time series microarray experiments . The proposed procedures identify gene functional patterns from the dynamics of a state-trajectory learned in the heterogeneous time series and the gradient information over time . Also , the trajectory learning with Back-propagation through time algorithm can recognize gene expression patterns vary over time . This may reveal much more information about the regulatory network underlying gene expressions . The analyzed data were extracted from spotted DNA microarrays in the budding yeast expression measurements , produced by Eisen et al. . The gene matrix contained 79 experiments over a variety of heterogeneous experiment conditions . The number of recognized gene patterns in our study ranged from two to ten and were divided into three cases . Optimal network architectures with different memory structures were selected based on Akaike and Bayesian information statistical criteria using two-way factorial design . The optimal model performance was compared to other popular gene classification algorithms such as Nearest Neighbor , Support Vector Machine , and Self-Organized Map . The reliability of the performance was verified with multiple iterated runs . [[EENNDD]] heterogeneous; backpropagation through time; probabilistic algorithms; trajectory learning; self-modifying machines; time lagged neural network; gene expression"}, "Melombong data ekspresi gen heterogen dengan rangkaian saraf berulang yang tertangguh masa Jenis ekspresi gen heterogen dapat memberikan gambaran yang lebih baik mengenai peranan biologi interaksi gen dengan persekitaran, perkembangan penyakit dan kesan ubat pada tahap molekul. Dalam makalah ini untuk kedua-dua tujuan penerokaan dan ramalan, Rangkaian Neural Berulang Waktu yang Ditinggalkan dengan pembelajaran lintasan dicadangkan untuk mengenal pasti dan mengklasifikasikan corak fungsional gen dari eksperimen mikroarray siri masa yang tidak heterogen. Prosedur yang dicadangkan mengenal pasti corak fungsi gen dari dinamika lintasan keadaan yang dipelajari dalam siri masa yang heterogen dan maklumat kecerunan dari masa ke masa. Juga, pembelajaran lintasan dengan penyebaran kembali melalui algoritma masa dapat mengenali corak ekspresi gen berbeza dari masa ke masa. Ini dapat mendedahkan lebih banyak maklumat mengenai rangkaian peraturan yang mendasari ungkapan gen. Data yang dianalisis diekstrak dari mikroarray DNA yang dilihat dalam pengukuran ekspresi ragi, yang dihasilkan oleh Eisen et al. . Matriks gen mengandungi 79 eksperimen terhadap pelbagai keadaan eksperimen yang heterogen. Jumlah corak gen yang diakui dalam kajian kami berkisar antara dua hingga sepuluh dan dibahagikan kepada tiga kes. Senibina rangkaian optimum dengan struktur memori yang berbeza dipilih berdasarkan kriteria statistik maklumat Akaike dan Bayesian menggunakan reka bentuk faktorial dua hala. Prestasi model yang optimum dibandingkan dengan algoritma klasifikasi gen lain yang popular seperti Jiran terdekat, Mesin Vektor Sokongan, dan Peta Organisasi Sendiri. Kebolehpercayaan prestasi disahkan dengan beberapa larian berulang. [[EENNDD]] heterogen; backpropagation melalui masa; algoritma probabilistik; pembelajaran lintasan; mesin ubahsuai sendiri; rangkaian saraf masa yang tertinggal; ungkapan gen"], [{"string": "Clustering seasonality patterns in the presence of errors Clustering is a very well studied problem that attempts to group similar data points . Most traditional clustering algorithms assume that the data is provided without measurement error . Often , however , real world data sets have such errors and one can obtain estimates of these errors . We present a clustering method that incorporates information contained in these error estimates . We present a new distance function that is based on the distribution of errors in data . Using a Gaussian model for errors , the distance function follows a Chi-Square distribution and is easy to compute . This distance function is used in hierarchical clustering to discover meaningful clusters . The distance function is scale-invariant so that clustering results are independent of units of measuring data . In the special case when the error distribution is the same for each attribute of data points , the rank order of pair-wise distances is the same for our distance function and the Euclidean distance function . The clustering method is applied to the seasonality estimation problem and experimental results are presented for the retail industry data as well as for simulated data , where it outperforms classical clustering methods .", "keywords": ["distance function", "seasonality", "time-series", "product life cycle", "gaussian distribution", "forecasting"], "combined": "Clustering seasonality patterns in the presence of errors Clustering is a very well studied problem that attempts to group similar data points . Most traditional clustering algorithms assume that the data is provided without measurement error . Often , however , real world data sets have such errors and one can obtain estimates of these errors . We present a clustering method that incorporates information contained in these error estimates . We present a new distance function that is based on the distribution of errors in data . Using a Gaussian model for errors , the distance function follows a Chi-Square distribution and is easy to compute . This distance function is used in hierarchical clustering to discover meaningful clusters . The distance function is scale-invariant so that clustering results are independent of units of measuring data . In the special case when the error distribution is the same for each attribute of data points , the rank order of pair-wise distances is the same for our distance function and the Euclidean distance function . The clustering method is applied to the seasonality estimation problem and experimental results are presented for the retail industry data as well as for simulated data , where it outperforms classical clustering methods . [[EENNDD]] distance function; seasonality; time-series; product life cycle; gaussian distribution; forecasting"}, "Pengelompokan pola musiman dengan adanya kesilapan Pengelompokan adalah masalah yang dikaji dengan baik yang cuba mengelompokkan titik data yang serupa. Sebilangan besar algoritma pengelompokan tradisional menganggap bahawa data diberikan tanpa ralat pengukuran. Namun, seringkali, set data dunia nyata mempunyai kesalahan seperti itu dan seseorang dapat memperoleh anggaran kesalahan ini. Kami menyajikan kaedah pengelompokan yang menggabungkan maklumat yang terdapat dalam anggaran ralat ini. Kami menyajikan fungsi jarak baru yang berdasarkan pengedaran kesalahan dalam data. Dengan menggunakan model Gaussian untuk kesilapan, fungsi jarak mengikuti taburan Chi-Square dan mudah dikira. Fungsi jarak ini digunakan dalam pengelompokan hierarki untuk mencari kelompok yang bermakna. Fungsi jarak tidak berubah-ubah sehingga hasil pengelompokan tidak bergantung pada unit pengukuran data. Dalam kes khas apabila taburan ralat adalah sama untuk setiap atribut titik data, urutan kedudukan jarak sepasang pasangan adalah sama untuk fungsi jarak kita dan fungsi jarak Euclidean. Metode pengelompokan diterapkan pada masalah estimasi musim dan hasil eksperimen disajikan untuk data industri runcit dan juga untuk data simulasi, di mana ia mengungguli kaedah pengelompokan klasik. [[EENNDD]] fungsi jarak; bermusim; siri masa; kitaran hayat produk; pengedaran gaussian; ramalan"], [{"string": "Towards NIC-based intrusion detection We present and evaluate a NIC-based network intrusion detection system . Intrusion detection at the NIC makes the system potentially tamper-proof and is naturally extensible to work in a distributed setting . Simple anomaly detection and signature detection based models have been implemented on the NIC firmware , which has its own processor and memory . We empirically evaluate such systems from the perspective of quality and performance bandwidth of acceptable messages under varying conditions of host load . The preliminary results we obtain are very encouraging and lead us to believe that such NIC-based security schemes could very well be a crucial part of next generation network security systems .", "keywords": ["data mining", "network security", "network interface cards", "nics", "network intrusion detection"], "combined": "Towards NIC-based intrusion detection We present and evaluate a NIC-based network intrusion detection system . Intrusion detection at the NIC makes the system potentially tamper-proof and is naturally extensible to work in a distributed setting . Simple anomaly detection and signature detection based models have been implemented on the NIC firmware , which has its own processor and memory . We empirically evaluate such systems from the perspective of quality and performance bandwidth of acceptable messages under varying conditions of host load . The preliminary results we obtain are very encouraging and lead us to believe that such NIC-based security schemes could very well be a crucial part of next generation network security systems . [[EENNDD]] data mining; network security; network interface cards; nics; network intrusion detection"}, "Ke arah pengesanan pencerobohan berasaskan NIC Kami mempersembahkan dan menilai sistem pengesanan pencerobohan rangkaian berasaskan NIC. Pengesanan pencerobohan di NIC menjadikan sistem berpotensi tahan terhadap gangguan dan secara semula jadi dapat digunakan untuk bekerja dalam suasana yang diedarkan. Model berdasarkan pengesanan anomali dan pengesanan tanda tangan telah dilaksanakan pada firmware NIC, yang mempunyai pemproses dan memori tersendiri. Kami secara empirikal menilai sistem tersebut dari perspektif lebar jalur prestasi dan prestasi mesej yang boleh diterima dalam keadaan beban host yang berbeza-beza. Hasil awal yang kami perolehi sangat menggembirakan dan mendorong kami untuk mempercayai bahawa skema keselamatan berasaskan NIC seperti ini boleh menjadi bahagian penting dalam sistem keselamatan rangkaian generasi akan datang. [[EENNDD]] perlombongan data; keselamatan rangkaian; kad antara muka rangkaian; nics; pengesanan pencerobohan rangkaian"], [{"string": "Experiences with mining temporal event sequences from electronic medical records : initial successes and some challenges The standardization and wider use of electronic medical records EMR creates opportunities for better understanding patterns of illness and care within and across medical systems . Our interest is in the temporal history of event codes embedded in patients ' records , specifically investigating frequently occurring sequences of event codes across patients . In studying data from more than 1.6 million patient histories at the University of Michigan Health system we quickly realized that frequent sequences , while providing one level of data reduction , still constitute a serious analytical challenge as many involve alternate serializations of the same sets of codes . To further analyze these sequences , we designed an approach where a partial order is mined from frequent sequences of codes . We demonstrate an EMR mining system called EMRView that enables exploration of the precedence relationships to quickly identify and visualize partial order information encoded in key classes of patients . We demonstrate some important nuggets learned through our approach and also outline key challenges for future research based on our experiences .", "keywords": ["medical informatics", "partial orders", "temporal data mining"], "combined": "Experiences with mining temporal event sequences from electronic medical records : initial successes and some challenges The standardization and wider use of electronic medical records EMR creates opportunities for better understanding patterns of illness and care within and across medical systems . Our interest is in the temporal history of event codes embedded in patients ' records , specifically investigating frequently occurring sequences of event codes across patients . In studying data from more than 1.6 million patient histories at the University of Michigan Health system we quickly realized that frequent sequences , while providing one level of data reduction , still constitute a serious analytical challenge as many involve alternate serializations of the same sets of codes . To further analyze these sequences , we designed an approach where a partial order is mined from frequent sequences of codes . We demonstrate an EMR mining system called EMRView that enables exploration of the precedence relationships to quickly identify and visualize partial order information encoded in key classes of patients . We demonstrate some important nuggets learned through our approach and also outline key challenges for future research based on our experiences . [[EENNDD]] medical informatics; partial orders; temporal data mining"}, "Pengalaman dengan perlombongan urutan peristiwa temporal dari rekod perubatan elektronik: kejayaan awal dan beberapa cabaran Penyeragaman dan penggunaan rekod perubatan elektronik yang lebih luas EMR memberi peluang untuk memahami corak penyakit dan perawatan yang lebih baik di dalam dan di seluruh sistem perubatan. Minat kami adalah dalam sejarah temporal kod peristiwa yang tertanam dalam rekod pesakit, secara khusus menyelidiki urutan kod peristiwa yang sering berlaku di seluruh pesakit. Dalam mengkaji data dari lebih daripada 1.6 juta sejarah pesakit di sistem Kesihatan University of Michigan, kami dengan cepat menyedari bahawa urutan yang kerap, sambil memberikan satu tahap pengurangan data, masih merupakan cabaran analitik yang serius kerana banyak yang melibatkan siri bersiri dari kod yang sama. Untuk menganalisis urutan ini dengan lebih lanjut, kami merancang pendekatan di mana pesanan separa ditambang dari urutan kod yang kerap. Kami menunjukkan sistem perlombongan EMR yang disebut EMRView yang membolehkan penerokaan hubungan keutamaan untuk mengenal pasti dan memvisualisasikan maklumat pesanan separa yang dikodkan dalam kelas utama pesakit. Kami menunjukkan beberapa nugget penting yang dipelajari melalui pendekatan kami dan juga menggariskan cabaran utama untuk penyelidikan masa depan berdasarkan pengalaman kami. [[EENNDD]] maklumat perubatan; pesanan separa; perlombongan data temporal"], [{"string": "The complexity of mining maximal frequent itemsets and maximal frequent patterns Mining maximal frequent itemsets is one of the most fundamental problems in data mining . In this paper we study the complexity-theoretic aspects of maximal frequent itemset mining , from the perspective of counting the number of solutions . We present the first formal proof that the problem of counting the number of distinct maximal frequent itemsets in a database of transactions , given an arbitrary support threshold , is #P - complete , thereby providing strong theoretical evidence that the problem of mining maximal frequent itemsets is NP-hard . This result is of particular interest since the associated decision problem of checking the existence of a maximal frequent itemset is in P. We also extend our complexity analysis to other similar data mining problems dealing with complex data structures , such as sequences , trees , and graphs , which have attracted intensive research interests in recent years . Normally , in these problems a partial order among frequent patterns can be defined in such a way as to preserve the downward closure property , with maximal frequent patterns being those without any successor with respect to this partial order . We investigate several variants of these mining problems in which the patterns of interest are subsequences , subtrees , or subgraphs , and show that the associated problems of counting the number of maximal frequent patterns are all either #P - complete or #P - hard .", "keywords": ["data mining", "maximal frequent itemset", "complexity", "nonnumerical algorithms and problems", "maximal frequent pattern"], "combined": "The complexity of mining maximal frequent itemsets and maximal frequent patterns Mining maximal frequent itemsets is one of the most fundamental problems in data mining . In this paper we study the complexity-theoretic aspects of maximal frequent itemset mining , from the perspective of counting the number of solutions . We present the first formal proof that the problem of counting the number of distinct maximal frequent itemsets in a database of transactions , given an arbitrary support threshold , is #P - complete , thereby providing strong theoretical evidence that the problem of mining maximal frequent itemsets is NP-hard . This result is of particular interest since the associated decision problem of checking the existence of a maximal frequent itemset is in P. We also extend our complexity analysis to other similar data mining problems dealing with complex data structures , such as sequences , trees , and graphs , which have attracted intensive research interests in recent years . Normally , in these problems a partial order among frequent patterns can be defined in such a way as to preserve the downward closure property , with maximal frequent patterns being those without any successor with respect to this partial order . We investigate several variants of these mining problems in which the patterns of interest are subsequences , subtrees , or subgraphs , and show that the associated problems of counting the number of maximal frequent patterns are all either #P - complete or #P - hard . [[EENNDD]] data mining; maximal frequent itemset; complexity; nonnumerical algorithms and problems; maximal frequent pattern"}, "Kerumitan perlombongan set item kerap maksimum dan corak kekerapan maksima Perlombongan set item kerap maksimum adalah salah satu masalah paling asas dalam perlombongan data. Dalam makalah ini kita mengkaji aspek kerumitan-teori dari perlombongan itemet kerap, dari perspektif menghitung jumlah penyelesaian. Kami mengemukakan bukti rasmi pertama bahawa masalah penghitungan bilangan set item kerap maksimum yang berbeza dalam pangkalan data urus niaga, yang diberi ambang sokongan sewenang-wenangnya, adalah #P - lengkap, dengan itu memberikan bukti teoritikal yang kuat bahawa masalah perlombongan set item paling kerap adalah NP-keras. Hasil ini menarik perhatian kerana masalah keputusan yang berkaitan untuk memeriksa keberadaan itemet yang kerap adalah di P. Kami juga memperluas analisis kerumitan kami kepada masalah perlombongan data serupa yang berkaitan dengan struktur data yang kompleks, seperti urutan, pokok, dan grafik , yang telah menarik minat penyelidikan intensif dalam beberapa tahun terakhir. Biasanya, dalam masalah-masalah ini, urutan separa di antara corak yang kerap dapat didefinisikan sedemikian rupa untuk memelihara harta penutupan ke bawah, dengan corak kerap yang maksimum adalah yang tanpa pengganti berkenaan dengan urutan separa ini. Kami menyiasat beberapa varian masalah penambangan ini di mana corak minatnya adalah turutan, subtitre, atau subgraf, dan menunjukkan bahawa masalah yang berkaitan dengan menghitung bilangan corak kekerapan maksimum adalah sama ada #P - lengkap atau #P - keras. [[EENNDD]] perlombongan data; itemset kerap maksimum; kerumitan; algoritma dan masalah bukan berangka; corak kekerapan maksimum"], [{"string": "Extracting collective probabilistic forecasts from web games Game sites on the World Wide Web draw people from around the world with specialized interests , skills , and knowledge . Data from the games often reflects the players ' expertise and will to win . We extract probabilistic forecasts from data obtained from three online games : the Hollywood Stock Exchange HSX , the Foresight Exchange FX , and the Formula One Pick Six F1P6 competition . We find that all three yield accurate forecasts of uncertain future events . In particular , prices of so-called `` movie stocks '' on HSX are good indicators of actual box office returns . Prices of HSX securities in Oscar , Emmy , and Grammy awards correlate well with observed frequencies of winning . FX prices are reliable indicators of future developments in science and technology . Collective predictions from players in the F1 competition serve as good forecasts of true race outcomes . In some cases , forecasts induced from game data are more reliable than expert opinions . We argue that web games naturally attract well-informed and well-motivated players , and thus offer a valuable and oft-overlooked source of high-quality data with significant predictive value .", "keywords": ["hollywood stock exchange", "formula one pick six competition", "collective probabilistic forecasts", "world wide web games", "artificial markets", "foresight exchange", "knowledge discovery"], "combined": "Extracting collective probabilistic forecasts from web games Game sites on the World Wide Web draw people from around the world with specialized interests , skills , and knowledge . Data from the games often reflects the players ' expertise and will to win . We extract probabilistic forecasts from data obtained from three online games : the Hollywood Stock Exchange HSX , the Foresight Exchange FX , and the Formula One Pick Six F1P6 competition . We find that all three yield accurate forecasts of uncertain future events . In particular , prices of so-called `` movie stocks '' on HSX are good indicators of actual box office returns . Prices of HSX securities in Oscar , Emmy , and Grammy awards correlate well with observed frequencies of winning . FX prices are reliable indicators of future developments in science and technology . Collective predictions from players in the F1 competition serve as good forecasts of true race outcomes . In some cases , forecasts induced from game data are more reliable than expert opinions . We argue that web games naturally attract well-informed and well-motivated players , and thus offer a valuable and oft-overlooked source of high-quality data with significant predictive value . [[EENNDD]] hollywood stock exchange; formula one pick six competition; collective probabilistic forecasts; world wide web games; artificial markets; foresight exchange; knowledge discovery"}, "Mengambil ramalan probabilistik kolektif dari permainan web Laman web permainan di World Wide Web menarik orang dari seluruh dunia dengan minat, kemahiran, dan pengetahuan khusus. Data dari permainan sering mencerminkan kepakaran dan kehendak pemain untuk menang. Kami mengambil ramalan probabilistik dari data yang diperoleh dari tiga permainan dalam talian: Hollywood Stock Exchange HSX, Foresight Exchange FX, dan pertandingan Formula One Pick Six F1P6. Kami dapati ketiga-tiga menghasilkan ramalan tepat mengenai kejadian masa depan yang tidak menentu. Khususnya, harga yang disebut \"stok filem\" di HSX adalah petunjuk yang baik untuk pulangan box office sebenar. Harga sekuriti HSX di Oscar, Emmy, dan anugerah Grammy berkorelasi dengan frekuensi kemenangan yang diperhatikan. Harga FX adalah petunjuk yang boleh dipercayai untuk perkembangan masa depan dalam sains dan teknologi. Ramalan kolektif dari pemain dalam pertandingan F1 berfungsi sebagai ramalan hasil perlumbaan yang benar. Dalam beberapa kes, ramalan yang disebabkan oleh data permainan lebih dipercayai daripada pendapat pakar. Kami berpendapat bahawa permainan web secara semula jadi menarik pemain yang berpengetahuan dan bermotivasi dengan baik, dan dengan itu menawarkan sumber data berkualiti tinggi yang berharga dan sering diabaikan dengan nilai ramalan yang signifikan. [[EENNDD]] bursa saham hollywood; formula satu memilih enam pertandingan; ramalan kebarangkalian kolektif; permainan web seluruh dunia; pasaran tiruan; pertukaran pandangan jauh; penemuan pengetahuan"], [{"string": "Optimizing web traffic via the media scheduling problem Website traffic varies through time in consistent and predictable ways , with highest traffic in the middle of the day . When providing media content to visitors , it is important to present repeat visitors with new content so that they keep coming back . In this paper we present an algorithm to balance the need to keep a website fresh with new content with the desire to present the best content to the most visitors at times of peak traffic . We formulate this as the media scheduling problem , where we attempt to maximize total clicks , given the overall traffic pattern and the time varying clickthrough rates of available media content . We present an efficient algorithm to perform this scheduling under certain conditions and apply this algorithm to real data obtained from server logs , showing evidence of significant improvements in traffic from our algorithmic schedules . Finally , we analyze the click data , presenting models for why and how the clickthrough rate for new content declines as it ages .", "keywords": ["user interaction", "media scheduling", "human response"], "combined": "Optimizing web traffic via the media scheduling problem Website traffic varies through time in consistent and predictable ways , with highest traffic in the middle of the day . When providing media content to visitors , it is important to present repeat visitors with new content so that they keep coming back . In this paper we present an algorithm to balance the need to keep a website fresh with new content with the desire to present the best content to the most visitors at times of peak traffic . We formulate this as the media scheduling problem , where we attempt to maximize total clicks , given the overall traffic pattern and the time varying clickthrough rates of available media content . We present an efficient algorithm to perform this scheduling under certain conditions and apply this algorithm to real data obtained from server logs , showing evidence of significant improvements in traffic from our algorithmic schedules . Finally , we analyze the click data , presenting models for why and how the clickthrough rate for new content declines as it ages . [[EENNDD]] user interaction; media scheduling; human response"}, "Mengoptimumkan lalu lintas web melalui masalah penjadualan media Trafik laman web berbeza mengikut masa dengan cara yang konsisten dan dapat diramalkan, dengan lalu lintas tertinggi pada tengah hari. Semasa menyediakan kandungan media kepada pelawat, penting untuk menunjukkan pengunjung baru dengan kandungan baru agar mereka terus kembali. Dalam makalah ini kami menyajikan algoritma untuk mengimbangi keperluan menjaga laman web segar dengan kandungan baru dengan keinginan untuk menyampaikan kandungan terbaik kepada pengunjung paling banyak pada waktu lalu lintas. Kami merumuskan ini sebagai masalah penjadualan media, di mana kami berusaha memaksimumkan jumlah klik, mengingat keseluruhan corak lalu lintas dan waktu yang berbeza-beza kadar klik lalu kandungan media yang tersedia. Kami menyajikan algoritma yang cekap untuk melaksanakan penjadualan ini dalam keadaan tertentu dan menerapkan algoritma ini pada data sebenar yang diperoleh dari log pelayan, yang menunjukkan bukti peningkatan lalu lintas yang ketara dari jadual algoritma kami. Akhirnya, kami menganalisis data klik, menyajikan model mengapa dan bagaimana kadar klik-tayang untuk kandungan baru menurun seiring bertambahnya usia. [[EENNDD]] interaksi pengguna; penjadualan media; tindak balas manusia"], [{"string": "On mining cross-graph quasi-cliques Joint mining of multiple data sets can often discover interesting , novel , and reliable patterns which can not be obtained solely from any single source . For example , in cross-market customer segmentation , a group of customers who behave similarly in multiple markets should be considered as a more coherent and more reliable cluster than clusters found in a single market . As another example , in bioinformatics , by joint mining of gene expression data and protein interaction data , we can find clusters of genes which show coherent expression patterns and also produce interacting proteins . Such clusters may be potential pathways . In this paper , we investigate a novel data mining problem , mining cross-graph quasi-cliques , which is generalized from several interesting applications such as cross-market customer segmentation and joint mining of gene expression data and protein interaction data . We build a general model for mining cross-graph quasi-cliques , show why the complete set of cross-graph quasi-cliques can not be found by previous data mining methods , and study the complexity of the problem . While the problem is difficult , we develop an efficient algorithm , Crochet , which exploits several interesting and effective techniques and heuristics to efficaciously mine cross-graph quasi-cliques . A systematic performance study is reported on both synthetic and real data sets . We demonstrate some interesting and meaningful cross-graph quasi-cliques in bioinformatics . The experimental results also show that algorithm Crochet is efficient and scalable .", "keywords": ["bioinformatics", "patterns", "graph mining"], "combined": "On mining cross-graph quasi-cliques Joint mining of multiple data sets can often discover interesting , novel , and reliable patterns which can not be obtained solely from any single source . For example , in cross-market customer segmentation , a group of customers who behave similarly in multiple markets should be considered as a more coherent and more reliable cluster than clusters found in a single market . As another example , in bioinformatics , by joint mining of gene expression data and protein interaction data , we can find clusters of genes which show coherent expression patterns and also produce interacting proteins . Such clusters may be potential pathways . In this paper , we investigate a novel data mining problem , mining cross-graph quasi-cliques , which is generalized from several interesting applications such as cross-market customer segmentation and joint mining of gene expression data and protein interaction data . We build a general model for mining cross-graph quasi-cliques , show why the complete set of cross-graph quasi-cliques can not be found by previous data mining methods , and study the complexity of the problem . While the problem is difficult , we develop an efficient algorithm , Crochet , which exploits several interesting and effective techniques and heuristics to efficaciously mine cross-graph quasi-cliques . A systematic performance study is reported on both synthetic and real data sets . We demonstrate some interesting and meaningful cross-graph quasi-cliques in bioinformatics . The experimental results also show that algorithm Crochet is efficient and scalable . [[EENNDD]] bioinformatics; patterns; graph mining"}, "Mengenai grafik silang silang perlombongan Penambangan bersama pelbagai set data sering kali dapat menemui corak menarik, baru, dan boleh dipercayai yang tidak dapat diperolehi hanya dari satu sumber tunggal. Sebagai contoh, dalam segmentasi pelanggan lintas pasaran, sekumpulan pelanggan yang berkelakuan serupa di beberapa pasaran harus dianggap sebagai kelompok yang lebih koheren dan lebih dipercayai daripada kelompok yang terdapat di satu pasaran. Sebagai contoh lain, dalam bioinformatik, dengan gabungan data ekspresi gen dan data interaksi protein, kita dapat menemui kumpulan gen yang menunjukkan corak ekspresi yang koheren dan juga menghasilkan protein yang berinteraksi. Kluster seperti itu mungkin merupakan jalan berpotensi. Dalam makalah ini, kami menyiasat masalah penambangan data baru, penambangan kuasi-silang grafik silang, yang digeneralisasi dari beberapa aplikasi menarik seperti segmentasi pelanggan lintas pasaran dan perlombongan bersama data ekspresi gen dan data interaksi protein. Kami membina model umum untuk perlombongan kuasi-silang grafik silang, menunjukkan mengapa set lengkap kuasi-silang grafik silang tidak dapat dijumpai dengan kaedah perlombongan data sebelumnya, dan mengkaji kerumitan masalah. Walaupun masalahnya sukar, kami mengembangkan algoritma yang cekap, Crochet, yang memanfaatkan beberapa teknik dan heuristik yang menarik dan berkesan untuk membuat kuasi-silang grafik silang dengan berkesan. Kajian prestasi sistematik dilaporkan pada kedua-dua kumpulan data sintetik dan sebenar. Kami menunjukkan beberapa kuasi-silang grafik silang yang menarik dan bermakna dalam bioinformatik. Hasil eksperimen juga menunjukkan bahawa algoritma Crochet cekap dan berskala. [[EENNDD]] bioinformatik; corak; perlombongan grafik"], [{"string": "Colibri : fast mining of large static and dynamic graphs Low-rank approximations of the adjacency matrix of a graph are essential in finding patterns such as communities and detecting anomalies . Additionally , it is desirable to track the low-rank structure as the graph evolves over time , efficiently and within limited storage . Real graphs typically have thousands or millions of nodes , but are usually very sparse . However , standard decompositions such as SVD do not preserve sparsity . This has led to the development of methods such as CUR and CMD , which seek a non-orthogonal basis by sampling the columns and\\/or rows of the sparse matrix . However , these approaches will typically produce overcomplete bases , which wastes both space and time . In this paper we propose the family of Colibri methods to deal with these challenges . Our version for static graphs , Colibri-S , iteratively finds a non-redundant basis and we prove that it has no loss of accuracy compared to the best competitors CUR and CMD , while achieving significant savings in space and time : on real data , Colibri-S requires much less space and is orders of magnitude faster in proportion to the square of the number of non-redundant columns . Additionally , we propose an efficient update algorithm for dynamic , time-evolving graphs , Colibri-D . Our evaluation on a large , real network traffic dataset shows that Colibri-D is over 100 times faster than the best published competitor CMD .", "keywords": ["low-rank approximation", "graph mining", "scalability"], "combined": "Colibri : fast mining of large static and dynamic graphs Low-rank approximations of the adjacency matrix of a graph are essential in finding patterns such as communities and detecting anomalies . Additionally , it is desirable to track the low-rank structure as the graph evolves over time , efficiently and within limited storage . Real graphs typically have thousands or millions of nodes , but are usually very sparse . However , standard decompositions such as SVD do not preserve sparsity . This has led to the development of methods such as CUR and CMD , which seek a non-orthogonal basis by sampling the columns and\\/or rows of the sparse matrix . However , these approaches will typically produce overcomplete bases , which wastes both space and time . In this paper we propose the family of Colibri methods to deal with these challenges . Our version for static graphs , Colibri-S , iteratively finds a non-redundant basis and we prove that it has no loss of accuracy compared to the best competitors CUR and CMD , while achieving significant savings in space and time : on real data , Colibri-S requires much less space and is orders of magnitude faster in proportion to the square of the number of non-redundant columns . Additionally , we propose an efficient update algorithm for dynamic , time-evolving graphs , Colibri-D . Our evaluation on a large , real network traffic dataset shows that Colibri-D is over 100 times faster than the best published competitor CMD . [[EENNDD]] low-rank approximation; graph mining; scalability"}, "Colibri: perlombongan pantas grafik statik dan dinamik yang besar Pendekatan peringkat rendah dari matriks adjacency dari grafik sangat penting dalam mencari corak seperti komuniti dan mengesan anomali. Selain itu, adalah wajar untuk mengesan struktur peringkat rendah kerana grafiknya berkembang dari masa ke masa, dengan cekap dan dalam simpanan terhad. Grafik sebenar biasanya mempunyai ribuan atau berjuta-juta nod, tetapi biasanya sangat jarang. Walau bagaimanapun, penguraian standard seperti SVD tidak mengekalkan kelangkaan. Ini telah menyebabkan pengembangan kaedah seperti CUR dan CMD, yang mencari asas bukan ortogonal dengan mengambil sampel lajur dan \\ / atau baris matriks jarang. Walau bagaimanapun, pendekatan ini biasanya akan menghasilkan asas yang tidak lengkap, yang membuang masa dan masa. Dalam makalah ini kami mencadangkan keluarga Colibri untuk menangani cabaran ini. Versi kami untuk grafik statik, Colibri-S, secara berulang mencari asas yang tidak berlebihan dan kami membuktikan bahawa ia tidak kehilangan ketepatan berbanding dengan CUR dan CMD pesaing terbaik, sambil mencapai penjimatan yang besar dalam ruang dan masa: pada data sebenar, Colibri -S memerlukan ruang yang jauh lebih sedikit dan pesanan magnitud lebih cepat berkadaran dengan kuadrat bilangan lajur yang tidak berlebihan. Di samping itu, kami mencadangkan algoritma kemas kini yang cekap untuk grafik dinamik dan berubah-ubah masa, Colibri-D. Penilaian kami terhadap kumpulan data lalu lintas rangkaian yang besar menunjukkan bahawa Colibri-D lebih 100 kali lebih pantas daripada CMD pesaing terbitan terbaik. [[EENNDD]] penghampiran peringkat rendah; perlombongan grafik; skalabiliti"], [{"string": "Mining adaptively frequent closed unlabeled rooted trees in data streams Closed patterns are powerful representatives of frequent patterns , since they eliminate redundant information . We propose a new approach for mining closed unlabeled rooted trees adaptively from data streams that change over time . Our approach is based on an efficient representation of trees and a low complexity notion of relaxed closed trees , and leads to an on-line strategy and an adaptive sliding window technique for dealing with changes over time . More precisely , we first present a general methodology to identify closed patterns in a data stream , using Galois Lattice Theory . Using this methodology , we then develop three closed tree mining algorithms : an incremental one IncTreeNat , a sliding-window based one , WinTreeNat , and finally one that mines closed trees adaptively from data streams , AdaTreeNat . To the best of our knowledge this is the first work on mining frequent closed trees in streaming data varying with time . We give a first experimental evaluation of the proposed algorithms .", "keywords": ["data streams", "concept drift", "closed mining", "patterns", "trees"], "combined": "Mining adaptively frequent closed unlabeled rooted trees in data streams Closed patterns are powerful representatives of frequent patterns , since they eliminate redundant information . We propose a new approach for mining closed unlabeled rooted trees adaptively from data streams that change over time . Our approach is based on an efficient representation of trees and a low complexity notion of relaxed closed trees , and leads to an on-line strategy and an adaptive sliding window technique for dealing with changes over time . More precisely , we first present a general methodology to identify closed patterns in a data stream , using Galois Lattice Theory . Using this methodology , we then develop three closed tree mining algorithms : an incremental one IncTreeNat , a sliding-window based one , WinTreeNat , and finally one that mines closed trees adaptively from data streams , AdaTreeNat . To the best of our knowledge this is the first work on mining frequent closed trees in streaming data varying with time . We give a first experimental evaluation of the proposed algorithms . [[EENNDD]] data streams; concept drift; closed mining; patterns; trees"}, "Perlombongan pokok berakar tanpa label yang sering ditutup secara adaptif dalam aliran data Corak tertutup merupakan wakil kuat bagi corak yang kerap, kerana ia menghilangkan maklumat yang berlebihan. Kami mencadangkan pendekatan baru untuk penambangan pokok berakar tertutup yang tidak berlabel secara adaptif dari aliran data yang berubah dari masa ke masa. Pendekatan kami didasarkan pada perwakilan pokok yang cekap dan konsep kerumitan rendah mengenai pokok tertutup yang santai, dan mengarah pada strategi on-line dan teknik slaid window adaptif untuk menangani perubahan dari masa ke masa. Lebih tepatnya, kami pertama kali menyajikan metodologi umum untuk mengenal pasti pola tertutup dalam aliran data, menggunakan Teori Kisi Galois. Dengan menggunakan metodologi ini, kami kemudian mengembangkan tiga algoritma perlombongan pokok tertutup: satu inkremental IncTreeNat, yang berdasarkan tingkap gelangsar, WinTreeNat, dan akhirnya yang menambang pokok tertutup secara adaptif dari aliran data, AdaTreeNat. Sepengetahuan kami, ini adalah kerja pertama melombong pokok yang sering ditutup dalam streaming data yang berbeza-beza mengikut masa. Kami memberikan penilaian eksperimental pertama terhadap algoritma yang dicadangkan. [[EENNDD]] aliran data; drift konsep; perlombongan tertutup; corak; pokok"], [{"string": "GPLAG : detection of software plagiarism by program dependence graph analysis Along with the blossom of open source projects comes the convenience for software plagiarism . A company , if less self-disciplined , may be tempted to plagiarize some open source projects for its own products . Although current plagiarism detection tools appear sufficient for academic use , they are nevertheless short for fighting against serious plagiarists . For example , disguises like statement reordering and code insertion can effectively confuse these tools . In this paper , we develop a new plagiarism detection tool , called GPLAG , which detects plagiarism by mining program dependence graphs PDGs . A PDG is a graphic representation of the data and control dependencies within a procedure . Because PDGs are nearly invariant during plagiarism , GPLAG is more effective than state-of-the-art tools for plagiarism detection . In order to make GPLAG scalable to large programs , a statistical lossy filter is proposed to prune the plagiarism search space . Experiment study shows that GPLAG is both effective and efficient : It detects plagiarism that easily slips over existing tools , and it usually takes a few seconds to find simulated plagiarism in programs having thousands of lines of code .", "keywords": ["program dependence graph", "graph mining", "software plagiarism detection"], "combined": "GPLAG : detection of software plagiarism by program dependence graph analysis Along with the blossom of open source projects comes the convenience for software plagiarism . A company , if less self-disciplined , may be tempted to plagiarize some open source projects for its own products . Although current plagiarism detection tools appear sufficient for academic use , they are nevertheless short for fighting against serious plagiarists . For example , disguises like statement reordering and code insertion can effectively confuse these tools . In this paper , we develop a new plagiarism detection tool , called GPLAG , which detects plagiarism by mining program dependence graphs PDGs . A PDG is a graphic representation of the data and control dependencies within a procedure . Because PDGs are nearly invariant during plagiarism , GPLAG is more effective than state-of-the-art tools for plagiarism detection . In order to make GPLAG scalable to large programs , a statistical lossy filter is proposed to prune the plagiarism search space . Experiment study shows that GPLAG is both effective and efficient : It detects plagiarism that easily slips over existing tools , and it usually takes a few seconds to find simulated plagiarism in programs having thousands of lines of code . [[EENNDD]] program dependence graph; graph mining; software plagiarism detection"}, "GPLAG: pengesanan plagiarisme perisian dengan analisis grafik ketergantungan program Seiring dengan berkembangnya projek sumber terbuka terdapat kemudahan untuk plagiarisme perisian. Sebuah syarikat, jika kurang berdisiplin, mungkin tergoda untuk menjiplak beberapa projek sumber terbuka untuk produknya sendiri. Walaupun alat pengesanan plagiarisme semasa nampaknya cukup untuk penggunaan akademik, mereka masih kekurangan untuk melawan penipuan yang serius. Contohnya, penyamaran seperti penyusunan semula penyataan dan penyisipan kod dapat mengelirukan alat ini dengan berkesan. Dalam makalah ini, kami mengembangkan alat pengesanan plagiarisme baru, yang disebut GPLAG, yang mengesan plagiarisme dengan grafik pergantungan program perlombongan PDG. PDG adalah gambaran grafik data dan kawalan kebergantungan dalam prosedur. Oleh kerana PDG hampir tidak berubah semasa melakukan plagiarisme, GPLAG lebih berkesan daripada alat canggih untuk mengesan plagiarisme. Untuk menjadikan GPLAG dapat disesuaikan dengan program besar, penapis statistik hilang dicadangkan untuk memangkas ruang carian plagiarisme. Kajian eksperimen menunjukkan bahawa GPLAG berkesan dan efisien: Ia mengesan plagiat yang mudah melepasi alat yang ada, dan biasanya memerlukan beberapa saat untuk mencari simulasi plagiarisme dalam program yang mempunyai ribuan baris kod. [[EENNDD]] grafik pergantungan program; perlombongan grafik; pengesanan plagiarisme perisian"], [{"string": "Real-time ranking with concept drift using expert advice In many practical applications , one is interested in generating a ranked list of items using information mined from continuous streams of data . For example , in the context of computer networks , one might want to generate lists of nodes ranked according to their susceptibility to attack . In addition , real-world data streams often exhibit concept drift , making the learning task even more challenging . We present an online learning approach to ranking with concept drift , using weighted majority techniques . By continuously modeling different snapshots of the data and tuning our measure of belief in these models over time , we capture changes in the underlying concept and adapt our predictions accordingly . We measure the performance of our algorithm on real electricity data as well as asynthetic data stream , and demonstrate that our approach to ranking from stream data outperforms previously known batch-learning methods and other online methods that do not account for concept drift .", "keywords": ["ranking", "data streams", "learning", "miscellaneous", "concept drift", "online learning"], "combined": "Real-time ranking with concept drift using expert advice In many practical applications , one is interested in generating a ranked list of items using information mined from continuous streams of data . For example , in the context of computer networks , one might want to generate lists of nodes ranked according to their susceptibility to attack . In addition , real-world data streams often exhibit concept drift , making the learning task even more challenging . We present an online learning approach to ranking with concept drift , using weighted majority techniques . By continuously modeling different snapshots of the data and tuning our measure of belief in these models over time , we capture changes in the underlying concept and adapt our predictions accordingly . We measure the performance of our algorithm on real electricity data as well as asynthetic data stream , and demonstrate that our approach to ranking from stream data outperforms previously known batch-learning methods and other online methods that do not account for concept drift . [[EENNDD]] ranking; data streams; learning; miscellaneous; concept drift; online learning"}, "Ranking masa nyata dengan konsep konsep menggunakan nasihat pakar Dalam banyak aplikasi praktikal, seseorang berminat untuk menghasilkan senarai item yang diperingkat menggunakan maklumat yang ditambang dari aliran data yang berterusan. Sebagai contoh, dalam konteks rangkaian komputer, seseorang mungkin ingin membuat senarai node yang diberi peringkat mengikut kerentanan mereka untuk menyerang. Di samping itu, aliran data dunia nyata sering menunjukkan konsep yang semakin meningkat, menjadikan tugas pembelajaran menjadi lebih mencabar. Kami menyajikan pendekatan pembelajaran dalam talian untuk peringkat dengan konsep konsep, menggunakan teknik majoriti tertimbang. Dengan terus memodelkan snapshot data yang berbeza dan menyesuaikan ukuran kepercayaan kami terhadap model-model ini dari masa ke masa, kami menangkap perubahan dalam konsep yang mendasari dan menyesuaikan ramalan kami dengan sewajarnya. Kami mengukur prestasi algoritma kami pada data elektrik sebenar dan juga aliran data asynthetic, dan menunjukkan bahawa pendekatan kami untuk pemeringkatan dari data aliran mengatasi kaedah pembelajaran kumpulan sebelumnya yang diketahui dan kaedah dalam talian lain yang tidak mengambil kira konsep konsep. [[EENNDD]] kedudukan; aliran data; belajar; pelbagai; drift konsep; pembelajaran dalam talian"], [{"string": "Maximally informative k-itemsets and their efficient discovery In this paper we present a new approach to mining binary data . We treat each binary feature item as a means of distinguishing two sets of examples . Our interest is in selecting from the total set of items an itemset of specified size , such that the database is partitioned with as uniform a distribution over the parts as possible . To achieve this goal , we propose the use of joint entropy as a quality measure for itemsets , and refer to optimal itemsets of cardinality k as maximally informative k-itemsets . We claim that this approach maximises distinctive power , as well as minimises redundancy within the feature set . A number of algorithms is presented for computing optimal itemsets efficiently .", "keywords": ["learning", "maximally informative k-itemsets", "systems and information theory", "information theory", "feature selection", "binary data", "subgroup discovery", "joint entropy"], "combined": "Maximally informative k-itemsets and their efficient discovery In this paper we present a new approach to mining binary data . We treat each binary feature item as a means of distinguishing two sets of examples . Our interest is in selecting from the total set of items an itemset of specified size , such that the database is partitioned with as uniform a distribution over the parts as possible . To achieve this goal , we propose the use of joint entropy as a quality measure for itemsets , and refer to optimal itemsets of cardinality k as maximally informative k-itemsets . We claim that this approach maximises distinctive power , as well as minimises redundancy within the feature set . A number of algorithms is presented for computing optimal itemsets efficiently . [[EENNDD]] learning; maximally informative k-itemsets; systems and information theory; information theory; feature selection; binary data; subgroup discovery; joint entropy"}, "K-itemets yang sangat bermaklumat dan penemuannya yang cekap Dalam makalah ini kami membentangkan pendekatan baru untuk melombong data binari. Kami memperlakukan setiap item ciri binari sebagai kaedah membezakan dua set contoh. Minat kami adalah untuk memilih dari jumlah set item yang satu set dengan ukuran yang ditentukan, sehingga pangkalan data dipisahkan dengan seragam sebaran mungkin. Untuk mencapai tujuan ini, kami mencadangkan penggunaan entropi bersama sebagai ukuran kualiti untuk set barang, dan merujuk pada set item kardinaliti yang optimum sebagai k-itemet yang paling bermaklumat. Kami mendakwa bahawa pendekatan ini memaksimumkan daya khas, dan juga meminimumkan kelebihan dalam set ciri. Sejumlah algoritma disajikan untuk mengira set item yang optimum dengan berkesan. [[EENNDD]] pembelajaran; maksimum k-item set maklumat; sistem dan teori maklumat; teori maklumat; pemilihan ciri; data binari; penemuan subkumpulan; entropi sendi"], [{"string": "On detecting differences between groups Understanding the differences between contrasting groups is a fundamental task in data analysis . This realization has led to the development of a new special purpose data mining technique , contrast-set mining . We undertook a study with a retail collaborator to compare contrast-set mining with existing rule-discovery techniques . To our surprise we observed that straightforward application of an existing commercial rule-discovery system , Magnum Opus , could successfully perform the contrast-set-mining task . This led to the realization that contrast-set mining is a special case of the more general rule-discovery task . We present the results of our study together with a proof of this conclusion .", "keywords": ["contrast-set discovery", "retailing", "information search and retrieval", "learning", "rule discovery"], "combined": "On detecting differences between groups Understanding the differences between contrasting groups is a fundamental task in data analysis . This realization has led to the development of a new special purpose data mining technique , contrast-set mining . We undertook a study with a retail collaborator to compare contrast-set mining with existing rule-discovery techniques . To our surprise we observed that straightforward application of an existing commercial rule-discovery system , Magnum Opus , could successfully perform the contrast-set-mining task . This led to the realization that contrast-set mining is a special case of the more general rule-discovery task . We present the results of our study together with a proof of this conclusion . [[EENNDD]] contrast-set discovery; retailing; information search and retrieval; learning; rule discovery"}, "Mengesan perbezaan antara kumpulan Memahami perbezaan antara kumpulan yang berbeza adalah tugas asas dalam analisis data. Kesedaran ini membawa kepada pengembangan teknik perlombongan data tujuan khas baru, perlombongan set kontras. Kami melakukan kajian dengan kolaborator runcit untuk membandingkan perlombongan set kontras dengan teknik penemuan peraturan yang ada. Yang mengejutkan kami, kami melihat bahawa aplikasi langsung sistem penemuan peraturan komersial yang ada, Magnum Opus, dapat berjaya melaksanakan tugas penambangan set-kontras. Ini membawa kepada kesedaran bahawa perlombongan set kontras adalah kes khas dari tugas penemuan peraturan yang lebih umum. Kami membentangkan hasil kajian kami bersama dengan bukti kesimpulan ini. [[EENNDD]] penemuan set kontras; peruncitan; pencarian dan pengambilan maklumat; belajar; penemuan peraturan"], [{"string": "Active exploration for learning rankings from clickthrough data We address the task of learning rankings of documents from search enginelogs of user behavior . Previous work on this problem has relied onpassively collected clickthrough data . In contrast , we show that anactive exploration strategy can provide data that leads to much fasterlearning . Specifically , we develop a Bayesian approach for selectingrankings to present users so that interactions result in more informativetraining data . Our results using the TREC-10 Web corpus , as well assynthetic data , demonstrate that a directed exploration strategy quicklyleads to users being presented improved rankings in an online learningsetting . We find that active exploration substantially outperformspassive observation and random exploration .", "keywords": ["clickthrough data", "web search", "information search and retrieval", "learning to rank", "active exploration"], "combined": "Active exploration for learning rankings from clickthrough data We address the task of learning rankings of documents from search enginelogs of user behavior . Previous work on this problem has relied onpassively collected clickthrough data . In contrast , we show that anactive exploration strategy can provide data that leads to much fasterlearning . Specifically , we develop a Bayesian approach for selectingrankings to present users so that interactions result in more informativetraining data . Our results using the TREC-10 Web corpus , as well assynthetic data , demonstrate that a directed exploration strategy quicklyleads to users being presented improved rankings in an online learningsetting . We find that active exploration substantially outperformspassive observation and random exploration . [[EENNDD]] clickthrough data; web search; information search and retrieval; learning to rank; active exploration"}, "Eksplorasi aktif untuk peringkat pembelajaran dari data klik lalu Kami menangani tugas mempelajari kedudukan dokumen dari enginelog carian tingkah laku pengguna. Kerja sebelumnya mengenai masalah ini telah bergantung pada data klik-tayang yang dikumpulkan secara tidak langsung. Sebaliknya, kami menunjukkan bahawa strategi penerokaan anaktif dapat menyediakan data yang membawa kepada pembelajaran yang lebih cepat. Secara khusus, kami mengembangkan pendekatan Bayesian untuk memilih peringkat untuk menghadirkan pengguna sehingga interaksi menghasilkan data latihan yang lebih informatif. Hasil kami menggunakan korpus Web TREC-10, serta data asynthetic, menunjukkan bahawa strategi penerokaan yang diarahkan dengan cepat membawa kepada pengguna yang diberi peringkat yang lebih baik dalam pembelajaran dalam talian. Kami mendapati bahawa penerokaan aktif secara signifikan melebihi pemerhatian yang luas dan penerokaan rawak. [[EENNDD]] data klik lalu; carian sesawang; pencarian dan pengambilan maklumat; belajar berpangkat; penerokaan aktif"], [{"string": "Evaluating similarity measures : a large-scale study in the orkut social network Online information services have grown too large for users to navigate without the help of automated tools such as collaborative filtering , which makes recommendations to users based on their collective past behavior . While many similarity measures have been proposed and individually evaluated , they have not been evaluated relative to each other in a large real-world environment . We present an extensive empirical comparison of six distinct measures of similarity for recommending online communities to members of the Orkut social network . We determine the usefulness of the different recommendations by actually measuring users ' propensity to visit and join recommended communities . We also examine how the ordering of recommendations influenced user selection , as well as interesting social issues that arise in recommending communities within a real social network .", "keywords": ["recommender system", "collaborative filtering", "on-line information services", "similarity measure", "social networks", "online communities"], "combined": "Evaluating similarity measures : a large-scale study in the orkut social network Online information services have grown too large for users to navigate without the help of automated tools such as collaborative filtering , which makes recommendations to users based on their collective past behavior . While many similarity measures have been proposed and individually evaluated , they have not been evaluated relative to each other in a large real-world environment . We present an extensive empirical comparison of six distinct measures of similarity for recommending online communities to members of the Orkut social network . We determine the usefulness of the different recommendations by actually measuring users ' propensity to visit and join recommended communities . We also examine how the ordering of recommendations influenced user selection , as well as interesting social issues that arise in recommending communities within a real social network . [[EENNDD]] recommender system; collaborative filtering; on-line information services; similarity measure; social networks; online communities"}, "Menilai ukuran kesamaan: kajian berskala besar di rangkaian sosial orkut Perkhidmatan maklumat dalam talian telah berkembang terlalu besar untuk pengguna menavigasi tanpa bantuan alat automatik seperti penapisan kolaboratif, yang membuat cadangan kepada pengguna berdasarkan tingkah laku masa lalu kolektif mereka. Walaupun banyak ukuran kesamaan telah diusulkan dan dinilai secara individu, mereka tidak dinilai satu sama lain dalam lingkungan dunia nyata yang besar. Kami menyajikan perbandingan empirikal dari enam ukuran kesamaan yang berbeza untuk mengesyorkan komuniti dalam talian kepada anggota rangkaian sosial Orkut. Kami menentukan kegunaan pelbagai cadangan dengan benar-benar mengukur kecenderungan pengguna untuk mengunjungi dan bergabung dengan komuniti yang disyorkan. Kami juga mengkaji bagaimana susunan cadangan mempengaruhi pemilihan pengguna, serta masalah sosial yang menarik yang timbul dalam mengesyorkan komuniti dalam rangkaian sosial yang sebenarnya. [[EENNDD]] sistem cadangan; penapisan kolaboratif; perkhidmatan maklumat dalam talian; ukuran kesamaan; rangkaian sosial; komuniti dalam talian"], [{"string": "Detection of emerging space-time clusters We propose a new class of spatio-temporal cluster detection methods designed for the rapid detection of emerging space-time clusters . We focus on the motivating application of prospective disease surveillance : detecting space-time clusters of disease cases resulting from an emerging disease outbreak . Automatic , real-time detection of outbreaks can enable rapid epidemiological response , potentially reducing rates of morbidity and mortality . Building on the prior work on spatial and space-time scan statistics , our methods combine time series analysis to determine how many cases we expect to observe for a given spatial region in a given time interval with new `` emerging cluster '' space-time scan statistics to decide whether an observed increase in cases in a region is significant , enabling fast and accurate detection of emerging outbreaks . We evaluate these methods on two types of simulated outbreaks : aerosol release of inhalational anthrax e.g. from a bioterrorist attack and FLOO `` Fictional Linear Onset Outbreak '' , injected into actual baseline data Emergency Department records and over-the-counter drug sales data from Allegheny County . We demonstrate that our methods are successful in rapidly detecting both outbreak types while keeping the number of false positives low , and show that our new `` emerging cluster '' scan statistics consistently outperform the standard `` persistent cluster '' scan statistics approach .", "keywords": ["cluster detection", "space-time scan statistics", "biosurveillance"], "combined": "Detection of emerging space-time clusters We propose a new class of spatio-temporal cluster detection methods designed for the rapid detection of emerging space-time clusters . We focus on the motivating application of prospective disease surveillance : detecting space-time clusters of disease cases resulting from an emerging disease outbreak . Automatic , real-time detection of outbreaks can enable rapid epidemiological response , potentially reducing rates of morbidity and mortality . Building on the prior work on spatial and space-time scan statistics , our methods combine time series analysis to determine how many cases we expect to observe for a given spatial region in a given time interval with new `` emerging cluster '' space-time scan statistics to decide whether an observed increase in cases in a region is significant , enabling fast and accurate detection of emerging outbreaks . We evaluate these methods on two types of simulated outbreaks : aerosol release of inhalational anthrax e.g. from a bioterrorist attack and FLOO `` Fictional Linear Onset Outbreak '' , injected into actual baseline data Emergency Department records and over-the-counter drug sales data from Allegheny County . We demonstrate that our methods are successful in rapidly detecting both outbreak types while keeping the number of false positives low , and show that our new `` emerging cluster '' scan statistics consistently outperform the standard `` persistent cluster '' scan statistics approach . [[EENNDD]] cluster detection; space-time scan statistics; biosurveillance"}, "Pengesanan kluster ruang-waktu yang baru muncul Kami mencadangkan kaedah pengesanan kelompok spatio-temporal baru yang dirancang untuk mengesan kluster ruang-waktu yang pesat. Kami memfokuskan pada penerapan pengawasan prospektif penyakit yang memotivasi: mengesan kelompok waktu penyakit yang berpunca dari wabak penyakit yang muncul. Pengesanan wabak secara automatik dalam masa nyata dapat memberi tindak balas epidemiologi yang cepat, yang berpotensi mengurangkan kadar morbiditi dan kematian. Berdasarkan karya sebelumnya mengenai statistik imbasan spasial dan ruang-waktu, kaedah kami menggabungkan analisis siri masa untuk menentukan berapa banyak kes yang kami harapkan untuk diperhatikan untuk wilayah spasial tertentu dalam selang waktu tertentu dengan ruang-ruang baru yang muncul mengimbas statistik untuk menentukan sama ada peningkatan kes yang diperhatikan di rantau ini adalah signifikan, yang memungkinkan pengesanan wabak yang cepat dan tepat. Kami menilai kaedah ini pada dua jenis wabak simulasi: pelepasan aerosol anthrax penyedutan, mis. dari serangan bioterorisme dan FLOO \"Fictional Linear Onset Outbreak\", disuntikkan ke dalam data asas sebenar rekod Jabatan Kecemasan dan data penjualan ubat bebas dari Allegheny County. Kami menunjukkan bahawa kaedah kami berjaya mengesan kedua-dua jenis wabak dengan cepat sambil mengekalkan bilangan positif palsu rendah, dan menunjukkan bahawa statistik imbasan \"kluster baru\" baru kami secara konsisten mengungguli pendekatan statistik imbasan \"kluster persisten\" standard. [[EENNDD]] pengesanan kluster; statistik imbasan masa-ruang; pengawasan biosur"], [{"string": "Personal privacy vs population privacy : learning to attack anonymization Over the last decade great strides have been made in developing techniques to compute functions privately . In particular , Differential Privacy gives strong promises about conclusions that can be drawn about an individual . In contrast , various syntactic methods for providing privacy criteria such as k-anonymity and l-diversity have been criticized for still allowing private information of an individual to be inferred . In this paper , we consider the ability of an attacker to use data meeting privacy definitions to build an accurate classifier . We demonstrate that even under Differential Privacy , such classifiers can be used to infer `` private '' attributes accurately in realistic data . We compare this to similar approaches for inference-based attacks on other forms of anonymized data . We show how the efficacy of all these attacks can be measured on the same scale , based on the probability of successfully inferring a private attribute . We observe that the accuracy of inference of private attributes for differentially private data and $ l $ - diverse data can be quite similar .", "keywords": ["differential privacy", "miscellaneous", "anonymization"], "combined": "Personal privacy vs population privacy : learning to attack anonymization Over the last decade great strides have been made in developing techniques to compute functions privately . In particular , Differential Privacy gives strong promises about conclusions that can be drawn about an individual . In contrast , various syntactic methods for providing privacy criteria such as k-anonymity and l-diversity have been criticized for still allowing private information of an individual to be inferred . In this paper , we consider the ability of an attacker to use data meeting privacy definitions to build an accurate classifier . We demonstrate that even under Differential Privacy , such classifiers can be used to infer `` private '' attributes accurately in realistic data . We compare this to similar approaches for inference-based attacks on other forms of anonymized data . We show how the efficacy of all these attacks can be measured on the same scale , based on the probability of successfully inferring a private attribute . We observe that the accuracy of inference of private attributes for differentially private data and $ l $ - diverse data can be quite similar . [[EENNDD]] differential privacy; miscellaneous; anonymization"}, "Privasi peribadi vs privasi penduduk: belajar menyerang anonimisasi Selama dekad terakhir telah dilakukan kemajuan besar dalam mengembangkan teknik untuk menghitung fungsi secara peribadi. Khususnya, Privasi Perbezaan memberikan janji-janji kuat mengenai kesimpulan yang dapat diambil mengenai seseorang individu. Sebaliknya, pelbagai kaedah sintaksis untuk menyediakan kriteria privasi seperti k-anonimiti dan kepelbagaian l telah dikritik kerana masih membenarkan maklumat peribadi seseorang disimpulkan. Dalam makalah ini, kami mempertimbangkan kemampuan penyerang untuk menggunakan definisi privasi pertemuan data untuk membina pengkelasan yang tepat. Kami menunjukkan bahawa walaupun di bawah Privasi Berbeza, pengklasifikasi semacam itu dapat digunakan untuk menyimpulkan atribut \"peribadi\" secara tepat dalam data realistik. Kami membandingkannya dengan pendekatan serupa untuk serangan berdasarkan inferensi terhadap bentuk data tanpa nama lain. Kami menunjukkan bagaimana keberkesanan semua serangan ini dapat diukur pada skala yang sama, berdasarkan kebarangkalian berjaya menyimpulkan atribut peribadi. Kami memerhatikan bahawa ketepatan inferensi atribut peribadi untuk data peribadi yang berbeza dan $ l $ - data yang pelbagai mungkin serupa. [[EENNDD]] privasi berbeza; pelbagai; tanpa nama"], [{"string": "BLOSOM : a framework for mining arbitrary boolean expressions We introduce a novel framework , called BLOSOM , for mining frequent boolean expressions over binary-valued datasets . We organize the space of boolean expressions into four categories : pure conjunctions , pure disjunctions , conjunction of disjunctions , and disjunction of conjunctions . We focus on mining the simplest expressions the minimal generators for each class . We also propose a closure operator for each class that yields closed boolean expressions . BLOSOM efficiently mines frequent boolean expressions by utilizing a number of methodical pruning techniques . Experiments showcase the behavior of BLOSOM , and an application study on a real dataset is also given .", "keywords": ["minimal generator", "closed itemsets", "boolean expression"], "combined": "BLOSOM : a framework for mining arbitrary boolean expressions We introduce a novel framework , called BLOSOM , for mining frequent boolean expressions over binary-valued datasets . We organize the space of boolean expressions into four categories : pure conjunctions , pure disjunctions , conjunction of disjunctions , and disjunction of conjunctions . We focus on mining the simplest expressions the minimal generators for each class . We also propose a closure operator for each class that yields closed boolean expressions . BLOSOM efficiently mines frequent boolean expressions by utilizing a number of methodical pruning techniques . Experiments showcase the behavior of BLOSOM , and an application study on a real dataset is also given . [[EENNDD]] minimal generator; closed itemsets; boolean expression"}, "BLOSOM: kerangka kerja untuk melombong ungkapan boolean sewenang-wenangnya Kami memperkenalkan kerangka baru, yang disebut BLOSOM, untuk melombong ungkapan boolean yang kerap berbanding set data bernilai binari. Kami mengatur ruang ungkapan boolean ke dalam empat kategori: konjungsi murni, disjungsi murni, konjungsi pemisah, dan pemutusan konjungsi. Kami menumpukan perhatian pada penggunaan ungkapan termudah yang paling minimum untuk setiap kelas. Kami juga mencadangkan operator penutupan untuk setiap kelas yang menghasilkan ungkapan boolean tertutup. BLOSOM secara berkesan menambang ungkapan boolean yang kerap dengan menggunakan sebilangan teknik pemangkasan metodis. Eksperimen menunjukkan tingkah laku BLOSOM, dan kajian aplikasi pada set data sebenar juga diberikan. [[EENNDD]] penjana minimum; set barang tertutup; ungkapan boolean"], [{"string": "Nantonac collaborative filtering : recommendation based on order responses A recommender system suggests the items expected to be preferred by the users . Recommender systems use collaborative filtering to recommend items by summarizing the preferences of people who have tendencies similar to the user preference . Traditionally , the degree of preference is represented by a scale , for example , one that ranges from one to five . This type of measuring technique is called the semantic differential SD method . Web adopted the ranking method , however , rather than the SD method , since the SD method is intrinsically not suited for representing individual preferences . In the ranking method , the preferences are represented by orders , which are sorted item sequences according to the users ' preferences . We here propose some methods to recommed items based on these order responses , and carry out the comparison experiments of these methods .", "keywords": ["collaborative filtering", "order", "recommender system"], "combined": "Nantonac collaborative filtering : recommendation based on order responses A recommender system suggests the items expected to be preferred by the users . Recommender systems use collaborative filtering to recommend items by summarizing the preferences of people who have tendencies similar to the user preference . Traditionally , the degree of preference is represented by a scale , for example , one that ranges from one to five . This type of measuring technique is called the semantic differential SD method . Web adopted the ranking method , however , rather than the SD method , since the SD method is intrinsically not suited for representing individual preferences . In the ranking method , the preferences are represented by orders , which are sorted item sequences according to the users ' preferences . We here propose some methods to recommed items based on these order responses , and carry out the comparison experiments of these methods . [[EENNDD]] collaborative filtering; order; recommender system"}, "Penapisan kolaboratif Nantonac: cadangan berdasarkan respons pesanan Sistem pengesyorkan menunjukkan item yang diharapkan dapat disukai oleh pengguna. Sistem penasihat menggunakan penapisan kolaboratif untuk mengesyorkan item dengan meringkaskan pilihan orang yang mempunyai kecenderungan yang serupa dengan pilihan pengguna. Secara tradisinya, tahap keutamaan diwakili oleh skala, misalnya, satu yang berkisar antara satu hingga lima. Teknik pengukuran jenis ini dipanggil kaedah SD pembezaan semantik. Web mengadopsi kaedah peringkat, bagaimanapun, daripada kaedah SD, kerana kaedah SD secara intrinsik tidak sesuai untuk mewakili pilihan individu. Dalam kaedah pemeringkatan, pilihan ditunjukkan oleh pesanan, yang disusun mengikut urutan item mengikut pilihan pengguna. Kami di sini mencadangkan beberapa kaedah untuk mengesyorkan item berdasarkan respons pesanan ini, dan menjalankan eksperimen perbandingan kaedah ini. [[EENNDD]] penapisan kolaboratif; pesanan; sistem cadangan"], [{"string": "Cross channel optimized marketing by reinforcement learning The issues of cross channel integration and customer life time value modeling are two of the most important topics surrounding customer relationship management CRM today . In the present paper , we describe and evaluate a novel solution that treats these two important issues in a unified framework of Markov Decision Processes MDP . In particular , we report on the results of a joint project between IBM Research and Saks Fifth Avenue to investigate the applicability of this technology to real world problems . The business problem we use as a testbed for our evaluation is that of optimizing direct mail campaign mailings for maximization of profits in the store channel . We identify a problem common to cross-channel CRM , which we call the Cross-Channel Challenge , due to the lack of explicit linking between the marketing actions taken in one channel and the customer responses obtained in another . We provide a solution for this problem based on old and new techniques in reinforcement learning . Our in-laboratory experimental evaluation using actual customer interaction data show that as much as 7 to 8 per cent increase in the store profits can be expected , by employing a mailing policy automatically generated by our methodology . These results confirm that our approach is valid in dealing with the cross channel CRM scenarios in the real world .", "keywords": ["reinforcement learning", "targeted marketing", "crm", "cost sensitive learning", "learning", "customer life time value"], "combined": "Cross channel optimized marketing by reinforcement learning The issues of cross channel integration and customer life time value modeling are two of the most important topics surrounding customer relationship management CRM today . In the present paper , we describe and evaluate a novel solution that treats these two important issues in a unified framework of Markov Decision Processes MDP . In particular , we report on the results of a joint project between IBM Research and Saks Fifth Avenue to investigate the applicability of this technology to real world problems . The business problem we use as a testbed for our evaluation is that of optimizing direct mail campaign mailings for maximization of profits in the store channel . We identify a problem common to cross-channel CRM , which we call the Cross-Channel Challenge , due to the lack of explicit linking between the marketing actions taken in one channel and the customer responses obtained in another . We provide a solution for this problem based on old and new techniques in reinforcement learning . Our in-laboratory experimental evaluation using actual customer interaction data show that as much as 7 to 8 per cent increase in the store profits can be expected , by employing a mailing policy automatically generated by our methodology . These results confirm that our approach is valid in dealing with the cross channel CRM scenarios in the real world . [[EENNDD]] reinforcement learning; targeted marketing; crm; cost sensitive learning; learning; customer life time value"}, "Pemasaran yang dioptimumkan lintas saluran dengan pembelajaran pengukuhan Isu-isu integrasi antara saluran dan pemodelan nilai masa hidup pelanggan adalah dua topik yang paling penting di sekitar CRM pengurusan hubungan pelanggan hari ini. Dalam makalah ini, kami menerangkan dan menilai penyelesaian baru yang menangani kedua-dua isu penting ini dalam kerangka bersatu Markov Decision Processes MDP. Khususnya, kami melaporkan hasil projek bersama antara IBM Research dan Saks Fifth Avenue untuk menyiasat penerapan teknologi ini untuk masalah dunia nyata. Masalah perniagaan yang kami gunakan sebagai ujian untuk penilaian kami ialah mengoptimumkan pengeposan kempen surat terus untuk memaksimumkan keuntungan di saluran kedai. Kami mengenal pasti masalah yang biasa terjadi pada CRM merentas saluran, yang kami sebut sebagai Cabaran Merentas Saluran, kerana kurangnya hubungan yang jelas antara tindakan pemasaran yang dilakukan dalam satu saluran dan respons pelanggan yang diperoleh di saluran yang lain. Kami menyediakan penyelesaian untuk masalah ini berdasarkan teknik lama dan baru dalam pembelajaran pengukuhan. Penilaian eksperimen di makmal kami menggunakan data interaksi pelanggan yang sebenarnya menunjukkan bahawa sebanyak 7 hingga 8 peratus kenaikan keuntungan kedai dapat diharapkan, dengan menggunakan polisi surat yang dihasilkan secara automatik oleh metodologi kami. Hasil ini mengesahkan bahawa pendekatan kami adalah sah dalam menangani senario CRM lintas saluran di dunia nyata. [[EENNDD]] pembelajaran pengukuhan; pemasaran yang disasarkan; crm; pembelajaran sensitif kos; belajar; nilai masa hidup pelanggan"], [{"string": "Enabling analysts in managed services for CRM analytics Data analytics tools and frameworks abound , yet rapid deployment of analytics solutions that deliver actionable insights from business data remains a challenge . The primary reason is that on-field practitioners are required to be both technically proficient and knowledgeable about the business . The recent abundance of unstructured business data has thrown up new opportunities for analytics , but has also multiplied the deployment challenge , since interpretation of concepts derived from textual sources require a deep understanding of the business . In such a scenario , a managed service for analytics comes up as the best alternative . A managed analytics service is centered around a business analyst who acts as a liaison between the business and the technology . This calls for new tools that assist the analyst to be efficient in the tasks that she needs to execute . Also , the analytics needs to be repeatable , in that the delivered insights should not depend heavily on the expertise of specific analysts . These factors lead us to identify new areas that open up for KDD research in terms of ` time-to-insight ' and repeatability for these analysts . We present our analytics framework in the form of a managed service offering for CRM analytics . We describe different analyst-centric tools using a case study from real-life engagements and demonstrate their effectiveness .", "keywords": ["general", "analytics service", "text mining"], "combined": "Enabling analysts in managed services for CRM analytics Data analytics tools and frameworks abound , yet rapid deployment of analytics solutions that deliver actionable insights from business data remains a challenge . The primary reason is that on-field practitioners are required to be both technically proficient and knowledgeable about the business . The recent abundance of unstructured business data has thrown up new opportunities for analytics , but has also multiplied the deployment challenge , since interpretation of concepts derived from textual sources require a deep understanding of the business . In such a scenario , a managed service for analytics comes up as the best alternative . A managed analytics service is centered around a business analyst who acts as a liaison between the business and the technology . This calls for new tools that assist the analyst to be efficient in the tasks that she needs to execute . Also , the analytics needs to be repeatable , in that the delivered insights should not depend heavily on the expertise of specific analysts . These factors lead us to identify new areas that open up for KDD research in terms of ` time-to-insight ' and repeatability for these analysts . We present our analytics framework in the form of a managed service offering for CRM analytics . We describe different analyst-centric tools using a case study from real-life engagements and demonstrate their effectiveness . [[EENNDD]] general; analytics service; text mining"}, "Mengaktifkan penganalisis dalam perkhidmatan terurus untuk analisis CRM Alat dan kerangka kerja analitik data banyak, namun penyebaran penyelesaian analitik yang cepat yang memberikan pandangan yang dapat diambil dari data perniagaan tetap menjadi cabaran. Sebab utama adalah bahawa pengamal di lapangan dikehendaki mahir dari segi teknikal dan berpengetahuan mengenai perniagaan. Banyaknya data perniagaan tidak berstruktur baru-baru ini telah membuka peluang baru untuk analisis, tetapi juga melipatgandakan cabaran penggunaan, kerana penafsiran konsep yang berasal dari sumber teks memerlukan pemahaman yang mendalam tentang perniagaan. Dalam senario seperti itu, perkhidmatan yang dikendalikan untuk analisis muncul sebagai alternatif terbaik. Perkhidmatan analisis terurus berpusat di sekitar penganalisis perniagaan yang bertindak sebagai penghubung antara perniagaan dan teknologi. Ini memerlukan alat baru yang membantu penganalisis cekap dalam tugas yang perlu dia laksanakan. Analisis juga perlu diulang, kerana pandangan yang disampaikan tidak boleh bergantung pada kepakaran penganalisis tertentu. Faktor-faktor ini mendorong kita untuk mengenal pasti bidang-bidang baru yang terbuka untuk penyelidikan KDD dari segi \"masa-ke-wawasan\" dan kebolehulangan bagi penganalisis ini. Kami membentangkan kerangka analisis kami dalam bentuk penawaran perkhidmatan terkelola untuk analisis CRM. Kami menerangkan pelbagai alat yang berpusatkan penganalisis menggunakan kajian kes dari penglibatan dalam kehidupan sebenar dan menunjukkan keberkesanannya. [[EENNDD]] umum; perkhidmatan analisis; perlombongan teks"], [{"string": "Information awareness : a prospective technical assessment Recent proposals to apply data mining systems to problems in law enforcement , national security , and fraud detection have attracted both media attention and technical critiques of their expected accuracy and impact on privacy . Unfortunately , the majority of technical critiques have been based on simplistic assumptions about data , classifiers , inference procedures , and the overall architecture of such systems . We consider these critiques in detail , and we construct a simulation model that more closely matches realistic systems . We show how both the accuracy and privacy impact of a hypothetical system could be substantially improved , and we discuss the necessary and sufficient conditions for this improvement to be achieved . This analysis is neither a defense nor a critique of any particular system concept . Rather , our model suggests alternative technical designs that could mitigate some concerns , but also raises more specific conditions that must be met for such systems to be both accurate and socially desirable .", "keywords": ["privacy", "ranking classifiers", "iterative classification", "technology assessment", "database applications", "relational data mining", "information awareness", "collective classification", "social network analysis", "tia"], "combined": "Information awareness : a prospective technical assessment Recent proposals to apply data mining systems to problems in law enforcement , national security , and fraud detection have attracted both media attention and technical critiques of their expected accuracy and impact on privacy . Unfortunately , the majority of technical critiques have been based on simplistic assumptions about data , classifiers , inference procedures , and the overall architecture of such systems . We consider these critiques in detail , and we construct a simulation model that more closely matches realistic systems . We show how both the accuracy and privacy impact of a hypothetical system could be substantially improved , and we discuss the necessary and sufficient conditions for this improvement to be achieved . This analysis is neither a defense nor a critique of any particular system concept . Rather , our model suggests alternative technical designs that could mitigate some concerns , but also raises more specific conditions that must be met for such systems to be both accurate and socially desirable . [[EENNDD]] privacy; ranking classifiers; iterative classification; technology assessment; database applications; relational data mining; information awareness; collective classification; social network analysis; tia"}, "Kesedaran maklumat: penilaian teknikal prospektif Cadangan terkini untuk menerapkan sistem perlombongan data untuk masalah dalam penegakan hukum, keamanan nasional, dan pengesanan penipuan telah menarik perhatian media dan kritikan teknikal mengenai ketepatan dan impak yang diharapkan terhadap privasi. Sayangnya, kebanyakan kritikan teknikal telah didasarkan pada anggapan sederhana mengenai data, pengklasifikasi, prosedur inferensi, dan keseluruhan struktur sistem tersebut. Kami mempertimbangkan kritikan ini secara terperinci, dan kami membina model simulasi yang lebih hampir sama dengan sistem realistik. Kami menunjukkan bagaimana kesan ketepatan dan privasi sistem hipotetis dapat ditingkatkan dengan ketara, dan kami membincangkan syarat-syarat yang perlu dan mencukupi agar peningkatan ini dapat dicapai. Analisis ini bukan merupakan pembelaan atau kritikan terhadap konsep sistem tertentu. Sebaliknya, model kami mencadangkan reka bentuk teknikal alternatif yang dapat mengurangkan beberapa kebimbangan, tetapi juga menimbulkan syarat yang lebih spesifik yang mesti dipenuhi agar sistem sedemikian tepat dan diinginkan secara sosial. [[EENNDD]] privasi; pengelasan peringkat; pengelasan berulang penilaian teknologi; aplikasi pangkalan data; perlombongan data hubungan; kesedaran maklumat; pengelasan kolektif; analisis rangkaian sosial; tia"], [{"string": "Incorporating site-level knowledge for incremental crawling of web forums : a list-wise strategy We study in this paper the problem of incremental crawling of web forums , which is a very fundamental yet challenging step in many web applications . Traditional approaches mainly focus on scheduling the revisiting strategy of each individual page . However , simply assigning different weights for different individual pages is usually inefficient in crawling forum sites because of the different characteristics between forum sites and general websites . Instead of treating each individual page independently , we propose a list-wise strategy by taking into account the site-level knowledge . Such site-level knowledge is mined through reconstructing the linking structure , called sitemap , for a given forum site . With the sitemap , posts from the same thread but distributed on various pages can be concatenated according to their timestamps . After that , for each thread , we employ a regression model to predict the time when the next post arrives . Based on this model , we develop an efficient crawler which is 260 % faster than some state-of-the-art methods in terms of fetching new generated content ; and meanwhile our crawler also ensure a high coverage ratio . Experimental results show promising performance of Coverage , Bandwidth utilization , and Timeliness of our crawler on 18 various forums .", "keywords": ["sitemap", "web forum", "incremental crawling"], "combined": "Incorporating site-level knowledge for incremental crawling of web forums : a list-wise strategy We study in this paper the problem of incremental crawling of web forums , which is a very fundamental yet challenging step in many web applications . Traditional approaches mainly focus on scheduling the revisiting strategy of each individual page . However , simply assigning different weights for different individual pages is usually inefficient in crawling forum sites because of the different characteristics between forum sites and general websites . Instead of treating each individual page independently , we propose a list-wise strategy by taking into account the site-level knowledge . Such site-level knowledge is mined through reconstructing the linking structure , called sitemap , for a given forum site . With the sitemap , posts from the same thread but distributed on various pages can be concatenated according to their timestamps . After that , for each thread , we employ a regression model to predict the time when the next post arrives . Based on this model , we develop an efficient crawler which is 260 % faster than some state-of-the-art methods in terms of fetching new generated content ; and meanwhile our crawler also ensure a high coverage ratio . Experimental results show promising performance of Coverage , Bandwidth utilization , and Timeliness of our crawler on 18 various forums . [[EENNDD]] sitemap; web forum; incremental crawling"}, "Menggabungkan pengetahuan peringkat laman web untuk perayapan bertahap untuk forum web: strategi yang sesuai dengan senarai Kami mengkaji dalam makalah ini masalah merangkak secara bertahap dari forum web, yang merupakan langkah yang sangat mendasar namun mencabar dalam banyak aplikasi web. Pendekatan tradisional terutamanya tertumpu pada menjadwalkan strategi menyemak semula setiap halaman individu. Walau bagaimanapun, hanya menetapkan bobot yang berbeza untuk halaman individu yang berbeza biasanya tidak cekap dalam merangkak laman forum kerana ciri-ciri yang berbeza antara laman forum dan laman web umum. Daripada melayan setiap halaman secara bebas, kami mencadangkan strategi berdasarkan senarai dengan mengambil kira pengetahuan di peringkat laman web. Pengetahuan di peringkat laman web ini ditambang melalui penyusunan semula struktur penghubung, yang disebut peta laman, untuk laman forum tertentu. Dengan peta laman, catatan dari utas yang sama tetapi diedarkan di pelbagai halaman dapat digabungkan mengikut cap waktu mereka. Selepas itu, untuk setiap utas, kami menggunakan model regresi untuk meramalkan waktu ketika pos seterusnya tiba. Berdasarkan model ini, kami mengembangkan crawler yang cekap yang 260% lebih cepat daripada beberapa kaedah canggih dari segi pengambilan kandungan yang dihasilkan baru; dan sementara itu crawler kami juga memastikan nisbah liputan tinggi. Hasil eksperimen menunjukkan prestasi yang menjanjikan Liputan, penggunaan Jalur Lebar, dan Ketepatan masa perayap kami di 18 pelbagai forum. [[EENNDD]] peta laman; forum web; merangkak bertahap"], [{"string": "Handling very large numbers of association rules in the analysis of microarray data The problem of analyzing microarray data became one of important topics in bioinformatics over the past several years , and different data mining techniques have been proposed for the analysis of such data . In this paper , we propose to use association rule discovery methods for determining associations among expression levels of different genes . One of the main problems related to the discovery of these associations is the scalability issue . Microarrays usually contain very large numbers of genes that are sometimes measured in 10,000 s. Therefore , analysis of such data can generate a very large number of associations that can often be measured in millions . The paper addresses this problem by presenting a method that enables biologists to evaluate these very large numbers of discovered association rules during the post-analysis stage of the data mining process . This is achieved by providing several rule evaluation operators , including rule grouping , filtering , browsing , and data inspection operators , that allow biologists to validate multiple individual gane regulation patterns at a time . By iteratively applying these operators , biologists can explore a significant part of all the initially generated rules in an acceptable period of time and thus answer biological questions that are of a particular interest to him or her . To validate our method , we tested our system on the microarray data pertaining to the studies of environmental hazards and their influence of gane expression processes . As a result , we managed to answer several questions that were of interest to the biologists that had collected this data .", "keywords": ["analysis of microarray data", "post-processing of discovered rules", "association rules", "bioinformatics", "rule filtering", "deduction", "expert-driven rule validation", "rule grouping"], "combined": "Handling very large numbers of association rules in the analysis of microarray data The problem of analyzing microarray data became one of important topics in bioinformatics over the past several years , and different data mining techniques have been proposed for the analysis of such data . In this paper , we propose to use association rule discovery methods for determining associations among expression levels of different genes . One of the main problems related to the discovery of these associations is the scalability issue . Microarrays usually contain very large numbers of genes that are sometimes measured in 10,000 s. Therefore , analysis of such data can generate a very large number of associations that can often be measured in millions . The paper addresses this problem by presenting a method that enables biologists to evaluate these very large numbers of discovered association rules during the post-analysis stage of the data mining process . This is achieved by providing several rule evaluation operators , including rule grouping , filtering , browsing , and data inspection operators , that allow biologists to validate multiple individual gane regulation patterns at a time . By iteratively applying these operators , biologists can explore a significant part of all the initially generated rules in an acceptable period of time and thus answer biological questions that are of a particular interest to him or her . To validate our method , we tested our system on the microarray data pertaining to the studies of environmental hazards and their influence of gane expression processes . As a result , we managed to answer several questions that were of interest to the biologists that had collected this data . [[EENNDD]] analysis of microarray data; post-processing of discovered rules; association rules; bioinformatics; rule filtering; deduction; expert-driven rule validation; rule grouping"}, "Mengendalikan sejumlah besar peraturan persatuan dalam analisis data microarray Masalah menganalisis data microarray menjadi salah satu topik penting dalam bioinformatics selama beberapa tahun terakhir, dan teknik perlombongan data yang berbeza telah diusulkan untuk analisis data tersebut. Dalam makalah ini, kami mencadangkan untuk menggunakan kaedah penemuan peraturan persatuan untuk menentukan hubungan antara tahap ekspresi gen yang berbeza. Salah satu masalah utama yang berkaitan dengan penemuan persatuan ini adalah masalah skalabiliti. Microarrays biasanya mengandungi sebilangan besar gen yang kadang-kadang diukur dalam 10,000 s. Oleh itu, analisis data sedemikian dapat menghasilkan sebilangan besar persatuan yang sering dapat diukur dalam berjuta-juta. Makalah ini mengatasi masalah ini dengan mengemukakan kaedah yang membolehkan ahli biologi menilai sebilangan besar peraturan pergaulan yang ditemui semasa peringkat pasca-analisis proses perlombongan data. Ini dicapai dengan menyediakan beberapa pengendali penilaian peraturan, termasuk pengelompokan peraturan, penyaringan, penyemakan imbas, dan pengendali pemeriksaan data, yang membolehkan ahli biologi mengesahkan beberapa pola peraturan individu pada satu masa. Dengan menggunakan pengendali ini secara berulang, ahli biologi dapat meneroka sebahagian besar dari semua peraturan yang dihasilkan pada jangka masa yang boleh diterima dan dengan itu menjawab soalan biologi yang menarik minatnya. Untuk mengesahkan kaedah kami, kami menguji sistem kami pada data microarray yang berkaitan dengan kajian bahaya persekitaran dan pengaruh mereka terhadap proses ekspresi gane. Hasilnya, kami berjaya menjawab beberapa soalan yang menarik bagi ahli biologi yang telah mengumpulkan data ini. [[EENNDD]] analisis data microarray; pasca pemprosesan peraturan yang ditemui; peraturan persatuan; bioinformatik; penapisan peraturan; potongan; pengesahan peraturan berdasarkan pakar; pengelompokan peraturan"], [{"string": "The UCI KDD archive of large data sets for data mining research and experimentation", "keywords": ["data archive"], "combined": "The UCI KDD archive of large data sets for data mining research and experimentation [[EENNDD]] data archive"}, "Arkib UCI KDD set data besar untuk penyelidikan dan eksperimen perlombongan data [[EENNDD]] arkib data"], [{"string": "Weighting versus pruning in rule validation for detecting network and host anomalies For intrusion detection , the LERAD algorithm learns a succinct set of comprehensible rules for detecting anomalies , which could be novel attacks . LERAD validates the learned rules on a separate held-out validation set and removes rules that cause false alarms . However , removing rules with possible high coverage can lead to missed detections . We propose to retain these rules and associate weights to them . We present three weighting schemes and our empirical results indicate that , for LERAD , rule weighting can detect more attacks than pruning with minimal computational overhead .", "keywords": ["rule weighting", "unauthorized access", "anomaly detection", "rule pruning", "invasive software", "machine learning"], "combined": "Weighting versus pruning in rule validation for detecting network and host anomalies For intrusion detection , the LERAD algorithm learns a succinct set of comprehensible rules for detecting anomalies , which could be novel attacks . LERAD validates the learned rules on a separate held-out validation set and removes rules that cause false alarms . However , removing rules with possible high coverage can lead to missed detections . We propose to retain these rules and associate weights to them . We present three weighting schemes and our empirical results indicate that , for LERAD , rule weighting can detect more attacks than pruning with minimal computational overhead . [[EENNDD]] rule weighting; unauthorized access; anomaly detection; rule pruning; invasive software; machine learning"}, "Pembobotan dan pemangkasan dalam pengesahan peraturan untuk mengesan anomali rangkaian dan host Untuk pengesanan pencerobohan, algoritma LERAD mempelajari sekumpulan peraturan yang mudah difahami untuk mengesan anomali, yang mungkin merupakan serangan baru. LERAD mengesahkan peraturan yang dipelajari pada set pengesahan tertahan yang terpisah dan membuang peraturan yang menyebabkan penggera palsu. Walau bagaimanapun, membuang peraturan dengan kemungkinan liputan tinggi boleh menyebabkan pengesanan tidak dijawab. Kami mencadangkan untuk mengekalkan peraturan ini dan mengaitkan bobot dengan peraturan tersebut. Kami membentangkan tiga skema pemberat dan hasil empirik kami menunjukkan bahawa, untuk LERAD, pemberat peraturan dapat mengesan lebih banyak serangan daripada pemangkasan dengan overhead komputasi minimum. [[EENNDD]] pemberat peraturan; akses tidak dibenarkan; pengesanan anomali; pemangkasan peraturan; perisian invasif; pembelajaran mesin"], [{"string": "Probabilistic workflow mining In several organizations , it has become increasingly popular to document and log the steps that makeup a typical business process . In some situations , a normative workflow model of such processes is developed , and it becomes important to know if such a model is actually being followed by analyzing the available activity logs . In other scenarios , no model is available and , with the purpose of evaluating cases or creating new production policies , one is interested in learning a workflow representation of such activities . In either case , machine learning tools that can mine workflow models are of great interest and still relatively unexplored . We present here a probabilistic workflow model and a corresponding learning algorithm that runs in polynomial time . We illustrate the algorithm on example data derived from a real world workflow .", "keywords": ["causal models", "workflow mining", "graphical models", "probability and statistics"], "combined": "Probabilistic workflow mining In several organizations , it has become increasingly popular to document and log the steps that makeup a typical business process . In some situations , a normative workflow model of such processes is developed , and it becomes important to know if such a model is actually being followed by analyzing the available activity logs . In other scenarios , no model is available and , with the purpose of evaluating cases or creating new production policies , one is interested in learning a workflow representation of such activities . In either case , machine learning tools that can mine workflow models are of great interest and still relatively unexplored . We present here a probabilistic workflow model and a corresponding learning algorithm that runs in polynomial time . We illustrate the algorithm on example data derived from a real world workflow . [[EENNDD]] causal models; workflow mining; graphical models; probability and statistics"}, "Perlombongan aliran kerja probabilistik Dalam beberapa organisasi, semakin popular untuk mendokumentasikan dan mencatat langkah-langkah yang membentuk proses perniagaan yang biasa. Dalam beberapa situasi, model aliran kerja normatif dari proses tersebut dikembangkan, dan menjadi penting untuk mengetahui apakah model sedemikian sebenarnya diikuti dengan menganalisis log aktiviti yang tersedia. Dalam senario lain, tidak ada model yang tersedia dan, dengan tujuan untuk menilai kes atau membuat polisi pengeluaran baru, seseorang berminat untuk mempelajari perwakilan aliran kerja dari kegiatan tersebut. Dalam kedua-dua kes tersebut, alat pembelajaran mesin yang dapat menambang model aliran kerja sangat menarik perhatian dan masih belum diterokai. Kami membentangkan di sini model aliran kerja probabilistik dan algoritma pembelajaran yang sesuai yang berjalan dalam masa polinomial. Kami menggambarkan algoritma pada contoh data yang berasal dari aliran kerja dunia nyata. [[EENNDD]] model kausal; perlombongan aliran kerja; model grafik; kebarangkalian dan statistik"], [{"string": "k-TTP : a new privacy model for large-scale distributed environments Secure multiparty computation allows parties to jointly compute a function of their private inputs without revealing anything but the output . Theoretical results 2 provide a general construction of such protocols for any function . Protocols obtained in this way are , however , inefficient , and thus , practically speaking , useless when a large number of participants are involved . The contribution of this paper is to define a new privacy model -- k-privacy -- by means of an innovative , yet natural generalization of the accepted trusted third party model . This allows implementing cryptographically secure efficient primitives for real-world large-scale distributed systems . As an example for the usefulness of the proposed model , we employ k-privacy to introduce a technique for obtaining knowledge -- by way of an association-rule mining algorithm -- from large-scale Data Grids , while ensuring that the privacy is cryptographically secure .", "keywords": ["privacy-preserving data mining", "distributed data mining", "privacy", "association rule mining"], "combined": "k-TTP : a new privacy model for large-scale distributed environments Secure multiparty computation allows parties to jointly compute a function of their private inputs without revealing anything but the output . Theoretical results 2 provide a general construction of such protocols for any function . Protocols obtained in this way are , however , inefficient , and thus , practically speaking , useless when a large number of participants are involved . The contribution of this paper is to define a new privacy model -- k-privacy -- by means of an innovative , yet natural generalization of the accepted trusted third party model . This allows implementing cryptographically secure efficient primitives for real-world large-scale distributed systems . As an example for the usefulness of the proposed model , we employ k-privacy to introduce a technique for obtaining knowledge -- by way of an association-rule mining algorithm -- from large-scale Data Grids , while ensuring that the privacy is cryptographically secure . [[EENNDD]] privacy-preserving data mining; distributed data mining; privacy; association rule mining"}, "k-TTP: model privasi baru untuk persekitaran yang diedarkan berskala besar Pengiraan multipartai yang selamat membolehkan pihak untuk bersama-sama menghitung fungsi input peribadi mereka tanpa mengungkapkan apa-apa kecuali hasilnya. Hasil teoritis 2 memberikan pembinaan umum protokol sedemikian untuk fungsi apa pun. Protokol yang diperoleh dengan cara ini, bagaimanapun, tidak cekap, dan dengan itu, secara praktiknya, tidak berguna ketika sebilangan besar peserta terlibat. Sumbangan makalah ini adalah untuk menentukan model privasi baru - k-privasi - dengan cara generalisasi yang inovatif, namun semula jadi dari model pihak ketiga yang dipercayai. Ini membolehkan pelaksanaan primitif cekap yang selamat secara kriptografi untuk sistem diedarkan berskala besar dunia nyata. Sebagai contoh untuk kegunaan model yang dicadangkan, kami menggunakan k-privacy untuk memperkenalkan teknik untuk memperoleh pengetahuan - melalui algoritma perlombongan peraturan persatuan - dari Grid Data berskala besar, sambil memastikan privasi itu secara kriptografi selamat. [[EENNDD]] perlombongan data yang memelihara privasi; perlombongan data yang diedarkan; privasi; perlombongan peraturan persatuan"], [{"string": "Sampling from large graphs Given a huge real graph , how can we derive a representative sample ? There are many known algorithms to compute interesting measures shortest paths , centrality , betweenness , etc. , but several of them become impractical for large graphs . Thus graph sampling is essential . The natural questions to ask are a which sampling method to use , b how small can the sample size be , and c how to scale up the measurements of the sample e.g. , the diameter , to get estimates for the large graph . The deeper , underlying question is subtle : how do we measure success ? . We answer the above questions , and test our answers by thorough experiments on several , diverse datasets , spanning thousands nodes and edges . We consider several sampling methods , propose novel methods to check the goodness of sampling , and develop a set of scaling laws that describe relations between the properties of the original and the sample . In addition to the theoretical contributions , the practical conclusions from our work are : Sampling strategies based on edge selection do not perform well ; simple uniform random node selection performs surprisingly well . Overall , best performing methods are the ones based on random-walks and `` forest fire '' ; they match very accurately both static as well as evolutionary graph patterns , with sample sizes down to about 15 % of the original graph .", "keywords": ["graph sampling", "scaling laws", "graph mining"], "combined": "Sampling from large graphs Given a huge real graph , how can we derive a representative sample ? There are many known algorithms to compute interesting measures shortest paths , centrality , betweenness , etc. , but several of them become impractical for large graphs . Thus graph sampling is essential . The natural questions to ask are a which sampling method to use , b how small can the sample size be , and c how to scale up the measurements of the sample e.g. , the diameter , to get estimates for the large graph . The deeper , underlying question is subtle : how do we measure success ? . We answer the above questions , and test our answers by thorough experiments on several , diverse datasets , spanning thousands nodes and edges . We consider several sampling methods , propose novel methods to check the goodness of sampling , and develop a set of scaling laws that describe relations between the properties of the original and the sample . In addition to the theoretical contributions , the practical conclusions from our work are : Sampling strategies based on edge selection do not perform well ; simple uniform random node selection performs surprisingly well . Overall , best performing methods are the ones based on random-walks and `` forest fire '' ; they match very accurately both static as well as evolutionary graph patterns , with sample sizes down to about 15 % of the original graph . [[EENNDD]] graph sampling; scaling laws; graph mining"}, "Persampelan dari grafik besar Memandangkan grafik nyata yang besar, bagaimana kita dapat memperoleh sampel yang representatif? Terdapat banyak algoritma yang diketahui untuk menghitung langkah-langkah menarik jalan terpendek, sentraliti, jarak antara, dan lain-lain, tetapi beberapa di antaranya menjadi tidak praktikal untuk grafik besar. Oleh itu, pensampelan grafik adalah mustahak. Soalan semula jadi yang harus ditanyakan adalah metode pengambilan sampel mana yang harus digunakan, b seberapa kecil ukuran sampel, dan c bagaimana meningkatkan ukuran sampel mis. , diameter, untuk mendapatkan anggaran bagi graf besar. Soalan yang lebih mendalam dan mendasar adalah halus: bagaimana kita mengukur kejayaan? . Kami menjawab soalan di atas, dan menguji jawapan kami dengan eksperimen menyeluruh pada beberapa kumpulan data yang pelbagai, merangkumi ribuan nod dan pinggir. Kami mempertimbangkan beberapa kaedah persampelan, mencadangkan kaedah baru untuk memeriksa kebaikan persampelan, dan mengembangkan sekumpulan undang-undang penskalaan yang menggambarkan hubungan antara sifat-sifat asli dan sampel. Sebagai tambahan kepada sumbangan teoritis, kesimpulan praktikal dari kerja kami adalah: Strategi pengambilan sampel berdasarkan pemilihan tepi tidak menunjukkan prestasi yang baik; pemilihan simpul rawak seragam sederhana menunjukkan prestasi yang baik Secara keseluruhan, kaedah berprestasi terbaik adalah kaedah berdasarkan jalan rawak dan \"kebakaran hutan\"; mereka sesuai dengan corak grafik statik dan evolusi dengan tepat, dengan ukuran sampel hingga kira-kira 15% daripada graf asal. [[EENNDD]] persampelan grafik; undang-undang penskalaan; perlombongan grafik"], [{"string": "Programming the K-means clustering algorithm in SQL Using SQL has not been considered an efficient and feasible way to implement data mining algorithms . Although this is true for many data mining , machine learning and statistical algorithms , this work shows it is feasible to get an efficient SQL implementation of the well-known K-means clustering algorithm that can work on top of a relational DBMS . The article emphasizes both correctness and performance . From a correctness point of view the article explains how to compute Euclidean distance , nearest-cluster queries and updating clustering results in SQL . From a performance point of view it is explained how to cluster large data sets defining and indexing tables to store and retrieve intermediate and final results , optimizing and avoiding joins , optimizing and simplifying clustering aggregations , and taking advantage of sufficient statistics . Experiments evaluate scalability with synthetic data sets varying size and dimensionality . The proposed K-means implementation can cluster large data sets and exhibits linear scalability .", "keywords": ["k-means", "sql", "clustering"], "combined": "Programming the K-means clustering algorithm in SQL Using SQL has not been considered an efficient and feasible way to implement data mining algorithms . Although this is true for many data mining , machine learning and statistical algorithms , this work shows it is feasible to get an efficient SQL implementation of the well-known K-means clustering algorithm that can work on top of a relational DBMS . The article emphasizes both correctness and performance . From a correctness point of view the article explains how to compute Euclidean distance , nearest-cluster queries and updating clustering results in SQL . From a performance point of view it is explained how to cluster large data sets defining and indexing tables to store and retrieve intermediate and final results , optimizing and avoiding joins , optimizing and simplifying clustering aggregations , and taking advantage of sufficient statistics . Experiments evaluate scalability with synthetic data sets varying size and dimensionality . The proposed K-means implementation can cluster large data sets and exhibits linear scalability . [[EENNDD]] k-means; sql; clustering"}, "Memprogram algoritma pengelompokan K-berarti dalam SQL Menggunakan SQL belum dianggap sebagai kaedah yang efisien dan layak untuk melaksanakan algoritma perlombongan data. Walaupun ini berlaku untuk banyak perlombongan data, pembelajaran mesin dan algoritma statistik, karya ini menunjukkan bahawa layak untuk mendapatkan implementasi SQL yang efisien dari algoritma pengelompokan K-means yang terkenal yang dapat berfungsi di atas DBMS hubungan. Artikel tersebut menekankan kepada ketepatan dan prestasi. Dari sudut pandang kebenaran, artikel menerangkan cara menghitung jarak Euclidean, pertanyaan kluster terdekat dan mengemas kini hasil pengelompokan dalam SQL. Dari sudut pandang prestasi dijelaskan bagaimana mengumpulkan kumpulan data besar yang menentukan dan mengindeks jadual untuk menyimpan dan mengambil hasil pertengahan dan akhir, mengoptimumkan dan menghindari gabungan, mengoptimumkan dan menyederhanakan penggabungan kelompok, dan memanfaatkan statistik yang mencukupi. Eksperimen menilai skalabiliti dengan set data sintetik yang berbeza ukuran dan dimensi. Pelaksanaan K-berarti yang dicadangkan dapat mengumpulkan kumpulan data yang besar dan menunjukkan skalabilitas linear. [[EENNDD]] k-bermaksud; sql; pengelompokan"], [{"string": "Mobile call graphs : beyond power-law and lognormal distributions We analyze a massive social network , gathered from the records of a large mobile phone operator , with more than a million users and tens of millions of calls . We examine the distributions of the number of phone calls per customer ; the total talk minutes per customer ; and the distinct number of calling partners per customer . We find that these distributions are skewed , and that they significantly deviate from what would be expected by power-law and lognormal distributions . To analyze our observed distributions of number of calls , distinct call partners , and total talk time , we propose PowerTrack , a method which fits a lesser known but more suitable distribution , namely the Double Pareto LogNormal DPLN distribution , to our data and track its parameters over time . Using PowerTrack , we find that our graph changes over time in a way consistent with a generative process that naturally results in the DPLN distributions we observe . Furthermore , we show that this generative process lends itself to a natural and appealing social wealth interpretation in the context of social networks such as ours . We discuss the application of those results to our model and to forecasting .", "keywords": ["generative process", "power laws", "dpln", "distribution"], "combined": "Mobile call graphs : beyond power-law and lognormal distributions We analyze a massive social network , gathered from the records of a large mobile phone operator , with more than a million users and tens of millions of calls . We examine the distributions of the number of phone calls per customer ; the total talk minutes per customer ; and the distinct number of calling partners per customer . We find that these distributions are skewed , and that they significantly deviate from what would be expected by power-law and lognormal distributions . To analyze our observed distributions of number of calls , distinct call partners , and total talk time , we propose PowerTrack , a method which fits a lesser known but more suitable distribution , namely the Double Pareto LogNormal DPLN distribution , to our data and track its parameters over time . Using PowerTrack , we find that our graph changes over time in a way consistent with a generative process that naturally results in the DPLN distributions we observe . Furthermore , we show that this generative process lends itself to a natural and appealing social wealth interpretation in the context of social networks such as ours . We discuss the application of those results to our model and to forecasting . [[EENNDD]] generative process; power laws; dpln; distribution"}, "Grafik panggilan mudah alih: di luar undang-undang kuasa dan pengedaran log abnormal Kami menganalisis rangkaian sosial yang besar, yang dikumpulkan dari rekod pengendali telefon bimbit yang besar, dengan lebih dari satu juta pengguna dan puluhan juta panggilan. Kami memeriksa sebaran jumlah panggilan telefon bagi setiap pelanggan; jumlah minit perbincangan bagi setiap pelanggan; dan bilangan rakan panggilan yang berbeza bagi setiap pelanggan. Kami mendapati bahawa pengedaran ini condong, dan bahawa mereka menyimpang secara signifikan dari apa yang diharapkan oleh pengedaran undang-undang kuasa dan log normal. Untuk menganalisis pengagihan jumlah panggilan, rakan panggilan yang berbeza, dan jumlah masa perbincangan kami, kami mencadangkan PowerTrack, kaedah yang sesuai dengan sebaran yang kurang dikenali tetapi lebih sesuai, iaitu pengedaran DPLN Log Parameter Pareto Berganda, ke data kami dan mengesan parameternya lebih masa . Dengan menggunakan PowerTrack, kami dapati grafik kami berubah dari masa ke masa dengan cara yang konsisten dengan proses generatif yang secara semula jadi menghasilkan pengedaran DPLN yang kami amati. Selanjutnya, kami menunjukkan bahawa proses generatif ini sesuai dengan tafsiran kekayaan sosial yang semula jadi dan menarik dalam konteks rangkaian sosial seperti kita. Kami membincangkan penerapan hasil tersebut pada model dan ramalan kami. [[EENNDD]] proses generatif; undang-undang kuasa; dpln; pengedaran"], [{"string": "Mining web logs for prediction models in WWW caching and prefetching Web caching and prefetching are well known strategies for improving the performance of Internet systems . When combined with web log mining , these strategies can decide to cache and prefetch web documents with higher accuracy . In this paper , we present an application of web log mining to obtain web-document access patterns and use these patterns to extend the well-known GDSF caching policies and prefetching policies . Using real web logs , we show that this application of data mining can achieve dramatic improvement to web-access performance .", "keywords": ["application to caching and prefetching on the www", "world wide web", "web log mining"], "combined": "Mining web logs for prediction models in WWW caching and prefetching Web caching and prefetching are well known strategies for improving the performance of Internet systems . When combined with web log mining , these strategies can decide to cache and prefetch web documents with higher accuracy . In this paper , we present an application of web log mining to obtain web-document access patterns and use these patterns to extend the well-known GDSF caching policies and prefetching policies . Using real web logs , we show that this application of data mining can achieve dramatic improvement to web-access performance . [[EENNDD]] application to caching and prefetching on the www; world wide web; web log mining"}, "Log web perlombongan untuk model ramalan dalam caching WWW dan pembuatan data Web caching dan prefetching adalah strategi terkenal untuk meningkatkan prestasi sistem Internet. Apabila digabungkan dengan perlombongan log web, strategi ini dapat memutuskan untuk menyimpan dan mengambil dokumen web dengan tepat. Dalam makalah ini, kami menyajikan aplikasi perlombongan log web untuk mendapatkan pola akses dokumen web dan menggunakan pola ini untuk memperluas dasar cache GDSF yang terkenal dan kebijakan pengambilan. Dengan menggunakan log web sebenar, kami menunjukkan bahawa aplikasi perlombongan data ini dapat mencapai peningkatan dramatik terhadap prestasi akses web. [[EENNDD]] aplikasi untuk caching dan prefetching di www; laman web seluruh dunia; perlombongan log web"], [{"string": "Efficiently handling feature redundancy in high-dimensional data High-dimensional data poses a severe challenge for data mining . Feature selection is a frequently used technique in pre-processing high-dimensional data for successful data mining . Traditionally , feature selection is focused on removing irrelevant features . However , for high-dimensional data , removing redundant features is equally critical . In this paper , we provide a study of feature redundancy in high-dimensional data and propose a novel correlation-based approach to feature selection within the filter model . The extensive empirical study using real-world data shows that the proposed approach is efficient and effective in removing redundant and irrelevant features .", "keywords": ["feature selection", "high-dimensional data", "redundancy"], "combined": "Efficiently handling feature redundancy in high-dimensional data High-dimensional data poses a severe challenge for data mining . Feature selection is a frequently used technique in pre-processing high-dimensional data for successful data mining . Traditionally , feature selection is focused on removing irrelevant features . However , for high-dimensional data , removing redundant features is equally critical . In this paper , we provide a study of feature redundancy in high-dimensional data and propose a novel correlation-based approach to feature selection within the filter model . The extensive empirical study using real-world data shows that the proposed approach is efficient and effective in removing redundant and irrelevant features . [[EENNDD]] feature selection; high-dimensional data; redundancy"}, "Pengendalian redundansi ciri dengan cekap dalam data dimensi tinggi Data dimensi tinggi menimbulkan cabaran yang teruk bagi perlombongan data. Pemilihan fitur adalah teknik yang sering digunakan dalam memproses data dimensi tinggi untuk perlombongan data yang berjaya. Secara tradisinya, pemilihan ciri difokuskan pada membuang ciri yang tidak berkaitan. Walau bagaimanapun, untuk data dimensi tinggi, membuang ciri berlebihan adalah sama pentingnya. Dalam makalah ini, kami memberikan kajian tentang redundansi fitur dalam data dimensi tinggi dan mengusulkan pendekatan berbasis korelasi baru untuk pemilihan fitur dalam model filter. Kajian empirikal yang luas menggunakan data dunia nyata menunjukkan bahawa pendekatan yang dicadangkan adalah cekap dan berkesan dalam menghilangkan ciri-ciri yang berlebihan dan tidak relevan. [[EENNDD]] pemilihan ciri; data dimensi tinggi; kelebihan"], [{"string": "Automating exploratory data analysis for efficient data mining", "keywords": ["encoding", "automation", "transformation", "attribute selection"], "combined": "Automating exploratory data analysis for efficient data mining [[EENNDD]] encoding; automation; transformation; attribute selection"}, "Mengautomasikan analisis data penerokaan untuk pengekodan perlombongan data yang cekap [[EENNDD]]; automasi; transformasi; pemilihan atribut"], [{"string": "Sustainable operation and management of data center chillers using temporal data mining Motivation : Data centers are a critical component of modern IT infrastructure but are also among the worst environmental offenders through their increasing energy usage and the resulting large carbon footprints . Efficient management of data centers , including power management , networking , and cooling infrastructure , is hence crucial to sustainability . In the absence of a ` first-principles ' approach to manage these complex components and their interactions , data-driven approaches have become attractive and tenable . Results : We present a temporal data mining solution to model and optimize performance of data center chillers , a key component of the cooling infrastructure . It helps bridge raw , numeric , time-series information from sensor streams toward higher level characterizations of chiller behavior , suitable for a data center engineer . To aid in this transduction , temporal data streams are first encoded into a symbolic representation , next run-length encoded segments are mined to form frequent motifs in time series , and finally these metrics are evaluated by their contributions to sustainability . A key innovation in our application is the ability to intersperse `` do n't care '' transitions e.g. , transients in continuous-valued time series data , an advantage we inherit by the application of frequent episode mining to symbolized representations of numeric time series . Our approach provides both qualitative and quantitative characterizations of the sensor streams to the data center engineer , to aid him in tuning chiller operating characteristics . This system is currently being prototyped for a data center managed by HP and experimental results from this application reveal the promise of our approach .", "keywords": ["data centers", "sustainability", "chillers", "motifs", "frequent episodes", "clustering"], "combined": "Sustainable operation and management of data center chillers using temporal data mining Motivation : Data centers are a critical component of modern IT infrastructure but are also among the worst environmental offenders through their increasing energy usage and the resulting large carbon footprints . Efficient management of data centers , including power management , networking , and cooling infrastructure , is hence crucial to sustainability . In the absence of a ` first-principles ' approach to manage these complex components and their interactions , data-driven approaches have become attractive and tenable . Results : We present a temporal data mining solution to model and optimize performance of data center chillers , a key component of the cooling infrastructure . It helps bridge raw , numeric , time-series information from sensor streams toward higher level characterizations of chiller behavior , suitable for a data center engineer . To aid in this transduction , temporal data streams are first encoded into a symbolic representation , next run-length encoded segments are mined to form frequent motifs in time series , and finally these metrics are evaluated by their contributions to sustainability . A key innovation in our application is the ability to intersperse `` do n't care '' transitions e.g. , transients in continuous-valued time series data , an advantage we inherit by the application of frequent episode mining to symbolized representations of numeric time series . Our approach provides both qualitative and quantitative characterizations of the sensor streams to the data center engineer , to aid him in tuning chiller operating characteristics . This system is currently being prototyped for a data center managed by HP and experimental results from this application reveal the promise of our approach . [[EENNDD]] data centers; sustainability; chillers; motifs; frequent episodes; clustering"}, "Operasi dan pengurusan penyejuk pusat data yang lestari menggunakan perlombongan data temporal Motivasi: Pusat data adalah komponen kritikal infrastruktur IT moden tetapi juga merupakan antara pelaku pelanggaran persekitaran terburuk melalui peningkatan penggunaan tenaga mereka dan kesan karbon besar yang dihasilkan. Pengurusan pusat data yang cekap, termasuk pengurusan kuasa, rangkaian, dan infrastruktur penyejukan, sangat penting untuk keberlanjutan. Sekiranya tidak ada pendekatan \"prinsip pertama\" untuk menguruskan komponen kompleks dan interaksi mereka, pendekatan berdasarkan data menjadi menarik dan dapat dipertahankan. Hasil: Kami menyajikan penyelesaian perlombongan data temporal untuk memodelkan dan mengoptimumkan prestasi penyejuk pusat data, komponen utama infrastruktur penyejukan. Ini membantu menjembatani maklumat mentah, berangka, dan siri masa dari aliran sensor ke arah ciri tingkah laku penyejuk yang lebih tinggi, sesuai untuk jurutera pusat data. Untuk membantu transduksi ini, aliran data temporal pertama kali dikodekan menjadi representasi simbolik, segmen yang dikodkan jangka masa berikutnya ditambang untuk membentuk motif yang kerap dalam siri masa, dan akhirnya metrik ini dinilai oleh sumbangan mereka terhadap keberlanjutan. Inovasi utama dalam aplikasi kami adalah keupayaan untuk menyebarkan peralihan \"tidak peduli\" mis. , sementara dalam data siri masa yang bernilai berterusan, satu kelebihan yang kita warisi dengan penerapan penambangan episod yang kerap kepada perwakilan simbol dari siri masa berangka. Pendekatan kami memberikan ciri-ciri kualitatif dan kuantitatif aliran sensor kepada jurutera pusat data, untuk membantunya dalam menyesuaikan ciri operasi penyejuk. Sistem ini sedang dibuat prototaip untuk pusat data yang dikendalikan oleh HP dan hasil eksperimen dari aplikasi ini menunjukkan janji pendekatan kami. [[EENNDD]] pusat data; kemampanan; penyejuk; motif; episod yang kerap; pengelompokan"], [{"string": "Empirical bayes screening for multi-item associations This paper considers the framework of the so-called `` market basket problem '' , in which a database of transactions is mined for the occurrence of unusually frequent item sets . In our case , `` unusually frequent '' involves estimates of the frequency of each item set divided by a baseline frequency computed as if items occurred independently . The focus is on obtaining reliable estimates of this measure of interestingness for all item sets , even item sets with relatively low frequencies . For example , in a medical database of patient histories , unusual item sets including the item `` patient death '' or other serious adverse event might hopefully be flagged with as few as 5 or 10 occurrences of the item set , it being unacceptable to require that item sets occur in as many as 0.1 % of millions of patient reports before the data mining algorithm detects a signal . Similar considerations apply in fraud detection applications . Thus we abandon the requirement that interesting item sets must contain a relatively large fixed minimal support , and adopt a criterion based on the results of fitting an empirical Bayes model to the item set counts . The model allows us to define a 95 % Bayesian lower confidence limit for the `` interestingness '' measure of every item set , whereupon the item sets can be ranked according to their empirical Bayes confidence limits . For item sets of size J 2 , we also distinguish between multi-item associations that can be explained by the observed J J-1 \\/ 2 pairwise associations , and item sets that are significantly more frequent than their pairwise associations would suggest . Such item sets can uncover complex or synergistic mechanisms generating multi-item associations . This methodology has been applied within the U.S. Food and Drug Administration FDA to databases of adverse drug reaction reports and within AT&T to customer international calling histories . We also present graphical techniques for exploring and understanding the modeling results .", "keywords": ["market basket problem", "association rules", "shrinkage estimation", "knowledge discovery", "empirical bayes methods", "statistical models", "gamma-poisson model"], "combined": "Empirical bayes screening for multi-item associations This paper considers the framework of the so-called `` market basket problem '' , in which a database of transactions is mined for the occurrence of unusually frequent item sets . In our case , `` unusually frequent '' involves estimates of the frequency of each item set divided by a baseline frequency computed as if items occurred independently . The focus is on obtaining reliable estimates of this measure of interestingness for all item sets , even item sets with relatively low frequencies . For example , in a medical database of patient histories , unusual item sets including the item `` patient death '' or other serious adverse event might hopefully be flagged with as few as 5 or 10 occurrences of the item set , it being unacceptable to require that item sets occur in as many as 0.1 % of millions of patient reports before the data mining algorithm detects a signal . Similar considerations apply in fraud detection applications . Thus we abandon the requirement that interesting item sets must contain a relatively large fixed minimal support , and adopt a criterion based on the results of fitting an empirical Bayes model to the item set counts . The model allows us to define a 95 % Bayesian lower confidence limit for the `` interestingness '' measure of every item set , whereupon the item sets can be ranked according to their empirical Bayes confidence limits . For item sets of size J 2 , we also distinguish between multi-item associations that can be explained by the observed J J-1 \\/ 2 pairwise associations , and item sets that are significantly more frequent than their pairwise associations would suggest . Such item sets can uncover complex or synergistic mechanisms generating multi-item associations . This methodology has been applied within the U.S. Food and Drug Administration FDA to databases of adverse drug reaction reports and within AT&T to customer international calling histories . We also present graphical techniques for exploring and understanding the modeling results . [[EENNDD]] market basket problem; association rules; shrinkage estimation; knowledge discovery; empirical bayes methods; statistical models; gamma-poisson model"}, "Pemeriksaan bayangan empirikal untuk persatuan multi-item Makalah ini mempertimbangkan kerangka apa yang disebut \"masalah keranjang pasaran\", di mana pangkalan data transaksi ditambang untuk berlakunya set item yang tidak biasa. Dalam kes kami, \"tidak biasa\" melibatkan anggaran kekerapan setiap item yang dibahagikan dengan frekuensi asas yang dihitung seolah-olah item berlaku secara bebas. Tumpuannya adalah untuk memperoleh anggaran yang boleh dipercayai mengenai ukuran minat ini untuk semua set item, bahkan set item dengan frekuensi yang agak rendah. Sebagai contoh, dalam pangkalan data perubatan sejarah pesakit, set item yang tidak biasa termasuk item \"kematian pesakit\" atau kejadian buruk yang serius diharapkan dapat ditandakan dengan sedikitnya 5 atau 10 kejadian item tersebut, tidak dapat diterima bahawa set item berlaku sebanyak 0.1% daripada berjuta-juta laporan pesakit sebelum algoritma perlombongan data mengesan isyarat. Pertimbangan serupa berlaku dalam aplikasi pengesanan penipuan. Oleh itu, kami meninggalkan syarat bahawa set item yang menarik mesti mengandungi sokongan minimum tetap yang agak besar, dan menggunakan kriteria berdasarkan hasil pemasangan model Bayes empirikal dengan jumlah set item. Model ini membolehkan kita menentukan had keyakinan 95% Bayesian yang lebih rendah untuk ukuran \"minat\" setiap set item, di mana set item dapat diperingkat mengikut had keyakinan Bayes empirik mereka. Untuk set item dengan ukuran J 2, kami juga membezakan antara persatuan berbilang item yang dapat dijelaskan oleh persatuan berpasangan J J-1 \\ / 2 yang diperhatikan, dan set item yang jauh lebih kerap daripada cadangan pasangan berpasangan mereka. Set item seperti itu dapat mengungkap mekanisme kompleks atau sinergistik yang menghasilkan persatuan pelbagai item. Metodologi ini telah diterapkan dalam FDA Pentadbiran Makanan dan Dadah A.S. ke pangkalan data laporan reaksi ubat-ubatan yang merugikan dan dalam AT&T kepada sejarah panggilan antarabangsa pelanggan. Kami juga menyajikan teknik grafik untuk meneroka dan memahami hasil pemodelan. [[EENNDD]] masalah keranjang pasaran; peraturan persatuan; anggaran pengecutan; penemuan pengetahuan; kaedah bayes empirikal; model statistik; model gamma-poisson"], [{"string": "Ranking-based clustering of heterogeneous information networks with star network schema A heterogeneous information network is an information network composed of multiple types of objects . Clustering on such a network may lead to better understanding of both hidden structures of the network and the individual role played by every object in each cluster . However , although clustering on homogeneous networks has been studied over decades , clustering on heterogeneous networks has not been addressed until recently . A recent study proposed a new algorithm , RankClus , for clustering on bi-typed heterogeneous networks . However , a real-world network may consist of more than two types , and the interactions among multi-typed objects play a key role at disclosing the rich semantics that a network carries . In this paper , we study clustering of multi-typed heterogeneous networks with a star network schema and propose a novel algorithm , NetClus , that utilizes links across multityped objects to generate high-quality net-clusters . An iterative enhancement method is developed that leads to effective ranking-based clustering in such heterogeneous networks . Our experiments on DBLP data show that NetClus generates more accurate clustering results than the baseline topic model algorithm PLSA and the recently proposed algorithm , RankClus . Further , NetClus generates informative clusters , presenting good ranking and cluster membership information for each attribute object in each net-cluster .", "keywords": ["heterogeneous information network", "clustering"], "combined": "Ranking-based clustering of heterogeneous information networks with star network schema A heterogeneous information network is an information network composed of multiple types of objects . Clustering on such a network may lead to better understanding of both hidden structures of the network and the individual role played by every object in each cluster . However , although clustering on homogeneous networks has been studied over decades , clustering on heterogeneous networks has not been addressed until recently . A recent study proposed a new algorithm , RankClus , for clustering on bi-typed heterogeneous networks . However , a real-world network may consist of more than two types , and the interactions among multi-typed objects play a key role at disclosing the rich semantics that a network carries . In this paper , we study clustering of multi-typed heterogeneous networks with a star network schema and propose a novel algorithm , NetClus , that utilizes links across multityped objects to generate high-quality net-clusters . An iterative enhancement method is developed that leads to effective ranking-based clustering in such heterogeneous networks . Our experiments on DBLP data show that NetClus generates more accurate clustering results than the baseline topic model algorithm PLSA and the recently proposed algorithm , RankClus . Further , NetClus generates informative clusters , presenting good ranking and cluster membership information for each attribute object in each net-cluster . [[EENNDD]] heterogeneous information network; clustering"}, "Pengelompokan berdasarkan rangkaian maklumat yang heterogen berdasarkan skema rangkaian bintang Rangkaian maklumat heterogen adalah rangkaian maklumat yang terdiri daripada pelbagai jenis objek. Penggabungan pada jaringan seperti itu dapat menyebabkan pemahaman yang lebih baik mengenai kedua struktur tersembunyi rangkaian dan peranan individu yang dimainkan oleh setiap objek dalam setiap kelompok. Namun, walaupun pengelompokan pada jaringan homogen telah dikaji selama beberapa dekad, pengelompokan pada jaringan heterogen belum ditangani sampai akhir-akhir ini. Satu kajian baru-baru ini mencadangkan algoritma baru, RankClus, untuk pengelompokan pada jaringan heterogen dwikaip. Walau bagaimanapun, rangkaian dunia nyata mungkin terdiri daripada lebih dari dua jenis, dan interaksi antara objek berbilang menaip memainkan peranan penting dalam mendedahkan semantik kaya yang dibawa oleh rangkaian. Dalam makalah ini, kami mempelajari pengelompokan rangkaian heterogen berbilang jenis dengan skema rangkaian bintang dan mengusulkan algoritma baru, NetClus, yang menggunakan pautan merentasi objek berbilang untuk menghasilkan kelompok bersih berkualiti tinggi. Kaedah peningkatan iteratif dikembangkan yang membawa kepada pengelompokan berdasarkan peringkat yang berkesan dalam rangkaian heterogen seperti itu. Eksperimen kami pada data DBLP menunjukkan bahawa NetClus menghasilkan hasil pengelompokan yang lebih tepat daripada algoritma model topik asas PLSA dan algoritma yang baru dicadangkan, RankClus. Selanjutnya, NetClus menghasilkan kluster bermaklumat, menyajikan maklumat kedudukan dan keanggotaan kluster yang baik untuk setiap objek atribut di setiap kelompok bersih. [[EENNDD]] rangkaian maklumat heterogen; pengelompokan"], [{"string": "An approach to spacecraft anomaly detection problem using kernel feature space Development of advanced anomaly detection and failure diagnosis technologies for spacecraft is a quite significant issue in the space industry , because the space environment is harsh , distant and uncertain . While several modern approaches based on qualitative reasoning , expert systems , and probabilistic reasoning have been developed recently for this purpose , any of them has a common difficulty in obtaining accurate and complete a priori knowledge on the space systems from human experts . A reasonable alternative to this conventional anomaly detection method is to reuse a vast amount of telemetry data which is multi-dimensional time-series continuously produced from a number of system components in the spacecraft . This paper proposes a novel `` knowledge-free '' anomaly detection method for spacecraft based on Kernel Feature Space and directional distribution , which constructs a system behavior model from the past normal telemetry data from a set of telemetry data in normal operation and monitors the current system status by checking incoming data with the model . In this method , we regard anomaly phenomena as unexpected changes of causal associations in the spacecraft system , and hypothesize that the significant causal associations inside the system will appear in the form of principal component directions in a high-dimensional non-linear feature space which is constructed by a kernel function and a set of data . We have confirmed the effectiveness of the proposed anomaly detection method by applying it to the telemetry data obtained from a simulator of an orbital transfer vehicle designed to make a rendezvous maneuver with the International Space Station .", "keywords": ["anomaly detection", "kernel feature space", "spacecraft", "von mises fisher distribution", "learning", "time series data", "principal component analysis"], "combined": "An approach to spacecraft anomaly detection problem using kernel feature space Development of advanced anomaly detection and failure diagnosis technologies for spacecraft is a quite significant issue in the space industry , because the space environment is harsh , distant and uncertain . While several modern approaches based on qualitative reasoning , expert systems , and probabilistic reasoning have been developed recently for this purpose , any of them has a common difficulty in obtaining accurate and complete a priori knowledge on the space systems from human experts . A reasonable alternative to this conventional anomaly detection method is to reuse a vast amount of telemetry data which is multi-dimensional time-series continuously produced from a number of system components in the spacecraft . This paper proposes a novel `` knowledge-free '' anomaly detection method for spacecraft based on Kernel Feature Space and directional distribution , which constructs a system behavior model from the past normal telemetry data from a set of telemetry data in normal operation and monitors the current system status by checking incoming data with the model . In this method , we regard anomaly phenomena as unexpected changes of causal associations in the spacecraft system , and hypothesize that the significant causal associations inside the system will appear in the form of principal component directions in a high-dimensional non-linear feature space which is constructed by a kernel function and a set of data . We have confirmed the effectiveness of the proposed anomaly detection method by applying it to the telemetry data obtained from a simulator of an orbital transfer vehicle designed to make a rendezvous maneuver with the International Space Station . [[EENNDD]] anomaly detection; kernel feature space; spacecraft; von mises fisher distribution; learning; time series data; principal component analysis"}, "Pendekatan untuk masalah pengesanan anomali kapal angkasa menggunakan ruang ciri kernel Pengembangan teknologi pengesanan anomali canggih dan kegagalan untuk kapal angkasa adalah masalah yang cukup signifikan dalam industri ruang angkasa, kerana persekitaran ruang angkasa keras, jauh dan tidak pasti. Walaupun beberapa pendekatan moden berdasarkan penaakulan kualitatif, sistem pakar, dan penaakulan probabilistik telah dikembangkan baru-baru ini untuk tujuan ini, salah satu daripadanya mempunyai kesukaran yang sama dalam memperoleh pengetahuan yang tepat dan lengkap apriori mengenai sistem ruang angkasa dari pakar manusia. Alternatif yang munasabah untuk kaedah pengesanan anomali konvensional ini adalah menggunakan kembali sejumlah besar data telemetri yang merupakan siri masa pelbagai dimensi yang dihasilkan secara berterusan dari sejumlah komponen sistem dalam kapal angkasa. Makalah ini mencadangkan kaedah pengesanan anomali novel \"tanpa pengetahuan\" untuk kapal angkasa berdasarkan Kernel Feature Space dan pengedaran arah, yang membina model tingkah laku sistem dari data telemetri biasa masa lalu dari satu set data telemetri dalam operasi normal dan memantau status sistem semasa dengan memeriksa data masuk dengan model. Dalam kaedah ini, kami menganggap fenomena anomali sebagai perubahan yang tidak dijangka dari hubungan kausal dalam sistem kapal angkasa, dan membuat hipotesis bahawa hubungan kausal yang signifikan di dalam sistem akan muncul dalam bentuk arah komponen utama dalam ruang ciri bukan linier dimensi tinggi yang dibina oleh fungsi kernel dan sekumpulan data. Kami telah mengesahkan keberkesanan kaedah pengesanan anomali yang dicadangkan dengan menerapkannya pada data telemetri yang diperoleh dari simulator kenderaan pemindahan orbit yang dirancang untuk membuat manuver pertemuan dengan Stesen Angkasa Antarabangsa. [[EENNDD]] pengesanan anomali; ruang ciri kernel; kapal angkasa; pengedaran nelayan von mises; belajar; data siri masa; analisis komponen utama"], [{"string": "Canonicalization of database records using adaptive similarity measures It is becoming increasingly common to construct databases from information automatically culled from many heterogeneous sources . For example , a research publication database can be constructed by automatically extracting titles , authors , and conference information from online papers . A common difficulty in consolidating data from multiple sources is that records are referenced in a variety of ways e.g. abbreviations , aliases , and misspellings . Therefore , it can be difficult to construct a single , standard representation to present to the user . We refer to the task of constructing this representation as canonicalization . Despite its importance , there is little existing work on canonicalization . In this paper , we explore the use of edit distance measures to construct a canonical representation that is `` central '' in the sense that it is most similar to each of the disparate records . This approach reduces the impact of noisy records on the canonical representation . Furthermore , because the user may prefer different styles of canonicalization , we show how different edit distance costs can result in different forms of canonicalization . For example , reducing the cost of character deletions can result in representations that favor abbreviated forms over expanded forms e.g. KDD versus Conference on Knowledge Discovery and Data Mining . We describe how to learn these costs from a small amount of manually annotated data using stochastic hill-climbing . Additionally , we investigate feature-based methods to learn ranking preferences over canonicalizations . These approaches can incorporate arbitrary textual evidence to select a canonical record . We evaluate our approach on a real-world publications database and show that our learning method results in a canonicalization solution that is robust to errors and easily customizable to user preferences .", "keywords": ["information extraction", "data cleaning"], "combined": "Canonicalization of database records using adaptive similarity measures It is becoming increasingly common to construct databases from information automatically culled from many heterogeneous sources . For example , a research publication database can be constructed by automatically extracting titles , authors , and conference information from online papers . A common difficulty in consolidating data from multiple sources is that records are referenced in a variety of ways e.g. abbreviations , aliases , and misspellings . Therefore , it can be difficult to construct a single , standard representation to present to the user . We refer to the task of constructing this representation as canonicalization . Despite its importance , there is little existing work on canonicalization . In this paper , we explore the use of edit distance measures to construct a canonical representation that is `` central '' in the sense that it is most similar to each of the disparate records . This approach reduces the impact of noisy records on the canonical representation . Furthermore , because the user may prefer different styles of canonicalization , we show how different edit distance costs can result in different forms of canonicalization . For example , reducing the cost of character deletions can result in representations that favor abbreviated forms over expanded forms e.g. KDD versus Conference on Knowledge Discovery and Data Mining . We describe how to learn these costs from a small amount of manually annotated data using stochastic hill-climbing . Additionally , we investigate feature-based methods to learn ranking preferences over canonicalizations . These approaches can incorporate arbitrary textual evidence to select a canonical record . We evaluate our approach on a real-world publications database and show that our learning method results in a canonicalization solution that is robust to errors and easily customizable to user preferences . [[EENNDD]] information extraction; data cleaning"}, "Kanonikalisasi rekod pangkalan data menggunakan ukuran kesamaan adaptif Menjadi semakin umum untuk membina pangkalan data dari maklumat yang secara automatik diambil dari banyak sumber heterogen. Sebagai contoh, pangkalan data penerbitan penyelidikan dapat dibina dengan mengekstrak tajuk, pengarang, dan maklumat persidangan dari makalah dalam talian secara automatik. Kesukaran umum dalam menyatukan data dari pelbagai sumber ialah rekod dirujuk dengan pelbagai cara mis. singkatan, alias, dan ejaan yang salah. Oleh itu, sukar untuk membina satu representasi standard untuk disampaikan kepada pengguna. Kami merujuk kepada tugas membina perwakilan ini sebagai kanonikalisasi. Walaupun pentingnya, terdapat sedikit kerja yang ada mengenai kanonikalisasi. Dalam makalah ini, kami meneroka penggunaan ukuran jarak edit untuk membina perwakilan kanonik yang \"pusat\" dalam arti bahawa ia paling serupa dengan setiap catatan yang berbeza. Pendekatan ini mengurangkan kesan rakaman bising pada perwakilan kanonik. Selanjutnya, kerana pengguna mungkin lebih suka gaya kanonikalisasi yang berbeza, kami menunjukkan bagaimana kos jarak pengeditan yang berbeza dapat menghasilkan bentuk kanonikalisasi yang berbeza. Sebagai contoh, pengurangan kos penghapusan watak boleh menghasilkan perwakilan yang menggemari bentuk singkatan daripada bentuk yang diperluas, mis. KDD lawan Persidangan mengenai Penemuan Pengetahuan dan Perlombongan Data. Kami menerangkan bagaimana mempelajari kos ini dari sebilangan kecil data anotasi secara manual menggunakan pendakian bukit stokastik. Selain itu, kami menyiasat kaedah berdasarkan ciri untuk mengetahui pilihan peringkat berbanding pengabaian. Pendekatan ini dapat menggabungkan bukti teks sewenang-wenangnya untuk memilih catatan kanonik. Kami menilai pendekatan kami pada pangkalan data penerbitan dunia nyata dan menunjukkan bahawa kaedah pembelajaran kami menghasilkan penyelesaian kanonikalisasi yang kuat terhadap kesilapan dan mudah disesuaikan dengan pilihan pengguna. [[EENNDD]] pengekstrakan maklumat; pembersihan data"], [{"string": "Constructing comprehensive summaries of large event sequences Event sequences capture system and user activity over time . Prior research on sequence mining has mostly focused on discovering local patterns . Though interesting , these patterns reveal local associations and fail to give a comprehensive summary of the entire event sequence . Moreover , the number of patterns discovered can be large . In this paper , we take an alternative approach and build short summaries that describe the entire sequence , while revealing local associations among events . We formally define the summarization problem as an optimization problem that balances between shortness of the summary and accuracy of the data description . We show that this problem can be solved optimally in polynomial time by using a combination of two dynamic-programming algorithms . We also explore more efficient greedy alternatives and demonstrate that they work well on large datasets . Experiments on both synthetic and real datasets illustrate that our algorithms are efficient and produce high-quality results , and reveal interesting local structures in the data .", "keywords": ["minimum description length", "log mining", "event sequences", "summarization", "dynamic programming"], "combined": "Constructing comprehensive summaries of large event sequences Event sequences capture system and user activity over time . Prior research on sequence mining has mostly focused on discovering local patterns . Though interesting , these patterns reveal local associations and fail to give a comprehensive summary of the entire event sequence . Moreover , the number of patterns discovered can be large . In this paper , we take an alternative approach and build short summaries that describe the entire sequence , while revealing local associations among events . We formally define the summarization problem as an optimization problem that balances between shortness of the summary and accuracy of the data description . We show that this problem can be solved optimally in polynomial time by using a combination of two dynamic-programming algorithms . We also explore more efficient greedy alternatives and demonstrate that they work well on large datasets . Experiments on both synthetic and real datasets illustrate that our algorithms are efficient and produce high-quality results , and reveal interesting local structures in the data . [[EENNDD]] minimum description length; log mining; event sequences; summarization; dynamic programming"}, "Menyusun ringkasan komprehensif urutan peristiwa besar Urutan menangkap sistem dan aktiviti pengguna dari masa ke masa. Penyelidikan sebelumnya mengenai perlombongan urutan kebanyakannya menumpukan pada mencari corak tempatan. Walaupun menarik, corak ini mengungkapkan persatuan tempatan dan gagal memberikan ringkasan menyeluruh keseluruhan urutan acara. Lebih-lebih lagi, jumlah corak yang ditemui boleh besar. Dalam makalah ini, kami mengambil pendekatan alternatif dan membina ringkasan pendek yang menerangkan keseluruhan urutan, sambil mengungkapkan pergaulan tempatan antara peristiwa. Kami secara formal mendefinisikan masalah ringkasan sebagai masalah pengoptimuman yang mengimbangi antara pendeknya ringkasan dan ketepatan keterangan data. Kami menunjukkan bahawa masalah ini dapat diselesaikan secara optimum dalam waktu polinomial dengan menggunakan gabungan dua algoritma pengaturcaraan dinamik. Kami juga meneroka alternatif tamak yang lebih cekap dan menunjukkan bahawa mereka berfungsi dengan baik pada set data yang besar. Eksperimen pada kedua-dua kumpulan data sintetik dan sebenar menggambarkan bahawa algoritma kami cekap dan menghasilkan hasil berkualiti tinggi, dan mendedahkan struktur tempatan yang menarik dalam data. [[EENNDD]] panjang keterangan minimum; perlombongan balak; urutan acara; ringkasan; pengaturcaraan dinamik"], [{"string": "Drosophila gene expression pattern annotation using sparse features and term-term interactions The Drosophila gene expression pattern images document the spatial and temporal dynamics of gene expression and they are valuable tools for explicating the gene functions , interaction , and networks during Drosophila embryogenesis . To provide text-based pattern searching , the images in the Berkeley Drosophila Genome Project BDGP study are annotated with ontology terms manually by human curators . We present a systematic approach for automating this task , because the number of images needing text descriptions is now rapidly increasing . We consider both improved feature representation and novel learning formulation to boost the annotation performance . For feature representation , we adapt the bag-of-words scheme commonly used in visual recognition problems so that the image group information in the BDGP study is retained . Moreover , images from multiple views can be integrated naturally in this representation . To reduce the quantization error caused by the bag-of-words representation , we propose an improved feature representation scheme based on the sparse learning technique . In the design of learning formulation , we propose a local regularization framework that can incorporate the correlations among terms explicitly . We further show that the resulting optimization problem admits an analytical solution . Experimental results show that the representation based on sparse learning outperforms the bag-of-words representation significantly . Results also show that incorporation of the term-term correlations improves the annotation performance consistently .", "keywords": ["regularization", "bag-of-words", "image annotation", "gene expression pattern", "sparse learning"], "combined": "Drosophila gene expression pattern annotation using sparse features and term-term interactions The Drosophila gene expression pattern images document the spatial and temporal dynamics of gene expression and they are valuable tools for explicating the gene functions , interaction , and networks during Drosophila embryogenesis . To provide text-based pattern searching , the images in the Berkeley Drosophila Genome Project BDGP study are annotated with ontology terms manually by human curators . We present a systematic approach for automating this task , because the number of images needing text descriptions is now rapidly increasing . We consider both improved feature representation and novel learning formulation to boost the annotation performance . For feature representation , we adapt the bag-of-words scheme commonly used in visual recognition problems so that the image group information in the BDGP study is retained . Moreover , images from multiple views can be integrated naturally in this representation . To reduce the quantization error caused by the bag-of-words representation , we propose an improved feature representation scheme based on the sparse learning technique . In the design of learning formulation , we propose a local regularization framework that can incorporate the correlations among terms explicitly . We further show that the resulting optimization problem admits an analytical solution . Experimental results show that the representation based on sparse learning outperforms the bag-of-words representation significantly . Results also show that incorporation of the term-term correlations improves the annotation performance consistently . [[EENNDD]] regularization; bag-of-words; image annotation; gene expression pattern; sparse learning"}, "Anotasi corak ekspresi gen Drosophila menggunakan ciri-ciri jarang dan interaksi jangka-panjang Imej corak ekspresi gen Drosophila mendokumentasikan dinamika spasial dan temporal ekspresi gen dan mereka adalah alat yang berguna untuk menjelaskan fungsi gen, interaksi, dan rangkaian semasa embriogenesis Drosophila. Untuk menyediakan pencarian corak berdasarkan teks, gambar-gambar dalam kajian Berkeley Drosophila Genome Project BDGP dianotasi dengan istilah ontologi secara manual oleh kurator manusia. Kami menyajikan pendekatan sistematik untuk mengotomatisasi tugas ini, kerana jumlah gambar yang memerlukan deskripsi teks kini meningkat dengan cepat. Kami menganggap perwakilan ciri yang diperbaiki dan rumusan pembelajaran novel untuk meningkatkan prestasi anotasi. Untuk perwakilan ciri, kami menyesuaikan skema kata-kata yang biasa digunakan dalam masalah pengenalan visual sehingga maklumat kumpulan gambar dalam kajian BDGP disimpan. Selain itu, gambar dari pelbagai pandangan dapat disatukan secara semula jadi dalam perwakilan ini. Untuk mengurangkan ralat kuantisasi yang disebabkan oleh perwakilan kata-kata, kami mencadangkan skema perwakilan ciri yang lebih baik berdasarkan teknik pembelajaran jarang. Dalam reka bentuk pembelajaran, kami mencadangkan kerangka kerja regulerisasi tempatan yang dapat menggabungkan korelasi antara istilah secara eksplisit. Kami selanjutnya menunjukkan bahawa masalah pengoptimuman yang dihasilkan mengakui penyelesaian analitis. Hasil eksperimen menunjukkan bahawa perwakilan berdasarkan pembelajaran jarang mengatasi perwakilan kata-kata dengan ketara. Hasil kajian juga menunjukkan bahawa penggabungan korelasi term-term meningkatkan prestasi anotasi secara konsisten. [[EENNDD]] regularisasi; beg perkataan; anotasi gambar; corak ekspresi gen; pembelajaran yang jarang"], [{"string": "Parallel mining of closed sequential patterns Discovery of sequential patterns is an essential data mining task with broad applications . Among several variations of sequential patterns , closed sequential pattern is the most useful one since it retains all the information of the complete pattern set but is often much more compact than it . Unfortunately , there is no parallel closed sequential pattern mining method proposed yet . In this paper we develop an algorithm , called Par-CSP Parallel Closed Sequential Pattern mining , to conduct parallel mining of closed sequential patterns on a distributed memory system . Par-CSP partitions the work among the processors by exploiting the divide-and-conquer property so that the overhead of interprocessor communication is minimized . Par-CSP applies dynamic scheduling to avoid processor idling . Moreover , it employs a technique , called selective sampling to address the load imbalance problem . We implement Par-CSP using MPI on a 64-node Linux cluster . Our experimental results show that Par-CSP attains good parallelization efficiencies on various input datasets .", "keywords": ["load balancing", "parallel algorithms", "sampling"], "combined": "Parallel mining of closed sequential patterns Discovery of sequential patterns is an essential data mining task with broad applications . Among several variations of sequential patterns , closed sequential pattern is the most useful one since it retains all the information of the complete pattern set but is often much more compact than it . Unfortunately , there is no parallel closed sequential pattern mining method proposed yet . In this paper we develop an algorithm , called Par-CSP Parallel Closed Sequential Pattern mining , to conduct parallel mining of closed sequential patterns on a distributed memory system . Par-CSP partitions the work among the processors by exploiting the divide-and-conquer property so that the overhead of interprocessor communication is minimized . Par-CSP applies dynamic scheduling to avoid processor idling . Moreover , it employs a technique , called selective sampling to address the load imbalance problem . We implement Par-CSP using MPI on a 64-node Linux cluster . Our experimental results show that Par-CSP attains good parallelization efficiencies on various input datasets . [[EENNDD]] load balancing; parallel algorithms; sampling"}, "Perlombongan selari pola berurutan tertutup Penemuan corak jujukan adalah tugas perlombongan data penting dengan aplikasi yang luas. Di antara beberapa variasi corak jujukan, corak urutan tertutup adalah yang paling berguna kerana menyimpan semua maklumat mengenai set corak yang lengkap tetapi selalunya jauh lebih padat daripadanya. Malangnya, belum ada kaedah perlombongan pola urutan tertutup yang dicadangkan. Dalam makalah ini kami mengembangkan algoritma, yang disebut Par-CSP Parallel Closed Sequential Pattern mining, untuk melakukan perlombongan selari pola berurutan tertutup pada sistem memori yang diedarkan. Par-CSP membahagikan kerja di antara pemproses dengan mengeksploitasi harta pembahagi dan penaklukan sehingga overhead komunikasi interprocessor diminimumkan. Par-CSP menerapkan penjadualan dinamik untuk mengelakkan pemproses tidak beroperasi. Lebih dari itu, ia menggunakan teknik, yang disebut pemilihan selektif untuk mengatasi masalah ketidakseimbangan beban. Kami melaksanakan Par-CSP menggunakan MPI pada kluster Linux 64-node. Hasil eksperimen kami menunjukkan bahawa Par-CSP mencapai kecekapan paralelisasi yang baik pada pelbagai set data input. [[EENNDD]] pengimbangan beban; algoritma selari; persampelan"], [{"string": "Enhanced max margin learning on multimodal data mining in a multimedia database The problem of multimodal data mining in a multimedia database can be addressed as a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variables . In this paper , built upon the existing literature on the max margin based learning , we develop a new max margin learning approach called Enhanced Max Margin Learning EMML framework . In addition , we apply EMML framework to developing an effective and efficient solution to the multimodal data mining problem in a multimedia database . The main contributions include : 1 we have developed a new max margin learning approach - the enhanced max margin learning framework that is much more efficient in learning with a much faster convergence rate , which is verified in empirical evaluations ; 2 we have applied this EMML approach to developing an effective and efficient solution to the multimodal data mining problem that is highly scalable in the sense that the query response time is independent of the database scale , allowing facilitating a multimodal data mining querying to a very large scale multimedia database , and excelling many existing multimodal data mining methods in the literature that do not scale up at all ; this advantage is also supported through the complexity analysis as well as empirical evaluations against a state-of-the-art multimodal data mining method from the literature . While EMML is a general framework , for the evaluation purpose , we apply it to the Berkeley Drosophila embryo image database , and report the performance comparison with a state-of-the-art multimodal data mining method .", "keywords": ["image annotation", "multimodal data mining", "image retrieval", "max margin"], "combined": "Enhanced max margin learning on multimodal data mining in a multimedia database The problem of multimodal data mining in a multimedia database can be addressed as a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variables . In this paper , built upon the existing literature on the max margin based learning , we develop a new max margin learning approach called Enhanced Max Margin Learning EMML framework . In addition , we apply EMML framework to developing an effective and efficient solution to the multimodal data mining problem in a multimedia database . The main contributions include : 1 we have developed a new max margin learning approach - the enhanced max margin learning framework that is much more efficient in learning with a much faster convergence rate , which is verified in empirical evaluations ; 2 we have applied this EMML approach to developing an effective and efficient solution to the multimodal data mining problem that is highly scalable in the sense that the query response time is independent of the database scale , allowing facilitating a multimodal data mining querying to a very large scale multimedia database , and excelling many existing multimodal data mining methods in the literature that do not scale up at all ; this advantage is also supported through the complexity analysis as well as empirical evaluations against a state-of-the-art multimodal data mining method from the literature . While EMML is a general framework , for the evaluation purpose , we apply it to the Berkeley Drosophila embryo image database , and report the performance comparison with a state-of-the-art multimodal data mining method . [[EENNDD]] image annotation; multimodal data mining; image retrieval; max margin"}, "Pembelajaran margin maksimum yang ditingkatkan pada perlombongan data multimodal dalam pangkalan data multimedia Masalah perlombongan data multimodal dalam pangkalan data multimedia dapat ditangani sebagai masalah ramalan berstruktur di mana kita belajar pemetaan dari input ke pemboleh ubah output berstruktur dan saling bergantung. Dalam makalah ini, yang dibangun berdasarkan literatur yang ada mengenai pembelajaran berdasarkan margin maksimum, kami mengembangkan pendekatan pembelajaran margin maksimum baru yang disebut kerangka EMML Pembelajaran Enhanced Max Margin. Selain itu, kami menerapkan kerangka EMML untuk mengembangkan penyelesaian yang efektif dan efisien untuk masalah perlombongan data multimodal dalam pangkalan data multimedia. Sumbangan utama merangkumi: 1 kami telah mengembangkan pendekatan pembelajaran margin maksimum baru - kerangka pembelajaran margin maksimum yang dipertingkatkan yang jauh lebih efisien dalam pembelajaran dengan kadar penumpuan yang lebih cepat, yang disahkan dalam penilaian empirikal; 2 kami telah menerapkan pendekatan EMML ini untuk mengembangkan penyelesaian yang efektif dan efisien untuk masalah perlombongan data multimodal yang sangat terukur dalam arti bahawa masa tindak balas pertanyaan tidak bergantung pada skala pangkalan data, yang memungkinkan memfasilitasi pertanyaan perlombongan data multimodal menjadi sangat besar skala pangkalan data multimedia, dan unggul banyak kaedah perlombongan data multimodal yang ada dalam literatur yang sama sekali tidak meningkat; kelebihan ini juga disokong melalui analisis kerumitan serta penilaian empirikal terhadap kaedah perlombongan data multimodal terkini dari literatur. Walaupun EMML adalah kerangka umum, untuk tujuan penilaian, kami menerapkannya ke pangkalan data gambar embrio Berkeley Drosophila, dan melaporkan perbandingan prestasi dengan kaedah perlombongan data multimodal canggih. [[EENNDD]] anotasi gambar; perlombongan data multimodal; pengambilan gambar; margin maksimum"], [{"string": "Transfer metric learning by learning task relationships Distance metric learning plays a very crucial role in many data mining algorithms because the performance of an algorithm relies heavily on choosing a good metric . However , the labeled data available in many applications is scarce and hence the metrics learned are often unsatisfactory . In this paper , we consider a transfer learning setting in which some related source tasks with labeled data are available to help the learning of the target task . We first propose a convex formulation for multi-task metric learning by modeling the task relationships in the form of a task covariance matrix . Then we regard transfer learning as a special case of multi-task learning and adapt the formulation of multi-task metric learning to the transfer learning setting for our method , called transfer metric learning TML . In TML , we learn the metric and the task covariances between the source tasks and the target task under a unified convex formulation . To solve the convex optimization problem , we use an alternating method in which each subproblem has an efficient solution . Experimental results on some commonly used transfer learning applications demonstrate the effectiveness of our method .", "keywords": ["transfer learning", "multi-task learning", "learning", "metric learning"], "combined": "Transfer metric learning by learning task relationships Distance metric learning plays a very crucial role in many data mining algorithms because the performance of an algorithm relies heavily on choosing a good metric . However , the labeled data available in many applications is scarce and hence the metrics learned are often unsatisfactory . In this paper , we consider a transfer learning setting in which some related source tasks with labeled data are available to help the learning of the target task . We first propose a convex formulation for multi-task metric learning by modeling the task relationships in the form of a task covariance matrix . Then we regard transfer learning as a special case of multi-task learning and adapt the formulation of multi-task metric learning to the transfer learning setting for our method , called transfer metric learning TML . In TML , we learn the metric and the task covariances between the source tasks and the target task under a unified convex formulation . To solve the convex optimization problem , we use an alternating method in which each subproblem has an efficient solution . Experimental results on some commonly used transfer learning applications demonstrate the effectiveness of our method . [[EENNDD]] transfer learning; multi-task learning; learning; metric learning"}, "Memindahkan pembelajaran metrik dengan belajar hubungan tugas Pembelajaran metrik jarak jauh memainkan peranan yang sangat penting dalam banyak algoritma perlombongan data kerana prestasi algoritma sangat bergantung pada memilih metrik yang baik. Walau bagaimanapun, data berlabel yang terdapat di banyak aplikasi jarang dan oleh itu metrik yang dipelajari seringkali tidak memuaskan. Dalam makalah ini, kami mempertimbangkan pengaturan pembelajaran transfer di mana beberapa tugas sumber yang berkaitan dengan data berlabel tersedia untuk membantu pembelajaran tugas sasaran. Mula-mula kami mencadangkan formulasi cembung untuk pembelajaran metrik multi-tugas dengan memodelkan hubungan tugas dalam bentuk matriks kovarians tugas. Kemudian kami menganggap pembelajaran pemindahan sebagai kes khusus pembelajaran multi-tugas dan menyesuaikan formulasi pembelajaran metrik pelbagai tugas dengan tetapan pembelajaran pemindahan untuk kaedah kami, yang disebut TML pembelajaran pemindahan metrik. Di TML, kami mempelajari metrik dan kovarian tugas antara tugas sumber dan tugas sasaran di bawah rumusan cembung terpadu. Untuk menyelesaikan masalah pengoptimuman cembung, kami menggunakan kaedah bolak-balik di mana setiap masalah mempunyai penyelesaian yang cekap. Hasil eksperimen pada beberapa aplikasi pembelajaran pemindahan yang biasa digunakan menunjukkan keberkesanan kaedah kami. [[EENNDD]] memindahkan pembelajaran; pembelajaran pelbagai tugas; belajar; pembelajaran metrik"], [{"string": "Fastanova : an efficient algorithm for genome-wide association study Studying the association between quantitative phenotype such as height or weight and single nucleotide polymorphisms SNPs is an important problem in biology . To understand underlying mechanisms of complex phenotypes , it is often necessary to consider joint genetic effects across multiple SNPs . ANOVA analysis of variance test is routinely used in association study . Important findings from studying gene-gene SNP-pair interactions are appearing in the literature . However , the number of SNPs can be up to millions . Evaluating joint effects of SNPs is a challenging task even for SNP-pairs . Moreover , with large number of SNPs correlated , permutation procedure is preferred over simple Bonferroni correction for properly controlling family-wise error rate and retaining mapping power , which dramatically increases the computational cost of association study . In this paper , we study the problem of finding SNP-pairs that have significant associations with a given quantitative phenotype . We propose an efficient algorithm , FastANOVA , for performing ANOVA tests on SNP-pairs in a batch mode , which also supports large permutation test . We derive an upper bound of SNP-pair ANOVA test , which can be expressed as the sum of two terms . The first term is based on single-SNP ANOVA test . The second term is based on the SNPs and independent of any phenotype permutation . Furthermore , SNP-pairs can be organized into groups , each of which shares a common upper bound . This allows for maximum reuse of intermediate computation , efficient upper bound estimation , and effective SNP-pair pruning . Consequently , FastANOVA only needs to perform the ANOVA test on a small number of candidate SNP-pairs without the risk of missing any significant ones . Extensive experiments demonstrate that FastANOVA is orders of magnitude faster than the brute-force implementation of ANOVA tests on all SNP pairs .", "keywords": ["anova test", "association study"], "combined": "Fastanova : an efficient algorithm for genome-wide association study Studying the association between quantitative phenotype such as height or weight and single nucleotide polymorphisms SNPs is an important problem in biology . To understand underlying mechanisms of complex phenotypes , it is often necessary to consider joint genetic effects across multiple SNPs . ANOVA analysis of variance test is routinely used in association study . Important findings from studying gene-gene SNP-pair interactions are appearing in the literature . However , the number of SNPs can be up to millions . Evaluating joint effects of SNPs is a challenging task even for SNP-pairs . Moreover , with large number of SNPs correlated , permutation procedure is preferred over simple Bonferroni correction for properly controlling family-wise error rate and retaining mapping power , which dramatically increases the computational cost of association study . In this paper , we study the problem of finding SNP-pairs that have significant associations with a given quantitative phenotype . We propose an efficient algorithm , FastANOVA , for performing ANOVA tests on SNP-pairs in a batch mode , which also supports large permutation test . We derive an upper bound of SNP-pair ANOVA test , which can be expressed as the sum of two terms . The first term is based on single-SNP ANOVA test . The second term is based on the SNPs and independent of any phenotype permutation . Furthermore , SNP-pairs can be organized into groups , each of which shares a common upper bound . This allows for maximum reuse of intermediate computation , efficient upper bound estimation , and effective SNP-pair pruning . Consequently , FastANOVA only needs to perform the ANOVA test on a small number of candidate SNP-pairs without the risk of missing any significant ones . Extensive experiments demonstrate that FastANOVA is orders of magnitude faster than the brute-force implementation of ANOVA tests on all SNP pairs . [[EENNDD]] anova test; association study"}, "Fastanova: algoritma yang cekap untuk kajian persatuan seluruh genom Mengkaji hubungan antara fenotip kuantitatif seperti tinggi atau berat dan polimorfisme nukleotida tunggal SNP adalah masalah penting dalam biologi. Untuk memahami mekanisme asas fenotip kompleks, selalunya perlu untuk mempertimbangkan kesan genetik sendi di pelbagai SNP. Analisis ANOVA ujian varians secara rutin digunakan dalam kajian persatuan. Penemuan penting dari mengkaji interaksi pasangan gen-gen SNP muncul dalam literatur. Walau bagaimanapun, jumlah SNP boleh mencapai berjuta-juta. Menilai kesan bersama SNP adalah tugas yang mencabar bahkan untuk pasangan SNP. Lebih-lebih lagi, dengan sebilangan besar SNP berkorelasi, prosedur permutasi lebih disukai daripada pembetulan Bonferroni yang mudah untuk mengawal kadar kesalahan keluarga dengan betul dan mengekalkan daya pemetaan, yang secara dramatik meningkatkan kos pengkomputeran kajian persatuan. Dalam makalah ini, kami mengkaji masalah mencari pasangan SNP yang mempunyai kaitan yang signifikan dengan fenotip kuantitatif yang diberikan. Kami mencadangkan algoritma yang cekap, FastANOVA, untuk melakukan ujian ANOVA pada pasangan SNP dalam mod kumpulan, yang juga menyokong ujian permutasi besar. Kami memperoleh ujian atas ANOVA pasangan SNP, yang dapat dinyatakan sebagai jumlah dua istilah. Istilah pertama berdasarkan ujian ANOVA SNP tunggal. Istilah kedua didasarkan pada SNP dan bebas dari permutasi fenotip. Selanjutnya, pasangan SNP dapat disusun ke dalam kumpulan, yang masing-masing mempunyai batas atas yang sama. Ini memungkinkan penggunaan semula pengiraan antara maksimum, anggaran batas atas yang cekap, dan pemangkasan pasangan SNP yang berkesan. Oleh itu, FastANOVA hanya perlu melakukan ujian ANOVA pada sebilangan kecil pasangan SNP calon tanpa risiko kehilangan yang signifikan. Eksperimen ekstensif menunjukkan bahawa FastANOVA adalah urutan magnitud lebih cepat daripada pelaksanaan ujian ANOVA brute-force pada semua pasangan SNP. [[EENNDD]] ujian anova; kajian persatuan"], [{"string": "Entity discovery and assignment for opinion mining applications Opinion mining became an important topic of study in recent years due to its wide range of applications . There are also many companies offering opinion mining services . One problem that has not been studied so far is the assignment of entities that have been talked about in each sentence . Let us use forum discussions about products as an example to make the problem concrete . In a typical discussion post , the author may give opinions on multiple products and also compare them . The issue is how to detect what products have been talked about in each sentence . If the sentence contains the product names , they need to be identified . We call this problem entity discovery . If the product names are not explicitly mentioned in the sentence but are implied due to the use of pronouns and language conventions , we need to infer the products . We call this problem entity assignment . These problems are important because without knowing what products each sentence talks about the opinion mined from the sentence is of little use . In this paper , we study these problems and propose two effective methods to solve the problems . Entity discovery is based on pattern discovery and entity assignment is based on mining of comparative sentences . Experimental results using a large number of forum posts demonstrate the effectiveness of the technique . Our system has also been successfully tested in a commercial setting .", "keywords": ["entity discovery", "sentiment analysis"], "combined": "Entity discovery and assignment for opinion mining applications Opinion mining became an important topic of study in recent years due to its wide range of applications . There are also many companies offering opinion mining services . One problem that has not been studied so far is the assignment of entities that have been talked about in each sentence . Let us use forum discussions about products as an example to make the problem concrete . In a typical discussion post , the author may give opinions on multiple products and also compare them . The issue is how to detect what products have been talked about in each sentence . If the sentence contains the product names , they need to be identified . We call this problem entity discovery . If the product names are not explicitly mentioned in the sentence but are implied due to the use of pronouns and language conventions , we need to infer the products . We call this problem entity assignment . These problems are important because without knowing what products each sentence talks about the opinion mined from the sentence is of little use . In this paper , we study these problems and propose two effective methods to solve the problems . Entity discovery is based on pattern discovery and entity assignment is based on mining of comparative sentences . Experimental results using a large number of forum posts demonstrate the effectiveness of the technique . Our system has also been successfully tested in a commercial setting . [[EENNDD]] entity discovery; sentiment analysis"}, "Penemuan entiti dan penugasan untuk aplikasi pertimbangan pendapat Perlombongan pendapat menjadi topik kajian penting dalam beberapa tahun kebelakangan ini kerana pelbagai aplikasinya. Terdapat juga banyak syarikat yang menawarkan perkhidmatan penambangan pendapat. Satu masalah yang belum dipelajari setakat ini adalah penugasan entiti yang telah dibincangkan dalam setiap ayat. Marilah kita menggunakan perbincangan forum mengenai produk sebagai contoh untuk menjadikan masalah itu menjadi konkrit. Dalam catatan perbincangan biasa, penulis boleh memberikan pendapat mengenai pelbagai produk dan juga membandingkannya. Masalahnya ialah bagaimana mengesan produk apa yang telah dibincangkan dalam setiap ayat. Sekiranya ayat itu mengandungi nama produk, mereka mesti dikenal pasti. Kami memanggil penemuan entiti masalah ini. Sekiranya nama produk tidak disebut secara jelas dalam ayat tetapi tersirat kerana penggunaan kata ganti nama dan konvensi bahasa, kita perlu menyimpulkan produk. Kami memanggil penugasan entiti masalah ini. Masalah-masalah ini penting kerana tanpa mengetahui produk apa yang dibincangkan oleh setiap ayat mengenai pendapat yang diturunkan dari ayat itu kurang berguna. Dalam makalah ini, kami mengkaji masalah ini dan mencadangkan dua kaedah yang berkesan untuk menyelesaikan masalah tersebut. Penemuan entiti berdasarkan penemuan corak dan penugasan entiti berdasarkan penambangan ayat perbandingan. Hasil eksperimen menggunakan sebilangan besar catatan forum menunjukkan keberkesanan teknik ini. Sistem kami juga telah berjaya diuji dalam suasana komersial. [[EENNDD]] penemuan entiti; analisis sentimen"], [{"string": "The data mining approach to automated software testing In today 's industry , the design of software tests is mostly based on the testers ' expertise , while test automation tools are limited to execution of pre-planned tests only . Evaluation of test outputs is also associated with a considerable effort by human testers who often have imperfect knowledge of the requirements specification . Not surprisingly , this manual approach to software testing results in heavy losses to the world 's economy . The costs of the so-called `` catastrophic '' software failures such as Mars Polar Lander shutdown in 1999 are even hard to measure . In this paper , we demonstrate the potential use of data mining algorithms for automated induction of functional requirements from execution data . The induced data mining models of tested software can be utilized for recovering missing and incomplete specifications , designing a minimal set of regression tests , and evaluating the correctness of software outputs when testing new , potentially flawed releases of the system . To study the feasibility of the proposed approach , we have applied a novel data mining algorithm called Info-Fuzzy Network IFN to execution data of a general-purpose code for solving partial differential equations . After being trained on a relatively small number of randomly generated input-output examples , the model constructed by the IFN algorithm has shown a clear capability to discriminate between correct and faulty versions of the program .", "keywords": ["info-fuzzy networks", "input-output analysis", "automated software testing", "finite element solver", "regression testing", "testing tools"], "combined": "The data mining approach to automated software testing In today 's industry , the design of software tests is mostly based on the testers ' expertise , while test automation tools are limited to execution of pre-planned tests only . Evaluation of test outputs is also associated with a considerable effort by human testers who often have imperfect knowledge of the requirements specification . Not surprisingly , this manual approach to software testing results in heavy losses to the world 's economy . The costs of the so-called `` catastrophic '' software failures such as Mars Polar Lander shutdown in 1999 are even hard to measure . In this paper , we demonstrate the potential use of data mining algorithms for automated induction of functional requirements from execution data . The induced data mining models of tested software can be utilized for recovering missing and incomplete specifications , designing a minimal set of regression tests , and evaluating the correctness of software outputs when testing new , potentially flawed releases of the system . To study the feasibility of the proposed approach , we have applied a novel data mining algorithm called Info-Fuzzy Network IFN to execution data of a general-purpose code for solving partial differential equations . After being trained on a relatively small number of randomly generated input-output examples , the model constructed by the IFN algorithm has shown a clear capability to discriminate between correct and faulty versions of the program . [[EENNDD]] info-fuzzy networks; input-output analysis; automated software testing; finite element solver; regression testing; testing tools"}, "Pendekatan perlombongan data untuk pengujian perisian automatik Dalam industri saat ini, reka bentuk ujian perisian kebanyakannya didasarkan pada kepakaran penguji, sementara alat automasi pengujian hanya terbatas pada pelaksanaan ujian pra-rancangan saja. Penilaian output ujian juga dikaitkan dengan usaha yang cukup besar oleh penguji manusia yang sering mempunyai pengetahuan yang tidak sempurna mengenai spesifikasi keperluan. Tidak menghairankan, pendekatan manual untuk pengujian perisian ini mengakibatkan kerugian besar bagi ekonomi dunia. Kos kegagalan perisian yang disebut \"malapetaka\" seperti penutupan Mars Polar Lander pada tahun 1999 bahkan sukar untuk diukur. Dalam makalah ini, kami menunjukkan potensi penggunaan algoritma perlombongan data untuk induksi automatik keperluan fungsional dari data pelaksanaan. Model perlombongan data yang diinduksi dari perisian yang diuji dapat digunakan untuk memulihkan spesifikasi yang hilang dan tidak lengkap, merancang satu set ujian regresi minimum, dan menilai kebenaran output perisian ketika menguji pelepasan sistem baru yang berpotensi cacat. Untuk mengkaji kemungkinan pendekatan yang dicadangkan, kami telah menerapkan algoritma perlombongan data baru yang disebut Info-Fuzzy Network IFN untuk melaksanakan data kod tujuan umum untuk menyelesaikan persamaan pembezaan separa. Setelah dilatih mengenai sebilangan kecil contoh input-output yang dihasilkan secara rawak, model yang dibina oleh algoritma IFN telah menunjukkan keupayaan yang jelas untuk membezakan antara versi program yang betul dan salah. [[EENNDD]] rangkaian maklumat-kabur; analisis input-output; ujian perisian automatik; pemecah unsur terhingga; ujian regresi; alat ujian"], [{"string": "Using ghost edges for classification in sparsely labeled networks We address the problem of classification in partially labeled networks a.k.a. within-network classification where observed class labels are sparse . Techniques for statistical relational learning have been shown to perform well on network classification tasks by exploiting dependencies between class labels of neighboring nodes . However , relational classifiers can fail when unlabeled nodes have too few labeled neighbors to support learning during training phase and\\/or inference during testing phase . This situation arises in real-world problems when observed labels are sparse . In this paper , we propose a novel approach to within-network classification that combines aspects of statistical relational learning and semi-supervised learning to improve classification performance in sparse networks . Our approach works by adding `` ghost edges '' to a network , which enable the flow of information from labeled to unlabeled nodes . Through experiments on real-world data sets , we demonstrate that our approach performs well across a range of conditions where existing approaches , such as collective classification and semi-supervised learning , fail . On all tasks , our approach improves area under the ROC curve AUC by up to 15 points over existing approaches . Furthermore , we demonstrate that our approach runs in time proportional to L \u2022 E , where L is the number of labeled nodes and E is the number of edges .", "keywords": ["statistical relational learning", "random walk", "collective classification", "semi-supervised learning"], "combined": "Using ghost edges for classification in sparsely labeled networks We address the problem of classification in partially labeled networks a.k.a. within-network classification where observed class labels are sparse . Techniques for statistical relational learning have been shown to perform well on network classification tasks by exploiting dependencies between class labels of neighboring nodes . However , relational classifiers can fail when unlabeled nodes have too few labeled neighbors to support learning during training phase and\\/or inference during testing phase . This situation arises in real-world problems when observed labels are sparse . In this paper , we propose a novel approach to within-network classification that combines aspects of statistical relational learning and semi-supervised learning to improve classification performance in sparse networks . Our approach works by adding `` ghost edges '' to a network , which enable the flow of information from labeled to unlabeled nodes . Through experiments on real-world data sets , we demonstrate that our approach performs well across a range of conditions where existing approaches , such as collective classification and semi-supervised learning , fail . On all tasks , our approach improves area under the ROC curve AUC by up to 15 points over existing approaches . Furthermore , we demonstrate that our approach runs in time proportional to L \u2022 E , where L is the number of labeled nodes and E is the number of edges . [[EENNDD]] statistical relational learning; random walk; collective classification; semi-supervised learning"}, "Menggunakan pinggir hantu untuk klasifikasi dalam rangkaian berlabel jarang Kami menangani masalah klasifikasi dalam rangkaian berlabel separa a.k.a. klasifikasi dalam rangkaian di mana label kelas yang diperhatikan jarang. Teknik untuk pembelajaran relasional statistik telah terbukti menunjukkan prestasi yang baik pada tugas klasifikasi rangkaian dengan mengeksploitasi kebergantungan antara label kelas nod jiran. Walau bagaimanapun, pengkelasan relasional boleh gagal apabila simpul yang tidak berlabel mempunyai jiran berlabel yang terlalu sedikit untuk menyokong pembelajaran semasa fasa latihan dan \\ / atau kesimpulan semasa fasa ujian. Keadaan ini timbul dalam masalah dunia nyata apabila label yang diperhatikan jarang. Dalam makalah ini, kami mengusulkan pendekatan baru untuk klasifikasi jaringan dalam yang menggabungkan aspek pembelajaran relasional statistik dan pembelajaran semi-penyeliaan untuk meningkatkan prestasi klasifikasi dalam jaringan jarang. Pendekatan kami berfungsi dengan menambahkan \"tepi hantu\" ke rangkaian, yang memungkinkan aliran maklumat dari node berlabel ke node berlabel. Melalui eksperimen pada set data dunia nyata, kami menunjukkan bahawa pendekatan kami berkinerja baik dalam pelbagai keadaan di mana pendekatan yang ada, seperti klasifikasi kolektif dan pembelajaran semi-diawasi, gagal. Pada semua tugas, pendekatan kami meningkatkan kawasan di bawah kurva ROC AUC hingga 15 mata berbanding pendekatan yang ada. Selanjutnya, kami menunjukkan bahawa pendekatan kami berjalan dalam masa yang sebanding dengan L \u2022 E, di mana L adalah bilangan nod berlabel dan E adalah bilangan tepi. [[EENNDD]] pembelajaran hubungan statistik; jalan rawak; pengelasan kolektif; pembelajaran separa penyeliaan"], [{"string": "Information-theoretic co-clustering Two-dimensional contingency or co-occurrence tables arise frequently in important applications such as text , web-log and market-basket data analysis . A basic problem in contingency table analysis is co-clustering : simultaneous clustering of the rows and columns . A novel theoretical formulation views the contingency table as an empirical joint probability distribution of two discrete random variables and poses the co-clustering problem as an optimization problem in information theory -- the optimal co-clustering maximizes the mutual information between the clustered random variables subject to constraints on the number of row and column clusters . We present an innovative co-clustering algorithm that monotonically increases the preserved mutual information by intertwining both the row and column clusterings at all stages . Using the practical example of simultaneous word-document clustering , we demonstrate that our algorithm works well in practice , especially in the presence of sparsity and high-dimensionality .", "keywords": ["co-clustering", "mutual information", "information search and retrieval", "information theory", "clustering"], "combined": "Information-theoretic co-clustering Two-dimensional contingency or co-occurrence tables arise frequently in important applications such as text , web-log and market-basket data analysis . A basic problem in contingency table analysis is co-clustering : simultaneous clustering of the rows and columns . A novel theoretical formulation views the contingency table as an empirical joint probability distribution of two discrete random variables and poses the co-clustering problem as an optimization problem in information theory -- the optimal co-clustering maximizes the mutual information between the clustered random variables subject to constraints on the number of row and column clusters . We present an innovative co-clustering algorithm that monotonically increases the preserved mutual information by intertwining both the row and column clusterings at all stages . Using the practical example of simultaneous word-document clustering , we demonstrate that our algorithm works well in practice , especially in the presence of sparsity and high-dimensionality . [[EENNDD]] co-clustering; mutual information; information search and retrieval; information theory; clustering"}, "Penggabungan bersama teori maklumat Jadual kontingensi dua dimensi atau kejadian bersama sering muncul dalam aplikasi penting seperti teks, log web dan analisis data keranjang pasaran. Masalah asas dalam analisis jadual kontingensi adalah pengelompokan bersama: pengelompokan baris dan lajur serentak. Rumusan teori novel melihat jadual kontingensi sebagai taburan kebarangkalian bersama empirikal dari dua pemboleh ubah rawak diskrit dan menimbulkan masalah kluster bersama sebagai masalah pengoptimuman dalam teori maklumat - penggabungan bersama yang optimum memaksimumkan maklumat bersama antara subjek pemboleh ubah rawak berkelompok kepada kekangan pada bilangan kelompok baris dan lajur. Kami menyajikan algoritma kluster bersama inovatif yang secara monoton meningkatkan maklumat bersama yang terpelihara dengan saling menggabungkan kumpulan baris dan lajur di semua peringkat. Dengan menggunakan contoh praktikal pengelompokan dokumen-dokumen serentak, kami menunjukkan bahawa algoritma kami berfungsi dengan baik dalam praktik, terutamanya dengan adanya kelangkaan dan dimensi tinggi. [[EENNDD]] penggabungan bersama; maklumat bersama; carian dan pengambilan maklumat; teori maklumat; pengelompokan"], [{"string": "Web object indexing using domain knowledge A web object is defined to represent any meaningful object embedded in web pages e.g. images , music or pointed to by hyperlinks e.g. downloadable files . In many cases , users would like to search for information of a certain ` object ' , rather than a web page containing the query terms . To facilitate web object searching and organizing , in this paper , we propose a novel approach to web object indexing , by discovering its inherent structure information with existed domain knowledge . In our approach , first , Layered LSI spaces are built for a better representation of the hierarchically structured domain knowledge , in order to emphasize the specific semantics and term space in each layer of the domain knowledge . Meanwhile , the web object representation is constructed by hyperlink analysis , and further pruned to remove the noises . Then an optimal matching between the web object and the domain knowledge is performed , in order to pick out the structure attributes of the web object from the knowledge . Finally , the obtained structure attributes are used to re-organize and index the web objects . Our approach also indicates a new promising way to use trust-worthy Deep Web knowledge to help organize dispersive information of Surface Web .", "keywords": ["indexing", "domain knowledge", "information retrieval", "confidence propagation", "music indexing", "web object", "link analysis", "latent semantic indexing"], "combined": "Web object indexing using domain knowledge A web object is defined to represent any meaningful object embedded in web pages e.g. images , music or pointed to by hyperlinks e.g. downloadable files . In many cases , users would like to search for information of a certain ` object ' , rather than a web page containing the query terms . To facilitate web object searching and organizing , in this paper , we propose a novel approach to web object indexing , by discovering its inherent structure information with existed domain knowledge . In our approach , first , Layered LSI spaces are built for a better representation of the hierarchically structured domain knowledge , in order to emphasize the specific semantics and term space in each layer of the domain knowledge . Meanwhile , the web object representation is constructed by hyperlink analysis , and further pruned to remove the noises . Then an optimal matching between the web object and the domain knowledge is performed , in order to pick out the structure attributes of the web object from the knowledge . Finally , the obtained structure attributes are used to re-organize and index the web objects . Our approach also indicates a new promising way to use trust-worthy Deep Web knowledge to help organize dispersive information of Surface Web . [[EENNDD]] indexing; domain knowledge; information retrieval; confidence propagation; music indexing; web object; link analysis; latent semantic indexing"}, "Pengindeksan objek web menggunakan pengetahuan domain Objek web ditakrifkan untuk mewakili apa-apa objek bermakna yang tertanam di laman web mis. gambar, muzik atau ditunjukkan oleh pautan hiperp. fail yang boleh dimuat turun. Dalam banyak kes, pengguna ingin mencari maklumat tentang `` objek '' tertentu, dan bukannya halaman web yang mengandungi istilah pertanyaan. Untuk memudahkan pencarian dan pengorganisasian objek web, dalam makalah ini, kami mengusulkan pendekatan baru untuk pengindeksan objek web, dengan menemukan maklumat struktur yang melekat dengan pengetahuan domain yang ada. Dalam pendekatan kami, pertama, ruang LSI Berlapis dibangun untuk representasi yang lebih baik dari pengetahuan domain yang tersusun secara hierarki, untuk menekankan semantik dan ruang istilah tertentu di setiap lapisan pengetahuan domain. Sementara itu, representasi objek web dibuat dengan analisis hyperlink, dan dipangkas lebih jauh untuk menghilangkan suara. Kemudian pencocokan optimum antara objek web dan pengetahuan domain dilakukan, untuk memilih atribut struktur objek web dari pengetahuan tersebut. Akhirnya, atribut struktur yang diperoleh digunakan untuk menyusun semula dan mengindeks objek web. Pendekatan kami juga menunjukkan cara baru yang menjanjikan untuk menggunakan pengetahuan Deep Web yang layak dipercayai untuk membantu mengatur maklumat penyebaran Surface Web. [[EENNDD]] pengindeksan; pengetahuan domain; pengambilan maklumat; penyebaran keyakinan; pengindeksan muzik; objek web; analisis pautan; pengindeksan semantik pendam"], [{"string": "Spatial-temporal causal modeling for climate change attribution Attribution of climate change to causal factors has been based predominantly on simulations using physical climate models , which have inherent limitations in describing such a complex and chaotic system . We propose an alternative , data centric , approach that relies on actual measurements of climate observations and human and natural forcing factors . Specifically , we develop a novel method to infer causality from spatial-temporal data , as well as a procedure to incorporate extreme value modeling into our method in order to address the attribution of extreme climate events , such as heatwaves . Our experimental results on a real world dataset indicate that changes in temperature are not solely accounted for by solar radiance , but attributed more significantly to CO2 and other greenhouse gases . Combined with extreme value modeling , we also show that there has been a significant increase in the intensity of extreme temperatures , and that such changes in extreme temperature are also attributable to greenhouse gases . These preliminary results suggest that our approach can offer a useful alternative to the simulation-based approach to climate modeling and attribution , and provide valuable insights from a fresh perspective .", "keywords": ["graphical granger modeling", "spatio-temporal causal modeling", "climate change attribution", "extreme value modeling"], "combined": "Spatial-temporal causal modeling for climate change attribution Attribution of climate change to causal factors has been based predominantly on simulations using physical climate models , which have inherent limitations in describing such a complex and chaotic system . We propose an alternative , data centric , approach that relies on actual measurements of climate observations and human and natural forcing factors . Specifically , we develop a novel method to infer causality from spatial-temporal data , as well as a procedure to incorporate extreme value modeling into our method in order to address the attribution of extreme climate events , such as heatwaves . Our experimental results on a real world dataset indicate that changes in temperature are not solely accounted for by solar radiance , but attributed more significantly to CO2 and other greenhouse gases . Combined with extreme value modeling , we also show that there has been a significant increase in the intensity of extreme temperatures , and that such changes in extreme temperature are also attributable to greenhouse gases . These preliminary results suggest that our approach can offer a useful alternative to the simulation-based approach to climate modeling and attribution , and provide valuable insights from a fresh perspective . [[EENNDD]] graphical granger modeling; spatio-temporal causal modeling; climate change attribution; extreme value modeling"}, "Pemodelan kausal spasial-temporal untuk atribusi perubahan iklim Atribusi perubahan iklim terhadap faktor-faktor kausal telah didasarkan terutama pada simulasi menggunakan model iklim fizikal, yang mempunyai batasan yang melekat dalam menggambarkan sistem yang begitu kompleks dan kacau. Kami mencadangkan pendekatan alternatif, yang berpusat pada data yang bergantung pada pengukuran sebenar pemerhatian iklim dan faktor penekanan manusia dan semula jadi. Secara khusus, kami mengembangkan metode baru untuk menyimpulkan kausalitas dari data spasial-temporal, serta prosedur untuk memasukkan pemodelan nilai ekstrem ke dalam metode kami untuk mengatasi atribusi peristiwa iklim yang ekstrim, seperti gelombang panas. Hasil eksperimen kami pada kumpulan data dunia nyata menunjukkan bahawa perubahan suhu tidak semata-mata diperhitungkan oleh cahaya matahari, tetapi dikaitkan dengan lebih ketara pada CO2 dan gas rumah hijau yang lain. Dikombinasikan dengan pemodelan nilai ekstrem, kami juga menunjukkan bahawa telah terjadi peningkatan yang signifikan dalam intensitas suhu ekstrem, dan bahawa perubahan suhu ekstrim tersebut juga disebabkan oleh gas rumah kaca. Hasil awal ini menunjukkan bahawa pendekatan kami dapat menawarkan alternatif yang berguna untuk pendekatan berasaskan simulasi untuk pemodelan iklim dan atribusi, dan memberikan pandangan berharga dari perspektif baru. [[EENNDD]] pemodelan granger grafik; pemodelan kausal spatio-temporal; atribusi perubahan iklim; pemodelan nilai yang melampau"], [{"string": "Pervasive parallelism in data mining : dataflow solution to co-clustering large and sparse Netflix data All Netflix Prize algorithms proposed so far are prohibitively costly for large-scale production systems . In this paper , we describe an efficient dataflow implementation of a collaborative filtering CF solution to the Netflix Prize problem 1 based on weighted coclustering 5 . The dataflow library we use facilitates the development of sophisticated parallel programs designed to fully utilize commodity multicore hardware , while hiding traditional difficulties such as queuing , threading , memory management , and deadlocks . The dataflow CF implementation first compresses the large , sparse training dataset into co-clusters . Then it generates recommendations by combining the average ratings of the co-clusters with the biases of the users and movies . When configured to identify 20x20 co-clusters in the Netflix training dataset , the implementation predicted over 100 million ratings in 16.31 minutes and achieved an RMSE of 0.88846 without any fine-tuning or domain knowledge . This is an effective real-time prediction runtime of 9.7 us per rating which is far superior to previously reported results . Moreover , the implemented co-clustering framework supports a wide variety of other large-scale data mining applications and forms the basis for predictive modeling on large , dyadic datasets 4 , 7 .", "keywords": ["scalability", "co-clustering", "predictive modeling", "dataflow"], "combined": "Pervasive parallelism in data mining : dataflow solution to co-clustering large and sparse Netflix data All Netflix Prize algorithms proposed so far are prohibitively costly for large-scale production systems . In this paper , we describe an efficient dataflow implementation of a collaborative filtering CF solution to the Netflix Prize problem 1 based on weighted coclustering 5 . The dataflow library we use facilitates the development of sophisticated parallel programs designed to fully utilize commodity multicore hardware , while hiding traditional difficulties such as queuing , threading , memory management , and deadlocks . The dataflow CF implementation first compresses the large , sparse training dataset into co-clusters . Then it generates recommendations by combining the average ratings of the co-clusters with the biases of the users and movies . When configured to identify 20x20 co-clusters in the Netflix training dataset , the implementation predicted over 100 million ratings in 16.31 minutes and achieved an RMSE of 0.88846 without any fine-tuning or domain knowledge . This is an effective real-time prediction runtime of 9.7 us per rating which is far superior to previously reported results . Moreover , the implemented co-clustering framework supports a wide variety of other large-scale data mining applications and forms the basis for predictive modeling on large , dyadic datasets 4 , 7 . [[EENNDD]] scalability; co-clustering; predictive modeling; dataflow"}, "Paralelisme meluas dalam perlombongan data: penyelesaian aliran data untuk mengumpulkan data Netflix yang besar dan jarang. Semua algoritma Hadiah Netflix yang dicadangkan setakat ini sangat mahal untuk sistem pengeluaran berskala besar. Dalam makalah ini, kami menerangkan pelaksanaan aliran data yang efisien penyelesaian CF penapisan kolaboratif untuk masalah Hadiah Netflix 1 berdasarkan coclustering 5 berwajaran. Perpustakaan aliran data yang kami gunakan memudahkan pengembangan program selari canggih yang dirancang untuk menggunakan sepenuhnya perkakasan multicore komoditi, sambil menyembunyikan kesukaran tradisional seperti mengantri, mengemas, pengurusan memori, dan kebuntuan. Pelaksanaan CF aliran data terlebih dahulu memampatkan set data latihan yang jarang dan jarang menjadi kluster bersama. Kemudian ia menghasilkan cadangan dengan menggabungkan penilaian rata-rata kluster bersama dengan bias pengguna dan filem. Apabila dikonfigurasi untuk mengenal pasti kluster 20x20 dalam set data latihan Netflix, pelaksanaannya meramalkan lebih dari 100 juta penilaian dalam 16.31 minit dan mencapai RMSE 0.88846 tanpa penyesuaian atau pengetahuan domain. Ini adalah jangka masa ramalan masa nyata yang berkesan sebanyak 9.7 kita setiap penilaian yang jauh lebih tinggi daripada hasil yang dilaporkan sebelumnya. Lebih-lebih lagi, kerangka kerja pengelompokan bersama yang dilaksanakan mendukung berbagai aplikasi perlombongan data berskala besar lainnya dan menjadi asas untuk pemodelan ramalan pada set data besar, dyadic 4, 7. [[EENNDD]] skalabiliti; penggabungan bersama; pemodelan ramalan; aliran data"], [{"string": "Fast collapsed gibbs sampling for latent dirichlet allocation In this paper we introduce a novel collapsed Gibbs sampling method for the widely used latent Dirichlet allocation LDA model . Our new method results in significant speedups on real world text corpora . Conventional Gibbs sampling schemes for LDA require O K operations per sample where K is the number of topics in the model . Our proposed method draws equivalent samples but requires on average significantly less then K operations per sample . On real-word corpora FastLDA can be as much as 8 times faster than the standard collapsed Gibbs sampler for LDA . No approximations are necessary , and we show that our fast sampling scheme produces exactly the same results as the standard but slower sampling scheme . Experiments on four real world data sets demonstrate speedups for a wide range of collection sizes . For the PubMed collection of over 8 million documents with a required computation time of 6 CPU months for LDA , our speedup of 5.7 can save 5 CPU months of computation .", "keywords": ["probabilistic algorithms", "latent dirichlet allocation", "sampling"], "combined": "Fast collapsed gibbs sampling for latent dirichlet allocation In this paper we introduce a novel collapsed Gibbs sampling method for the widely used latent Dirichlet allocation LDA model . Our new method results in significant speedups on real world text corpora . Conventional Gibbs sampling schemes for LDA require O K operations per sample where K is the number of topics in the model . Our proposed method draws equivalent samples but requires on average significantly less then K operations per sample . On real-word corpora FastLDA can be as much as 8 times faster than the standard collapsed Gibbs sampler for LDA . No approximations are necessary , and we show that our fast sampling scheme produces exactly the same results as the standard but slower sampling scheme . Experiments on four real world data sets demonstrate speedups for a wide range of collection sizes . For the PubMed collection of over 8 million documents with a required computation time of 6 CPU months for LDA , our speedup of 5.7 can save 5 CPU months of computation . [[EENNDD]] probabilistic algorithms; latent dirichlet allocation; sampling"}, "Pensampelan gibbs cepat runtuh untuk peruntukan laten laten Dalam makalah ini kami memperkenalkan kaedah persampelan Gibbs runtuh novel untuk model LDA peruntukan laten laten yang banyak digunakan. Kaedah baru kami menghasilkan peningkatan yang ketara pada korporat teks dunia nyata. Skema persampelan Gibbs konvensional untuk LDA memerlukan operasi O K setiap sampel di mana K adalah bilangan topik dalam model. Kaedah yang dicadangkan kami mengambil sampel yang setara tetapi memerlukan secara signifikan lebih sedikit daripada operasi K setiap sampel. Pada syarikat sebenar FastLDA boleh menjadi 8 kali lebih cepat daripada sampler Gibbs yang runtuh standard untuk LDA. Tidak diperlukan penghampiran, dan kami menunjukkan bahawa skema persampelan cepat kami menghasilkan hasil yang sama persis dengan skema persampelan standard tetapi lebih perlahan. Eksperimen pada empat set data dunia nyata menunjukkan peningkatan untuk pelbagai ukuran koleksi. Untuk koleksi PubMed lebih dari 8 juta dokumen dengan masa pengiraan yang diperlukan 6 bulan CPU untuk LDA, peningkatan 5,7 kami dapat menjimatkan 5 bulan CPU pengiraan. [[EENNDD]] algoritma probabilistik; peruntukan dirichlet pendam; persampelan"], [{"string": "Smoothing techniques for adaptive online language models : topic tracking in tweet streams We are interested in the problem of tracking broad topics such as `` baseball '' and `` fashion '' in continuous streams of short texts , exemplified by tweets from the microblogging service Twitter . The task is conceived as a language modeling problem where per-topic models are trained using hashtags in the tweet stream , which serve as proxies for topic labels . Simple perplexity-based classifiers are then applied to filter the tweet stream for topics of interest . Within this framework , we evaluate , both intrinsically and extrinsically , smoothing techniques for integrating `` foreground '' models to capture recency and `` background '' models to combat sparsity , as well as different techniques for retaining history . Experiments show that unigram language models smoothed using a normalized extension of stupid backoff and a simple queue for history retention performs well on the task .", "keywords": ["stream processing", "general", "twitter", "tdt"], "combined": "Smoothing techniques for adaptive online language models : topic tracking in tweet streams We are interested in the problem of tracking broad topics such as `` baseball '' and `` fashion '' in continuous streams of short texts , exemplified by tweets from the microblogging service Twitter . The task is conceived as a language modeling problem where per-topic models are trained using hashtags in the tweet stream , which serve as proxies for topic labels . Simple perplexity-based classifiers are then applied to filter the tweet stream for topics of interest . Within this framework , we evaluate , both intrinsically and extrinsically , smoothing techniques for integrating `` foreground '' models to capture recency and `` background '' models to combat sparsity , as well as different techniques for retaining history . Experiments show that unigram language models smoothed using a normalized extension of stupid backoff and a simple queue for history retention performs well on the task . [[EENNDD]] stream processing; general; twitter; tdt"}, "Teknik melicinkan untuk model bahasa dalam talian yang adaptif: penjejakan topik dalam aliran tweet Kami berminat dalam masalah mengesan topik yang luas seperti \"baseball\" dan \"fesyen\" dalam aliran teks pendek yang berterusan, yang dicontohkan oleh tweet dari perkhidmatan mikroblog Twitter. Tugas ini difahami sebagai masalah pemodelan bahasa di mana model per topik dilatih menggunakan hashtag dalam aliran tweet, yang berfungsi sebagai proksi untuk label topik. Pengelaskan berdasarkan kebingungan sederhana kemudian digunakan untuk menyaring aliran tweet untuk topik yang menarik. Dalam kerangka ini, kami menilai, baik secara intrinsik maupun ekstrinsik, teknik melicinkan untuk mengintegrasikan model \"latar depan\" untuk menangkap model kebelakangan dan \"latar belakang\" untuk memerangi sparsity, serta teknik yang berbeza untuk mempertahankan sejarah. Eksperimen menunjukkan bahawa model bahasa unigram dilancarkan dengan menggunakan lanjutan backoff bodoh yang dinormalkan dan barisan ringkas untuk pengekalan sejarah berfungsi dengan baik pada tugas tersebut. [[EENNDD]] pemprosesan aliran; umum; twitter; tdt"], [{"string": "To buy or not to buy : mining airfare data to minimize ticket purchase price As product prices become increasingly available on the World Wide Web , consumers attempt to understand how corporations vary these prices over time . However , corporations change prices based on proprietary algorithms and hidden variables e.g. , the number of unsold seats on a flight . Is it possible to develop data mining techniques that will enable consumers to predict price changes under these conditions ? This paper reports on a pilot study in the domain of airline ticket prices where we recorded over 12,000 price observations over a 41 day period . When trained on this data , Hamlet -- our multi-strategy data mining algorithm -- generated a predictive model that saved 341 simulated passengers $ 198,074 by advising them when to buy and when to postpone ticket purchases . Remarkably , a clairvoyant algorithm with complete knowledge of future prices could save at most $ 320,572 in our simulation , thus HAMLET 's savings were 61.8 % of optimal . The algorithm 's savings of $ 198,074 represents an average savings of 23.8 % for the 341 passengers for whom savings are possible . Overall , HAMLET saved 4.4 % of the ticket price averaged over the entire set of 4,488 simulated passengers . Our pilot study suggests that mining of price data available over the web has the potential to save consumers substantial sums of money per annum .", "keywords": ["internet", "airline price prediction", "learning", "price mining", "web mining"], "combined": "To buy or not to buy : mining airfare data to minimize ticket purchase price As product prices become increasingly available on the World Wide Web , consumers attempt to understand how corporations vary these prices over time . However , corporations change prices based on proprietary algorithms and hidden variables e.g. , the number of unsold seats on a flight . Is it possible to develop data mining techniques that will enable consumers to predict price changes under these conditions ? This paper reports on a pilot study in the domain of airline ticket prices where we recorded over 12,000 price observations over a 41 day period . When trained on this data , Hamlet -- our multi-strategy data mining algorithm -- generated a predictive model that saved 341 simulated passengers $ 198,074 by advising them when to buy and when to postpone ticket purchases . Remarkably , a clairvoyant algorithm with complete knowledge of future prices could save at most $ 320,572 in our simulation , thus HAMLET 's savings were 61.8 % of optimal . The algorithm 's savings of $ 198,074 represents an average savings of 23.8 % for the 341 passengers for whom savings are possible . Overall , HAMLET saved 4.4 % of the ticket price averaged over the entire set of 4,488 simulated passengers . Our pilot study suggests that mining of price data available over the web has the potential to save consumers substantial sums of money per annum . [[EENNDD]] internet; airline price prediction; learning; price mining; web mining"}, "Untuk membeli atau tidak untuk membeli: melombong data tambang untuk meminimumkan harga pembelian tiket Oleh kerana harga produk semakin tersedia di World Wide Web, pengguna cuba memahami bagaimana syarikat mengubah harga ini dari masa ke masa. Walau bagaimanapun, syarikat mengubah harga berdasarkan algoritma proprietari dan pemboleh ubah tersembunyi mis. , bilangan tempat duduk yang tidak terjual dalam penerbangan. Adakah mungkin untuk mengembangkan teknik perlombongan data yang akan membolehkan pengguna meramalkan perubahan harga dalam keadaan ini? Makalah ini melaporkan kajian rintis dalam domain harga tiket penerbangan di mana kami mencatatkan lebih dari 12,000 pemerhatian harga dalam jangka masa 41 hari. Semasa dilatih mengenai data ini, Hamlet - algoritma perlombongan data pelbagai strategi kami - menghasilkan model ramalan yang menyelamatkan 341 penumpang simulasi $ 198,074 dengan menasihati mereka kapan hendak membeli dan kapan menangguhkan pembelian tiket. Hebatnya, algoritma yang berwawasan dengan pengetahuan lengkap mengenai harga masa depan dapat menjimatkan paling banyak $ 320,572 dalam simulasi kami, dengan itu penjimatan HAMLET adalah 61.8% optimum. Penjimatan algoritma sebanyak $ 198,074 mewakili penjimatan purata 23.8% untuk 341 penumpang yang mungkin dapat dijimatkan. Secara keseluruhan, HAMLET menjimatkan 4.4% dari harga tiket rata-rata berbanding keseluruhan 4,488 penumpang simulasi. Kajian rintis kami menunjukkan bahawa perlombongan data harga yang ada di web berpotensi menjimatkan pengguna sejumlah besar wang setahun. [[EENNDD]] internet; ramalan harga syarikat penerbangan; belajar; perlombongan harga; perlombongan web"], [{"string": "Estimating rates of rare events at multiple resolutions We consider the problem of estimating occurrence rates of rare eventsfor extremely sparse data , using pre-existing hierarchies to perform inference at multiple resolutions . In particular , we focus on the problem of estimating click rates for webpage , advertisement pairs called impressions where both the pages and the ads are classified into hierarchies that capture broad contextual information at different levels of granularity . Typically the click rates are low and the coverage of the hierarchies is sparse . To overcome these difficulties we devise a sampling method whereby we analyze aspecially chosen sample of pages in the training set , and then estimate click rates using a two-stage model . The first stage imputes the number of webpage , ad pairs at all resolutions of the hierarchy to adjust for the sampling bias . The second stage estimates clickrates at all resolutions after incorporating correlations among sibling nodes through a tree-structured Markov model . Both models are scalable and suited to large scale data mining applications . On a real-world dataset consisting of 1\\/2 billion impressions , we demonstrate that even with 95 % negative non-clicked events in the training set , our method can effectively discriminate extremely rare events in terms of their click propensity .", "keywords": ["internet advertising", "imputation", "tree-structured markov model", "hypertext/hypermedia", "on-line information services", "contextual matching", "miscellaneous", "hierarchy", "clickthrough rate", "maximum entropy"], "combined": "Estimating rates of rare events at multiple resolutions We consider the problem of estimating occurrence rates of rare eventsfor extremely sparse data , using pre-existing hierarchies to perform inference at multiple resolutions . In particular , we focus on the problem of estimating click rates for webpage , advertisement pairs called impressions where both the pages and the ads are classified into hierarchies that capture broad contextual information at different levels of granularity . Typically the click rates are low and the coverage of the hierarchies is sparse . To overcome these difficulties we devise a sampling method whereby we analyze aspecially chosen sample of pages in the training set , and then estimate click rates using a two-stage model . The first stage imputes the number of webpage , ad pairs at all resolutions of the hierarchy to adjust for the sampling bias . The second stage estimates clickrates at all resolutions after incorporating correlations among sibling nodes through a tree-structured Markov model . Both models are scalable and suited to large scale data mining applications . On a real-world dataset consisting of 1\\/2 billion impressions , we demonstrate that even with 95 % negative non-clicked events in the training set , our method can effectively discriminate extremely rare events in terms of their click propensity . [[EENNDD]] internet advertising; imputation; tree-structured markov model; hypertext/hypermedia; on-line information services; contextual matching; miscellaneous; hierarchy; clickthrough rate; maximum entropy"}, "Mengira kadar kejadian jarang pada pelbagai resolusi Kami menganggap masalah menganggarkan kadar kejadian kejadian jarang untuk data yang sangat jarang, menggunakan hierarki yang sudah ada untuk melakukan inferensi pada beberapa resolusi. Khususnya, kami memusatkan perhatian pada masalah menganggarkan kadar klik untuk halaman web, pasangan iklan yang disebut tayangan di mana kedua-dua halaman dan iklan diklasifikasikan ke dalam hierarki yang menangkap maklumat kontekstual yang luas pada tahap perincian yang berbeza. Biasanya kadar klik rendah dan liputan hierarki jarang. Untuk mengatasi kesulitan ini, kami merancang kaedah pengambilan sampel di mana kami menganalisis contoh halaman yang dipilih secara khusus dalam set latihan, dan kemudian menganggarkan kadar klik menggunakan model dua tahap. Tahap pertama menyiratkan bilangan halaman web, pasangan iklan pada semua resolusi hierarki untuk menyesuaikan dengan bias pengambilan sampel. Tahap kedua menganggarkan kadar klik pada semua resolusi setelah memasukkan korelasi antara simpul saudara melalui model Markov berstruktur pokok. Kedua-dua model ini berskala dan sesuai untuk aplikasi perlombongan data berskala besar. Pada kumpulan data dunia nyata yang terdiri daripada 1 \\ / 2 bilion tayangan, kami menunjukkan bahawa walaupun dengan peristiwa negatif 95% yang tidak diklik dalam set latihan, kaedah kami dapat membezakan peristiwa yang sangat jarang berlaku dari segi kecenderungan klik mereka. [[EENNDD]] pengiklanan internet; imputasi; model markov berstruktur pokok; hiperteks / hipermedia; perkhidmatan maklumat dalam talian; pemadanan kontekstual; pelbagai; hierarki; kadar klik lalu; entropi maksimum"], [{"string": "Joint optimization of wrapper generation and template detection Many websites have large collections of pages generated dynamically from an underlying structured source like a database . The data of a category are typically encoded into similar pages by a common script or template . In recent years , some value-added services , such as comparison shopping and vertical search in a specific domain , have motivated the research of extraction technologies with high accuracy . Almost all previous works assume that input pages of a wrapper induction system conform to a common template and they can be easily identified in terms of a common schema of URL . However , we observed that it is hard to distinguish different templates using dynamic URLs today . Moreover , since extraction accuracy heavily depends on how consistent input pages are , we argue that it is risky to determine whether pages share a common template solely based on URLs . Instead , we propose a new approach that utilizes similarity between pages to detect templates . Our approach separates pages with notable inner differences and then generates wrappers , respectively . Experimental results show that our proposed approach is feasible and effective for improving extraction accuracy .", "keywords": ["information extraction", "template detection", "wrapper", "miscellaneous"], "combined": "Joint optimization of wrapper generation and template detection Many websites have large collections of pages generated dynamically from an underlying structured source like a database . The data of a category are typically encoded into similar pages by a common script or template . In recent years , some value-added services , such as comparison shopping and vertical search in a specific domain , have motivated the research of extraction technologies with high accuracy . Almost all previous works assume that input pages of a wrapper induction system conform to a common template and they can be easily identified in terms of a common schema of URL . However , we observed that it is hard to distinguish different templates using dynamic URLs today . Moreover , since extraction accuracy heavily depends on how consistent input pages are , we argue that it is risky to determine whether pages share a common template solely based on URLs . Instead , we propose a new approach that utilizes similarity between pages to detect templates . Our approach separates pages with notable inner differences and then generates wrappers , respectively . Experimental results show that our proposed approach is feasible and effective for improving extraction accuracy . [[EENNDD]] information extraction; template detection; wrapper; miscellaneous"}, "Pengoptimuman bersama penghasilan pembungkus dan templat templat Banyak laman web mempunyai koleksi halaman besar yang dihasilkan secara dinamik dari sumber terstruktur yang mendasari seperti pangkalan data. Data kategori biasanya dikodkan ke halaman yang serupa dengan skrip atau templat yang sama. Dalam beberapa tahun kebelakangan ini, beberapa perkhidmatan bernilai tambah, seperti perbandingan perbandingan dan pencarian menegak dalam domain tertentu, telah mendorong penyelidikan teknologi pengekstrakan dengan ketepatan tinggi. Hampir semua karya sebelumnya mengandaikan bahawa halaman input sistem induksi pembungkus sesuai dengan templat biasa dan mereka dapat dikenali dengan mudah dari segi skema umum URL. Walau bagaimanapun, kami melihat bahawa sukar untuk membezakan templat yang berbeza menggunakan URL dinamik hari ini. Lebih-lebih lagi, kerana ketepatan pengekstrakan sangat bergantung pada seberapa banyak halaman input yang konsisten, kami berpendapat bahawa berisiko untuk menentukan sama ada halaman berkongsi templat umum hanya berdasarkan URL. Sebaliknya, kami mencadangkan pendekatan baru yang menggunakan kesamaan antara halaman untuk mengesan templat. Pendekatan kami memisahkan halaman dengan perbezaan dalaman yang ketara dan kemudian menghasilkan pembungkus, masing-masing. Hasil eksperimen menunjukkan bahawa pendekatan yang dicadangkan kami dapat dilaksanakan dan berkesan untuk meningkatkan ketepatan pengekstrakan. [[EENNDD]] pengekstrakan maklumat; pengesanan templat; pembungkus; pelbagai"], [{"string": "On interactive visualization of high-dimensional data using the hyperbolic plane We propose a novel projection based visualization method for high-dimensional datasets by combining concepts from MDS and the geometry of the hyperbolic spaces . Our approach Hyperbolic Multi-Dimensional Scaling H-MDS extends earlier work 7 using hyperbolic spaces for visualization of tree structures data `` hyperbolic tree browser '' . By borrowing concepts from multi-dimensional scaling we map proximity data directly into the 2-dimensional hyperbolic space H2 . This removes the restriction to `` quasihierarchical '' , graph-based data -- limiting previous work . Since a suitable distance function can convert all kinds of data to proximity or distance-based data this type of data can be considered the most general . We used the circular Poincar\u00e9 model of the H2 which allows effective human-computer interaction : by moving the `` focus '' via mouse the user can navigate in the data without loosing the `` context '' . In H2 the `` fish-eye '' behavior originates not simply by a non-linear view transformation but rather by extraordinary , non-Euclidean properties of the H2 . Especially , the exponential growth of length and area of the underlying space makes the H2 a prime target for mapping hierarchical and now also high-dimensional data . We present several high-dimensional mapping examples including synthetic and real world data and a successful application for unstructured text . By analyzing and integrating multiple film critiques from news : rec . art . movies . reviews and the internet movie database , each movie becomes placed within the H2 . Here the idea is , that related films share more words in their reviews than unrelated . Their semantic proximity leads to a closer arrangement . The result is a kind of high-level content structured display allowing the user to explore the `` space of movies '' .", "keywords": ["text mining", "focus+context", "hyperbolic multi-dimensional scaling", "visualizing high-dimensional data", "semantic browsing", "infoviz", "h-mds", "interaction styles"], "combined": "On interactive visualization of high-dimensional data using the hyperbolic plane We propose a novel projection based visualization method for high-dimensional datasets by combining concepts from MDS and the geometry of the hyperbolic spaces . Our approach Hyperbolic Multi-Dimensional Scaling H-MDS extends earlier work 7 using hyperbolic spaces for visualization of tree structures data `` hyperbolic tree browser '' . By borrowing concepts from multi-dimensional scaling we map proximity data directly into the 2-dimensional hyperbolic space H2 . This removes the restriction to `` quasihierarchical '' , graph-based data -- limiting previous work . Since a suitable distance function can convert all kinds of data to proximity or distance-based data this type of data can be considered the most general . We used the circular Poincar\u00e9 model of the H2 which allows effective human-computer interaction : by moving the `` focus '' via mouse the user can navigate in the data without loosing the `` context '' . In H2 the `` fish-eye '' behavior originates not simply by a non-linear view transformation but rather by extraordinary , non-Euclidean properties of the H2 . Especially , the exponential growth of length and area of the underlying space makes the H2 a prime target for mapping hierarchical and now also high-dimensional data . We present several high-dimensional mapping examples including synthetic and real world data and a successful application for unstructured text . By analyzing and integrating multiple film critiques from news : rec . art . movies . reviews and the internet movie database , each movie becomes placed within the H2 . Here the idea is , that related films share more words in their reviews than unrelated . Their semantic proximity leads to a closer arrangement . The result is a kind of high-level content structured display allowing the user to explore the `` space of movies '' . [[EENNDD]] text mining; focus+context; hyperbolic multi-dimensional scaling; visualizing high-dimensional data; semantic browsing; infoviz; h-mds; interaction styles"}, "Mengenai interaktif visualisasi data dimensi tinggi menggunakan satah hiperbolik Kami mencadangkan kaedah visualisasi berasaskan unjuran novel untuk set data dimensi tinggi dengan menggabungkan konsep dari MDS dan geometri ruang hiperbolik. Pendekatan kami Hyperbolic Multi-Dimensional Scaling H-MDS memperluas kerja sebelumnya 7 menggunakan ruang hiperbolik untuk visualisasi data struktur pokok \"penyemak imbas pokok hiperbolik\". Dengan meminjam konsep dari penskalaan pelbagai dimensi kami memetakan data jarak langsung ke ruang hiperbolik 2 dimensi H2. Ini menghilangkan sekatan untuk `` quasihierarchical '', data berdasarkan grafik - membatasi kerja sebelumnya. Oleh kerana fungsi jarak yang sesuai dapat mengubah semua jenis data menjadi jarak atau data berdasarkan jarak, jenis data ini dapat dianggap paling umum. Kami menggunakan model Poincar\u00e9 edaran H2 yang membolehkan interaksi manusia-komputer yang berkesan: dengan menggerakkan \"fokus\" melalui tetikus, pengguna dapat menavigasi data tanpa kehilangan \"konteks\". Dalam H2, tingkah laku \"fish-eye\" bukan hanya berasal dari transformasi pandangan tidak linear tetapi oleh sifat H2 yang luar biasa dan bukan Euclidean. Terutama, pertumbuhan panjang dan luas ruang yang mendasari menjadikan H2 sebagai sasaran utama untuk memetakan hierarki dan sekarang juga data dimensi tinggi. Kami mengemukakan beberapa contoh pemetaan dimensi tinggi termasuk data sintetik dan dunia nyata dan aplikasi yang berjaya untuk teks tidak berstruktur. Dengan menganalisis dan mengintegrasikan pelbagai kritikan filem dari berita: rec. seni. wayang . ulasan dan pangkalan data filem internet, setiap filem dimasukkan ke dalam H2. Inilah idenya, filem berkaitan berkongsi lebih banyak perkataan dalam ulasannya daripada yang tidak berkaitan. Kedekatan semantik mereka membawa kepada pengaturan yang lebih dekat. Hasilnya adalah sejenis paparan berstruktur konten tingkat tinggi yang memungkinkan pengguna untuk meneroka \"ruang filem\". [[EENNDD]] perlombongan teks; fokus + konteks; penskalaan pelbagai dimensi hiperbolik; menggambarkan data dimensi tinggi; melayari semantik; infoviz; h-mds; gaya interaksi"], [{"string": "Influence and correlation in social networks In many online social systems , social ties between users play an important role in dictating their behavior . One of the ways this can happen is through social influence , the phenomenon that the actions of a user can induce his\\/her friends to behave in a similar way . In systems where social influence exists , ideas , modes of behavior , or new technologies can diffuse through the network like an epidemic . Therefore , identifying and understanding social influence is of tremendous interest from both analysis and design points of view . This is a difficult task in general , since there are factors such as homophily or unobserved confounding variables that can induce statistical correlation between the actions of friends in a social network . Distinguishing influence from these is essentially the problem of distinguishing correlation from causality , a notoriously hard statistical problem . In this paper we study this problem systematically . We define fairly general models that replicate the aforementioned sources of social correlation . We then propose two simple tests that can identify influence as a source of social correlation when the time series of user actions is available . We give a theoretical justification of one of the tests by proving that with high probability it succeeds in ruling out influence in a rather general model of social correlation . We also simulate our tests on a number of examples designed by randomly generating actions of nodes on a real social network from Flickr according to one of several models . Simulation results confirm that our test performs well on these data . Finally , we apply them to real tagging data on Flickr , exhibiting that while there is significant social correlation in tagging behavior on this system , this correlation can not be attributed to social influence .", "keywords": ["social networks", "tagging", "social influence", "correlation"], "combined": "Influence and correlation in social networks In many online social systems , social ties between users play an important role in dictating their behavior . One of the ways this can happen is through social influence , the phenomenon that the actions of a user can induce his\\/her friends to behave in a similar way . In systems where social influence exists , ideas , modes of behavior , or new technologies can diffuse through the network like an epidemic . Therefore , identifying and understanding social influence is of tremendous interest from both analysis and design points of view . This is a difficult task in general , since there are factors such as homophily or unobserved confounding variables that can induce statistical correlation between the actions of friends in a social network . Distinguishing influence from these is essentially the problem of distinguishing correlation from causality , a notoriously hard statistical problem . In this paper we study this problem systematically . We define fairly general models that replicate the aforementioned sources of social correlation . We then propose two simple tests that can identify influence as a source of social correlation when the time series of user actions is available . We give a theoretical justification of one of the tests by proving that with high probability it succeeds in ruling out influence in a rather general model of social correlation . We also simulate our tests on a number of examples designed by randomly generating actions of nodes on a real social network from Flickr according to one of several models . Simulation results confirm that our test performs well on these data . Finally , we apply them to real tagging data on Flickr , exhibiting that while there is significant social correlation in tagging behavior on this system , this correlation can not be attributed to social influence . [[EENNDD]] social networks; tagging; social influence; correlation"}, "Pengaruh dan korelasi dalam rangkaian sosial Dalam banyak sistem sosial dalam talian, hubungan sosial antara pengguna memainkan peranan penting dalam menentukan tingkah laku mereka. Salah satu cara ini dapat terjadi adalah melalui pengaruh sosial, fenomena tindakan pengguna dapat mendorong rakannya berkelakuan dengan cara yang serupa. Dalam sistem di mana pengaruh sosial ada, idea, cara tingkah laku, atau teknologi baru dapat meresap melalui rangkaian seperti wabak. Oleh itu, mengenal pasti dan memahami pengaruh sosial sangat menarik dari sudut pandangan analisis dan reka bentuk. Ini adalah tugas yang sukar pada umumnya, kerana terdapat faktor-faktor seperti pemboleh ubah homofili atau pembaur yang tidak dapat diperhatikan yang dapat mendorong korelasi statistik antara tindakan rakan dalam rangkaian sosial. Membezakan pengaruh dari ini pada dasarnya adalah masalah membezakan korelasi dari sebab-akibat, masalah statistik yang sangat sukar. Dalam makalah ini kita mengkaji masalah ini secara sistematik. Kami menentukan model yang agak umum yang mereplikasi sumber korelasi sosial yang disebutkan di atas. Kami kemudian mencadangkan dua ujian mudah yang dapat mengenal pasti pengaruh sebagai sumber korelasi sosial apabila rangkaian masa tindakan pengguna tersedia. Kami memberikan pembenaran teoritis salah satu ujian dengan membuktikan bahawa dengan kebarangkalian tinggi ia berjaya menolak pengaruh dalam model korelasi sosial yang agak umum. Kami juga mensimulasikan ujian kami pada sejumlah contoh yang dirancang dengan menghasilkan tindakan nod secara rawak pada rangkaian sosial sebenar dari Flickr mengikut salah satu daripada beberapa model. Hasil simulasi mengesahkan bahawa ujian kami menunjukkan prestasi yang baik pada data ini. Akhirnya, kami menerapkannya pada data penandaan sebenar di Flickr, menunjukkan bahawa walaupun terdapat korelasi sosial yang signifikan dalam tingkah laku pemberian tag pada sistem ini, korelasi ini tidak dapat dikaitkan dengan pengaruh sosial. [[EENNDD]] rangkaian sosial; penandaan; pengaruh sosial; korelasi"], [{"string": "Mining closed relational graphs with connectivity constraints Relational graphs are widely used in modeling large scale networks such as biological networks and social networks . In this kind of graph , connectivity becomes critical in identifying highly associated groups and clusters . In this paper , we investigate the issues of mining closed frequent graphs with connectivity constraints in massive relational graphs where each graph has around 10K nodes and 1M edges . We adopt the concept of edge connectivity and apply the results from graph theory , to speed up the mining process . Two approaches are developed to handle different mining requests : CloseCut , a pattern-growth approach , and splat , a pattern-reduction approach . We have applied these methods in biological datasets and found the discovered patterns interesting .", "keywords": ["connectivity", "graph", "closed pattern"], "combined": "Mining closed relational graphs with connectivity constraints Relational graphs are widely used in modeling large scale networks such as biological networks and social networks . In this kind of graph , connectivity becomes critical in identifying highly associated groups and clusters . In this paper , we investigate the issues of mining closed frequent graphs with connectivity constraints in massive relational graphs where each graph has around 10K nodes and 1M edges . We adopt the concept of edge connectivity and apply the results from graph theory , to speed up the mining process . Two approaches are developed to handle different mining requests : CloseCut , a pattern-growth approach , and splat , a pattern-reduction approach . We have applied these methods in biological datasets and found the discovered patterns interesting . [[EENNDD]] connectivity; graph; closed pattern"}, "Melombong grafik relasi tertutup dengan kekangan penyambungan Graf relasional banyak digunakan dalam memodelkan rangkaian berskala besar seperti rangkaian biologi dan rangkaian sosial. Dalam grafik seperti ini, penyambungan menjadi penting dalam mengenal pasti kumpulan dan kelompok yang sangat berkaitan. Dalam makalah ini, kami menyiasat masalah penambangan grafik yang sering ditutup dengan kekangan penyambungan dalam grafik hubungan besar-besaran di mana setiap graf mempunyai sekitar 10K nod dan tepi 1M. Kami mengadopsi konsep sambungan tepi dan menerapkan hasil dari teori grafik, untuk mempercepat proses perlombongan. Dua pendekatan dikembangkan untuk menangani permintaan perlombongan yang berbeza: CloseCut, pendekatan pertumbuhan pola, dan percikan, pendekatan pengurangan pola. Kami telah menggunakan kaedah ini dalam kumpulan data biologi dan mendapati corak yang ditemui menarik. [[EENNDD]] penyambungan; graf; corak tertutup"], [{"string": "Discovering complex matchings across web query interfaces : a correlation mining approach To enable information integration , schema matching is a critical step for discovering semantic correspondences of attributes across heterogeneous sources . While complex matchings are common , because of their far more complex search space , most existing techniques focus on simple 1:1 matchings . To tackle this challenge , this paper takes a conceptually novel approach by viewing schema matching as correlation mining , for our task of matching Web query interfaces to integrate the myriad databases on the Internet . On this `` deep Web , '' query interfaces generally form complex matchings between attribute groups e.g. , author corresponds to first name , last name in the Books domain . We observe that the co-occurrences patterns across query interfaces often reveal such complex semantic relationships : grouping attributes e.g. , first name , last name tend to be co-present in query interfaces and thus positively correlated . In contrast , synonym attributes are negatively correlated because they rarely co-occur . This insight enables us to discover complex matchings by a correlation mining approach . In particular , we develop the DCM framework , which consists of data preparation , dual mining of positive and negative correlations , and finally matching selection . Unlike previous correlation mining algorithms , which mainly focus on finding strong positive correlations , our algorithm cares both positive and negative correlations , especially the subtlety of negative correlations , due to its special importance in schema matching . This leads to the introduction of a new correlation measure , $ H$ - measure , distinct from those proposed in previous work . We evaluate our approach extensively and the results show good accuracy for discovering complex matchings .", "keywords": ["data integration", "schema matching", "correlation mining", "heterogeneous databases", "correlation measure", "deep web"], "combined": "Discovering complex matchings across web query interfaces : a correlation mining approach To enable information integration , schema matching is a critical step for discovering semantic correspondences of attributes across heterogeneous sources . While complex matchings are common , because of their far more complex search space , most existing techniques focus on simple 1:1 matchings . To tackle this challenge , this paper takes a conceptually novel approach by viewing schema matching as correlation mining , for our task of matching Web query interfaces to integrate the myriad databases on the Internet . On this `` deep Web , '' query interfaces generally form complex matchings between attribute groups e.g. , author corresponds to first name , last name in the Books domain . We observe that the co-occurrences patterns across query interfaces often reveal such complex semantic relationships : grouping attributes e.g. , first name , last name tend to be co-present in query interfaces and thus positively correlated . In contrast , synonym attributes are negatively correlated because they rarely co-occur . This insight enables us to discover complex matchings by a correlation mining approach . In particular , we develop the DCM framework , which consists of data preparation , dual mining of positive and negative correlations , and finally matching selection . Unlike previous correlation mining algorithms , which mainly focus on finding strong positive correlations , our algorithm cares both positive and negative correlations , especially the subtlety of negative correlations , due to its special importance in schema matching . This leads to the introduction of a new correlation measure , $ H$ - measure , distinct from those proposed in previous work . We evaluate our approach extensively and the results show good accuracy for discovering complex matchings . [[EENNDD]] data integration; schema matching; correlation mining; heterogeneous databases; correlation measure; deep web"}, "Mencari pencocokan kompleks di antara muka pertanyaan web: pendekatan perlombongan korelasi Untuk membolehkan integrasi maklumat, pemadanan skema adalah langkah kritikal untuk menemui korespondensi semantik atribut di seluruh sumber yang heterogen. Walaupun pencocokan kompleks adalah perkara biasa, kerana ruang pencariannya yang jauh lebih kompleks, kebanyakan teknik yang ada tertumpu pada pemadanan sederhana 1: 1. Untuk mengatasi cabaran ini, makalah ini menggunakan pendekatan baru secara konseptual dengan melihat pemadanan skema sebagai penambangan korelasi, untuk tugas kita memadankan antara muka pertanyaan Web untuk mengintegrasikan pelbagai pangkalan data di Internet. Pada \"Web dalam\" ini, antara muka pertanyaan biasanya membentuk padanan kompleks antara kumpulan atribut, mis. , penulis sesuai dengan nama depan, nama belakang dalam domain Buku. Kami memerhatikan bahawa corak kejadian bersama di antara muka pertanyaan sering mengungkapkan hubungan semantik yang kompleks: atribut pengelompokan, mis. , nama depan, nama belakang cenderung wujud bersama dalam antara muka pertanyaan dan berkorelasi positif. Sebaliknya, atribut sinonim berkorelasi negatif kerana jarang berlaku bersama. Wawasan ini membolehkan kita menemui padanan yang kompleks dengan pendekatan perlombongan korelasi. Khususnya, kami mengembangkan kerangka DCM, yang terdiri dari penyediaan data, penambangan ganda korelasi positif dan negatif, dan akhirnya pemilihan yang sepadan. Tidak seperti algoritma perlombongan korelasi sebelumnya, yang terutama menumpukan pada mencari korelasi positif yang kuat, algoritma kami menjaga korelasi positif dan negatif, terutama kehalusan korelasi negatif, kerana kepentingan khusus dalam pemadanan skema. Ini membawa kepada pengenalan ukuran korelasi baru, ukuran $ H $, berbeza dengan yang dicadangkan dalam karya sebelumnya. Kami menilai pendekatan kami secara meluas dan hasilnya menunjukkan ketepatan yang baik untuk menemui padanan yang kompleks. [[EENNDD]] penyatuan data; pemadanan skema; perlombongan korelasi; pangkalan data heterogen; ukuran korelasi; web dalam"], [{"string": "A probabilistic model for personalized tag prediction Social tagging systems have become increasingly popular for sharing and organizing web resources . Tag prediction is a common feature of social tagging systems . Social tagging by nature is an incremental process , meaning that once a user has saved a web page with tags , the tagging system can provide more accurate predictions for the user , based on user 's incremental behaviors . However , existing tag prediction methods do not consider this important factor , in which their training and test datasets are either split by a fixed time stamp or randomly sampled from a larger corpus . In our temporal experiments , we perform a time-sensitive sampling on an existing public dataset , resulting in a new scenario which is much closer to `` real-world '' . In this paper , we address the problem of tag prediction by proposing a probabilistic model for personalized tag prediction . The model is a Bayesian approach , and integrates three factors - ego-centric effect , environmental effects and web page content . Two methods - both intuitive calculation and learning optimization - are provided for parameter estimation . Pure graphbased methods which may have significant constraints such as every user , every item and every tag has to occur in at least p posts , can not make a prediction in most of `` real world '' cases while our model improves the F-measure by over 30 % compared to a leading algorithm , in our `` real-world '' use case .", "keywords": ["personalized tag prediction", "social tagging", "tag prediction", "information search and retrieval"], "combined": "A probabilistic model for personalized tag prediction Social tagging systems have become increasingly popular for sharing and organizing web resources . Tag prediction is a common feature of social tagging systems . Social tagging by nature is an incremental process , meaning that once a user has saved a web page with tags , the tagging system can provide more accurate predictions for the user , based on user 's incremental behaviors . However , existing tag prediction methods do not consider this important factor , in which their training and test datasets are either split by a fixed time stamp or randomly sampled from a larger corpus . In our temporal experiments , we perform a time-sensitive sampling on an existing public dataset , resulting in a new scenario which is much closer to `` real-world '' . In this paper , we address the problem of tag prediction by proposing a probabilistic model for personalized tag prediction . The model is a Bayesian approach , and integrates three factors - ego-centric effect , environmental effects and web page content . Two methods - both intuitive calculation and learning optimization - are provided for parameter estimation . Pure graphbased methods which may have significant constraints such as every user , every item and every tag has to occur in at least p posts , can not make a prediction in most of `` real world '' cases while our model improves the F-measure by over 30 % compared to a leading algorithm , in our `` real-world '' use case . [[EENNDD]] personalized tag prediction; social tagging; tag prediction; information search and retrieval"}, "Model probabilistik untuk ramalan tag yang diperibadikan Sistem penandaan sosial menjadi semakin popular untuk berkongsi dan mengatur sumber web. Ramalan tag adalah ciri umum sistem penandaan sosial. Pemberian tag sosial secara semula jadi adalah proses tambahan, yang bermaksud bahawa setelah pengguna menyimpan halaman web dengan tag, sistem penandaan dapat memberikan ramalan yang lebih tepat untuk pengguna, berdasarkan perilaku tambahan pengguna. Walau bagaimanapun, kaedah ramalan teg yang ada tidak menganggap faktor penting ini, di mana set data latihan dan ujian mereka dibahagi dengan cap waktu tetap atau diambil sampel secara rawak dari korpus yang lebih besar. Dalam eksperimen temporal kami, kami melakukan pensampelan sensitif waktu pada set data publik yang ada, menghasilkan senario baru yang jauh lebih dekat dengan \"dunia nyata\". Dalam makalah ini, kami mengatasi masalah ramalan tag dengan mencadangkan model probabilistik untuk ramalan tag yang diperibadikan. Model ini adalah pendekatan Bayesian, dan mengintegrasikan tiga faktor - kesan ego-centric, kesan persekitaran dan kandungan halaman web. Dua kaedah - pengiraan intuitif dan pengoptimuman pembelajaran - disediakan untuk anggaran parameter. Kaedah berdasarkan grafik murni yang mungkin mempunyai kekangan yang ketara seperti setiap pengguna, setiap item dan setiap tag harus berlaku dalam sekurang-kurangnya catatan p, tidak dapat membuat ramalan dalam kebanyakan kes \"dunia nyata\" sementara model kami meningkatkan ukuran F lebih daripada 30% berbanding algoritma terkemuka, dalam kes penggunaan \"dunia nyata\" kami. [[EENNDD]] ramalan teg diperibadikan; penandaan sosial; ramalan tag; pencarian dan pencarian maklumat"], [{"string": "Mining knowledge-sharing sites for viral marketing Viral marketing takes advantage of networks of influence among customers to inexpensively achieve large changes in behavior . Our research seeks to put it on a firmer footing by mining these networks from data , building probabilistic models of them , and using these models to choose the best viral marketing plan . Knowledge-sharing sites , where customers review products and advise each other , are a fertile source for this type of data mining . In this paper we extend our previous techniques , achieving a large reduction in computational cost , and apply them to data from a knowledge-sharing site . We optimize the amount of marketing funds spent on each customer , rather than just making a binary decision on whether to market to him . We take into account the fact that knowledge of the network is partial , and that gathering that knowledge can itself have a cost . Our results show the robustness and utility of our approach .", "keywords": ["probabilistic models", "viral marketing", "direct marketing", "social networks", "linear models", "knowledge sharing"], "combined": "Mining knowledge-sharing sites for viral marketing Viral marketing takes advantage of networks of influence among customers to inexpensively achieve large changes in behavior . Our research seeks to put it on a firmer footing by mining these networks from data , building probabilistic models of them , and using these models to choose the best viral marketing plan . Knowledge-sharing sites , where customers review products and advise each other , are a fertile source for this type of data mining . In this paper we extend our previous techniques , achieving a large reduction in computational cost , and apply them to data from a knowledge-sharing site . We optimize the amount of marketing funds spent on each customer , rather than just making a binary decision on whether to market to him . We take into account the fact that knowledge of the network is partial , and that gathering that knowledge can itself have a cost . Our results show the robustness and utility of our approach . [[EENNDD]] probabilistic models; viral marketing; direct marketing; social networks; linear models; knowledge sharing"}, "Melombong laman perkongsian pengetahuan untuk pemasaran virus Pemasaran virus memanfaatkan rangkaian pengaruh di antara pelanggan untuk mencapai perubahan besar dalam tingkah laku. Penyelidikan kami bertujuan untuk meletakkannya pada tahap yang lebih tegas dengan melombong rangkaian ini dari data, membangun model probabilistik dari mereka, dan menggunakan model-model ini untuk memilih rancangan pemasaran viral terbaik. Laman perkongsian pengetahuan, di mana pelanggan mengkaji produk dan saling menasihati, adalah sumber yang subur untuk jenis perlombongan data ini. Dalam makalah ini kami memperluas teknik sebelumnya, mencapai pengurangan besar dalam kos komputasi, dan menerapkannya ke data dari laman perkongsian pengetahuan. Kami mengoptimumkan jumlah dana pemasaran yang dibelanjakan untuk setiap pelanggan, dan bukan hanya membuat keputusan binari mengenai apakah akan memasarkan kepadanya. Kami mengambil kira hakikat bahawa pengetahuan mengenai rangkaian adalah separa, dan pengumpulan pengetahuan itu sendiri boleh menanggung kos. Hasil kami menunjukkan ketahanan dan kegunaan pendekatan kami. [[EENNDD]] model probabilistik; pemasaran viral; pemasaran langsung; rangkaian sosial; model linear; perkongsian pengetahuan"], [{"string": "Incremental maintenance of quotient cube for median Data cube pre-computation is an important concept for supporting OLAP Online Analytical Processing and has been studied extensively . It is often not feasible to compute a complete data cube due to the huge storage requirement . Recently proposed quotient cube addressed this issue through a partitioning method that groups cube cells into equivalence partitions . Such an approach is not only useful for distributive aggregate functions such as SUM but can also be applied to the holistic aggregate functions like MEDIAN . Maintaining a data cube for holistic aggregation is a hard problem since its difficulty lies in the fact that history tuple values must be kept in order to compute the new aggregate when tuples are inserted or deleted . The quotient cube makes the problem harder since we also need to maintain the equivalence classes . In this paper , we introduce two techniques called addset data structure and sliding window to deal with this problem . We develop efficient algorithms for maintaining a quotient cube with holistic aggregation functions that takes up reasonably small storage space . Performance study shows that our algorithms are effective , efficient and scalable over large databases .", "keywords": ["data cube", "holistic aggregation"], "combined": "Incremental maintenance of quotient cube for median Data cube pre-computation is an important concept for supporting OLAP Online Analytical Processing and has been studied extensively . It is often not feasible to compute a complete data cube due to the huge storage requirement . Recently proposed quotient cube addressed this issue through a partitioning method that groups cube cells into equivalence partitions . Such an approach is not only useful for distributive aggregate functions such as SUM but can also be applied to the holistic aggregate functions like MEDIAN . Maintaining a data cube for holistic aggregation is a hard problem since its difficulty lies in the fact that history tuple values must be kept in order to compute the new aggregate when tuples are inserted or deleted . The quotient cube makes the problem harder since we also need to maintain the equivalence classes . In this paper , we introduce two techniques called addset data structure and sliding window to deal with this problem . We develop efficient algorithms for maintaining a quotient cube with holistic aggregation functions that takes up reasonably small storage space . Performance study shows that our algorithms are effective , efficient and scalable over large databases . [[EENNDD]] data cube; holistic aggregation"}, "Pemeliharaan tambahan bagi kubus bagi pra-pengiraan Data kubus adalah konsep penting untuk menyokong Pemprosesan Analisis Dalam Talian OLAP dan telah dikaji secara meluas. Selalunya tidak dapat dilakukan untuk menghitung kubus data yang lengkap kerana keperluan penyimpanan yang besar. Quient cube yang baru-baru ini dicadangkan menangani masalah ini melalui kaedah pembahagian yang mengelompokkan sel kubus menjadi partisi kesetaraan. Pendekatan sedemikian tidak hanya berguna untuk fungsi agregat distributif seperti SUM tetapi juga dapat diterapkan pada fungsi agregat holistik seperti MEDIAN. Mengekalkan kubus data untuk agregasi holistik adalah masalah yang sukar kerana kesukarannya terletak pada fakta bahawa nilai tuple sejarah harus disimpan untuk menghitung agregat baru ketika tupel dimasukkan atau dihapus. Kuota bagi menjadikan masalah lebih sukar kerana kita juga perlu mengekalkan kelas kesetaraan. Dalam makalah ini, kami memperkenalkan dua teknik yang disebut struktur data addset dan sliding window untuk menangani masalah ini. Kami mengembangkan algoritma yang cekap untuk mengekalkan kubus hasil bagi dengan fungsi pengagregatan holistik yang memerlukan ruang penyimpanan yang agak kecil. Kajian prestasi menunjukkan bahawa algoritma kami berkesan, cekap dan berskala berbanding pangkalan data yang besar. [[EENNDD]] kiub data; agregasi holistik"], [{"string": "Mining correlated bursty topic patterns from coordinated text streams Previous work on text mining has almost exclusively focused on a single stream . However , we often have available multiple text streams indexed by the same set of time points called coordinated text streams , which offer new opportunities for text mining . For example , when a major event happens , all the news articles published by different agencies in different languages tend to cover the same event for a certain period , exhibiting a correlated bursty topic pattern in all the news article streams . In general , mining correlated bursty topic patterns from coordinated text streams can reveal interesting latent associations or events behind these streams . In this paper , we define and study this novel text mining problem . We propose a general probabilistic algorithm which can effectively discover correlated bursty patterns and their bursty periods across text streams even if the streams have completely different vocabularies e.g. , English vs Chinese . Evaluation of the proposed method on a news data set and a literature data set shows that it can effectively discover quite meaningful topic patterns from both data sets : the patterns discovered from the news data set accurately reveal the major common events covered in the two streams of news articles in English and Chinese , respectively , while the patterns discovered from two database publication streams match well with the major research paradigm shifts in database research . Since the proposed method is general and does not require the streams to share vocabulary , it can be applied to any coordinated text streams to discover correlated topic patterns that burst in multiple streams in the same period .", "keywords": ["reinforcement", "correlated bursty patterns", "coordinated streams"], "combined": "Mining correlated bursty topic patterns from coordinated text streams Previous work on text mining has almost exclusively focused on a single stream . However , we often have available multiple text streams indexed by the same set of time points called coordinated text streams , which offer new opportunities for text mining . For example , when a major event happens , all the news articles published by different agencies in different languages tend to cover the same event for a certain period , exhibiting a correlated bursty topic pattern in all the news article streams . In general , mining correlated bursty topic patterns from coordinated text streams can reveal interesting latent associations or events behind these streams . In this paper , we define and study this novel text mining problem . We propose a general probabilistic algorithm which can effectively discover correlated bursty patterns and their bursty periods across text streams even if the streams have completely different vocabularies e.g. , English vs Chinese . Evaluation of the proposed method on a news data set and a literature data set shows that it can effectively discover quite meaningful topic patterns from both data sets : the patterns discovered from the news data set accurately reveal the major common events covered in the two streams of news articles in English and Chinese , respectively , while the patterns discovered from two database publication streams match well with the major research paradigm shifts in database research . Since the proposed method is general and does not require the streams to share vocabulary , it can be applied to any coordinated text streams to discover correlated topic patterns that burst in multiple streams in the same period . [[EENNDD]] reinforcement; correlated bursty patterns; coordinated streams"}, "Perlombongan melengkapkan corak topik burst dari aliran teks yang diselaraskan. Kerja sebelumnya dalam perlombongan teks hampir secara eksklusif tertumpu pada satu aliran. Walau bagaimanapun, kami sering mempunyai banyak aliran teks yang diindeks oleh sekumpulan titik waktu yang sama yang disebut aliran teks yang diselaraskan, yang menawarkan peluang baru untuk penambangan teks. Sebagai contoh, ketika peristiwa besar berlaku, semua artikel berita yang diterbitkan oleh agensi yang berlainan dalam bahasa yang berlainan cenderung merangkumi peristiwa yang sama untuk jangka waktu tertentu, menunjukkan corak topik pecah yang berkorelasi di semua aliran artikel berita. Secara amnya, corak topik pecah yang berkaitan dengan perlombongan dari aliran teks yang diselaraskan dapat mendedahkan perkaitan atau peristiwa terpendam yang menarik di sebalik aliran ini. Dalam makalah ini, kami mendefinisikan dan mengkaji masalah perlombongan teks novel ini. Kami mencadangkan algoritma probabilistik umum yang dapat dengan berkesan menemui corak pecah berkorelasi dan tempoh pecahnya di seluruh aliran teks walaupun aliran mempunyai kosa kata yang sama sekali berbeza, mis. , Bahasa Inggeris vs Bahasa Cina. Penilaian kaedah yang dicadangkan pada kumpulan data berita dan satu set data literatur menunjukkan bahawa ia dapat dengan berkesan menemui corak topik yang cukup bermakna dari kedua set data: corak yang ditemui dari kumpulan data berita secara tepat menunjukkan peristiwa umum utama yang diliputi dalam dua aliran artikel berita dalam bahasa Inggeris dan Cina, masing-masing, sementara pola yang dijumpai dari dua aliran penerbitan pangkalan data sesuai dengan perubahan paradigma penyelidikan utama dalam penyelidikan pangkalan data. Oleh kerana kaedah yang dicadangkan adalah umum dan tidak memerlukan aliran untuk berkongsi perbendaharaan kata, kaedah ini dapat diterapkan pada aliran teks yang terkoordinasi untuk mengetahui corak topik yang berkorelasi yang meletus dalam beberapa aliran dalam tempoh yang sama. [[EENNDD]] peneguhan; corak pecah berkorelasi; aliran yang diselaraskan"], [{"string": "A mixture model for contextual text mining Contextual text mining is concerned with extracting topical themes from a text collection with context information e.g. , time and location and comparing\\/analyzing the variations of themes over different contexts . Since the topics covered in a document are usually related to the context of the document , analyzing topical themes within context can potentially reveal many interesting theme patterns . In this paper , we generalize some of these models proposed in the previous work and we propose a new general probabilistic model for contextual text mining that can cover several existing models as special cases . Specifically , we extend the probabilistic latent semantic analysis PLSA model by introducing context variables to model the context of a document . The proposed mixture model , called contextual probabilistic latent semantic analysis CPLSA model , can be applied to many interesting mining tasks , such as temporal text mining , spatiotemporal text mining , author-topic analysis , and cross-collection comparative analysis . Empirical experiments show that the proposed mixture model can discover themes and their contextual variations effectively .", "keywords": ["em algorithm", "context", "theme pattern", "information search and retrieval", "mixture model", "contextual text mining", "clustering"], "combined": "A mixture model for contextual text mining Contextual text mining is concerned with extracting topical themes from a text collection with context information e.g. , time and location and comparing\\/analyzing the variations of themes over different contexts . Since the topics covered in a document are usually related to the context of the document , analyzing topical themes within context can potentially reveal many interesting theme patterns . In this paper , we generalize some of these models proposed in the previous work and we propose a new general probabilistic model for contextual text mining that can cover several existing models as special cases . Specifically , we extend the probabilistic latent semantic analysis PLSA model by introducing context variables to model the context of a document . The proposed mixture model , called contextual probabilistic latent semantic analysis CPLSA model , can be applied to many interesting mining tasks , such as temporal text mining , spatiotemporal text mining , author-topic analysis , and cross-collection comparative analysis . Empirical experiments show that the proposed mixture model can discover themes and their contextual variations effectively . [[EENNDD]] em algorithm; context; theme pattern; information search and retrieval; mixture model; contextual text mining; clustering"}, "Model campuran untuk perlombongan teks kontekstual Perlombongan teks kontekstual berkaitan dengan mengekstrak tema topikal dari koleksi teks dengan maklumat konteks mis. , masa dan lokasi dan membandingkan \\ / menganalisis variasi tema dalam konteks yang berbeza. Oleh kerana topik yang diliputi dalam dokumen biasanya berkaitan dengan konteks dokumen, menganalisis tema topikal dalam konteks berpotensi mengungkapkan banyak pola tema yang menarik. Dalam makalah ini, kami menggeneralisasikan beberapa model ini yang dicadangkan dalam karya sebelumnya dan kami mencadangkan model probabilistik umum baru untuk perlombongan teks kontekstual yang dapat merangkumi beberapa model yang ada sebagai kes khas. Secara khusus, kami memperluaskan model PLSA analisis semantik probabilistik dengan memperkenalkan pemboleh ubah konteks untuk memodelkan konteks dokumen. Model campuran yang dicadangkan, yang disebut model semantik semantik probabilistik kontekstual, model CPLSA, dapat diterapkan pada banyak tugas perlombongan yang menarik, seperti perlombongan teks temporal, perlombongan teks spatiotemporal, analisis topik pengarang, dan analisis perbandingan koleksi silang. Eksperimen empirikal menunjukkan bahawa model campuran yang dicadangkan dapat menemui tema dan variasi kontekstualnya dengan berkesan. [[EENNDD]] algoritma em; konteks; corak tema; carian dan pengambilan maklumat; model campuran; perlombongan teks kontekstual; pengelompokan"], [{"string": "Systematic data selection to mine concept-drifting data streams One major problem of existing methods to mine data streams is that it makes ad hoc choices to combine most recent data with some amount of old data to search the new hypothesis . The assumption is that the additional old data always helps produce a more accurate hypothesis than using the most recent data only . We first criticize this notion and point out that using old data blindly is not better than `` gambling '' ; in other words , it helps increase the accuracy only if we are `` lucky . '' We discuss and analyze the situations where old data will help and what kind of old data will help . The practical problem on choosing the right example from old data is due to the formidable cost to compare different possibilities and models . This problem will go away if we have an algorithm that is extremely efficient to compare all sensible choices with little extra cost . Based on this observation , we propose a simple , efficient and accurate cross-validation decision tree ensemble method .", "keywords": ["data streams", "decision trees", "concept-drift"], "combined": "Systematic data selection to mine concept-drifting data streams One major problem of existing methods to mine data streams is that it makes ad hoc choices to combine most recent data with some amount of old data to search the new hypothesis . The assumption is that the additional old data always helps produce a more accurate hypothesis than using the most recent data only . We first criticize this notion and point out that using old data blindly is not better than `` gambling '' ; in other words , it helps increase the accuracy only if we are `` lucky . '' We discuss and analyze the situations where old data will help and what kind of old data will help . The practical problem on choosing the right example from old data is due to the formidable cost to compare different possibilities and models . This problem will go away if we have an algorithm that is extremely efficient to compare all sensible choices with little extra cost . Based on this observation , we propose a simple , efficient and accurate cross-validation decision tree ensemble method . [[EENNDD]] data streams; decision trees; concept-drift"}, "Pemilihan data yang sistematik untuk menambang aliran data yang mengalir konsep Salah satu masalah utama kaedah yang ada untuk menambang aliran data adalah bahawa ia membuat pilihan ad hoc untuk menggabungkan data terbaru dengan sejumlah data lama untuk mencari hipotesis baru. Anggapannya adalah bahawa data lama tambahan selalu membantu menghasilkan hipotesis yang lebih tepat daripada menggunakan data terbaru sahaja. Kami pertama kali mengkritik gagasan ini dan menunjukkan bahawa menggunakan data lama secara membuta tuli tidak lebih baik daripada \"perjudian\"; dengan kata lain, ia membantu meningkatkan ketepatan hanya jika kita \"bernasib baik. Kami membincangkan dan menganalisis situasi di mana data lama akan membantu dan jenis data lama yang akan membantu. Masalah praktikal dalam memilih contoh yang tepat dari data lama adalah kerana kos yang sukar untuk membandingkan kemungkinan dan model yang berbeza. Masalah ini akan hilang jika kita mempunyai algoritma yang sangat efisien untuk membandingkan semua pilihan yang masuk akal dengan sedikit kos tambahan. Berdasarkan pemerhatian ini, kami mencadangkan kaedah ensembel pokok keputusan pengesahan silang yang mudah, cekap dan tepat. [[EENNDD]] aliran data; pokok keputusan; konsep-drift"], [{"string": "Combining partitions by probabilistic label aggregation Data clustering represents an important tool in exploratory data analysis . The lack of objective criteria render model selection as well as the identification of robust solutions particularly difficult . The use of a stability assessment and the combination of multiple clustering solutions represents an important ingredient to achieve the goal of finding useful partitions . In this work , we propose a novel way of combining multiple clustering solutions for both , hard and soft partitions : the approach is based on modeling the probability that two objects are grouped together . An efficient EM optimization strategy is employed in order to estimate the model parameters . Our proposal can also be extended in order to emphasize the signal more strongly by weighting individual base clustering solutions according to their consistency with the prediction for previously unseen objects . In addition to that , the probabilistic model supports an out-of-sample extension that i makes it possible to assign previously unseen objects to classes of the combined solution and ii renders the efficient aggregation of solutions possible . In this work , we also shed some light on the usefulness of such combination approaches . In the experimental result section , we demonstrate the competitive performance of our proposal in comparison with other recently proposed methods for combining multiple classifications of a finite data set .", "keywords": ["consensus partition", "re-sampling", "learning", "clustering"], "combined": "Combining partitions by probabilistic label aggregation Data clustering represents an important tool in exploratory data analysis . The lack of objective criteria render model selection as well as the identification of robust solutions particularly difficult . The use of a stability assessment and the combination of multiple clustering solutions represents an important ingredient to achieve the goal of finding useful partitions . In this work , we propose a novel way of combining multiple clustering solutions for both , hard and soft partitions : the approach is based on modeling the probability that two objects are grouped together . An efficient EM optimization strategy is employed in order to estimate the model parameters . Our proposal can also be extended in order to emphasize the signal more strongly by weighting individual base clustering solutions according to their consistency with the prediction for previously unseen objects . In addition to that , the probabilistic model supports an out-of-sample extension that i makes it possible to assign previously unseen objects to classes of the combined solution and ii renders the efficient aggregation of solutions possible . In this work , we also shed some light on the usefulness of such combination approaches . In the experimental result section , we demonstrate the competitive performance of our proposal in comparison with other recently proposed methods for combining multiple classifications of a finite data set . [[EENNDD]] consensus partition; re-sampling; learning; clustering"}, "Menggabungkan partisi dengan agregasi label probabilistik Pengelompokan data mewakili alat penting dalam analisis data penerokaan. Kekurangan kriteria objektif menjadikan pemilihan model serta pengenalpastian penyelesaian yang tepat sangat sukar. Penggunaan penilaian kestabilan dan gabungan beberapa penyelesaian pengelompokan merupakan ramuan penting untuk mencapai tujuan mencari partisi yang berguna. Dalam karya ini, kami mencadangkan cara baru untuk menggabungkan beberapa penyelesaian pengelompokan untuk kedua-dua partisi keras dan lembut: pendekatan ini didasarkan pada pemodelan kebarangkalian dua objek dikelompokkan bersama. Strategi pengoptimuman EM yang efisien digunakan untuk menganggar parameter model. Cadangan kami juga dapat diperluaskan untuk menekankan isyarat dengan lebih kuat dengan menimbang penyelesaian pengelompokan asas individu mengikut kesesuaiannya dengan ramalan untuk objek yang sebelumnya tidak terlihat. Di samping itu, model probabilistik menyokong peluasan di luar sampel yang memungkinkan saya menetapkan objek yang tidak pernah dilihat sebelumnya ke kelas penyelesaian gabungan dan ii menjadikan agregasi penyelesaian yang berkesan mungkin. Dalam karya ini, kami juga memberi penerangan tentang kegunaan pendekatan gabungan tersebut. Di bahagian hasil eksperimen, kami menunjukkan prestasi kompetitif cadangan kami berbanding dengan kaedah lain yang dicadangkan baru-baru ini untuk menggabungkan beberapa klasifikasi set data terhingga. [[EENNDD]] pemisahan konsensus; persampelan semula; belajar; pengelompokan"], [{"string": "Neighbor query friendly compression of social networks Compressing social networks can substantially facilitate mining and advanced analysis of large social networks . Preferably , social networks should be compressed in a way that they still can be queried efficiently without decompression . Arguably , neighbor queries , which search for all neighbors of a query vertex , are the most essential operations on social networks . Can we compress social networks effectively in a neighbor query friendly manner , that is , neighbor queries still can be answered in sublinear time using the compression ? In this paper , we develop an effective social network compression approach achieved by a novel Eulerian data structure using multi-position linearizations of directed graphs . Our method comes with a nontrivial theoretical bound on the compression rate . To the best of our knowledge , our approach is the first that can answer both out-neighbor and in-neighbor queries in sublinear time . An extensive empirical study on more than a dozen benchmark real data sets verifies our design .", "keywords": ["mpk linearization", "compression", "social networks"], "combined": "Neighbor query friendly compression of social networks Compressing social networks can substantially facilitate mining and advanced analysis of large social networks . Preferably , social networks should be compressed in a way that they still can be queried efficiently without decompression . Arguably , neighbor queries , which search for all neighbors of a query vertex , are the most essential operations on social networks . Can we compress social networks effectively in a neighbor query friendly manner , that is , neighbor queries still can be answered in sublinear time using the compression ? In this paper , we develop an effective social network compression approach achieved by a novel Eulerian data structure using multi-position linearizations of directed graphs . Our method comes with a nontrivial theoretical bound on the compression rate . To the best of our knowledge , our approach is the first that can answer both out-neighbor and in-neighbor queries in sublinear time . An extensive empirical study on more than a dozen benchmark real data sets verifies our design . [[EENNDD]] mpk linearization; compression; social networks"}, "Jaringan pemampatan mesra tetangga mesra Kompres rangkaian sosial dapat memudahkan perlombongan dan analisis lanjutan rangkaian sosial yang besar. Sebaiknya, rangkaian sosial harus dikompresi dengan cara yang masih dapat disoal dengan cekap tanpa penyahmampatan. Boleh dikatakan, pertanyaan tetangga, yang mencari semua jiran dari titik pertanyaan, adalah operasi yang paling penting di rangkaian sosial. Bolehkah kita memampatkan rangkaian sosial dengan berkesan dengan cara pertanyaan tetangga yang ramah, iaitu, pertanyaan tetangga masih dapat dijawab dalam waktu sublinear menggunakan pemampatan? Dalam makalah ini, kami mengembangkan pendekatan pemampatan rangkaian sosial yang efektif yang dicapai oleh struktur data Eulerian novel menggunakan linearisasi multi-posisi grafik terarah. Kaedah kami dilengkapi dengan teori tidak terikat pada kadar mampatan. Sepengetahuan kami, pendekatan kami adalah yang pertama yang dapat menjawab pertanyaan luar-jiran dan dalam-jiran dalam waktu sublinear. Kajian empirikal yang luas di lebih daripada sedozen set data sebenar penanda aras mengesahkan reka bentuk kami. [[EENNDD]] linearisasi mpk; pemampatan; rangkaian sosial"], [{"string": "Robust information-theoretic clustering How do we find a natural clustering of a real world point set , which contains an unknown number of clusters with different shapes , and which may be contaminated by noise ? Most clustering algorithms were designed with certain assumptions Gaussianity , they often require the user to give input parameters , and they are sensitive to noise . In this paper , we propose a robust framework for determining a natural clustering of a given data set , based on the minimum description length MDL principle . The proposed framework , Robust Information-theoretic Clustering RIC , is orthogonal to any known clustering algorithm : given a preliminary clustering , RIC purifies these clusters from noise , and adjusts the clusterings such that it simultaneously determines the most natural amount and shape subspace of the clusters . Our RIC method can be combined with any clustering technique ranging from K-means and K-medoids to advanced methods such as spectral clustering . In fact , RIC is even able to purify and improve an initial coarse clustering , even if we start with very simple methods such as grid-based space partitioning . Moreover , RIC scales well with the data set size . Extensive experiments on synthetic and real world data sets validate the proposed RIC framework .", "keywords": ["data summarization", "parameter-free data mining", "noise-robustness", "clustering"], "combined": "Robust information-theoretic clustering How do we find a natural clustering of a real world point set , which contains an unknown number of clusters with different shapes , and which may be contaminated by noise ? Most clustering algorithms were designed with certain assumptions Gaussianity , they often require the user to give input parameters , and they are sensitive to noise . In this paper , we propose a robust framework for determining a natural clustering of a given data set , based on the minimum description length MDL principle . The proposed framework , Robust Information-theoretic Clustering RIC , is orthogonal to any known clustering algorithm : given a preliminary clustering , RIC purifies these clusters from noise , and adjusts the clusterings such that it simultaneously determines the most natural amount and shape subspace of the clusters . Our RIC method can be combined with any clustering technique ranging from K-means and K-medoids to advanced methods such as spectral clustering . In fact , RIC is even able to purify and improve an initial coarse clustering , even if we start with very simple methods such as grid-based space partitioning . Moreover , RIC scales well with the data set size . Extensive experiments on synthetic and real world data sets validate the proposed RIC framework . [[EENNDD]] data summarization; parameter-free data mining; noise-robustness; clustering"}, "Pengelompokan teori-maklumat yang mantap Bagaimana kita dapat menemukan pengelompokan semula jadi dari set titik dunia nyata, yang mengandungi sejumlah kelompok yang tidak diketahui dengan bentuk yang berbeza, dan yang mungkin tercemar oleh bunyi bising? Sebilangan besar algoritma pengelompokan dirancang dengan andaian tertentu Gaussianity, mereka sering memerlukan pengguna memberikan parameter input, dan mereka sensitif terhadap kebisingan. Dalam makalah ini, kami mengusulkan kerangka kerja yang kuat untuk menentukan pengelompokan semula jadi kumpulan data tertentu, berdasarkan prinsip MDL panjang keterangan minimum. Rangka kerja yang dicadangkan, RIC Information-theoretic Clustering RIC, adalah ortogonal kepada mana-mana algoritma pengelompokan yang diketahui: dengan pengumpulan awal, RIC membersihkan kelompok ini dari kebisingan, dan menyesuaikan pengelompokan sehingga secara bersamaan menentukan ruang dan bentuk ruang yang paling semula jadi dari kelompok . Kaedah RIC kami dapat digabungkan dengan teknik pengelompokan mulai dari K-berarti dan K-medoid hingga metode lanjutan seperti pengelompokan spektrum. Sebenarnya, RIC bahkan dapat membersihkan dan memperbaiki pengelompokan kasar awal, walaupun kita mulai dengan kaedah yang sangat sederhana seperti partisi ruang berasaskan grid. Lebih-lebih lagi, skala RIC sesuai dengan ukuran set data. Eksperimen yang meluas pada set data sintetik dan dunia nyata mengesahkan rangka kerja RIC yang dicadangkan. [[EENNDD]] ringkasan data; perlombongan data tanpa parameter; ketahanan bunyi; pengelompokan"], [{"string": "Using a knowledge cache for interactive discovery of association rules", "keywords": ["decision support"], "combined": "Using a knowledge cache for interactive discovery of association rules [[EENNDD]] decision support"}, "Menggunakan cache pengetahuan untuk penemuan interaktif peraturan persatuan [[EENNDD]] sokongan keputusan"], [{"string": "Efficient algorithms for constructing decision trees with constraints", "keywords": ["decision tree", "decision support", "classification"], "combined": "Efficient algorithms for constructing decision trees with constraints [[EENNDD]] decision tree; decision support; classification"}, "Algoritma yang cekap untuk membina pokok keputusan dengan kekangan [[EENNDD]] pohon keputusan; sokongan keputusan; pengelasan"], [{"string": "Toward autonomic grids : analyzing the job flow with affinity streaming The Affinity Propagation AP clustering algorithm proposed by Frey and Dueck 2007 provides an understandable , nearly optimal summary of a dataset , albeit with quadratic computational complexity . This paper , motivated by Autonomic Computing , extends AP to the data streaming framework . Firstly a hierarchical strategy is used to reduce the complexity to O N1 + \u03b5 ; the distortion loss incurred is analyzed in relation with the dimension of the data items . Secondly , a coupling with a change detection test is used to cope with non-stationary data distribution , and rebuild the model as needed . The presented approach StrAP is applied to the stream of jobs submitted to the EGEE Grid , providing an understandable description of the job flow and enabling the system administrator to spot online some sources of failures .", "keywords": ["affinity propagation", "online clustering", "autonomic computing"], "combined": "Toward autonomic grids : analyzing the job flow with affinity streaming The Affinity Propagation AP clustering algorithm proposed by Frey and Dueck 2007 provides an understandable , nearly optimal summary of a dataset , albeit with quadratic computational complexity . This paper , motivated by Autonomic Computing , extends AP to the data streaming framework . Firstly a hierarchical strategy is used to reduce the complexity to O N1 + \u03b5 ; the distortion loss incurred is analyzed in relation with the dimension of the data items . Secondly , a coupling with a change detection test is used to cope with non-stationary data distribution , and rebuild the model as needed . The presented approach StrAP is applied to the stream of jobs submitted to the EGEE Grid , providing an understandable description of the job flow and enabling the system administrator to spot online some sources of failures . [[EENNDD]] affinity propagation; online clustering; autonomic computing"}, "Ke arah grid autonomi: menganalisis aliran pekerjaan dengan aliran afinitas Algoritma pengelompokan afinitas AP yang dicadangkan oleh Frey dan Dueck 2007 memberikan ringkasan yang hampir dapat difahami dari set data, walaupun dengan kerumitan komputasi kuadratik. Makalah ini, dimotivasi oleh Autonomic Computing, memperluas AP ke kerangka streaming data. Pertama strategi hierarki digunakan untuk mengurangkan kerumitan kepada O N1 + \u03b5; kerugian penyelewengan yang berlaku dianalisis berkaitan dengan dimensi item data. Kedua, gandingan dengan ujian pengesanan perubahan digunakan untuk mengatasi distribusi data yang tidak bergerak, dan membangun kembali model sesuai kebutuhan. Pendekatan yang disajikan StrAP diterapkan pada aliran pekerjaan yang diserahkan ke EGEE Grid, memberikan keterangan yang dapat difahami mengenai aliran pekerjaan dan memungkinkan pentadbir sistem untuk melihat secara online beberapa sumber kegagalan. [[EENNDD]] penyebaran pertalian; pengelompokan dalam talian; pengkomputeran autonomi"], [{"string": "Discovering roll-up dependencies", "keywords": ["logical design"], "combined": "Discovering roll-up dependencies [[EENNDD]] logical design"}, "Mencari kebergantungan penggabungan [[EENNDD]] reka bentuk logik"], [{"string": "Time and sample efficient discovery of Markov blankets and direct causal relations Data Mining with Bayesian Network learning has two important characteristics : under conditions learned edges between variables correspond to casual influences , and second , for every variable T in the network a special subset Markov Blanket identifiable by the network is the minimal variable set required to predict T. However , all known algorithms learning a complete BN do not scale up beyond a few hundred variables . On the other hand , all known sound algorithms learning a local region of the network require an exponential number of training instances to the size of the learned region . The contribution of this paper is two-fold . We introduce a novel local algorithm that returns all variables with direct edges to and from a target variable T as well as a local algorithm that returns the Markov Blanket of T. Both algorithms i are sound , ii can be run efficiently in datasets with thousands of variables , and iii significantly outperform in terms of approximating the true neighborhood previous state-of-the-art algorithms using only a fraction of the training size required by the existing methods . A fundamental difference between our approach and existing ones is that the required sample depends on the generating graph connectivity and not the size of the local region ; this yields up to exponential savings in sample relative to previously known algorithms . The results presented here are promising not only for discovery of local causal structure , and variable selection for classification , but also for the induction of complete BNs .", "keywords": ["novel data mining algorithms", "robust and scalable statistical methods", "learning", "bayesian networks"], "combined": "Time and sample efficient discovery of Markov blankets and direct causal relations Data Mining with Bayesian Network learning has two important characteristics : under conditions learned edges between variables correspond to casual influences , and second , for every variable T in the network a special subset Markov Blanket identifiable by the network is the minimal variable set required to predict T. However , all known algorithms learning a complete BN do not scale up beyond a few hundred variables . On the other hand , all known sound algorithms learning a local region of the network require an exponential number of training instances to the size of the learned region . The contribution of this paper is two-fold . We introduce a novel local algorithm that returns all variables with direct edges to and from a target variable T as well as a local algorithm that returns the Markov Blanket of T. Both algorithms i are sound , ii can be run efficiently in datasets with thousands of variables , and iii significantly outperform in terms of approximating the true neighborhood previous state-of-the-art algorithms using only a fraction of the training size required by the existing methods . A fundamental difference between our approach and existing ones is that the required sample depends on the generating graph connectivity and not the size of the local region ; this yields up to exponential savings in sample relative to previously known algorithms . The results presented here are promising not only for discovery of local causal structure , and variable selection for classification , but also for the induction of complete BNs . [[EENNDD]] novel data mining algorithms; robust and scalable statistical methods; learning; bayesian networks"}, "Masa dan contoh penemuan selimut Markov yang cekap dan hubungan sebab-akibat langsung Perlombongan Data dengan pembelajaran Rangkaian Bayesian mempunyai dua ciri penting: di bawah keadaan yang dipelajari tepi antara pemboleh ubah sesuai dengan pengaruh kasual, dan kedua, untuk setiap pemboleh ubah T dalam rangkaian subset khas Markov Blanket dapat dikenal pasti oleh rangkaian adalah set pemboleh ubah minimum yang diperlukan untuk meramalkan T. Walau bagaimanapun, semua algoritma yang diketahui mempelajari BN yang lengkap tidak meningkat melebihi beberapa ratus pemboleh ubah. Sebaliknya, semua algoritma bunyi yang diketahui yang mempelajari wilayah tempatan rangkaian memerlukan jumlah latihan yang eksponensial dengan ukuran wilayah yang dipelajari. Sumbangan kertas kerja ini dua kali ganda. Kami memperkenalkan algoritma tempatan baru yang mengembalikan semua pemboleh ubah dengan pinggir langsung ke dan dari pemboleh ubah sasaran T serta algoritma tempatan yang mengembalikan Markov Blanket of T. Kedua-dua algoritma i adalah baik, ii dapat dijalankan dengan cekap dalam set data dengan ribuan pemboleh ubah, dan iii secara signifikan mengungguli dari segi menghampiri algoritma terkini yang terkini dengan menggunakan hanya sebahagian kecil dari ukuran latihan yang diperlukan oleh kaedah yang ada. Perbezaan mendasar antara pendekatan kami dan pendekatan yang ada adalah bahawa sampel yang diperlukan bergantung pada penghasilan grafik yang dihasilkan dan bukan ukuran wilayah tempatan; ini menghasilkan penjimatan eksponensial dalam sampel berbanding dengan algoritma yang diketahui sebelumnya. Hasil yang disajikan di sini menjanjikan tidak hanya untuk penemuan struktur kausal tempatan, dan pemilihan variabel untuk klasifikasi, tetapi juga untuk induksi BN yang lengkap. [[EENNDD]] algoritma perlombongan data baru; kaedah statistik yang mantap dan berskala; belajar; rangkaian bayesian"], [{"string": "V-Miner : using enhanced parallel coordinates to mine product design and test data Analyzing data to find trends , correlations , and stable patterns is an important task in many industrial applications . This paper proposes a new technique based on parallel coordinate visualization . Previous work on parallel coordinate methods has shown that they are effective only when variables that are correlated and\\/or show similar patterns are displayed adjacently . Although current parallel coordinate tools allow the user to manually rearrange the order of variables , this process is very time-consuming when the number of variables is large . Automated assistance is required . This paper introduces an edit-distance based technique to rearrange variables so that interesting change patterns can be easily detected visually . The Visual Miner V-Miner software includes both automated methods for visualizing common patterns and a query tool that enables the user to describe specific target patterns to be mined or displayed by the system . In addition , the system can filter data according to rules sets imported from other data mining tools . This feature was found very helpful in practice , because it enables decision makers to visually identify interesting rules and data segments for further analysis or data mining . This paper begins with an introduction to the proposed techniques and the V-Miner system . Next , a case study illustrates how V-Miner has been used at Motorola to guide product design and test decisions .", "keywords": ["parallel coordinate visualization", "change patterns", "rules"], "combined": "V-Miner : using enhanced parallel coordinates to mine product design and test data Analyzing data to find trends , correlations , and stable patterns is an important task in many industrial applications . This paper proposes a new technique based on parallel coordinate visualization . Previous work on parallel coordinate methods has shown that they are effective only when variables that are correlated and\\/or show similar patterns are displayed adjacently . Although current parallel coordinate tools allow the user to manually rearrange the order of variables , this process is very time-consuming when the number of variables is large . Automated assistance is required . This paper introduces an edit-distance based technique to rearrange variables so that interesting change patterns can be easily detected visually . The Visual Miner V-Miner software includes both automated methods for visualizing common patterns and a query tool that enables the user to describe specific target patterns to be mined or displayed by the system . In addition , the system can filter data according to rules sets imported from other data mining tools . This feature was found very helpful in practice , because it enables decision makers to visually identify interesting rules and data segments for further analysis or data mining . This paper begins with an introduction to the proposed techniques and the V-Miner system . Next , a case study illustrates how V-Miner has been used at Motorola to guide product design and test decisions . [[EENNDD]] parallel coordinate visualization; change patterns; rules"}, "V-Miner: menggunakan koordinat selari yang ditingkatkan untuk melombong reka bentuk produk dan data ujian Menganalisis data untuk mencari tren, korelasi, dan corak stabil adalah tugas penting dalam banyak aplikasi industri. Makalah ini mencadangkan teknik baru berdasarkan visualisasi koordinat selari. Kerja sebelumnya mengenai kaedah koordinat selari telah menunjukkan bahawa ia berkesan hanya apabila pemboleh ubah yang berkorelasi dan \\ / atau menunjukkan corak serupa dipaparkan secara bersesuaian. Walaupun alat koordinat selari semasa membolehkan pengguna menyusun semula urutan pemboleh ubah secara manual, proses ini sangat memakan masa apabila jumlah pemboleh ubahnya besar. Bantuan automatik diperlukan. Makalah ini memperkenalkan teknik berdasarkan jarak edit untuk menyusun semula pemboleh ubah supaya corak perubahan yang menarik dapat dikesan dengan mudah secara visual. Perisian Visual Miner V-Miner merangkumi kedua-dua kaedah automatik untuk memvisualisasikan pola umum dan alat pertanyaan yang membolehkan pengguna menerangkan corak sasaran tertentu untuk ditambang atau ditampilkan oleh sistem. Di samping itu, sistem dapat menyaring data mengikut set peraturan yang diimport dari alat perlombongan data lain. Ciri ini didapati sangat berguna dalam praktiknya, kerana ia membolehkan para pembuat keputusan mengenal pasti secara visual peraturan dan segmen data yang menarik untuk analisis lebih lanjut atau data mining. Makalah ini dimulakan dengan pengenalan teknik yang dicadangkan dan sistem V-Miner. Seterusnya, kajian kes menggambarkan bagaimana V-Miner telah digunakan di Motorola untuk memandu reka bentuk produk dan keputusan ujian. [[EENNDD]] visualisasi koordinat selari; ubah corak; peraturan"], [{"string": "Ensemble pruning via individual contribution ordering An ensemble is a set of learned models that make decisions collectively . Although an ensemble is usually more accurate than a single learner , existing ensemble methods often tend to construct unnecessarily large ensembles , which increases the memory consumption and computational cost . Ensemble pruning tackles this problem by selecting a subset of ensemble members to form subensembles that are subject to less resource consumption and response time with accuracy that is similar to or better than the original ensemble . In this paper , we analyze the accuracy\\/diversity trade-off and prove that classifiers that are more accurate and make more predictions in the minority group are more important for subensemble construction . Based on the gained insights , a heuristic metric that considers both accuracy and diversity is proposed to explicitly evaluate each individual classifier 's contribution to the whole ensemble . By incorporating ensemble members in decreasing order of their contributions , subensembles are formed such that users can select the top $ p $ percent of ensemble members , depending on their resource availability and tolerable waiting time , for predictions . Experimental results on 26 UCI data sets show that subensembles formed by the proposed EPIC Ensemble Pruning via Individual Contribution ordering algorithm outperform the original ensemble and a state-of-the-art ensemble pruning method , Orientation Ordering OO .", "keywords": ["ensemble learning", "ensemble pruning"], "combined": "Ensemble pruning via individual contribution ordering An ensemble is a set of learned models that make decisions collectively . Although an ensemble is usually more accurate than a single learner , existing ensemble methods often tend to construct unnecessarily large ensembles , which increases the memory consumption and computational cost . Ensemble pruning tackles this problem by selecting a subset of ensemble members to form subensembles that are subject to less resource consumption and response time with accuracy that is similar to or better than the original ensemble . In this paper , we analyze the accuracy\\/diversity trade-off and prove that classifiers that are more accurate and make more predictions in the minority group are more important for subensemble construction . Based on the gained insights , a heuristic metric that considers both accuracy and diversity is proposed to explicitly evaluate each individual classifier 's contribution to the whole ensemble . By incorporating ensemble members in decreasing order of their contributions , subensembles are formed such that users can select the top $ p $ percent of ensemble members , depending on their resource availability and tolerable waiting time , for predictions . Experimental results on 26 UCI data sets show that subensembles formed by the proposed EPIC Ensemble Pruning via Individual Contribution ordering algorithm outperform the original ensemble and a state-of-the-art ensemble pruning method , Orientation Ordering OO . [[EENNDD]] ensemble learning; ensemble pruning"}, "Pemangkasan ensemble melalui urutan sumbangan individu Ensemble adalah sekumpulan model terpelajar yang membuat keputusan secara kolektif. Walaupun ensemble biasanya lebih tepat daripada pelajar tunggal, kaedah ensemble yang ada sering cenderung untuk membina ensemble besar yang tidak perlu, yang meningkatkan penggunaan memori dan kos pengiraan. Pemangkasan ensemble mengatasi masalah ini dengan memilih subset anggota ensemble untuk membentuk sub-kumpulan yang dikenakan penggunaan sumber daya dan masa tindak balas yang kurang dengan ketepatan yang serupa dengan atau lebih baik daripada ensemble asal. Dalam makalah ini, kami menganalisis ketepatan \\ / kepelbagaian pertukaran dan membuktikan bahawa pengkelasan yang lebih tepat dan membuat lebih banyak ramalan dalam kumpulan minoriti adalah lebih penting untuk pembinaan subsembel. Berdasarkan pandangan yang diperoleh, metrik heuristik yang mempertimbangkan ketepatan dan kepelbagaian diusulkan untuk menilai secara eksplisit setiap sumbangan pengklasifikasi individu untuk keseluruhan kumpulan. Dengan menggabungkan anggota ensemble dalam susunan sumbangan mereka yang berkurang, sub-kumpulan dibentuk sedemikian rupa sehingga pengguna dapat memilih $ ensemble $ p $ peratus anggota ensemble teratas, bergantung pada ketersediaan sumber daya dan masa menunggu yang boleh diterima, untuk ramalan. Hasil eksperimen pada 26 set data UCI menunjukkan bahawa subensembel yang dibentuk oleh EPIC Ensemble Pruning yang dicadangkan melalui algoritma pesanan Sumbangan Individu mengatasi ensemble asal dan kaedah pemangkasan ensemble canggih, Orientation Ordering OO. [[EENNDD]] pembelajaran ensembel; pemangkasan ensemble"], [{"string": "Learning incoherent sparse and low-rank patterns from multiple tasks We consider the problem of learning incoherent sparse and low-rank patterns from multiple tasks . Our approach is based on a linear multi-task learning formulation , in which the sparse and low-rank patterns are induced by a cardinality regularization term and a low-rank constraint , respectively . This formulation is non-convex ; we convert it into its convex surrogate , which can be routinely solved via semidefinite programming for small-size problems . We propose to employ the general projected gradient scheme to efficiently solve such a convex surrogate ; however , in the optimization formulation , the objective function is non-differentiable and the feasible domain is non-trivial . We present the procedures for computing the projected gradient and ensuring the global convergence of the projected gradient scheme . The computation of projected gradient involves a constrained optimization problem ; we show that the optimal solution to such a problem can be obtained via solving an unconstrained optimization subproblem and an Euclidean projection subproblem . In addition , we present two projected gradient algorithms and discuss their rates of convergence . Experimental results on benchmark data sets demonstrate the effectiveness of the proposed multi-task learning formulation and the efficiency of the proposed projected gradient algorithms .", "keywords": ["trace norm", "sparse and low-rank patterns", "multi-task learning"], "combined": "Learning incoherent sparse and low-rank patterns from multiple tasks We consider the problem of learning incoherent sparse and low-rank patterns from multiple tasks . Our approach is based on a linear multi-task learning formulation , in which the sparse and low-rank patterns are induced by a cardinality regularization term and a low-rank constraint , respectively . This formulation is non-convex ; we convert it into its convex surrogate , which can be routinely solved via semidefinite programming for small-size problems . We propose to employ the general projected gradient scheme to efficiently solve such a convex surrogate ; however , in the optimization formulation , the objective function is non-differentiable and the feasible domain is non-trivial . We present the procedures for computing the projected gradient and ensuring the global convergence of the projected gradient scheme . The computation of projected gradient involves a constrained optimization problem ; we show that the optimal solution to such a problem can be obtained via solving an unconstrained optimization subproblem and an Euclidean projection subproblem . In addition , we present two projected gradient algorithms and discuss their rates of convergence . Experimental results on benchmark data sets demonstrate the effectiveness of the proposed multi-task learning formulation and the efficiency of the proposed projected gradient algorithms . [[EENNDD]] trace norm; sparse and low-rank patterns; multi-task learning"}, "Belajar corak jarang dan peringkat rendah yang tidak koheren dari pelbagai tugas Kami menganggap masalah pembelajaran corak jarang dan peringkat rendah yang tidak koheren dari pelbagai tugas. Pendekatan kami didasarkan pada formulasi pembelajaran multi-tugas linier, di mana corak jarang dan rendah dipengaruhi oleh istilah regularisasi kardinaliti dan batasan peringkat rendah. Rumusan ini tidak cembung; kami mengubahnya menjadi pengganti cembungnya, yang dapat diselesaikan secara rutin melalui pengaturcaraan semidefinite untuk masalah kecil. Kami mencadangkan untuk menggunakan skema kecerunan yang diproyeksikan secara umum untuk menyelesaikan penggantian cembung seperti itu dengan cekap; namun, dalam rumusan pengoptimuman, fungsi objektif tidak dapat dibezakan dan domain yang layak tidak sepele. Kami membentangkan prosedur untuk menghitung kecerunan yang diproyeksikan dan memastikan penumpuan global skema kecerunan yang diproyeksikan. Pengiraan kecerunan yang diproyeksikan melibatkan masalah pengoptimuman yang dibatasi; kami menunjukkan bahawa penyelesaian yang optimum untuk masalah seperti itu dapat diperoleh dengan menyelesaikan sub-masalah pengoptimuman yang tidak terkawal dan sub-masalah unjuran Euclidean. Sebagai tambahan, kami membentangkan dua algoritma kecerunan yang diproyeksikan dan membincangkan kadar penumpuannya. Hasil eksperimen pada set data penanda aras menunjukkan keberkesanan rumusan pembelajaran multi-tugas yang dicadangkan dan kecekapan algoritma kecerunan yang diproyeksikan. [[EENNDD]] norma jejak; corak jarang dan rendah; pembelajaran pelbagai tugas"], [{"string": "Consensus group stable feature selection Stability is an important yet under-addressed issue in feature selection from high-dimensional and small sample data . In this paper , we show that stability of feature selection has a strong dependency on sample size . We propose a novel framework for stable feature selection which first identifies consensus feature groups from subsampling of training samples , and then performs feature selection by treating each consensus feature group as a single entity . Experiments on both synthetic and real-world data sets show that an algorithm developed under this framework is effective at alleviating the problem of small sample size and leads to more stable feature selection results and comparable or better generalization performance than state-of-the-art feature selection algorithms . Synthetic data sets and algorithm source code are available at http:\\/\\/www.cs.binghamton.edu\\/~lyu\\/KDD09\\/ .", "keywords": ["ensemble", "high-dimensional data", "small sample", "feature selection", "stability"], "combined": "Consensus group stable feature selection Stability is an important yet under-addressed issue in feature selection from high-dimensional and small sample data . In this paper , we show that stability of feature selection has a strong dependency on sample size . We propose a novel framework for stable feature selection which first identifies consensus feature groups from subsampling of training samples , and then performs feature selection by treating each consensus feature group as a single entity . Experiments on both synthetic and real-world data sets show that an algorithm developed under this framework is effective at alleviating the problem of small sample size and leads to more stable feature selection results and comparable or better generalization performance than state-of-the-art feature selection algorithms . Synthetic data sets and algorithm source code are available at http:\\/\\/www.cs.binghamton.edu\\/~lyu\\/KDD09\\/ . [[EENNDD]] ensemble; high-dimensional data; small sample; feature selection; stability"}, "Pemilihan ciri stabil kumpulan konsensus Kestabilan adalah masalah penting tetapi tidak dapat ditangani dalam pemilihan ciri dari data sampel dimensi tinggi dan kecil. Dalam makalah ini, kami menunjukkan bahawa kestabilan pemilihan ciri mempunyai ketergantungan yang kuat pada ukuran sampel. Kami mencadangkan kerangka baru untuk pemilihan ciri stabil yang pertama kali mengenal pasti kumpulan ciri konsensus dari sampel sampel latihan, dan kemudian melakukan pemilihan fitur dengan memperlakukan setiap kumpulan fitur konsensus sebagai satu kesatuan. Eksperimen pada kedua-dua set data sintetik dan dunia nyata menunjukkan bahawa algoritma yang dikembangkan di bawah kerangka ini berkesan untuk mengurangkan masalah saiz sampel kecil dan membawa kepada hasil pemilihan ciri yang lebih stabil dan prestasi generalisasi yang setanding atau lebih baik daripada yang terkini. algoritma pemilihan ciri. Kumpulan data sintetik dan kod sumber algoritma boleh didapati di http: \\ / \\ / www.cs.binghamton.edu \\ / ~ lyu \\ / KDD09 \\ /. [[EENNDD]] ensemble; data dimensi tinggi; sampel kecil; pemilihan ciri; kestabilan"], [{"string": "A fast algorithm for finding frequent episodes in event streams Frequent episode discovery is a popular framework for mining data available as a long sequence of events . An episode is essentially a short ordered sequence of event types and the frequency of an episode is some suitable measure of how often the episode occurs in the data sequence . Recently , we proposed a new frequency measure for episodes based on the notion of non-overlapped occurrences of episodes in the event sequence , and showed that , such a definition , in addition to yielding computationally efficient algorithms , has some important theoretical properties in connecting frequent episode discovery with HMM learning . This paper presents some new algorithms for frequent episode discovery under this non-overlapped occurrences-based frequency definition . The algorithms presented here are better by a factor of N , where N denotes the size of episodes being discovered in terms of both time and space complexities when compared to existing methods for frequent episode discovery . We show through some simulation experiments , that our algorithms are very efficient . The new algorithms presented here have arguably the least possible orders of spaceand time complexities for the task of frequent episode discovery .", "keywords": ["event streams", "frequent episodes", "temporal data mining", "non-overlapped occurrences"], "combined": "A fast algorithm for finding frequent episodes in event streams Frequent episode discovery is a popular framework for mining data available as a long sequence of events . An episode is essentially a short ordered sequence of event types and the frequency of an episode is some suitable measure of how often the episode occurs in the data sequence . Recently , we proposed a new frequency measure for episodes based on the notion of non-overlapped occurrences of episodes in the event sequence , and showed that , such a definition , in addition to yielding computationally efficient algorithms , has some important theoretical properties in connecting frequent episode discovery with HMM learning . This paper presents some new algorithms for frequent episode discovery under this non-overlapped occurrences-based frequency definition . The algorithms presented here are better by a factor of N , where N denotes the size of episodes being discovered in terms of both time and space complexities when compared to existing methods for frequent episode discovery . We show through some simulation experiments , that our algorithms are very efficient . The new algorithms presented here have arguably the least possible orders of spaceand time complexities for the task of frequent episode discovery . [[EENNDD]] event streams; frequent episodes; temporal data mining; non-overlapped occurrences"}, "Algoritma pantas untuk mencari episod yang kerap dalam aliran peristiwa Penemuan episod yang kerap adalah kerangka yang popular untuk data perlombongan yang tersedia sebagai urutan peristiwa yang panjang. Episod pada dasarnya adalah urutan jenis peristiwa yang disusun pendek dan kekerapan episod adalah beberapa ukuran yang sesuai untuk seberapa kerap episod itu berlaku dalam urutan data. Baru-baru ini, kami mencadangkan ukuran frekuensi baru untuk episod berdasarkan tanggapan mengenai kejadian episod yang tidak bertindih dalam urutan peristiwa, dan menunjukkan bahawa, definisi seperti itu, selain menghasilkan algoritma yang efisien secara komputasi, mempunyai beberapa sifat teori penting dalam menghubungkan kerap penemuan episod dengan pembelajaran HMM. Makalah ini mengemukakan beberapa algoritma baru untuk penemuan episod yang kerap di bawah definisi frekuensi berdasarkan kejadian yang tidak bertindih. Algoritma yang disajikan di sini lebih baik oleh faktor N, di mana N menunjukkan ukuran episod yang ditemui dari segi kerumitan masa dan ruang jika dibandingkan dengan kaedah yang ada untuk penemuan episod yang kerap. Kami menunjukkan melalui beberapa eksperimen simulasi, bahawa algoritma kami sangat cekap. Algoritma baru yang disajikan di sini mempunyai pesanan ruang dan kerumitan masa yang paling mungkin untuk tugas penemuan episod yang kerap. [[EENNDD]] aliran acara; episod yang kerap; perlombongan data temporal; kejadian tidak bertindih"], [{"string": "CVS : a Correlation-Verification based Smoothing technique on information retrieval and term clustering As information volume in enterprise systems and in the Web grows rapidly , how to accurately retrieve information is an important research area . Several corpus based smoothing techniques have been proposed to address the data sparsity and synonym problems faced by information retrieval systems . Such smoothing techniques are often unable to discover and utilize the correlations among terms . We propose CVS , a Correlation-Verification based Smoothing method , that considers co-occurrence information in smoothing . Strongly correlated terms in a document are identified by their co-occurrence frequencies in the document . To avoid missing correlated terms with low co-occurrence frequencies but specific to the theme of the document , the joint distributions of terms in the document are compared with those in the corpus for statistical significance . A common approach to apply corpus based smoothing techniques to information retrieval is by refining the vector representations of documents . This paper investigates the effects of corpus based smoothing on information retrieval by query expansion using term clusters generated from a term clustering process . The results can also be viewed in light of the effects of smoothing on clustering . Empirical studies show that our approach outperforms previous corpus based smoothing techniques . It improves retrieval effectiveness by 14.6 % . The results demonstrate that corpus based smoothing can be used for query expansion by term clustering .", "keywords": ["text mining", "query expansion", "information retrieval", "content analysis and indexing", "term clustering"], "combined": "CVS : a Correlation-Verification based Smoothing technique on information retrieval and term clustering As information volume in enterprise systems and in the Web grows rapidly , how to accurately retrieve information is an important research area . Several corpus based smoothing techniques have been proposed to address the data sparsity and synonym problems faced by information retrieval systems . Such smoothing techniques are often unable to discover and utilize the correlations among terms . We propose CVS , a Correlation-Verification based Smoothing method , that considers co-occurrence information in smoothing . Strongly correlated terms in a document are identified by their co-occurrence frequencies in the document . To avoid missing correlated terms with low co-occurrence frequencies but specific to the theme of the document , the joint distributions of terms in the document are compared with those in the corpus for statistical significance . A common approach to apply corpus based smoothing techniques to information retrieval is by refining the vector representations of documents . This paper investigates the effects of corpus based smoothing on information retrieval by query expansion using term clusters generated from a term clustering process . The results can also be viewed in light of the effects of smoothing on clustering . Empirical studies show that our approach outperforms previous corpus based smoothing techniques . It improves retrieval effectiveness by 14.6 % . The results demonstrate that corpus based smoothing can be used for query expansion by term clustering . [[EENNDD]] text mining; query expansion; information retrieval; content analysis and indexing; term clustering"}, "CVS: teknik Smoothing berdasarkan Correlation-Verification pada pencarian maklumat dan penggabungan istilah Oleh kerana jumlah maklumat dalam sistem perusahaan dan di Web berkembang dengan pesat, bagaimana mendapatkan maklumat dengan tepat adalah kawasan penyelidikan yang penting. Beberapa teknik pelicinan berdasarkan korpus telah diusulkan untuk mengatasi masalah data dan masalah sinonim yang dihadapi oleh sistem pengambilan maklumat. Teknik melicinkan seperti itu sering kali tidak dapat menemui dan memanfaatkan hubungan antara istilah. Kami mencadangkan CVS, kaedah Smoothing berdasarkan Correlation-Verification, yang mempertimbangkan maklumat kejadian bersama dalam melicinkan. Istilah yang sangat berkorelasi dalam dokumen dikenal pasti oleh kekerapan kejadian bersama dalam dokumen. Untuk mengelakkan hilangnya istilah berkorelasi dengan frekuensi kejadian rendah tetapi khusus untuk tema dokumen, pengagihan gabungan istilah dalam dokumen dibandingkan dengan istilah dalam korporat untuk kepentingan statistik. Pendekatan umum untuk menerapkan teknik pelicinan berdasarkan korpus untuk pengambilan maklumat adalah dengan memperbaiki representasi vektor dokumen. Makalah ini menyiasat kesan penghalusan berdasarkan korpus pada pencarian maklumat dengan pengembangan pertanyaan menggunakan kelompok istilah yang dihasilkan dari proses penggabungan istilah. Hasilnya juga dapat dilihat berdasarkan kesan kelancaran pengelompokan. Kajian empirikal menunjukkan bahawa pendekatan kita mengatasi teknik pelicinan berdasarkan korpus sebelumnya. Ia meningkatkan keberkesanan pengambilan sebanyak 14.6%. Hasilnya menunjukkan bahawa pelicinan berdasarkan korpus dapat digunakan untuk pengembangan permintaan dengan penggabungan istilah. [[EENNDD]] perlombongan teks; pengembangan pertanyaan; pengambilan maklumat; analisis kandungan dan pengindeksan; penggabungan istilah"], [{"string": "Relational learning via collective matrix factorization Relational learning is concerned with predicting unknown values of a relation , given a database of entities and observed relations among entities . An example of relational learning is movie rating prediction , where entities could include users , movies , genres , and actors . Relations encode users ' ratings of movies , movies ' genres , and actors ' roles in movies . A common prediction technique given one pairwise relation , for example a #users x #movies ratings matrix , is low-rank matrix factorization . In domains with multiple relations , represented as multiple matrices , we may improve predictive accuracy by exploiting information from one relation while predicting another . To this end , we propose a collective matrix factorization model : we simultaneously factor several matrices , sharing parameters among factors when an entity participates in multiple relations . Each relation can have a different value type and error distribution ; so , we allow nonlinear relationships between the parameters and outputs , using Bregman divergences to measure error . We extend standard alternating projection algorithms to our model , and derive an efficient Newton update for the projection . Furthermore , we propose stochastic optimization methods to deal with large , sparse matrices . Our model generalizes several existing matrix factorization methods , and therefore yields new large-scale optimization algorithms for these problems . Our model can handle any pairwise relational schema and a wide variety of error models . We demonstrate its efficiency , as well as the benefit of sharing parameters among relations .", "keywords": ["stochastic approximation", "matrix factorization", "relational learning"], "combined": "Relational learning via collective matrix factorization Relational learning is concerned with predicting unknown values of a relation , given a database of entities and observed relations among entities . An example of relational learning is movie rating prediction , where entities could include users , movies , genres , and actors . Relations encode users ' ratings of movies , movies ' genres , and actors ' roles in movies . A common prediction technique given one pairwise relation , for example a #users x #movies ratings matrix , is low-rank matrix factorization . In domains with multiple relations , represented as multiple matrices , we may improve predictive accuracy by exploiting information from one relation while predicting another . To this end , we propose a collective matrix factorization model : we simultaneously factor several matrices , sharing parameters among factors when an entity participates in multiple relations . Each relation can have a different value type and error distribution ; so , we allow nonlinear relationships between the parameters and outputs , using Bregman divergences to measure error . We extend standard alternating projection algorithms to our model , and derive an efficient Newton update for the projection . Furthermore , we propose stochastic optimization methods to deal with large , sparse matrices . Our model generalizes several existing matrix factorization methods , and therefore yields new large-scale optimization algorithms for these problems . Our model can handle any pairwise relational schema and a wide variety of error models . We demonstrate its efficiency , as well as the benefit of sharing parameters among relations . [[EENNDD]] stochastic approximation; matrix factorization; relational learning"}, "Pembelajaran relasional melalui faktorisasi matriks kolektif Pembelajaran relasional berkaitan dengan meramalkan nilai hubungan yang tidak diketahui, diberikan pangkalan data entiti dan hubungan yang diperhatikan antara entiti. Contoh pembelajaran relasional adalah ramalan penilaian filem, di mana entiti boleh merangkumi pengguna, filem, genre, dan pelakon. Hubungan menyandikan penilaian pengguna terhadap filem, genre filem, dan peranan pelakon dalam filem. Teknik ramalan biasa yang diberikan satu hubungan berpasangan, misalnya matriks penilaian #users x #movies, adalah pemfaktoran matriks peringkat rendah. Dalam domain dengan pelbagai hubungan, yang ditunjukkan sebagai beberapa matriks, kami dapat meningkatkan ketepatan ramalan dengan memanfaatkan maklumat dari satu hubungan sambil meramalkan hubungan yang lain. Untuk tujuan ini, kami mencadangkan model pemodelan matriks kolektif: kami secara bersamaan memfaktorkan beberapa matriks, berkongsi parameter antara faktor apabila entiti mengambil bahagian dalam pelbagai hubungan. Setiap hubungan boleh mempunyai jenis nilai dan taburan ralat yang berbeza; jadi, kami membenarkan hubungan tidak linier antara parameter dan output, menggunakan perbezaan Bregman untuk mengukur ralat. Kami memperluas algoritma unjuran penggantian standard ke model kami, dan memperoleh kemas kini Newton yang cekap untuk unjuran tersebut. Selanjutnya, kami mencadangkan kaedah pengoptimuman stokastik untuk menangani matriks yang jarang dan besar. Model kami menggeneralisasikan beberapa kaedah pemfaktoran matriks yang ada, dan dengan itu menghasilkan algoritma pengoptimuman skala besar baru untuk masalah ini. Model kami dapat menangani sebarang skema hubungan berpasangan dan pelbagai model ralat. Kami menunjukkan kecekapannya, dan juga manfaat berkongsi parameter antara hubungan. [[EENNDD]] pendekatan stokastik; pemfaktoran matriks; pembelajaran hubungan"], [{"string": "Why collective inference improves relational classification Procedures for collective inference make simultaneous statistical judgments about the same variables for a set of related data instances . For example , collective inference could be used to simultaneously classify a set of hyperlinked documents or infer the legitimacy of a set of related financial transactions . Several recent studies indicate that collective inference can significantly reduce classification error when compared with traditional inference techniques . We investigate the underlying mechanisms for this error reduction by reviewing past work on collective inference and characterizing different types of statistical models used for making inference in relational data . We show important differences among these models , and we characterize the necessary and sufficient conditions for reduced classification error based on experiments with real and simulated data .", "keywords": ["relational learning", "learning", "collective inference", "probabilistic relational models", "models"], "combined": "Why collective inference improves relational classification Procedures for collective inference make simultaneous statistical judgments about the same variables for a set of related data instances . For example , collective inference could be used to simultaneously classify a set of hyperlinked documents or infer the legitimacy of a set of related financial transactions . Several recent studies indicate that collective inference can significantly reduce classification error when compared with traditional inference techniques . We investigate the underlying mechanisms for this error reduction by reviewing past work on collective inference and characterizing different types of statistical models used for making inference in relational data . We show important differences among these models , and we characterize the necessary and sufficient conditions for reduced classification error based on experiments with real and simulated data . [[EENNDD]] relational learning; learning; collective inference; probabilistic relational models; models"}, "Mengapa inferens kolektif meningkatkan klasifikasi hubungan Prosedur untuk inferens kolektif membuat penilaian statistik serentak mengenai pemboleh ubah yang sama untuk sekumpulan contoh data yang berkaitan. Sebagai contoh, inferens kolektif dapat digunakan untuk mengklasifikasikan serangkaian dokumen berangkai yang berlebihan atau menyimpulkan kesahihan sekumpulan transaksi kewangan yang berkaitan. Beberapa kajian baru-baru ini menunjukkan bahawa kesimpulan kolektif dapat mengurangkan kesalahan klasifikasi secara signifikan jika dibandingkan dengan teknik inferensi tradisional. Kami menyiasat mekanisme yang mendasari pengurangan ralat ini dengan meninjau karya lalu mengenai kesimpulan kolektif dan mencirikan pelbagai jenis model statistik yang digunakan untuk membuat kesimpulan dalam data hubungan. Kami menunjukkan perbezaan penting di antara model-model ini, dan kami mencirikan syarat yang diperlukan dan mencukupi untuk mengurangkan ralat klasifikasi berdasarkan eksperimen dengan data nyata dan simulasi. [[EENNDD]] pembelajaran hubungan; belajar; inferens kolektif; model hubungan probabilistik; model"], [{"string": "Improved robustness of signature-based near-replica detection via lexicon randomization Detection of near duplicate documents is an important problem in many data mining and information filtering applications . When faced with massive quantities of data , traditional duplicate detection techniques relying on direct inter-document similarity computation e.g. , using the cosine measure are often not feasible given the time and memory performance constraints . On the other hand , fingerprint-based methods , such as I-Match , are very attractive computationally but may be brittle with respect to small changes to document content . We focus on approaches to near-replica detection that are based upon large-collection statistics and present a general technique of increasing their robustness via multiple lexicon randomization . In experiments with large web-page and spam-email datasets the proposed method is shown to consistently outperform traditional I-Match , with the relative improvement in duplicate-document recall reaching as high as 40-60 % . The large gains in detection accuracy are offset by only small increases in computational requirements .", "keywords": ["spam filtering", "web mining", "deduplication", "data cleaning"], "combined": "Improved robustness of signature-based near-replica detection via lexicon randomization Detection of near duplicate documents is an important problem in many data mining and information filtering applications . When faced with massive quantities of data , traditional duplicate detection techniques relying on direct inter-document similarity computation e.g. , using the cosine measure are often not feasible given the time and memory performance constraints . On the other hand , fingerprint-based methods , such as I-Match , are very attractive computationally but may be brittle with respect to small changes to document content . We focus on approaches to near-replica detection that are based upon large-collection statistics and present a general technique of increasing their robustness via multiple lexicon randomization . In experiments with large web-page and spam-email datasets the proposed method is shown to consistently outperform traditional I-Match , with the relative improvement in duplicate-document recall reaching as high as 40-60 % . The large gains in detection accuracy are offset by only small increases in computational requirements . [[EENNDD]] spam filtering; web mining; deduplication; data cleaning"}, "Peningkatan pengesanan dekat-replika berasaskan tandatangan melalui pengacakan leksikon Pengesanan dokumen pendua hampir merupakan masalah penting dalam banyak aplikasi perlombongan data dan penyaringan maklumat. Apabila berhadapan dengan sejumlah besar data, teknik pengesanan pendua tradisional bergantung pada pengiraan kesamaan antara dokumen langsung, mis. , penggunaan ukuran kosinus sering tidak dapat dilaksanakan memandangkan kekangan masa dan prestasi memori. Sebaliknya, kaedah berdasarkan sidik jari, seperti I-Match, sangat menarik secara komputasi tetapi mungkin rapuh berkaitan dengan perubahan kecil pada kandungan dokumen. Kami memfokuskan pada pendekatan untuk mendeteksi replika dekat yang berdasarkan statistik pengumpulan besar dan menyajikan teknik umum untuk meningkatkan ketahanan mereka melalui pengacakan pelbagai leksikon. Dalam eksperimen dengan set data laman web dan e-mel yang besar, kaedah yang dicadangkan ditunjukkan secara konsisten mengatasi I-Match tradisional, dengan peningkatan relatif dalam penarikan balik pendua-dokumen mencapai setinggi 40-60%. Keuntungan besar dalam ketepatan pengesanan diimbangi oleh hanya peningkatan kecil dalam keperluan pengiraan. [[EENNDD]] penapisan spam; perlombongan web; deduplikasi; pembersihan data"], [{"string": "SPIN : mining maximal frequent subgraphs from graph databases One fundamental challenge for mining recurring subgraphs from semi-structured data sets is the overwhelming abundance of such patterns . In large graph databases , the total number of frequent subgraphs can become too large to allow a full enumeration using reasonable computational resources . In this paper , we propose a new algorithm that mines only maximal frequent subgraphs , i.e. subgraphs that are not a part of any other frequent subgraphs . This may exponentially decrease the size of the output set in the best case ; in our experiments on practical data sets , mining maximal frequent subgraphs reduces the total number of mined patterns by two to three orders of magnitude . Our method first mines all frequent trees from a general graph database and then reconstructs all maximal subgraphs from the mined trees . Using two chemical structure benchmarks and a set of synthetic graph data sets , we demonstrate that , in addition to decreasing the output size , our algorithm can achieve a five-fold speed up over the current state-of-the-art subgraph mining algorithms .", "keywords": ["subgraph mining", "spanning tree"], "combined": "SPIN : mining maximal frequent subgraphs from graph databases One fundamental challenge for mining recurring subgraphs from semi-structured data sets is the overwhelming abundance of such patterns . In large graph databases , the total number of frequent subgraphs can become too large to allow a full enumeration using reasonable computational resources . In this paper , we propose a new algorithm that mines only maximal frequent subgraphs , i.e. subgraphs that are not a part of any other frequent subgraphs . This may exponentially decrease the size of the output set in the best case ; in our experiments on practical data sets , mining maximal frequent subgraphs reduces the total number of mined patterns by two to three orders of magnitude . Our method first mines all frequent trees from a general graph database and then reconstructs all maximal subgraphs from the mined trees . Using two chemical structure benchmarks and a set of synthetic graph data sets , we demonstrate that , in addition to decreasing the output size , our algorithm can achieve a five-fold speed up over the current state-of-the-art subgraph mining algorithms . [[EENNDD]] subgraph mining; spanning tree"}, "SPIN: melombong subgraf paling kerap dari pangkalan data grafik Salah satu cabaran mendasar untuk melombong subgraf berulang dari set data separa berstruktur adalah banyaknya corak seperti itu. Dalam pangkalan data grafik yang besar, jumlah subgraf yang kerap boleh menjadi terlalu besar untuk membolehkan penghitungan penuh menggunakan sumber pengiraan yang munasabah. Dalam makalah ini, kami mencadangkan algoritma baru yang hanya menggunakan subgraf kerap maksimum, iaitu subgraf yang bukan merupakan bahagian dari subgraf kerap yang lain. Ini dapat mengurangkan ukuran output yang ditetapkan secara eksponen dalam keadaan terbaik; dalam eksperimen kami pada set data praktikal, perlombongan subgraf maksimum yang kerap mengurangkan jumlah corak yang dilombong sebanyak dua hingga tiga pesanan magnitud. Kaedah kami terlebih dahulu melombong semua pokok yang kerap dari pangkalan data grafik umum dan kemudian membina semula semua subgraf maksimum dari pokok yang dilombong. Dengan menggunakan dua penanda aras struktur kimia dan satu set data grafik sintetik, kami menunjukkan bahawa, selain mengurangkan ukuran output, algoritma kami dapat mencapai kelajuan lima kali ganda berbanding algoritma perlombongan subgraf terkini. [[EENNDD]] perlombongan subgraf; pokok span"], [{"string": "Graph-based anomaly detection Anomaly detection is an area that has received much attention in recent years . It has a wide variety of applications , including fraud detection and network intrusion detection . A good deal of research has been performed in this area , often using strings or attribute-value data as the medium from which anomalies are to be extracted . Little work , however , has focused on anomaly detection in graph-based data . In this paper , we introduce two techniques for graph-based anomaly detection . In addition , we introduce a new method for calculating the regularity of a graph , with applications to anomaly detection . We hypothesize that these methods will prove useful both for finding anomalies , and for determining the likelihood of successful anomaly detection within graph-based data . We provide experimental results using both real-world network intrusion data and artificially-created data .", "keywords": ["anomaly detection", "graph regularity"], "combined": "Graph-based anomaly detection Anomaly detection is an area that has received much attention in recent years . It has a wide variety of applications , including fraud detection and network intrusion detection . A good deal of research has been performed in this area , often using strings or attribute-value data as the medium from which anomalies are to be extracted . Little work , however , has focused on anomaly detection in graph-based data . In this paper , we introduce two techniques for graph-based anomaly detection . In addition , we introduce a new method for calculating the regularity of a graph , with applications to anomaly detection . We hypothesize that these methods will prove useful both for finding anomalies , and for determining the likelihood of successful anomaly detection within graph-based data . We provide experimental results using both real-world network intrusion data and artificially-created data . [[EENNDD]] anomaly detection; graph regularity"}, "Pengesanan anomali berdasarkan grafik Pengesanan anomali adalah kawasan yang telah mendapat banyak perhatian dalam beberapa tahun terakhir. Ini memiliki berbagai macam aplikasi, termasuk pengesanan penipuan dan pengesanan pencerobohan jaringan. Banyak kajian telah dilakukan di daerah ini, sering menggunakan data rentetan atau nilai atribut sebagai media dari mana anomali diekstraksi. Namun, sedikit usaha telah memfokuskan pada pengesanan anomali dalam data berdasarkan grafik. Dalam makalah ini, kami memperkenalkan dua teknik untuk pengesanan anomali berdasarkan grafik. Sebagai tambahan, kami memperkenalkan kaedah baru untuk mengira keteraturan grafik, dengan aplikasi untuk pengesanan anomali. Kami membuat hipotesis bahawa kaedah ini akan terbukti berguna untuk mencari anomali, dan untuk menentukan kemungkinan pengesanan anomali yang berjaya dalam data berdasarkan grafik. Kami memberikan hasil eksperimen menggunakan data pencerobohan rangkaian dunia nyata dan data buatan. [[EENNDD]] pengesanan anomali; keteraturan grafik"], [{"string": "Generalized component analysis for text with heterogeneous attributes We present a class of richly structured , undirected hidden variable models suitable for simultaneously modeling text along with other attributes encoded in different modalities . Our model generalizes techniques such as principal component analysis to heterogeneous data types . In contrast to other approaches , this framework allows modalities such as words , authors and timestamps to be captured in their natural , probabilistic encodings . A latent space representation for a previously unseen document can be obtained through a fast matrix multiplication using our method . We demonstrate the effectiveness of our framework on the task of author prediction from 13 years of the NIPS conference proceedings and for a recipient prediction task using a 10-month academic email archive of a researcher . Our approach should be more broadly applicable to many real-world applications where one wishes to efficiently make predictions for a large number of potential outputs using dimensionality reduction in a well defined probabilistic framework .", "keywords": ["text mining", "author prediction", "multimodal heterogeneous data", "learning", "topic modeling", "undirected graphical models", "recipient prediction"], "combined": "Generalized component analysis for text with heterogeneous attributes We present a class of richly structured , undirected hidden variable models suitable for simultaneously modeling text along with other attributes encoded in different modalities . Our model generalizes techniques such as principal component analysis to heterogeneous data types . In contrast to other approaches , this framework allows modalities such as words , authors and timestamps to be captured in their natural , probabilistic encodings . A latent space representation for a previously unseen document can be obtained through a fast matrix multiplication using our method . We demonstrate the effectiveness of our framework on the task of author prediction from 13 years of the NIPS conference proceedings and for a recipient prediction task using a 10-month academic email archive of a researcher . Our approach should be more broadly applicable to many real-world applications where one wishes to efficiently make predictions for a large number of potential outputs using dimensionality reduction in a well defined probabilistic framework . [[EENNDD]] text mining; author prediction; multimodal heterogeneous data; learning; topic modeling; undirected graphical models; recipient prediction"}, "Analisis komponen umum untuk teks dengan atribut heterogen Kami menyajikan kelas model pemboleh ubah tersembunyi yang sangat terstruktur dan tidak terarah yang sesuai untuk memodelkan teks secara serentak bersama dengan atribut lain yang dikodkan dalam cara yang berbeza. Model kami membuat generalisasi teknik seperti analisis komponen utama kepada jenis data yang heterogen. Berbeza dengan pendekatan lain, kerangka ini memungkinkan modaliti seperti kata, pengarang dan cap waktu ditangkap dalam pengekodan probabilistik semula jadi. Perwakilan ruang laten untuk dokumen yang sebelumnya tidak dapat dilihat dapat diperoleh melalui pendaraban matriks cepat menggunakan kaedah kami. Kami menunjukkan keberkesanan kerangka kerja kami terhadap tugas ramalan penulis dari 13 tahun prosiding persidangan NIPS dan untuk tugas ramalan penerima menggunakan arkib e-mel akademik 10 bulan penyelidik. Pendekatan kami harus lebih luas berlaku untuk banyak aplikasi dunia nyata di mana seseorang ingin membuat ramalan dengan berkesan untuk sejumlah besar output berpotensi menggunakan pengurangan dimensi dalam kerangka probabilistik yang ditentukan. [[EENNDD]] perlombongan teks; ramalan pengarang; data heterogen multimodal; belajar; pemodelan topik; model grafik yang tidak diarahkan; ramalan penerima"], [{"string": "Cut-and-stitch : efficient parallel learning of linear dynamical systems on smps Multi-core processors with ever increasing number of cores per chip are becoming prevalent in modern parallel computing . Our goal is to make use of the multi-core as well as multi-processor architectures to speed up data mining algorithms . Specifically , we present a parallel algorithm for approximate learning of Linear Dynamical Systems LDS , also known as Kalman Filters KF . LDSs are widely used in time series analysis such as motion capture modeling , visual tracking etc. . We propose Cut-And-Stitch CAS , a novel method to handle the data dependencies from the chain structure of hidden variables in LDS , so as to parallelize the EM-based parameter learning algorithm . We implement the algorithm using OpenMP on both a supercomputer and a quad-core commercial desktop . The experimental results show that parallel algorithms using Cut-And-Stitch achieve comparable accuracy and almost linear speedups over the serial version . In addition , Cut-And-Stitch can be generalized to other models with similar linear structures such as Hidden Markov Models HMM and Switching Kalman Filters SKF .", "keywords": ["linear dynamical systems", "multi-core", "kalman filters", "optimization", "expectation maximization", "openmp"], "combined": "Cut-and-stitch : efficient parallel learning of linear dynamical systems on smps Multi-core processors with ever increasing number of cores per chip are becoming prevalent in modern parallel computing . Our goal is to make use of the multi-core as well as multi-processor architectures to speed up data mining algorithms . Specifically , we present a parallel algorithm for approximate learning of Linear Dynamical Systems LDS , also known as Kalman Filters KF . LDSs are widely used in time series analysis such as motion capture modeling , visual tracking etc. . We propose Cut-And-Stitch CAS , a novel method to handle the data dependencies from the chain structure of hidden variables in LDS , so as to parallelize the EM-based parameter learning algorithm . We implement the algorithm using OpenMP on both a supercomputer and a quad-core commercial desktop . The experimental results show that parallel algorithms using Cut-And-Stitch achieve comparable accuracy and almost linear speedups over the serial version . In addition , Cut-And-Stitch can be generalized to other models with similar linear structures such as Hidden Markov Models HMM and Switching Kalman Filters SKF . [[EENNDD]] linear dynamical systems; multi-core; kalman filters; optimization; expectation maximization; openmp"}, "Cut-and-stitch: pembelajaran selari yang cekap sistem dinamik linier pada pemproses multi-teras smps dengan bilangan teras per cip yang semakin meningkat menjadi lazim dalam pengkomputeran selari moden. Matlamat kami adalah untuk menggunakan arkitek multi-core dan multi-prosesor untuk mempercepat algoritma perlombongan data. Secara khusus, kami menyajikan algoritma selari untuk perkiraan pembelajaran Linear Dynamical Systems LDS, juga dikenal sebagai Kalman Filters KF. LDS banyak digunakan dalam analisis siri masa seperti pemodelan tangkapan gerakan, penjejakan visual dan lain-lain. Kami mencadangkan Cut-And-Stitch CAS, kaedah baru untuk menangani kebergantungan data dari struktur rantai pemboleh ubah tersembunyi di LDS, sehingga dapat menyelaraskan algoritma pembelajaran parameter berasaskan EM. Kami menerapkan algoritma menggunakan OpenMP pada komputer komersil superkomputer dan quad-core. Hasil eksperimen menunjukkan bahawa algoritma selari menggunakan Cut-And-Stitch mencapai ketepatan yang setanding dan kepantasan hampir linear berbanding versi siri. Di samping itu, Cut-And-Stitch dapat digeneralisasikan kepada model lain dengan struktur linear yang serupa seperti Hidden Markov Models HMM dan Switching Kalman Filters SKF. [[EENNDD]] sistem dinamik linear; pelbagai teras; penapis kalman; pengoptimuman; memaksimumkan jangkaan; bukamp"], [{"string": "Diversified ranking on large graphs : an optimization viewpoint Diversified ranking on graphs is a fundamental mining task and has a variety of high-impact applications . There are two important open questions here . The first challenge is the measure - how to quantify the goodness of a given top-k ranking list that captures both the relevance and the diversity ? The second challenge lies in the algorithmic aspect - how to find an optimal , or near-optimal , top-k ranking list that maximizes the measure we defined in a scalable way ? In this paper , we address these challenges from an optimization point of view . Firstly , we propose a goodness measure for a given top-k ranking list . The proposed goodness measure intuitively captures both a the relevance between each individual node in the ranking list and the query ; and b the diversity among different nodes in the ranking list . Moreover , we propose a scalable algorithm linear wrt the size of the graph that generates a provably near-optimal solution . The experimental evaluations on real graphs demonstrate its effectiveness and efficiency .", "keywords": ["ranking", "graph mining", "diversity", "scalability"], "combined": "Diversified ranking on large graphs : an optimization viewpoint Diversified ranking on graphs is a fundamental mining task and has a variety of high-impact applications . There are two important open questions here . The first challenge is the measure - how to quantify the goodness of a given top-k ranking list that captures both the relevance and the diversity ? The second challenge lies in the algorithmic aspect - how to find an optimal , or near-optimal , top-k ranking list that maximizes the measure we defined in a scalable way ? In this paper , we address these challenges from an optimization point of view . Firstly , we propose a goodness measure for a given top-k ranking list . The proposed goodness measure intuitively captures both a the relevance between each individual node in the ranking list and the query ; and b the diversity among different nodes in the ranking list . Moreover , we propose a scalable algorithm linear wrt the size of the graph that generates a provably near-optimal solution . The experimental evaluations on real graphs demonstrate its effectiveness and efficiency . [[EENNDD]] ranking; graph mining; diversity; scalability"}, "Peringkat yang beragam pada grafik besar: sudut pandang pengoptimuman Peringkat yang beragam pada grafik adalah tugas perlombongan asas dan mempunyai pelbagai aplikasi berimpak tinggi. Terdapat dua soalan terbuka penting di sini. Cabaran pertama adalah ukuran - bagaimana untuk mengukur kebaikan senarai ranking top-k tertentu yang menangkap relevansi dan kepelbagaian? Cabaran kedua terletak pada aspek algoritma - bagaimana mencari senarai peringkat teratas yang optimum, atau hampir optimum, yang memaksimumkan ukuran yang kita tentukan dengan cara yang boleh diskalakan? Dalam makalah ini, kami menangani cabaran ini dari sudut pengoptimuman. Pertama, kami mencadangkan ukuran kebaikan untuk senarai kedudukan k-top tertentu. Ukuran kebaikan yang dicadangkan secara intuitif menangkap kedua-dua perkaitan antara setiap simpul individu dalam senarai ranking dan pertanyaan; dan b kepelbagaian antara nod yang berlainan dalam senarai ranking. Lebih-lebih lagi, kami mencadangkan algoritma linear yang boleh diskalakan dengan ukuran grafik yang menghasilkan penyelesaian yang hampir hampir optimum. Penilaian eksperimen pada grafik nyata menunjukkan keberkesanan dan kecekapannya. [[EENNDD]] kedudukan; perlombongan grafik; kepelbagaian; skalabiliti"], [{"string": "Privacy preserving mining of association rules We present a framework for mining association rules from transactions consisting of categorical items where the data has been randomized to preserve privacy of individual transactions . While it is feasible to recover association rules and preserve privacy using a straightforward `` uniform '' randomization , the discovered rules can unfortunately be exploited to find privacy breaches . We analyze the nature of privacy breaches and propose a class of randomization operators that are much more effective than uniform randomization in limiting the breaches . We derive formulae for an unbiased support estimator and its variance , which allow us to recover itemset supports from randomized datasets , and show how to incorporate these formulae into mining algorithms . Finally , we present experimental results that validate the algorithm by applying it on real datasets .", "keywords": ["deduction", "unauthorized access"], "combined": "Privacy preserving mining of association rules We present a framework for mining association rules from transactions consisting of categorical items where the data has been randomized to preserve privacy of individual transactions . While it is feasible to recover association rules and preserve privacy using a straightforward `` uniform '' randomization , the discovered rules can unfortunately be exploited to find privacy breaches . We analyze the nature of privacy breaches and propose a class of randomization operators that are much more effective than uniform randomization in limiting the breaches . We derive formulae for an unbiased support estimator and its variance , which allow us to recover itemset supports from randomized datasets , and show how to incorporate these formulae into mining algorithms . Finally , we present experimental results that validate the algorithm by applying it on real datasets . [[EENNDD]] deduction; unauthorized access"}, "Privasi menjaga perlombongan peraturan persatuan Kami menyajikan kerangka untuk peraturan persatuan perlombongan dari transaksi yang terdiri daripada item kategori di mana data telah secara rawak untuk menjaga privasi transaksi individu. Walaupun layak untuk memulihkan peraturan pergaulan dan menjaga privasi menggunakan pengacakan \"seragam\" langsung, peraturan yang ditemui sayangnya dapat dieksploitasi untuk mencari pelanggaran privasi. Kami menganalisis sifat pelanggaran privasi dan mencadangkan kelas pengendali rawak yang jauh lebih berkesan daripada pengacakan seragam dalam membatasi pelanggaran. Kami memperoleh formula untuk penganggar sokongan yang tidak berat sebelah dan variansnya, yang memungkinkan kami untuk mendapatkan sokongan itemet dari set data rawak, dan menunjukkan cara memasukkan formula ini ke dalam algoritma perlombongan. Akhirnya, kami membentangkan hasil eksperimen yang mengesahkan algoritma dengan menerapkannya pada set data sebenar. [[EENNDD]] pemotongan; akses tidak dibenarkan"], [{"string": "Mining distance-based outliers in near linear time with randomization and a simple pruning rule Defining outliers by their distance to neighboring examples is a popular approach to finding unusual examples in a data set . Recently , much work has been conducted with the goal of finding fast algorithms for this task . We show that a simple nested loop algorithm that in the worst case is quadratic can give near linear time performance when the data is in random order and a simple pruning rule is used . We test our algorithm on real high-dimensional data sets with millions of examples and show that the near linear scaling holds over several orders of magnitude . Our average case analysis suggests that much of the efficiency is because the time to process non-outliers , which are the majority of examples , does not depend on the size of the data set .", "keywords": ["anomaly detection", "outliers", "distance-based operations", "diskbased algorithms"], "combined": "Mining distance-based outliers in near linear time with randomization and a simple pruning rule Defining outliers by their distance to neighboring examples is a popular approach to finding unusual examples in a data set . Recently , much work has been conducted with the goal of finding fast algorithms for this task . We show that a simple nested loop algorithm that in the worst case is quadratic can give near linear time performance when the data is in random order and a simple pruning rule is used . We test our algorithm on real high-dimensional data sets with millions of examples and show that the near linear scaling holds over several orders of magnitude . Our average case analysis suggests that much of the efficiency is because the time to process non-outliers , which are the majority of examples , does not depend on the size of the data set . [[EENNDD]] anomaly detection; outliers; distance-based operations; diskbased algorithms"}, "Melombong outlier berdasarkan jarak dalam waktu linear hampir dengan rawak dan peraturan pemangkasan yang sederhana Menentukan outliers dengan jarak mereka ke contoh jiran adalah pendekatan yang popular untuk mencari contoh yang tidak biasa dalam satu set data. Baru-baru ini, banyak kerja yang dilakukan dengan tujuan mencari algoritma pantas untuk tugas ini. Kami menunjukkan bahawa algoritma gelung bersarang sederhana yang dalam keadaan terburuk adalah kuadratik dapat memberikan prestasi masa linier hampir apabila data berada dalam urutan rawak dan peraturan pemangkasan sederhana digunakan. Kami menguji algoritma kami pada set data dimensi tinggi yang nyata dengan berjuta-juta contoh dan menunjukkan bahawa penskalaan linier hampir merangkumi beberapa pesanan besar. Analisis kes rata-rata kami menunjukkan bahawa banyak kecekapan adalah kerana masa untuk memproses non-outlier, yang merupakan sebahagian besar contoh, tidak bergantung pada ukuran set data. [[EENNDD]] pengesanan anomali; outliers; operasi berdasarkan jarak; algoritma berasaskan disk"], [{"string": "Incremental context mining for adaptive document classification Automatic document classification DC is essential for the management of information and knowledge . This paper explores two practical issues in DC : 1 each document has its context of discussion , and 2 both the content and vocabulary of the document database is intrinsically evolving . The issues call for adaptive document classification ADC that adapts a DC system to the evolving contextual requirement of each document category , so that input documents may be classified based on their contexts of discussion . We present an incremental context mining technique to tackle the challenges of ADC . Theoretical analyses and empirical results show that , given a text hierarchy , the mining technique is efficient in incrementally maintaining the evolving contextual requirement of each category . Based on the contextual requirements mined by the system , higher-precision DC may be achieved with better efficiency .", "keywords": ["incremental mining", "context text mining", "adaptive document classification"], "combined": "Incremental context mining for adaptive document classification Automatic document classification DC is essential for the management of information and knowledge . This paper explores two practical issues in DC : 1 each document has its context of discussion , and 2 both the content and vocabulary of the document database is intrinsically evolving . The issues call for adaptive document classification ADC that adapts a DC system to the evolving contextual requirement of each document category , so that input documents may be classified based on their contexts of discussion . We present an incremental context mining technique to tackle the challenges of ADC . Theoretical analyses and empirical results show that , given a text hierarchy , the mining technique is efficient in incrementally maintaining the evolving contextual requirement of each category . Based on the contextual requirements mined by the system , higher-precision DC may be achieved with better efficiency . [[EENNDD]] incremental mining; context text mining; adaptive document classification"}, "Perlombongan konteks tambahan untuk klasifikasi dokumen adaptif Klasifikasi dokumen automatik DC sangat penting untuk pengurusan maklumat dan pengetahuan. Makalah ini meneroka dua masalah praktikal di DC: 1 setiap dokumen mempunyai konteks perbincangannya, dan 2 kandungan dan perbendaharaan kata pangkalan data dokumen secara intrinsik berkembang. Masalahnya memerlukan ADC klasifikasi dokumen adaptif yang menyesuaikan sistem DC dengan keperluan kontekstual yang berubah dari setiap kategori dokumen, sehingga dokumen input dapat diklasifikasikan berdasarkan konteks perbincangannya. Kami menyajikan teknik penambangan konteks tambahan untuk menangani cabaran ADC. Analisis teoritis dan hasil empirik menunjukkan bahawa, mengingat hierarki teks, teknik perlombongan efisien dalam meningkatkan secara bertahap keperluan kontekstual dari setiap kategori. Berdasarkan keperluan kontekstual yang ditambang oleh sistem, DC berketepatan tinggi dapat dicapai dengan kecekapan yang lebih baik. [[EENNDD]] perlombongan tambahan; perlombongan teks konteks; pengelasan dokumen adaptif"], [{"string": "Pattern lattice traversal by selective jumps Regardless of the frequent patterns to discover , either the full frequent patterns or the condensed ones , either closed or maximal , the strategy always includes the traversal of the lattice of candidate patterns . We study the existing depth versus breadth traversal approaches for generating candidate patterns and propose in this paper a new traversal approach that jumps in the search space among only promising nodes . Our leaping approach avoids nodes that would not participate in the answer set and reduce drastically the number of candidate patterns . We use this approach to efficiently pinpoint maximal patterns at the border of the frequent patterns in the lattice and collect enough information in the process to generate all subsequent patterns .", "keywords": ["nonnumerical algorithms and problems"], "combined": "Pattern lattice traversal by selective jumps Regardless of the frequent patterns to discover , either the full frequent patterns or the condensed ones , either closed or maximal , the strategy always includes the traversal of the lattice of candidate patterns . We study the existing depth versus breadth traversal approaches for generating candidate patterns and propose in this paper a new traversal approach that jumps in the search space among only promising nodes . Our leaping approach avoids nodes that would not participate in the answer set and reduce drastically the number of candidate patterns . We use this approach to efficiently pinpoint maximal patterns at the border of the frequent patterns in the lattice and collect enough information in the process to generate all subsequent patterns . [[EENNDD]] nonnumerical algorithms and problems"}, "Corak kisi melintang dengan lompatan selektif Terlepas dari corak yang sering dijumpai, sama ada corak kerap penuh atau pola pekat, baik tertutup atau maksimal, strategi selalu merangkumi lintasan kisi pola calon. Kami mengkaji pendekatan melintasi kedalaman berbanding luas untuk menghasilkan corak calon dan mencadangkan dalam makalah ini pendekatan melintasi baru yang melonjak di ruang carian di antara node yang hanya menjanjikan. Pendekatan lompatan kami mengelakkan simpul yang tidak akan mengambil bahagian dalam set jawapan dan mengurangkan secara drastik bilangan corak calon. Kami menggunakan pendekatan ini untuk menentukan corak maksimum dengan cekap di sempadan corak kerap dalam kisi dan mengumpulkan maklumat yang cukup dalam proses untuk menghasilkan semua corak berikutnya. [[EENNDD]] algoritma dan masalah bukan angka"], [{"string": "Using randomized response techniques for privacy-preserving data mining Privacy is an important issue in data mining and knowledge discovery . In this paper , we propose to use the randomized response techniques to conduct the data mining computation . Specially , we present a method to build decision tree classifiers from the disguised data . We conduct experiments to compare the accuracy of our decision tree with the one built from the original undisguised data . Our results show that although the data are disguised , our method can still achieve fairly high accuracy . We also show how the parameter used in the randomized response techniques affects the accuracy of the results .", "keywords": ["data mining", "privacy", "decision tree", "database applications"], "combined": "Using randomized response techniques for privacy-preserving data mining Privacy is an important issue in data mining and knowledge discovery . In this paper , we propose to use the randomized response techniques to conduct the data mining computation . Specially , we present a method to build decision tree classifiers from the disguised data . We conduct experiments to compare the accuracy of our decision tree with the one built from the original undisguised data . Our results show that although the data are disguised , our method can still achieve fairly high accuracy . We also show how the parameter used in the randomized response techniques affects the accuracy of the results . [[EENNDD]] data mining; privacy; decision tree; database applications"}, "Menggunakan teknik tindak balas rawak untuk pelestarian data menjaga privasi Privasi adalah masalah penting dalam perlombongan data dan penemuan pengetahuan. Dalam makalah ini, kami mencadangkan untuk menggunakan teknik tindak balas rawak untuk melakukan pengiraan data mining. Khususnya, kami menyajikan kaedah untuk membuat pengelasan pokok keputusan dari data yang menyamar. Kami menjalankan eksperimen untuk membandingkan ketepatan keputusan kami dengan keputusan yang dibina dari data asal yang tidak disamar. Hasil kajian kami menunjukkan bahawa walaupun data tersembunyi, kaedah kami masih dapat mencapai ketepatan yang cukup tinggi. Kami juga menunjukkan bagaimana parameter yang digunakan dalam teknik tindak balas rawak mempengaruhi ketepatan hasil. [[EENNDD]] perlombongan data; privasi; pokok keputusan; aplikasi pangkalan data"], [{"string": "Learning classifiers from only positive and unlabeled data The input to an algorithm that learns a binary classifier normally consists of two sets of examples , where one set consists of positive examples of the concept to be learned , and the other set consists of negative examples . However , it is often the case that the available training data are an incomplete set of positive examples , and a set of unlabeled examples , some of which are positive and some of which are negative . The problem solved in this paper is how to learn a standard binary classifier given a nontraditional training set of this nature . Under the assumption that the labeled examples are selected randomly from the positive examples , we show that a classifier trained on positive and unlabeled examples predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive . We show how to use this result in two different ways to learn a classifier from a nontraditional training set . We then apply these two new methods to solve a real-world problem : identifying protein records that should be included in an incomplete specialized molecular biology database . Our experiments in this domain show that models trained using the new methods perform better than the current state-of-the-art biased SVM method for learning from positive and unlabeled examples .", "keywords": ["unlabeled examples", "bioinformatics", "text mining", "supervised learning"], "combined": "Learning classifiers from only positive and unlabeled data The input to an algorithm that learns a binary classifier normally consists of two sets of examples , where one set consists of positive examples of the concept to be learned , and the other set consists of negative examples . However , it is often the case that the available training data are an incomplete set of positive examples , and a set of unlabeled examples , some of which are positive and some of which are negative . The problem solved in this paper is how to learn a standard binary classifier given a nontraditional training set of this nature . Under the assumption that the labeled examples are selected randomly from the positive examples , we show that a classifier trained on positive and unlabeled examples predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive . We show how to use this result in two different ways to learn a classifier from a nontraditional training set . We then apply these two new methods to solve a real-world problem : identifying protein records that should be included in an incomplete specialized molecular biology database . Our experiments in this domain show that models trained using the new methods perform better than the current state-of-the-art biased SVM method for learning from positive and unlabeled examples . [[EENNDD]] unlabeled examples; bioinformatics; text mining; supervised learning"}, "Pembelajaran pengklasifikasi dari hanya data positif dan tidak berlabel Input ke algoritma yang mempelajari pengkelasan binari biasanya terdiri daripada dua set contoh, di mana satu set terdiri daripada contoh positif konsep yang akan dipelajari, dan kumpulan lain terdiri daripada contoh negatif. Walau bagaimanapun, seringkali data latihan yang tersedia adalah sekumpulan contoh positif yang tidak lengkap, dan sekumpulan contoh yang tidak berlabel, ada yang positif dan ada yang negatif. Masalah yang diselesaikan dalam makalah ini adalah bagaimana mempelajari pengkelasan perduaan standard yang diberi set latihan nontradisional seperti ini. Dengan anggapan bahawa contoh berlabel dipilih secara rawak dari contoh positif, kami menunjukkan bahawa pengkelasan yang dilatih mengenai contoh positif dan tidak berlabel meramalkan kebarangkalian yang berbeza hanya dengan faktor tetap dari kebarangkalian bersyarat yang sebenarnya menjadi positif. Kami menunjukkan cara menggunakan hasil ini dengan dua cara yang berbeza untuk mempelajari pengkelasan dari set latihan tidak tradisional. Kami kemudian menggunakan dua kaedah baru ini untuk menyelesaikan masalah dunia nyata: mengenal pasti rekod protein yang harus dimasukkan dalam pangkalan data biologi molekul khusus yang tidak lengkap. Eksperimen kami dalam domain ini menunjukkan bahawa model yang dilatih menggunakan kaedah baru berprestasi lebih baik daripada kaedah SVM bias terkini yang terkini untuk belajar dari contoh positif dan tidak berlabel. [[EENNDD]] contoh yang tidak berlabel; bioinformatik; perlombongan teks; pembelajaran yang diselia"], [{"string": "On the potential of domain literature for clustering and Bayesian network learning Thanks to its increasing availability , electronic literature can now be a major source of information when developing complex statistical models where data is scarce or contains much noise . This raises the question of how to integrate information from domain literature with statistical data . Because quantifying similarities or dependencies between variables is a basic building block in knowledge discovery , we consider here the following question . Which vector representations of text and which statistical scores of similarity or dependency support best the use of literature in statistical models ? For the text source , we assume to have annotations for the domain variables as short free-text descriptions and optionally to have a large literature repository from which we can further expand the annotations . For evaluation , we contrast the variables similarities or dependencies obtained from text using different annotation sources and vector representations with those obtained from measurement data or expert assessments . Specifically , we consider two learning problems : clustering and Bayesian network learning . Firstly , we report performance against an expert reference for clustering yeast genes from textual annotations . Secondly , we assess the agreement between text-based and data-based scores of variable dependencies when learning Bayesian network substructures for the task of modeling the joint distribution of clinical measurements of ovarian tumors .", "keywords": ["data mining", "text mining", "bayesian networks", "clustering"], "combined": "On the potential of domain literature for clustering and Bayesian network learning Thanks to its increasing availability , electronic literature can now be a major source of information when developing complex statistical models where data is scarce or contains much noise . This raises the question of how to integrate information from domain literature with statistical data . Because quantifying similarities or dependencies between variables is a basic building block in knowledge discovery , we consider here the following question . Which vector representations of text and which statistical scores of similarity or dependency support best the use of literature in statistical models ? For the text source , we assume to have annotations for the domain variables as short free-text descriptions and optionally to have a large literature repository from which we can further expand the annotations . For evaluation , we contrast the variables similarities or dependencies obtained from text using different annotation sources and vector representations with those obtained from measurement data or expert assessments . Specifically , we consider two learning problems : clustering and Bayesian network learning . Firstly , we report performance against an expert reference for clustering yeast genes from textual annotations . Secondly , we assess the agreement between text-based and data-based scores of variable dependencies when learning Bayesian network substructures for the task of modeling the joint distribution of clinical measurements of ovarian tumors . [[EENNDD]] data mining; text mining; bayesian networks; clustering"}, "Mengenai potensi sastera domain untuk pengelompokan dan pembelajaran jaringan Bayes. Berkat ketersediaannya yang semakin meningkat, sastera elektronik kini dapat menjadi sumber utama maklumat ketika mengembangkan model statistik yang kompleks di mana data jarang atau mengandung banyak kebisingan. Ini menimbulkan persoalan bagaimana mengintegrasikan maklumat dari literatur domain dengan data statistik. Oleh kerana mengukur persamaan atau kebergantungan antara pemboleh ubah adalah blok asas dalam penemuan pengetahuan, kami mempertimbangkan di sini persoalan berikut. Vektor representasi teks mana dan skor statistik kesamaan atau kebergantungan yang menyokong penggunaan literatur terbaik dalam model statistik? Untuk sumber teks, kami menganggap mempunyai anotasi untuk pemboleh ubah domain sebagai keterangan teks bebas pendek dan pilihan untuk memiliki repositori literatur yang besar dari mana kami dapat memperluas anotasi lebih lanjut. Untuk penilaian, kami membezakan persamaan pemboleh ubah atau kebergantungan yang diperoleh dari teks menggunakan sumber anotasi dan representasi vektor yang berbeza dengan yang diperoleh dari data pengukuran atau penilaian pakar. Secara khusus, kami mempertimbangkan dua masalah pembelajaran: pengelompokan dan pembelajaran rangkaian Bayesian. Pertama, kami melaporkan prestasi berdasarkan rujukan pakar untuk mengumpulkan gen ragi dari anotasi teks. Kedua, kami menilai kesepakatan antara skor ketergantungan berubah berdasarkan teks dan data ketika mempelajari substruktur rangkaian Bayesian untuk tugas memodelkan pengedaran bersama pengukuran klinikal tumor ovari. [[EENNDD]] perlombongan data; perlombongan teks; rangkaian bayesian; pengelompokan"], [{"string": "Generating semantic annotations for frequent patterns with context analysis As a fundamental data mining task , frequent pattern mining has widespread applications in many different domains . Research in frequent pattern mining has so far mostly focused on developing efficient algorithms to discover various kinds of frequent patterns , but little attention has been paid to the important nextstep - interpreting the discovered frequent patterns . Although some recent work has studied the compression and summarization of frequent patterns , the proposed techniques can only annotate a frequent pattern with non-semantical information e.g. support , which provides only limited help for a user to understand the patterns . In this paper , we propose the novel problem of generating semantic annotations for frequent patterns . The goal is to annotate a frequent pattern with in-depth , concise , and structured information that can better indicate the hidden meanings of the pattern . We propose a general approach to generate such anannotation for a frequent pattern by constructing its context model , selecting informative context indicators , and extracting representative transactions and semantically similar patterns . This general approach has potentially many applications such as generating a dictionary-like description for a pattern , finding synonym patterns , discovering semantic relations , and summarizing semantic classes of a set of frequent patterns . Experiments on different datasets show that our approach is effective in generating semantic pattern annotations .", "keywords": ["pattern context", "pattern semantic analysis", "pattern annotation", "frequent pattern"], "combined": "Generating semantic annotations for frequent patterns with context analysis As a fundamental data mining task , frequent pattern mining has widespread applications in many different domains . Research in frequent pattern mining has so far mostly focused on developing efficient algorithms to discover various kinds of frequent patterns , but little attention has been paid to the important nextstep - interpreting the discovered frequent patterns . Although some recent work has studied the compression and summarization of frequent patterns , the proposed techniques can only annotate a frequent pattern with non-semantical information e.g. support , which provides only limited help for a user to understand the patterns . In this paper , we propose the novel problem of generating semantic annotations for frequent patterns . The goal is to annotate a frequent pattern with in-depth , concise , and structured information that can better indicate the hidden meanings of the pattern . We propose a general approach to generate such anannotation for a frequent pattern by constructing its context model , selecting informative context indicators , and extracting representative transactions and semantically similar patterns . This general approach has potentially many applications such as generating a dictionary-like description for a pattern , finding synonym patterns , discovering semantic relations , and summarizing semantic classes of a set of frequent patterns . Experiments on different datasets show that our approach is effective in generating semantic pattern annotations . [[EENNDD]] pattern context; pattern semantic analysis; pattern annotation; frequent pattern"}, "Menghasilkan anotasi semantik untuk corak kerap dengan analisis konteks Sebagai tugas perlombongan data asas, perlombongan corak yang kerap mempunyai aplikasi yang meluas di banyak domain yang berbeza. Penyelidikan dalam perlombongan corak yang kerap sejauh ini kebanyakannya berfokus pada pengembangan algoritma yang cekap untuk menemui pelbagai jenis corak yang kerap, tetapi sedikit perhatian telah diberikan kepada langkah penting seterusnya - menafsirkan pola yang sering dijumpai. Walaupun beberapa karya baru-baru ini telah mengkaji pemampatan dan penjumlahan corak yang kerap, teknik yang dicadangkan hanya dapat memberi penjelasan mengenai corak yang kerap dengan maklumat bukan semantik, mis. sokongan, yang hanya memberikan bantuan terhad untuk pengguna memahami corak. Dalam makalah ini, kami mencadangkan masalah novel menghasilkan anotasi semantik untuk corak yang kerap. Tujuannya adalah untuk memberi penjelasan mengenai corak yang kerap dengan maklumat yang mendalam, ringkas, dan tersusun yang dapat menunjukkan maksud corak yang tersembunyi dengan lebih baik. Kami mencadangkan pendekatan umum untuk menghasilkan anotasi semacam itu untuk pola yang kerap dengan membina model konteksnya, memilih petunjuk konteks yang bermaklumat, dan mengekstrak transaksi perwakilan dan pola yang serupa secara semantik. Pendekatan umum ini berpotensi banyak aplikasi seperti menghasilkan deskripsi seperti kamus untuk corak, mencari corak sinonim, menemui hubungan semantik, dan meringkaskan kelas semantik sekumpulan corak yang kerap. Eksperimen pada kumpulan data yang berbeza menunjukkan bahawa pendekatan kami berkesan dalam menghasilkan anotasi corak semantik. [[EENNDD]] konteks corak; analisis semantik corak; penjelasan corak; corak yang kerap"], [{"string": "Algorithms for storytelling We formulate a new data mining problem called it storytelling as a generalization of redescription mining . In traditional redescription mining , we are given a set of objects and a collection of subsets defined over these objects . The goal is to view the set system as a vocabulary and identify two expressions in this vocabulary that induce the same set of objects . Storytelling , on the other hand , aims to explicitly relate object sets that are disjoint and hence , maximally dissimilar by finding a chain of approximate redescriptions between the sets . This problem finds applications in bioinformatics , for instance , where the biologist is trying to relate a set of genes expressed in one experiment to another set , implicated in a different pathway . We outline an efficient storytelling implementation that embeds the CART wheels redescription mining algorithm in an A \\* search procedure , using the former to supply next move operators on search branches to the latter . This approach is practical and effective for mining large datasets and , at the same time , exploits the structure of partitions imposed by the given vocabulary . Three application case studies are presented : a study of word overlaps in large English dictionaries , exploring connections between genesets in a bioinformatics dataset , and relating publications in the PubMed index of abstracts .", "keywords": ["redescription", "learning", "storytelling"], "combined": "Algorithms for storytelling We formulate a new data mining problem called it storytelling as a generalization of redescription mining . In traditional redescription mining , we are given a set of objects and a collection of subsets defined over these objects . The goal is to view the set system as a vocabulary and identify two expressions in this vocabulary that induce the same set of objects . Storytelling , on the other hand , aims to explicitly relate object sets that are disjoint and hence , maximally dissimilar by finding a chain of approximate redescriptions between the sets . This problem finds applications in bioinformatics , for instance , where the biologist is trying to relate a set of genes expressed in one experiment to another set , implicated in a different pathway . We outline an efficient storytelling implementation that embeds the CART wheels redescription mining algorithm in an A \\* search procedure , using the former to supply next move operators on search branches to the latter . This approach is practical and effective for mining large datasets and , at the same time , exploits the structure of partitions imposed by the given vocabulary . Three application case studies are presented : a study of word overlaps in large English dictionaries , exploring connections between genesets in a bioinformatics dataset , and relating publications in the PubMed index of abstracts . [[EENNDD]] redescription; learning; storytelling"}, "Algoritma untuk penceritaan Kami merumuskan masalah perlombongan data baru yang menyebutnya penceritaan sebagai generalisasi perlombongan reka bentuk semula. Dalam perlombongan reka bentuk semula tradisional, kita diberi sekumpulan objek dan kumpulan subset yang ditentukan di atas objek-objek ini. Tujuannya adalah untuk melihat sistem set sebagai perbendaharaan kata dan mengenal pasti dua ungkapan dalam perbendaharaan kata ini yang mendorong set objek yang sama. Bercerita, sebaliknya, bertujuan untuk secara eksplisit menghubungkan kumpulan objek yang terpisah dan oleh itu, secara maksimum tidak sama dengan mencari rantai penafsiran semula antara set. Masalah ini menemui aplikasi dalam bioinformatik, misalnya, di mana ahli biologi cuba mengaitkan satu set gen yang dinyatakan dalam satu eksperimen dengan satu set yang lain, yang terlibat dalam jalan yang berbeza. Kami menggariskan pelaksanaan penceritaan yang efisien yang menyertakan algoritma perlombongan reka bentuk roda CART dalam prosedur carian A \\ *, menggunakan yang pertama untuk membekalkan operator bergerak seterusnya di cabang carian kepada yang terakhir. Pendekatan ini praktikal dan berkesan untuk melombong set data yang besar dan, pada masa yang sama, memanfaatkan struktur partisi yang dikenakan oleh perbendaharaan kata yang diberikan. Tiga kajian kes aplikasi disajikan: kajian pertindihan kata dalam kamus bahasa Inggeris yang besar, meneroka hubungan antara gen dalam kumpulan data bioinformatik, dan berkaitan penerbitan dalam indeks abstrak PubMed. [[EENNDD]] reka bentuk semula; belajar; bercerita"], [{"string": "Mining and summarizing customer reviews Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services . As e-commerce is becoming more and more popular , the number of customer reviews that a product receives grows rapidly . For a popular product , the number of reviews can be in hundreds or even thousands . This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product . It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions . For the manufacturer , there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products . In this research , we aim to mine and to summarize all the customer reviews of a product . This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative . We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization . Our task is performed in three steps : 1 mining product features that have been commented on by customers ; 2 identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative ; 3 summarizing the results . This paper proposes several novel techniques to perform these tasks . Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques .", "keywords": ["summarization", "reviews", "sentiment classification", "text mining"], "combined": "Mining and summarizing customer reviews Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services . As e-commerce is becoming more and more popular , the number of customer reviews that a product receives grows rapidly . For a popular product , the number of reviews can be in hundreds or even thousands . This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product . It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions . For the manufacturer , there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products . In this research , we aim to mine and to summarize all the customer reviews of a product . This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative . We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization . Our task is performed in three steps : 1 mining product features that have been commented on by customers ; 2 identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative ; 3 summarizing the results . This paper proposes several novel techniques to perform these tasks . Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques . [[EENNDD]] summarization; reviews; sentiment classification; text mining"}, "Melombong dan meringkaskan ulasan pelanggan Pedagang yang menjual produk di Web sering meminta pelanggan mereka untuk mengkaji produk yang telah mereka beli dan perkhidmatan yang berkaitan. Oleh kerana e-commerce semakin popular, jumlah ulasan pelanggan yang diterima oleh produk meningkat dengan pesat. Untuk produk yang popular, jumlah ulasan boleh mencapai ratusan bahkan ribuan. Ini menyukarkan calon pelanggan untuk membacanya untuk membuat keputusan yang tepat mengenai apakah akan membeli produk tersebut. Ini juga menyukarkan pengeluar produk untuk mengawasi dan mengurus pendapat pelanggan. Bagi pengilang, terdapat kesulitan tambahan kerana banyak laman web penjual boleh menjual produk yang sama dan biasanya pengeluar menghasilkan banyak jenis produk. Dalam penyelidikan ini, kami bertujuan untuk menambang dan meringkaskan semua ulasan pelanggan terhadap sesuatu produk. Tugas ringkasan ini berbeza dengan ringkasan teks tradisional kerana kami hanya menggunakan ciri-ciri produk di mana pelanggan telah menyatakan pendapat mereka dan sama ada pendapat itu positif atau negatif. Kami tidak meringkaskan ulasan dengan memilih subset atau menulis semula beberapa ayat asal dari ulasan untuk menangkap perkara utama seperti dalam ringkasan teks klasik. Tugas kami dilaksanakan dalam tiga langkah: 1 ciri produk perlombongan yang telah diulas oleh pelanggan; 2 mengenal pasti ayat pendapat dalam setiap ulasan dan memutuskan sama ada setiap ayat pendapat itu positif atau negatif; 3 merumuskan keputusan. Makalah ini mencadangkan beberapa teknik baru untuk melaksanakan tugas-tugas ini. Hasil eksperimen kami menggunakan tinjauan dari sejumlah produk yang dijual dalam talian menunjukkan keberkesanan teknik tersebut. [[EENNDD]] ringkasan; ulasan; klasifikasi sentimen; perlombongan teks"], [{"string": "Maximizing the spread of influence through a social network Models for the processes by which ideas and influence propagate through a social network have been studied in a number of domains , including the diffusion of medical and technological innovations , the sudden and widespread adoption of various strategies in game-theoretic settings , and the effects of `` word of mouth '' in the promotion of new products . Recently , motivated by the design of viral marketing strategies , Domingos and Richardson posed a fundamental algorithmic problem for such social network processes : if we can try to convince a subset of individuals to adopt a new product or innovation , and the goal is to trigger a large cascade of further adoptions , which set of individuals should we target ? We consider this problem in several of the most widely studied models in social network analysis . The optimization problem of selecting the most influential nodes is NP-hard here , and we provide the first provable approximation guarantees for efficient algorithms . Using an analysis framework based on submodular functions , we show that a natural greedy strategy obtains a solution that is provably within 63 % of optimal for several classes of models ; our framework suggests a general approach for reasoning about the performance guarantees of algorithms for these types of influence problems in social networks . We also provide computational experiments on large collaboration networks , showing that in addition to their provable guarantees , our approximation algorithms significantly out-perform node-selection heuristics based on the well-studied notions of degree centrality and distance centrality from the field of social networks .", "keywords": ["diffusion of innovations", "viral marketing", "social networks", "nonnumerical algorithms and problems", "approximation algorithms"], "combined": "Maximizing the spread of influence through a social network Models for the processes by which ideas and influence propagate through a social network have been studied in a number of domains , including the diffusion of medical and technological innovations , the sudden and widespread adoption of various strategies in game-theoretic settings , and the effects of `` word of mouth '' in the promotion of new products . Recently , motivated by the design of viral marketing strategies , Domingos and Richardson posed a fundamental algorithmic problem for such social network processes : if we can try to convince a subset of individuals to adopt a new product or innovation , and the goal is to trigger a large cascade of further adoptions , which set of individuals should we target ? We consider this problem in several of the most widely studied models in social network analysis . The optimization problem of selecting the most influential nodes is NP-hard here , and we provide the first provable approximation guarantees for efficient algorithms . Using an analysis framework based on submodular functions , we show that a natural greedy strategy obtains a solution that is provably within 63 % of optimal for several classes of models ; our framework suggests a general approach for reasoning about the performance guarantees of algorithms for these types of influence problems in social networks . We also provide computational experiments on large collaboration networks , showing that in addition to their provable guarantees , our approximation algorithms significantly out-perform node-selection heuristics based on the well-studied notions of degree centrality and distance centrality from the field of social networks . [[EENNDD]] diffusion of innovations; viral marketing; social networks; nonnumerical algorithms and problems; approximation algorithms"}, "Memaksimumkan penyebaran pengaruh melalui rangkaian sosial Model untuk proses di mana idea dan pengaruh menyebarkan melalui rangkaian sosial telah dikaji dalam sejumlah domain, termasuk penyebaran inovasi perubatan dan teknologi, penerapan pelbagai strategi secara tiba-tiba dan meluas dalam tetapan teori permainan, dan kesan \"dari mulut ke mulut\" dalam promosi produk baru. Baru-baru ini, didorong oleh perancangan strategi pemasaran viral, Domingos dan Richardson menimbulkan masalah algoritma asas untuk proses rangkaian sosial seperti itu: jika kita dapat meyakinkan sekumpulan individu untuk menggunakan produk atau inovasi baru, dan tujuannya adalah untuk memicu lata besar penerapan selanjutnya, kumpulan individu mana yang harus kita sasarkan? Kami mempertimbangkan masalah ini dalam beberapa model yang paling banyak dikaji dalam analisis rangkaian sosial. Masalah pengoptimuman memilih node yang paling berpengaruh adalah NP-hard di sini, dan kami memberikan jaminan penghampiran pertama yang dapat dibuktikan untuk algoritma yang cekap. Dengan menggunakan kerangka analisis berdasarkan fungsi submodular, kami menunjukkan bahawa strategi tamak semula jadi memperoleh penyelesaian yang dapat dibuktikan dalam 63% optimum untuk beberapa kelas model; kerangka kerja kami mencadangkan pendekatan umum untuk membuat pertimbangan mengenai jaminan prestasi algoritma untuk jenis masalah pengaruh dalam rangkaian sosial. Kami juga menyediakan eksperimen komputasi pada rangkaian kolaborasi yang besar, yang menunjukkan bahawa sebagai tambahan kepada jaminan yang dapat dibuktikan, algoritma penghampiran kami dengan signifikan melakukan heuristik pemilihan nod berdasarkan konsep yang dipelajari dengan baik mengenai sentraliti darjah dan sentraliti jarak dari bidang rangkaian sosial. [[EENNDD]] penyebaran inovasi; pemasaran viral; rangkaian sosial; algoritma dan masalah bukan berangka; algoritma penghampiran"], [{"string": "Fast window correlations over uncooperative time series Data arriving in time order a data stream arises in fields including physics , finance , medicine , and music , to name a few . Often the data comes from sensors in physics and medicine for example whose data rates continue to improve dramatically as sensor technology improves . Further , the number of sensors is increasing , so correlating data between sensors becomes ever more critical in order to distill knowlege from the data . In many applications such as finance , recent correlations are of far more interest than long-term correlation , so correlation over sliding windows windowed correlation is the desired operation . Fast response is desirable in many applications e.g. , to aim a telescope at an activity of interest or to perform a stock trade . These three factors -- data size , windowed correlation , and fast response -- motivate this work . Previous work 10 , 14 showed how to compute Pearson correlation using Fast Fourier Transforms and Wavelet transforms , but such techniques do n't work for time series in which the energy is spread over many frequency components , thus resembling white noise . For such `` uncooperative '' time series , this paper shows how to combine several simple techniques -- sketches random projections , convolution , structured random vectors , grid structures , and combinatorial design -- to achieve high performance windowed Pearson correlation over a variety of data sets .", "keywords": ["data structures", "randomized algorithms", "time series", "correlation"], "combined": "Fast window correlations over uncooperative time series Data arriving in time order a data stream arises in fields including physics , finance , medicine , and music , to name a few . Often the data comes from sensors in physics and medicine for example whose data rates continue to improve dramatically as sensor technology improves . Further , the number of sensors is increasing , so correlating data between sensors becomes ever more critical in order to distill knowlege from the data . In many applications such as finance , recent correlations are of far more interest than long-term correlation , so correlation over sliding windows windowed correlation is the desired operation . Fast response is desirable in many applications e.g. , to aim a telescope at an activity of interest or to perform a stock trade . These three factors -- data size , windowed correlation , and fast response -- motivate this work . Previous work 10 , 14 showed how to compute Pearson correlation using Fast Fourier Transforms and Wavelet transforms , but such techniques do n't work for time series in which the energy is spread over many frequency components , thus resembling white noise . For such `` uncooperative '' time series , this paper shows how to combine several simple techniques -- sketches random projections , convolution , structured random vectors , grid structures , and combinatorial design -- to achieve high performance windowed Pearson correlation over a variety of data sets . [[EENNDD]] data structures; randomized algorithms; time series; correlation"}, "Korelasi tetingkap cepat terhadap siri masa yang tidak kooperatif. Data tiba dalam urutan masa aliran data muncul dalam bidang termasuk fizik, kewangan, perubatan, dan muzik, untuk beberapa nama. Selalunya data berasal dari sensor dalam fizik dan perubatan misalnya, kadar datanya terus meningkat secara mendadak seiring dengan peningkatan teknologi sensor. Selanjutnya, jumlah sensor semakin meningkat, sehingga data yang menghubungkan antara sensor menjadi lebih penting untuk menyaring pengetahuan dari data. Dalam banyak aplikasi seperti kewangan, korelasi baru-baru ini jauh lebih menarik daripada korelasi jangka panjang, jadi korelasi terhadap korelasi tingkap slaid windows adalah operasi yang diinginkan. Respons pantas diperlukan dalam banyak aplikasi mis. , untuk mengarahkan teleskop pada aktiviti yang menarik atau untuk melakukan perdagangan saham. Ketiga faktor ini - ukuran data, korelasi berjendela, dan tindak balas pantas - memotivasi kerja ini. Kerja sebelumnya 10, 14 menunjukkan cara menghitung korelasi Pearson menggunakan Transformasi Fourier Cepat dan transformasi Wavelet, tetapi teknik seperti itu tidak berfungsi untuk siri masa di mana tenaga tersebar di banyak komponen frekuensi, sehingga menyerupai suara putih. Untuk siri masa \"tidak bekerjasama\" seperti ini, makalah ini menunjukkan bagaimana menggabungkan beberapa teknik mudah - lakaran unjuran rawak, konvolusi, vektor rawak berstruktur, struktur grid, dan reka bentuk kombinatorial - untuk mencapai prestasi tinggi korelasi Pearson dengan pelbagai set data. [[EENNDD]] struktur data; algoritma rawak; siri masa; korelasi"], [{"string": "Graphs over time : densification laws , shrinking diameters and possible explanations How do real graphs evolve over time ? What are `` normal '' growth patterns in social , technological , and information networks ? Many studies have discovered patterns in static graphs , identifying properties in a single snapshot of a large network , or in a very small number of snapshots ; these include heavy tails for in - and out-degree distributions , communities , small-world phenomena , and others . However , given the lack of information about network evolution over long periods , it has been hard to convert these findings into statements about trends over time . Here we study a wide range of real graphs , and we observe some surprising phenomena . First , most of these graphs densify over time , with the number of edges growing super-linearly in the number of nodes . Second , the average distance between nodes often shrinks over time , in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes like O log n or O log log n . Existing graph generation models do not exhibit these types of behavior , even at a qualitative level . We provide a new graph generator , based on a `` forest fire '' spreading process , that has a simple , intuitive justification , requires very few parameters like the `` flammability '' of nodes , and produces graphs exhibiting the full range of properties observed both in prior work and in the present study .", "keywords": ["densification power laws", "small-world phenomena", "graph mining", "heavy-tailed distributions", "graph generators"], "combined": "Graphs over time : densification laws , shrinking diameters and possible explanations How do real graphs evolve over time ? What are `` normal '' growth patterns in social , technological , and information networks ? Many studies have discovered patterns in static graphs , identifying properties in a single snapshot of a large network , or in a very small number of snapshots ; these include heavy tails for in - and out-degree distributions , communities , small-world phenomena , and others . However , given the lack of information about network evolution over long periods , it has been hard to convert these findings into statements about trends over time . Here we study a wide range of real graphs , and we observe some surprising phenomena . First , most of these graphs densify over time , with the number of edges growing super-linearly in the number of nodes . Second , the average distance between nodes often shrinks over time , in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes like O log n or O log log n . Existing graph generation models do not exhibit these types of behavior , even at a qualitative level . We provide a new graph generator , based on a `` forest fire '' spreading process , that has a simple , intuitive justification , requires very few parameters like the `` flammability '' of nodes , and produces graphs exhibiting the full range of properties observed both in prior work and in the present study . [[EENNDD]] densification power laws; small-world phenomena; graph mining; heavy-tailed distributions; graph generators"}, "Grafik dari masa ke masa: undang-undang kepadatan, pengecutan diameter dan penjelasan yang mungkin Bagaimana grafik sebenar berkembang dari masa ke masa? Apa pola pertumbuhan \"normal\" dalam rangkaian sosial, teknologi, dan maklumat? Banyak kajian telah menemui corak dalam grafik statik, mengenal pasti sifat dalam satu gambar rangkaian besar, atau dalam jumlah tangkapan yang sangat kecil; ini termasuk ekor berat untuk pengedaran dalam, dan luar darjah, komuniti, fenomena dunia kecil, dan lain-lain. Namun, mengingat kekurangan maklumat mengenai evolusi rangkaian dalam jangka masa yang panjang, sukar untuk mengubah penemuan ini menjadi pernyataan mengenai trend dari masa ke masa. Di sini kita mengkaji pelbagai grafik nyata, dan kita melihat beberapa fenomena yang mengejutkan. Pertama, sebilangan besar graf ini semakin padat dari masa ke masa, dengan bilangan tepi tumbuh secara super linear dalam bilangan nod. Kedua, jarak rata-rata antara nod sering menyusut dari masa ke masa, berbeza dengan kebijaksanaan konvensional bahawa parameter jarak seperti itu harus meningkat dengan perlahan sebagai fungsi bilangan nod seperti O log n atau O log log n. Model penjanaan grafik yang ada tidak menunjukkan jenis tingkah laku ini, bahkan pada tahap kualitatif. Kami menyediakan penjana grafik baru, berdasarkan proses penyebaran \"kebakaran hutan\", yang mempunyai justifikasi yang mudah dan intuitif, memerlukan sangat sedikit parameter seperti \"kebakaran\" nod, dan menghasilkan grafik yang menunjukkan pelbagai sifat diperhatikan dalam kerja sebelumnya dan dalam kajian ini. [[EENNDD]] undang-undang kuasa pemadatan; fenomena dunia kecil; perlombongan grafik; pengedaran ekor berat; penjana grafik"], [{"string": "A large-scale analysis of query logs for assessing personalization opportunities Query logs , the patterns of activity left by millions of users , contain a wealth of information that can be mined to aid personalization . We perform a large-scale study of Yahoo ! search engine logs , tracking 1.35 million browser-cookies over a period of 6 months . We define metrics to address questions such as 1 How much history is available ? , 2 How do users ' topical interests vary , as reflected by their queries ? , and 3 What can we learn from user clicks ? We find that there is significantly more expected history for the user of a randomly picked query than for a randomly picked user . We show that users exhibit consistent topical interests that vary between users . We also see that user clicks indicate a variety of special interests . Our findings shed light on user activity and can inform future personalization efforts .", "keywords": ["personalization", "miscellaneous", "query logs", "user interests", "user history", "categorization", "clustering"], "combined": "A large-scale analysis of query logs for assessing personalization opportunities Query logs , the patterns of activity left by millions of users , contain a wealth of information that can be mined to aid personalization . We perform a large-scale study of Yahoo ! search engine logs , tracking 1.35 million browser-cookies over a period of 6 months . We define metrics to address questions such as 1 How much history is available ? , 2 How do users ' topical interests vary , as reflected by their queries ? , and 3 What can we learn from user clicks ? We find that there is significantly more expected history for the user of a randomly picked query than for a randomly picked user . We show that users exhibit consistent topical interests that vary between users . We also see that user clicks indicate a variety of special interests . Our findings shed light on user activity and can inform future personalization efforts . [[EENNDD]] personalization; miscellaneous; query logs; user interests; user history; categorization; clustering"}, "Analisis skala besar log pertanyaan untuk menilai peluang pemperibadian Log pertanyaan, corak aktiviti yang ditinggalkan oleh berjuta-juta pengguna, mengandungi banyak maklumat yang dapat ditambang untuk membantu pemperibadian. Kami melakukan kajian besar-besaran mengenai Yahoo! log enjin carian, menjejaki 1.35 juta kuki penyemak imbas dalam jangka masa 6 bulan. Kami menentukan metrik untuk menjawab soalan seperti 1 Berapa banyak sejarah yang ada? , 2 Bagaimana minat topikal pengguna berbeza, seperti yang ditunjukkan oleh pertanyaan mereka? , dan 3 Apa yang dapat kita pelajari dari klik pengguna? Kami mendapati bahawa terdapat sejarah yang jauh lebih diharapkan bagi pengguna pertanyaan yang dipilih secara rawak daripada pengguna yang dipilih secara rawak. Kami menunjukkan bahawa pengguna menunjukkan minat topikal yang konsisten yang berbeza antara pengguna. Kami juga melihat bahawa klik pengguna menunjukkan pelbagai minat khusus. Penemuan kami memberi penerangan mengenai aktiviti pengguna dan dapat memberitahu usaha pemperibadian masa depan. [[EENNDD]] pemperibadian; pelbagai; log pertanyaan; minat pengguna; sejarah pengguna; pengkategorian; pengelompokan"], [{"string": "Support envelopes : a technique for exploring the structure of association patterns This paper introduces support envelopes -- a new tool for analyzing association patterns -- and illustrates some of their properties , applications , and possible extensions . Specifically , the support envelope for a transaction data set and a specified pair of positive integers m , n consists of the items and transactions that need to be searched to find any association pattern involving m or more transactions and n or more items . For any transaction data set with M transactions and N items , there is a unique lattice of at most M \\* N support envelopes that captures the structure of the association patterns in that data set . Because support envelopes are not encumbered by a support threshold , this support lattice provides a complete view of the association structure of the data set , including association patterns that have low support . Furthermore , the boundary of the support lattice -- the support boundary -- has at most min M , N envelopes and is especially interesting since it bounds the maximum sizes of potential association patterns -- not only for frequent , closed , and maximal itemsets , but also for patterns , such as error-tolerant itemsets , that are more general . The association structure can be represented graphically as a two-dimensional scatter plot of the m , n values associated with the support envelopes of the data set , a feature that is useful in the exploratory analysis of association patterns . Finally , the algorithm to compute support envelopes is simple and computationally efficient , and it is straightforward to parallelize the process of finding all the support envelopes .", "keywords": ["association analysis", "formal concept analysis", "error-tolerant itemsets", "support envelope"], "combined": "Support envelopes : a technique for exploring the structure of association patterns This paper introduces support envelopes -- a new tool for analyzing association patterns -- and illustrates some of their properties , applications , and possible extensions . Specifically , the support envelope for a transaction data set and a specified pair of positive integers m , n consists of the items and transactions that need to be searched to find any association pattern involving m or more transactions and n or more items . For any transaction data set with M transactions and N items , there is a unique lattice of at most M \\* N support envelopes that captures the structure of the association patterns in that data set . Because support envelopes are not encumbered by a support threshold , this support lattice provides a complete view of the association structure of the data set , including association patterns that have low support . Furthermore , the boundary of the support lattice -- the support boundary -- has at most min M , N envelopes and is especially interesting since it bounds the maximum sizes of potential association patterns -- not only for frequent , closed , and maximal itemsets , but also for patterns , such as error-tolerant itemsets , that are more general . The association structure can be represented graphically as a two-dimensional scatter plot of the m , n values associated with the support envelopes of the data set , a feature that is useful in the exploratory analysis of association patterns . Finally , the algorithm to compute support envelopes is simple and computationally efficient , and it is straightforward to parallelize the process of finding all the support envelopes . [[EENNDD]] association analysis; formal concept analysis; error-tolerant itemsets; support envelope"}, "Sampul sokongan: teknik untuk meneroka struktur corak pergaulan Kertas ini memperkenalkan sampul sokongan - alat baru untuk menganalisis corak hubungan - dan menggambarkan beberapa sifat, aplikasi, dan kemungkinan peluasannya. Secara khusus, sampul sokongan untuk kumpulan data transaksi dan pasangan bilangan bulat positif m, n terdiri daripada item dan transaksi yang perlu dicari untuk mencari corak perkaitan yang melibatkan transaksi m atau lebih dan item n atau lebih. Untuk sebarang set data transaksi dengan urus niaga M dan item N, terdapat kisi unik paling banyak sampul sokongan M \\ * N yang menangkap struktur corak hubungan dalam kumpulan data tersebut. Oleh kerana sampul sokongan tidak dibebani oleh ambang sokongan, kisi sokongan ini memberikan gambaran lengkap mengenai struktur perkaitan set data, termasuk corak hubungan yang mempunyai sokongan rendah. Selanjutnya, batas kisi sokongan - sempadan sokongan - mempunyai selimut minimum M, N dan sangat menarik kerana ia menghadkan ukuran maksimum corak hubungan berpotensi - bukan hanya untuk kumpulan barang yang kerap, tertutup, dan maksimum, tetapi juga untuk corak, seperti set item yang bertolak ansur, yang lebih umum. Struktur perkaitan dapat ditunjukkan secara grafik sebagai plot penyebaran dua dimensi dari nilai m, n yang berkaitan dengan sampul sokongan kumpulan data, satu ciri yang berguna dalam analisis eksplorasi corak persatuan. Akhirnya, algoritma untuk mengira sampul sokongan adalah mudah dan cekap dalam pengiraan, dan mudah untuk menyelaraskan proses mencari semua sampul sokongan. [[EENNDD]] analisis persatuan; analisis konsep formal; set item yang bertolak ansur; sampul surat sokongan"], [{"string": "Hypergraph spectral learning for multi-label classification A hypergraph is a generalization of the traditional graph in which the edges are arbitrary non-empty subsets of the vertex set . It has been applied successfully to capture high-order relations in various domains . In this paper , we propose a hypergraph spectral learning formulation for multi-label classification , where a hypergraph is constructed to exploit the correlation information among different labels . We show that the proposed formulation leads to an eigenvalue problem , which may be computationally expensive especially for large-scale problems . To reduce the computational cost , we propose an approximate formulation , which is shown to be equivalent to a least squares problem under a mild condition . Based on the approximate formulation , efficient algorithms for solving least squares problems can be applied to scale the formulation to very large data sets . In addition , existing regularization techniques for least squares can be incorporated into the model for improved generalization performance . We have conducted experiments using large-scale benchmark data sets , and experimental results show that the proposed hypergraph spectral learning formulation is effective in capturing the high-order relations in multi-label problems . Results also indicate that the approximate formulation is much more efficient than the original one , while keeping competitive classification performance .", "keywords": ["spectral learning", "multi-label classification", "regularization", "hypergraph", "least squares", "efficiency", "canonical correlation analysis"], "combined": "Hypergraph spectral learning for multi-label classification A hypergraph is a generalization of the traditional graph in which the edges are arbitrary non-empty subsets of the vertex set . It has been applied successfully to capture high-order relations in various domains . In this paper , we propose a hypergraph spectral learning formulation for multi-label classification , where a hypergraph is constructed to exploit the correlation information among different labels . We show that the proposed formulation leads to an eigenvalue problem , which may be computationally expensive especially for large-scale problems . To reduce the computational cost , we propose an approximate formulation , which is shown to be equivalent to a least squares problem under a mild condition . Based on the approximate formulation , efficient algorithms for solving least squares problems can be applied to scale the formulation to very large data sets . In addition , existing regularization techniques for least squares can be incorporated into the model for improved generalization performance . We have conducted experiments using large-scale benchmark data sets , and experimental results show that the proposed hypergraph spectral learning formulation is effective in capturing the high-order relations in multi-label problems . Results also indicate that the approximate formulation is much more efficient than the original one , while keeping competitive classification performance . [[EENNDD]] spectral learning; multi-label classification; regularization; hypergraph; least squares; efficiency; canonical correlation analysis"}, "Pembelajaran spektrum hipergraf untuk klasifikasi pelbagai label Sebuah hipergraf adalah generalisasi graf tradisional di mana pinggirnya adalah subset sewenang-wenang yang tidak kosong dari set bucu. Telah berjaya diterapkan untuk menangkap hubungan pesanan tinggi dalam pelbagai domain. Dalam makalah ini, kami mengusulkan rumusan pembelajaran spektrum hipergrafi untuk klasifikasi pelbagai label, di mana hipergraf dibina untuk mengeksploitasi maklumat korelasi antara label yang berbeza. Kami menunjukkan bahawa rumusan yang dicadangkan membawa kepada masalah nilai eigen, yang mungkin mahal secara komputasi terutama untuk masalah skala besar. Untuk mengurangkan kos pengiraan, kami mencadangkan rumusan yang diperkirakan setara dengan masalah kuasa dua dengan keadaan ringan. Berdasarkan perumusan anggaran, algoritma yang cekap untuk menyelesaikan masalah kuadrat terkecil dapat diterapkan untuk skala formulasi ke set data yang sangat besar. Di samping itu, teknik regulasi yang ada untuk kotak paling sedikit dapat dimasukkan ke dalam model untuk peningkatan prestasi generalisasi. Kami telah menjalankan eksperimen dengan menggunakan set data penanda aras berskala besar, dan hasil eksperimen menunjukkan bahawa rumusan pembelajaran spektrum hipergrafi yang dicadangkan berkesan dalam menangkap hubungan pesanan tinggi dalam masalah berbilang label. Hasilnya juga menunjukkan bahawa rumusan anggaran jauh lebih efisien daripada yang asli, sambil mengekalkan prestasi klasifikasi yang kompetitif. [[EENNDD]] pembelajaran spektrum; klasifikasi pelbagai label; pengaturcaraan; hipergraf; petak paling sedikit; kecekapan; analisis korelasi kanonik"], [{"string": "A general framework for accurate and fast regression by data summarization in random decision trees Predicting the values of continuous variable as a function of several independent variables is one of the most important problems for data mining . A very large number of regression methods , both parametric and nonparametric , have been proposed in the past . However , since the list is quite extensive and many of these models make rather explicit , strong yet different assumptions about the type of applicable problems and involve a lot of parameters and options , choosing the appropriate regression methodology and then specifying the parameter values is a none-trivial , sometimes frustrating , task for data mining practitioners . Choosing the inappropriate methodology can have rather disappointing results . This issue is against the general utility of data mining software . For example , linear regression methods are straightforward and well-understood . However , since the linear assumption is very strong , its performance is compromised for complicated non-linear problems . Kernel-based methods perform quite well if the kernel functions are selected correctly . In this paper , we propose a straightforward approach based on summarizing the training data using an ensemble of random decisions trees . It requires very little knowledge from the user , yet is applicable to every type of regression problem that we are currently aware of . We have experimented on a wide range of problems including those that parametric methods performwell , a large selection of benchmark datasets for nonparametric regression , as well as highly non-linear stochastic problems . Our results are either significantly better than or identical to many approaches that are known to perform well on these problems .", "keywords": ["regression", "decision trees", "random"], "combined": "A general framework for accurate and fast regression by data summarization in random decision trees Predicting the values of continuous variable as a function of several independent variables is one of the most important problems for data mining . A very large number of regression methods , both parametric and nonparametric , have been proposed in the past . However , since the list is quite extensive and many of these models make rather explicit , strong yet different assumptions about the type of applicable problems and involve a lot of parameters and options , choosing the appropriate regression methodology and then specifying the parameter values is a none-trivial , sometimes frustrating , task for data mining practitioners . Choosing the inappropriate methodology can have rather disappointing results . This issue is against the general utility of data mining software . For example , linear regression methods are straightforward and well-understood . However , since the linear assumption is very strong , its performance is compromised for complicated non-linear problems . Kernel-based methods perform quite well if the kernel functions are selected correctly . In this paper , we propose a straightforward approach based on summarizing the training data using an ensemble of random decisions trees . It requires very little knowledge from the user , yet is applicable to every type of regression problem that we are currently aware of . We have experimented on a wide range of problems including those that parametric methods performwell , a large selection of benchmark datasets for nonparametric regression , as well as highly non-linear stochastic problems . Our results are either significantly better than or identical to many approaches that are known to perform well on these problems . [[EENNDD]] regression; decision trees; random"}, "Kerangka umum untuk regresi yang tepat dan cepat dengan ringkasan data di pohon keputusan rawak Meramalkan nilai pemboleh ubah berterusan sebagai fungsi beberapa pemboleh ubah bebas adalah salah satu masalah yang paling penting untuk perlombongan data. Sebilangan besar kaedah regresi, baik parametrik dan nonparametrik, telah dicadangkan pada masa lalu. Walau bagaimanapun, kerana senarai ini cukup luas dan banyak model ini membuat andaian yang agak eksplisit, kuat tetapi berbeza mengenai jenis masalah yang berlaku dan melibatkan banyak parameter dan pilihan, memilih metodologi regresi yang sesuai dan kemudian menentukan nilai parameter tidak ada -tugas yang remeh, kadang-kadang mengecewakan, bagi pengamal perlombongan data. Memilih metodologi yang tidak sesuai boleh memberikan hasil yang agak mengecewakan. Masalah ini bertentangan dengan utiliti umum perisian perlombongan data. Contohnya, kaedah regresi linear adalah mudah dan difahami dengan baik. Walau bagaimanapun, kerana anggapan linear sangat kuat, kinerjanya terganggu kerana masalah bukan linear yang rumit. Kaedah berasaskan kernel berfungsi dengan baik jika fungsi kernel dipilih dengan betul. Dalam makalah ini, kami mengusulkan pendekatan langsung berdasarkan meringkaskan data latihan menggunakan ensemble pohon keputusan rawak. Ini memerlukan pengetahuan yang sangat sedikit dari pengguna, namun dapat diterapkan pada setiap jenis masalah regresi yang saat ini kita sedari. Kami telah bereksperimen pada berbagai masalah termasuk yang menggunakan kaedah parametrik, pilihan set data penanda aras untuk regresi nonparametrik, dan juga masalah stokastik yang sangat tidak linear. Hasil kami sama ada lebih baik daripada atau sama dengan banyak pendekatan yang diketahui menunjukkan prestasi yang baik terhadap masalah ini. [[EENNDD]] regresi; pokok keputusan; rawak"], [{"string": "Eigenspace-based anomaly detection in computer systems We report on an automated runtime anomaly detection method at the application layer of multi-node computer systems . Although several network management systems are available in the market , none of them have sufficient capabilities to detect faults in multi-tier Web-based systems with redundancy . We model a Web-based system as a weighted graph , where each node represents a `` service '' and each edge represents a dependency between services . Since the edge weights vary greatly over time , the problem we address is that of anomaly detection from a time sequence of graphs . In our method , we first extract a feature vector from the adjacency matrix that represents the activities of all of the services . The heart of our method is to use the principal eigenvector of the eigenclusters of the graph . Then we derive a probability distribution for an anomaly measure defined for a time-series of directional data derived from the graph sequence . Given a critical probability , the threshold value is adaptively updated using a novel online algorithm . We demonstrate that a fault in a Web application can be automatically detected and the faulty services are identified without using detailed knowledge of the behavior of the system .", "keywords": ["time sequence of graphs", "singular value decomposition", "learning", "principal eigenvector", "perron-frobenius theorem", "von mises-fisher distribution"], "combined": "Eigenspace-based anomaly detection in computer systems We report on an automated runtime anomaly detection method at the application layer of multi-node computer systems . Although several network management systems are available in the market , none of them have sufficient capabilities to detect faults in multi-tier Web-based systems with redundancy . We model a Web-based system as a weighted graph , where each node represents a `` service '' and each edge represents a dependency between services . Since the edge weights vary greatly over time , the problem we address is that of anomaly detection from a time sequence of graphs . In our method , we first extract a feature vector from the adjacency matrix that represents the activities of all of the services . The heart of our method is to use the principal eigenvector of the eigenclusters of the graph . Then we derive a probability distribution for an anomaly measure defined for a time-series of directional data derived from the graph sequence . Given a critical probability , the threshold value is adaptively updated using a novel online algorithm . We demonstrate that a fault in a Web application can be automatically detected and the faulty services are identified without using detailed knowledge of the behavior of the system . [[EENNDD]] time sequence of graphs; singular value decomposition; learning; principal eigenvector; perron-frobenius theorem; von mises-fisher distribution"}, "Pengesanan anomali berasaskan Eigenspace dalam sistem komputer Kami melaporkan kaedah pengesanan anomali runtime automatik pada lapisan aplikasi sistem komputer multi-node. Walaupun beberapa sistem pengurusan rangkaian tersedia di pasaran, tidak satu pun dari mereka memiliki kemampuan yang cukup untuk mengesan kesalahan dalam sistem berasaskan Web pelbagai peringkat dengan kelebihan. Kami memodelkan sistem berasaskan Web sebagai grafik berwajaran, di mana setiap simpul mewakili \"perkhidmatan\" dan setiap kelebihan mewakili ketergantungan antara perkhidmatan. Oleh kerana berat tepi sangat berbeza dari masa ke masa, masalah yang kita hadapi adalah pengesanan anomali dari urutan masa grafik. Dalam kaedah kami, pertama-tama kami mengekstrak vektor ciri dari matriks adjacency yang mewakili aktiviti semua perkhidmatan. Inti kaedah kami adalah dengan menggunakan eigenvector utama eigenclusters grafik. Kemudian kami memperoleh sebaran kebarangkalian untuk ukuran anomali yang ditentukan untuk siri masa data arah yang berasal dari urutan grafik. Memandangkan kebarangkalian kritikal, nilai ambang dikemas kini secara adaptif menggunakan algoritma dalam talian baru. Kami menunjukkan bahawa kesalahan dalam aplikasi Web dapat dikesan secara automatik dan perkhidmatan yang salah dikenal pasti tanpa menggunakan pengetahuan terperinci mengenai tingkah laku sistem. [[EENNDD]] urutan masa graf; penguraian nilai tunggal; belajar; eigenvector utama; teorema perron-frobenius; pengedaran von mises-Fisher"], [{"string": "Short term performance forecasting in enterprise systems We use data mining and machine learning techniques to predict upcoming periods of high utilization or poor performance in enterprise systems . The abundant data available and complexity of these systems defies human characterization or static models and makes the task suitable for data mining techniques . We formulate the problem as one of classification : given current and past information about the system 's behavior , can we forecast whether the system will meet its performance targets over the next hour ? Using real data gathered from several enterprise systems in Hewlett-Packard , we compare several approaches ranging from time series to Bayesian networks . Besides establishing the predictive power of these approaches our study analyzes three dimensions that are important for their application as a stand alone tool . First , it quantifies the gain in accuracy of multivariate prediction methods over simple statistical univariate methods . Second , it quantifies the variations in accuracy when using different classes of system and workload features . Third , it establishes that models induced using combined data from various systems generalize well and are applicable to new systems , enabling accurate predictions on systems with insufficient historical data . Together this analysis offers a promising outlook on the development of tools to automate assignment of resources to stabilize performance , e.g. , adding servers to a cluster and allow opportunistic job scheduling e.g. , backups or virus scans .", "keywords": ["enterprise systems", "system management", "performance forecasting"], "combined": "Short term performance forecasting in enterprise systems We use data mining and machine learning techniques to predict upcoming periods of high utilization or poor performance in enterprise systems . The abundant data available and complexity of these systems defies human characterization or static models and makes the task suitable for data mining techniques . We formulate the problem as one of classification : given current and past information about the system 's behavior , can we forecast whether the system will meet its performance targets over the next hour ? Using real data gathered from several enterprise systems in Hewlett-Packard , we compare several approaches ranging from time series to Bayesian networks . Besides establishing the predictive power of these approaches our study analyzes three dimensions that are important for their application as a stand alone tool . First , it quantifies the gain in accuracy of multivariate prediction methods over simple statistical univariate methods . Second , it quantifies the variations in accuracy when using different classes of system and workload features . Third , it establishes that models induced using combined data from various systems generalize well and are applicable to new systems , enabling accurate predictions on systems with insufficient historical data . Together this analysis offers a promising outlook on the development of tools to automate assignment of resources to stabilize performance , e.g. , adding servers to a cluster and allow opportunistic job scheduling e.g. , backups or virus scans . [[EENNDD]] enterprise systems; system management; performance forecasting"}, "Ramalan prestasi jangka pendek dalam sistem perusahaan Kami menggunakan teknik perlombongan data dan pembelajaran mesin untuk meramalkan jangka masa penggunaan yang tinggi atau prestasi yang buruk dalam sistem perusahaan. Banyak data yang ada dan kerumitan sistem ini menentang pencirian manusia atau model statik dan menjadikan tugas itu sesuai untuk teknik perlombongan data. Kami merumuskan masalah sebagai salah satu klasifikasi: mengingat maklumat semasa dan masa lalu mengenai tingkah laku sistem, dapatkah kita meramalkan apakah sistem akan memenuhi target prestasinya selama satu jam berikutnya? Dengan menggunakan data sebenar yang dikumpulkan dari beberapa sistem perusahaan di Hewlett-Packard, kami membandingkan beberapa pendekatan mulai dari siri waktu hingga rangkaian Bayesian. Selain menentukan kekuatan ramalan pendekatan ini, kajian kami menganalisis tiga dimensi yang penting untuk aplikasi mereka sebagai alat yang berdiri sendiri. Pertama, ia mengukur peningkatan ketepatan kaedah ramalan multivariate berbanding kaedah univariat statistik sederhana. Kedua, ia mengukur variasi ketepatan ketika menggunakan kelas yang berbeza dari ciri sistem dan beban kerja. Ketiga, menetapkan bahwa model yang diinduksi menggunakan gabungan data dari berbagai sistem menggeneralisasikan dengan baik dan dapat diterapkan pada sistem baru, memungkinkan ramalan yang tepat pada sistem dengan data sejarah yang tidak mencukupi. Bersama-sama analisis ini menawarkan pandangan yang menjanjikan mengenai pengembangan alat untuk mengautomasikan penugasan sumber untuk menstabilkan prestasi, mis. , menambah pelayan ke kluster dan membenarkan penjadualan pekerjaan oportunistik mis. , sandaran atau imbasan virus. [[EENNDD]] sistem perusahaan; pengurusan sistem; ramalan prestasi"], [{"string": "A streaming ensemble algorithm SEA for large-scale classification Ensemble methods have recently garnered a great deal of attention in the machine learning community . Techniques such as Boosting and Bagging have proven to be highly effective but require repeated resampling of the training data , making them inappropriate in a data mining context . The methods presented in this paper take advantage of plentiful data , building separate classifiers on sequential chunks of training points . These classifiers are combined into a fixed-size ensemble using a heuristic replacement strategy . The result is a fast algorithm for large-scale or streaming data that classifies as well as a single decision tree built on all the data , requires approximately constant memory , and adjusts quickly to concept drift .", "keywords": ["streaming data", "ensemble classification", "database applications"], "combined": "A streaming ensemble algorithm SEA for large-scale classification Ensemble methods have recently garnered a great deal of attention in the machine learning community . Techniques such as Boosting and Bagging have proven to be highly effective but require repeated resampling of the training data , making them inappropriate in a data mining context . The methods presented in this paper take advantage of plentiful data , building separate classifiers on sequential chunks of training points . These classifiers are combined into a fixed-size ensemble using a heuristic replacement strategy . The result is a fast algorithm for large-scale or streaming data that classifies as well as a single decision tree built on all the data , requires approximately constant memory , and adjusts quickly to concept drift . [[EENNDD]] streaming data; ensemble classification; database applications"}, "Algoritma ensemble streaming SEA untuk klasifikasi skala besar Kaedah ensemble baru-baru ini mendapat banyak perhatian dalam komuniti pembelajaran mesin. Teknik seperti Boosting and Bagging telah terbukti sangat berkesan tetapi memerlukan pengambilan sampel data latihan berulang kali, menjadikannya tidak sesuai dalam konteks perlombongan data. Kaedah yang disajikan dalam makalah ini memanfaatkan banyak data, membina pengkelasan yang berasingan pada sekumpulan titik latihan yang berurutan. Pengelasan ini digabungkan menjadi ensemble ukuran tetap menggunakan strategi penggantian heuristik. Hasilnya adalah algoritma cepat untuk data berskala besar atau streaming yang mengklasifikasikan dan juga keputusan tunggal yang dibina berdasarkan semua data, memerlukan kira-kira memori tetap, dan menyesuaikan diri dengan cepat ke konsep konsep. [[EENNDD]] streaming data; klasifikasi ensemble; aplikasi pangkalan data"], [{"string": "Classification features for attack detection in collaborative recommender systems Collaborative recommender systems are highly vulnerable to attack . Attackers can use automated means to inject a large number of biased profiles into such a system , resulting in recommendations that favor or disfavor given items . Since collaborative recommender systems must be open to user input , it is difficult to design a system that can not be so attacked . Researchers studying robust recommendation have therefore begun to identify types of attacks and study mechanisms for recognizing and defeating them . In this paper , we propose and study different attributes derived from user profiles for their utility in attack detection . We show that a machine learning classification approach that includes attributes derived from attack models is more successful than more generalized detection algorithms previously studied .", "keywords": ["systems and software", "collaborative filtering", "information search and retrieval", "learning", "robustness", "recommender systems", "attack detection"], "combined": "Classification features for attack detection in collaborative recommender systems Collaborative recommender systems are highly vulnerable to attack . Attackers can use automated means to inject a large number of biased profiles into such a system , resulting in recommendations that favor or disfavor given items . Since collaborative recommender systems must be open to user input , it is difficult to design a system that can not be so attacked . Researchers studying robust recommendation have therefore begun to identify types of attacks and study mechanisms for recognizing and defeating them . In this paper , we propose and study different attributes derived from user profiles for their utility in attack detection . We show that a machine learning classification approach that includes attributes derived from attack models is more successful than more generalized detection algorithms previously studied . [[EENNDD]] systems and software; collaborative filtering; information search and retrieval; learning; robustness; recommender systems; attack detection"}, "Ciri klasifikasi untuk pengesanan serangan dalam sistem pengesyoran kolaboratif Sistem pengesyoran kolaboratif sangat rentan terhadap serangan. Penyerang dapat menggunakan cara automatik untuk menyuntikkan sebilangan besar profil berat sebelah ke dalam sistem seperti itu, menghasilkan rekomendasi yang memihak atau tidak menyukai item yang diberikan. Oleh kerana sistem pengesyoran kolaboratif mesti terbuka untuk input pengguna, sukar untuk merancang sistem yang tidak dapat diserang begitu. Oleh itu, penyelidik yang mengkaji cadangan yang kuat telah mula mengenal pasti jenis serangan dan mengkaji mekanisme untuk mengenali dan mengalahkannya. Dalam makalah ini, kami mencadangkan dan mengkaji atribut yang berbeza yang berasal dari profil pengguna untuk kegunaannya dalam pengesanan serangan. Kami menunjukkan bahawa pendekatan klasifikasi pembelajaran mesin yang merangkumi atribut yang berasal dari model serangan lebih berjaya daripada algoritma pengesanan yang lebih umum yang dikaji sebelumnya. [[EENNDD]] sistem dan perisian; penapisan kolaboratif; carian dan pengambilan maklumat; belajar; ketahanan; sistem pengesyorkan; pengesanan serangan"], [{"string": "Document preprocessing for naive Bayes classification and clustering with mixture of multinomials Naive Bayes classifier has long been used for text categorization tasks . Its sibling from the unsupervised world , the probabilistic mixture of multinomial models , has likewise been successfully applied to text clustering problems . Despite the strong independence assumptions that these models make , their attractiveness come from low computational cost , relatively low memory consumption , ability to handle heterogeneous features and multiple classes , and often competitiveness with the top of the line models . Recently , there has been several attempts to alleviate the problems of Naive Bayes by performing heuristic feature transformations , such as IDF , normalization by the length of the documents and taking the logarithms of the counts . We justify the use of these techniques and apply them to two problems : classification of products in Yahoo ! Shopping and clustering the vectors of collocated terms in user queries to Yahoo ! Search . The experimental evaluation allows us to draw conclusions about the promise that these transformations carry with regard to alleviating the strong assumptions of the multinomial model .", "keywords": ["data transformations", "naive bayes", "classification", "learning", "mixture of multinomials", "clustering"], "combined": "Document preprocessing for naive Bayes classification and clustering with mixture of multinomials Naive Bayes classifier has long been used for text categorization tasks . Its sibling from the unsupervised world , the probabilistic mixture of multinomial models , has likewise been successfully applied to text clustering problems . Despite the strong independence assumptions that these models make , their attractiveness come from low computational cost , relatively low memory consumption , ability to handle heterogeneous features and multiple classes , and often competitiveness with the top of the line models . Recently , there has been several attempts to alleviate the problems of Naive Bayes by performing heuristic feature transformations , such as IDF , normalization by the length of the documents and taking the logarithms of the counts . We justify the use of these techniques and apply them to two problems : classification of products in Yahoo ! Shopping and clustering the vectors of collocated terms in user queries to Yahoo ! Search . The experimental evaluation allows us to draw conclusions about the promise that these transformations carry with regard to alleviating the strong assumptions of the multinomial model . [[EENNDD]] data transformations; naive bayes; classification; learning; mixture of multinomials; clustering"}, "Pemrosesan dokumen untuk klasifikasi dan pengelompokan Bayes naif dengan campuran multinomial Pengkelasan Naive Bayes telah lama digunakan untuk tugas pengkategorian teks. Saudara kandungnya dari dunia yang tidak diawasi, campuran probabilistik model multinomial, juga berjaya diterapkan pada masalah pengelompokan teks. Walaupun terdapat andaian kemandirian yang kuat yang dibuat oleh model-model ini, daya tarikan mereka berasal dari kos pengiraan yang rendah, penggunaan memori yang agak rendah, keupayaan untuk menangani ciri-ciri heterogen dan pelbagai kelas, dan sering kali bersaing dengan model-model teratas. Baru-baru ini, terdapat beberapa usaha untuk meringankan masalah Naive Bayes dengan melakukan transformasi ciri heuristik, seperti IDF, normalisasi dengan panjang dokumen dan mengambil logaritma pengiraan. Kami membenarkan penggunaan teknik ini dan menerapkannya pada dua masalah: pengelasan produk di Yahoo! Membeli-belah dan mengumpulkan vektor istilah gabungan dalam pertanyaan pengguna ke Yahoo! Cari. Penilaian eksperimen membolehkan kita membuat kesimpulan mengenai janji yang dilakukan oleh transformasi ini untuk mengurangkan andaian kuat dari model multinomial. [[EENNDD]] transformasi data; bayes naif; pengelasan; belajar; campuran multinomial; pengelompokan"], [{"string": "Dimension induced clustering It is commonly assumed that high-dimensional datasets contain points most of which are located in low-dimensional manifolds . Detection of low-dimensional clusters is an extremely useful task for performing operations such as clustering and classification , however , it is a challenging computational problem . In this paper we study the problem of finding subsets of points with low intrinsic dimensionality . Our main contribution is to extend the definition of fractal correlation dimension , which measures average volume growth rate , in order to estimate the intrinsic dimensionality of the data in local neighborhoods . We provide a careful analysis of several key examples in order to demonstrate the properties of our measure . Based on our proposed measure , we introduce a novel approach to discover clusters with low dimensionality . The resulting algorithms extend previous density based measures , which have been successfully used for clustering . We demonstrate the effectiveness of our algorithms for discovering low-dimensional m-flats embedded in high dimensional spaces , and for detecting low-rank sub-matrices .", "keywords": ["miscellaneous", "clustering", "fractal dimension"], "combined": "Dimension induced clustering It is commonly assumed that high-dimensional datasets contain points most of which are located in low-dimensional manifolds . Detection of low-dimensional clusters is an extremely useful task for performing operations such as clustering and classification , however , it is a challenging computational problem . In this paper we study the problem of finding subsets of points with low intrinsic dimensionality . Our main contribution is to extend the definition of fractal correlation dimension , which measures average volume growth rate , in order to estimate the intrinsic dimensionality of the data in local neighborhoods . We provide a careful analysis of several key examples in order to demonstrate the properties of our measure . Based on our proposed measure , we introduce a novel approach to discover clusters with low dimensionality . The resulting algorithms extend previous density based measures , which have been successfully used for clustering . We demonstrate the effectiveness of our algorithms for discovering low-dimensional m-flats embedded in high dimensional spaces , and for detecting low-rank sub-matrices . [[EENNDD]] miscellaneous; clustering; fractal dimension"}, "Pengelompokan dimensi dimensi Umumnya diasumsikan bahawa set data dimensi tinggi mengandungi titik yang kebanyakannya terletak dalam manifold dimensi rendah. Pengesanan kelompok dimensi rendah adalah tugas yang sangat berguna untuk melakukan operasi seperti pengelompokan dan klasifikasi, namun, ini adalah masalah komputasi yang mencabar. Dalam makalah ini kita mengkaji masalah mencari subset titik dengan dimensi intrinsik rendah. Sumbangan utama kami adalah untuk memperluas definisi dimensi korelasi fraktal, yang mengukur kadar pertumbuhan jumlah purata, untuk menganggarkan dimensi intrinsik data di kawasan sekitar. Kami memberikan analisis yang teliti terhadap beberapa contoh utama untuk menunjukkan sifat ukuran kami. Berdasarkan ukuran yang dicadangkan, kami memperkenalkan pendekatan baru untuk menemui kelompok dengan dimensi rendah. Algoritma yang dihasilkan memperluas ukuran berdasarkan kepadatan sebelumnya, yang telah berjaya digunakan untuk pengelompokan. Kami menunjukkan keberkesanan algoritma kami untuk menemui m-flat dimensi rendah yang tertanam di ruang dimensi tinggi, dan untuk mengesan sub-matriks peringkat rendah. [[EENNDD]] pelbagai; pengelompokan; dimensi fraktal"], [{"string": "Large linear classification when data can not fit in memory Recent advances in linear classification have shown that for applications such as document classification , the training can be extremely efficient . However , most of the existing training methods are designed by assuming that data can be stored in the computer memory . These methods can not be easily applied to data larger than the memory capacity due to the random access to the disk . We propose and analyze a block minimization framework for data larger than the memory size . At each step a block of data is loaded from the disk and handled by certain learning methods . We investigate two implementations of the proposed framework for primal and dual SVMs , respectively . As data can not fit in memory , many design considerations are very different from those for traditional algorithms . Experiments using data sets 20 times larger than the memory demonstrate the effectiveness of the proposed method .", "keywords": ["large scale learning", "svm", "block minimization"], "combined": "Large linear classification when data can not fit in memory Recent advances in linear classification have shown that for applications such as document classification , the training can be extremely efficient . However , most of the existing training methods are designed by assuming that data can be stored in the computer memory . These methods can not be easily applied to data larger than the memory capacity due to the random access to the disk . We propose and analyze a block minimization framework for data larger than the memory size . At each step a block of data is loaded from the disk and handled by certain learning methods . We investigate two implementations of the proposed framework for primal and dual SVMs , respectively . As data can not fit in memory , many design considerations are very different from those for traditional algorithms . Experiments using data sets 20 times larger than the memory demonstrate the effectiveness of the proposed method . [[EENNDD]] large scale learning; svm; block minimization"}, "Klasifikasi linear besar apabila data tidak dapat dimasukkan ke dalam memori Kemajuan terkini dalam klasifikasi linear menunjukkan bahawa untuk aplikasi seperti klasifikasi dokumen, latihan dapat sangat efisien. Walau bagaimanapun, kebanyakan kaedah latihan yang ada dirancang dengan mengandaikan bahawa data dapat disimpan dalam memori komputer. Kaedah ini tidak dapat diterapkan dengan mudah pada data yang lebih besar daripada kapasitas memori kerana akses rawak ke disk. Kami mencadangkan dan menganalisis kerangka kerja pengurangan blok untuk data yang lebih besar daripada ukuran memori. Pada setiap langkah, sekumpulan data dimuat dari cakera dan dikendalikan dengan kaedah pembelajaran tertentu. Kami menyelidiki dua pelaksanaan kerangka kerja yang dicadangkan untuk SVM primal dan ganda. Oleh kerana data tidak dapat dimuatkan dalam memori, banyak pertimbangan reka bentuk sangat berbeza dengan data algoritma tradisional. Eksperimen menggunakan set data 20 kali lebih besar daripada memori menunjukkan keberkesanan kaedah yang dicadangkan. [[EENNDD]] pembelajaran berskala besar; svm; pengurangan blok"], [{"string": "Bypass rates : reducing query abandonment using negative inferences We introduce a new approach to analyzing click logs by examining both the documents that are clicked and those that are bypassed-documents returned higher in the ordering of the search results but skipped by the user . This approach complements the popular click-through rate analysis , and helps to draw negative inferences in the click logs . We formulate a natural objective that finds sets of results that are unlikely to be collectively bypassed by a typical user . This is closely related to the problem of reducing query abandonment . We analyze a greedy approach to optimizing this objective , and establish theoretical guarantees of its performance . We evaluate our approach on a large set of queries , and demonstrate that it compares favorably to the maximal marginal relevance approach on a number of metrics including mean average precision and mean reciprocal rank .", "keywords": ["bypass rates", "query abandonment", "random walk", "similarity search", "relevance"], "combined": "Bypass rates : reducing query abandonment using negative inferences We introduce a new approach to analyzing click logs by examining both the documents that are clicked and those that are bypassed-documents returned higher in the ordering of the search results but skipped by the user . This approach complements the popular click-through rate analysis , and helps to draw negative inferences in the click logs . We formulate a natural objective that finds sets of results that are unlikely to be collectively bypassed by a typical user . This is closely related to the problem of reducing query abandonment . We analyze a greedy approach to optimizing this objective , and establish theoretical guarantees of its performance . We evaluate our approach on a large set of queries , and demonstrate that it compares favorably to the maximal marginal relevance approach on a number of metrics including mean average precision and mean reciprocal rank . [[EENNDD]] bypass rates; query abandonment; random walk; similarity search; relevance"}, "Bypass rate: mengurangkan pengabaian pertanyaan menggunakan inferens negatif Kami memperkenalkan pendekatan baru untuk menganalisis log klik dengan memeriksa kedua-dua dokumen yang diklik dan dokumen yang dipintas-dikembalikan lebih tinggi dalam susunan hasil carian tetapi dilangkau oleh pengguna. Pendekatan ini melengkapkan analisis kadar klik-tayang yang popular, dan membantu membuat kesimpulan negatif dalam log klik. Kami merumuskan objektif semula jadi yang menemui set hasil yang tidak mungkin dilewati secara kolektif oleh pengguna biasa. Ini berkait rapat dengan masalah mengurangkan pengabaian pertanyaan. Kami menganalisis pendekatan tamak untuk mengoptimumkan objektif ini, dan menetapkan jaminan teoritikal terhadap prestasinya. Kami menilai pendekatan kami pada sejumlah besar pertanyaan, dan menunjukkan bahawa ia membandingkan dengan pendekatan relevan marginal maksimum pada sejumlah metrik termasuk ketepatan rata-rata rata-rata dan peringkat timbal balik rata-rata. [[EENNDD]] kadar pintasan; pengabaian pertanyaan; jalan rawak; carian kesamaan; kesesuaian"], [{"string": "A hybrid unsupervised approach for document clustering We propose a hybrid , unsupervised document clustering approach that combines a hierarchical clustering algorithm with Expectation Maximization . We developed several heuristics to automatically select a subset of the clusters generated by the first algorithm as the initial points of the second one . Furthermore , our initialization algorithm generates not only an initial model for the iterative refinement algorithm but also an estimate of the model dimension , thus eliminating another important element of human supervision . We have evaluated the proposed system on five real-world document collections . The results show that our approach generates clustering solutions of higher quality than both its individual components .", "keywords": ["information search and retrieval", "em initialization", "unsupervised clustering"], "combined": "A hybrid unsupervised approach for document clustering We propose a hybrid , unsupervised document clustering approach that combines a hierarchical clustering algorithm with Expectation Maximization . We developed several heuristics to automatically select a subset of the clusters generated by the first algorithm as the initial points of the second one . Furthermore , our initialization algorithm generates not only an initial model for the iterative refinement algorithm but also an estimate of the model dimension , thus eliminating another important element of human supervision . We have evaluated the proposed system on five real-world document collections . The results show that our approach generates clustering solutions of higher quality than both its individual components . [[EENNDD]] information search and retrieval; em initialization; unsupervised clustering"}, "Pendekatan hibrid tanpa pengawasan untuk pengelompokan dokumen Kami mencadangkan pendekatan pengelompokan dokumen hibrid tanpa pengawasan yang menggabungkan algoritma pengelompokan hierarki dengan Maksimum Pengharapan. Kami mengembangkan beberapa heuristik untuk memilih subkumpulan kelompok yang dihasilkan oleh algoritma pertama secara automatik sebagai titik awal yang kedua. Selanjutnya, algoritma inisialisasi kami menghasilkan bukan sahaja model awal untuk algoritma penyempurnaan berulang tetapi juga anggaran dimensi model, sehingga menghilangkan satu lagi elemen penting pengawasan manusia. Kami telah menilai sistem yang dicadangkan pada lima koleksi dokumen dunia nyata. Hasilnya menunjukkan bahawa pendekatan kami menghasilkan penyelesaian pengelompokan dengan kualiti yang lebih tinggi daripada kedua-dua komponennya. [[EENNDD]] carian dan pengambilan maklumat; inisialisasi em; pengelompokan tanpa pengawasan"], [{"string": "Distributed data mining in a chain store database of short transactions In this paper , we broaden the horizon of traditional rule mining by introducing a new framework of causality rule mining in a distributed chain store database . Specifically , the causality rule explored in this paper consists of a sequence of triggering events and a set of consequential events , and is designed with the capability of mining non-sequential , inter-transaction information . Hence , the causality rule mining provides a very general framework for rule derivation . Note , however , that the procedure of causality rule mining is very costly particularly in the presence of a huge number of candidate sets and a distributed database , and in our opinion , can not be dealt with by direct extensions from existing rule mining methods . Consequently , we devise in this paper a series of level matching algorithms , including Level Matching abbreviatedly as LM , Level Matching with Selective Scan abbreviatedly as LMS , and Distributed Level Matching abbreviatedly as Distibuted LM , to minimize the computing cost needed for the distributed data mining of causality rules . In addition , the phenomena of time window constraints are also taken into consideration for the development of our algorithms . As a result of properly employing the technologies of level matching and selective scan , the proposed algorithms present good efficiency and scalability in the mining of local and global causality rules . Scale-up experiments show that the proposed algorithms scale well with the number of sites and the number of customer transactions . Index Terms : knowledge discovery , distributed data mining causality rules , triggering events , consequential events", "keywords": ["deduction", "representations", "number-theoretic computations"], "combined": "Distributed data mining in a chain store database of short transactions In this paper , we broaden the horizon of traditional rule mining by introducing a new framework of causality rule mining in a distributed chain store database . Specifically , the causality rule explored in this paper consists of a sequence of triggering events and a set of consequential events , and is designed with the capability of mining non-sequential , inter-transaction information . Hence , the causality rule mining provides a very general framework for rule derivation . Note , however , that the procedure of causality rule mining is very costly particularly in the presence of a huge number of candidate sets and a distributed database , and in our opinion , can not be dealt with by direct extensions from existing rule mining methods . Consequently , we devise in this paper a series of level matching algorithms , including Level Matching abbreviatedly as LM , Level Matching with Selective Scan abbreviatedly as LMS , and Distributed Level Matching abbreviatedly as Distibuted LM , to minimize the computing cost needed for the distributed data mining of causality rules . In addition , the phenomena of time window constraints are also taken into consideration for the development of our algorithms . As a result of properly employing the technologies of level matching and selective scan , the proposed algorithms present good efficiency and scalability in the mining of local and global causality rules . Scale-up experiments show that the proposed algorithms scale well with the number of sites and the number of customer transactions . Index Terms : knowledge discovery , distributed data mining causality rules , triggering events , consequential events [[EENNDD]] deduction; representations; number-theoretic computations"}, "Perlombongan data yang diedarkan dalam pangkalan data kedai rantai transaksi pendek Dalam makalah ini, kami memperluas cakrawala perlombongan peraturan tradisional dengan memperkenalkan kerangka baru perlombongan peraturan kausalitas dalam pangkalan data kedai rantai tersebar. Secara khusus, aturan kausalitas yang dieksplorasi dalam makalah ini terdiri dari urutan peristiwa yang memicu dan sekumpulan peristiwa yang berurutan, dan dirancang dengan kemampuan untuk melombong maklumat yang tidak berurutan, antara transaksi. Oleh itu, perlombongan peraturan sebab-akibat menyediakan kerangka kerja yang sangat umum untuk pembuatan peraturan. Walau bagaimanapun, perhatikan bahawa prosedur perlombongan peraturan kausalitas sangat mahal terutama dengan adanya sejumlah besar calon dan pangkalan data yang diedarkan, dan pada pendapat kami, tidak dapat ditangani dengan penyambungan langsung dari kaedah perlombongan peraturan yang ada. Oleh yang demikian, kami menyusun dalam makalah ini serangkaian algoritma pemadanan tahap, termasuk Pencocokan Tingkat disingkat sebagai LM, Pencocokan Tingkat dengan Pemindaian Selektif disingkat sebagai LMS, dan Pencocokan Tingkat Terdistribusi disingkat sebagai LM Terdistribusi, untuk meminimumkan kos pengkomputeran yang diperlukan untuk perlombongan data yang diedarkan. peraturan sebab-akibat. Di samping itu, fenomena kekangan tetingkap masa juga diambil kira untuk pengembangan algoritma kami. Sebagai hasil penggunaan teknologi pemadanan tahap dan pemindaian selektif dengan betul, algoritma yang dicadangkan menunjukkan kecekapan dan skalabilitas yang baik dalam perlombongan peraturan sebab-akibat tempatan dan global. Eksperimen penambahbaikan menunjukkan bahawa algoritma yang dicadangkan sesuai dengan jumlah laman web dan jumlah transaksi pelanggan. Syarat Indeks: penemuan pengetahuan, peraturan kausaliti perlombongan data yang diedarkan, peristiwa pencetus, pemotongan peristiwa seterusnya [[EENNDD]]; perwakilan; pengiraan nombor-teori"], [{"string": "Constraint-driven clustering Clustering methods can be either data-driven or need-driven . Data-driven methods intend to discover the true structure of the underlying data while need-driven methods aims at organizing the true structure to meet certain application requirements . Thus , need-driven e.g. constrained clustering is able to find more useful and actionable clusters in applications such as energy aware sensor networks , privacy preservation , and market segmentation . However , the existing methods of constrained clustering require users to provide the number of clusters , which is often unknown in advance , but has a crucial impact on the clustering result . In this paper , we argue that a more natural way to generate actionable clusters is to let the application-specific constraints decide the number of clusters . For this purpose , we introduce a novel cluster model , Constraint-Driven Clustering CDC , which finds an a priori unspecified number of compact clusters that satisfy all user-provided constraints . Two general types of constraints are considered , i.e. minimum significance constraints and minimum variance constraints , as well as combinations of these two types . We prove the NP-hardness of the CDC problem with different constraints . We propose a novel dynamic data structure , the CD-Tree , which organizes data points in leaf nodes such that each leaf node approximately satisfies the CDC constraints and minimizes the objective function . Based on CD-Trees , we develop an efficient algorithm to solve the new clustering problem . Our experimental evaluation on synthetic and real datasets demonstrates the quality of the generated clusters and the scalability of the algorithm .", "keywords": ["constraints", "np-hardness", "clustering"], "combined": "Constraint-driven clustering Clustering methods can be either data-driven or need-driven . Data-driven methods intend to discover the true structure of the underlying data while need-driven methods aims at organizing the true structure to meet certain application requirements . Thus , need-driven e.g. constrained clustering is able to find more useful and actionable clusters in applications such as energy aware sensor networks , privacy preservation , and market segmentation . However , the existing methods of constrained clustering require users to provide the number of clusters , which is often unknown in advance , but has a crucial impact on the clustering result . In this paper , we argue that a more natural way to generate actionable clusters is to let the application-specific constraints decide the number of clusters . For this purpose , we introduce a novel cluster model , Constraint-Driven Clustering CDC , which finds an a priori unspecified number of compact clusters that satisfy all user-provided constraints . Two general types of constraints are considered , i.e. minimum significance constraints and minimum variance constraints , as well as combinations of these two types . We prove the NP-hardness of the CDC problem with different constraints . We propose a novel dynamic data structure , the CD-Tree , which organizes data points in leaf nodes such that each leaf node approximately satisfies the CDC constraints and minimizes the objective function . Based on CD-Trees , we develop an efficient algorithm to solve the new clustering problem . Our experimental evaluation on synthetic and real datasets demonstrates the quality of the generated clusters and the scalability of the algorithm . [[EENNDD]] constraints; np-hardness; clustering"}, "Pengelompokan yang didorong oleh kendala Kaedah kluster boleh didorong oleh data atau berdasarkan keperluan. Kaedah berdasarkan data bermaksud untuk mengetahui struktur sebenar data yang mendasari sementara kaedah berdasarkan keperluan bertujuan untuk mengatur struktur sebenar untuk memenuhi keperluan aplikasi tertentu. Oleh itu, didorong oleh keperluan mis. pengelompokan terkendali dapat menemukan kelompok yang lebih berguna dan dapat ditindaklanjuti dalam aplikasi seperti rangkaian sensor yang menyedari tenaga, pemeliharaan privasi, dan segmentasi pasar. Namun, kaedah pengelompokan terkendali yang ada mengharuskan pengguna memberikan jumlah kelompok, yang sering tidak diketahui sebelumnya, tetapi memberi kesan penting pada hasil pengelompokan. Dalam makalah ini, kami berpendapat bahawa cara yang lebih semula jadi untuk menghasilkan kelompok yang dapat ditindaklanjuti adalah dengan membiarkan batasan khusus aplikasi menentukan jumlah kelompok. Untuk tujuan ini, kami memperkenalkan model kluster baru, Constraint-Driven Clustering CDC, yang menemui sebilangan besar kluster kompak yang belum ditentukan sebelumnya yang memenuhi semua kekangan yang disediakan pengguna. Dua jenis kekangan umum dipertimbangkan, iaitu batasan kepentingan minimum dan batasan varians minimum, serta gabungan kedua-dua jenis ini. Kami membuktikan NP-kekerasan masalah CDC dengan kekangan yang berbeza. Kami mencadangkan struktur data dinamik baru, CD-Tree, yang mengatur titik data dalam simpul daun sehingga setiap simpul daun memenuhi batasan CDC dan meminimumkan fungsi objektif. Berdasarkan CD-Trees, kami mengembangkan algoritma yang cekap untuk menyelesaikan masalah pengelompokan baru. Penilaian eksperimental kami pada kumpulan data sintetik dan nyata menunjukkan kualiti kluster yang dihasilkan dan skalabilitas algoritma. [[EENNDD]] kekangan; np-kekerasan; pengelompokan"], [{"string": "Correlating synchronous and asynchronous data streams In a variety of modern mining applications , data are commonly viewed as infinite time ordered data streams rather as finite data sets stored on disk . This view challenges fundamental assumptions commonly made in the context of several data mining algorithms . In this paper , we study the problem of identifying correlations between multiple data streams . In particular , we propose algorithms capable of capturing correlations between multiple continuous data streams in a highly efficient and accurate manner . Our algorithms and techniques are applicable in the case of both synchronous and asynchronous data streaming environments . We capture correlations between multiple streams using the well known technique of Singular Value Decomposition SVD . Correlations between data items , and the SVD technique in particular , have been repeatedly utilized in an off-line non stream data mining problems , for example forecasting , approximate query answering , and data reduction . We propose a methodology based on a combination of dimensionality reduction and sampling to make the SVD technique suitable for a data stream context . Our techniques are approximate , trading accuracy with performance , and we analytically quantify this tradeoff . We present a through experimental evaluation , using both real and synthetic data sets , from a prototype implementation of our technique , investigating the impact of various parameters in the accuracy of the overall computation . Our results indicate , that correlations between multiple data streams can be identified very efficiently and accurately . The algorithms proposed herein , are presented as generic tools , with a multitude of applications on data stream mining problems .", "keywords": ["data streams", "singular value decomposition", "approximate computation"], "combined": "Correlating synchronous and asynchronous data streams In a variety of modern mining applications , data are commonly viewed as infinite time ordered data streams rather as finite data sets stored on disk . This view challenges fundamental assumptions commonly made in the context of several data mining algorithms . In this paper , we study the problem of identifying correlations between multiple data streams . In particular , we propose algorithms capable of capturing correlations between multiple continuous data streams in a highly efficient and accurate manner . Our algorithms and techniques are applicable in the case of both synchronous and asynchronous data streaming environments . We capture correlations between multiple streams using the well known technique of Singular Value Decomposition SVD . Correlations between data items , and the SVD technique in particular , have been repeatedly utilized in an off-line non stream data mining problems , for example forecasting , approximate query answering , and data reduction . We propose a methodology based on a combination of dimensionality reduction and sampling to make the SVD technique suitable for a data stream context . Our techniques are approximate , trading accuracy with performance , and we analytically quantify this tradeoff . We present a through experimental evaluation , using both real and synthetic data sets , from a prototype implementation of our technique , investigating the impact of various parameters in the accuracy of the overall computation . Our results indicate , that correlations between multiple data streams can be identified very efficiently and accurately . The algorithms proposed herein , are presented as generic tools , with a multitude of applications on data stream mining problems . [[EENNDD]] data streams; singular value decomposition; approximate computation"}, "Mengaitkan aliran data segerak dan tak segerak Dalam pelbagai aplikasi perlombongan moden, data biasanya dilihat sebagai aliran data yang disusun tanpa batas waktu dan bukannya set data terhingga yang disimpan pada cakera. Pandangan ini mencabar andaian asas yang biasa dibuat dalam konteks beberapa algoritma perlombongan data. Dalam makalah ini, kami mengkaji masalah mengenal pasti korelasi antara pelbagai aliran data. Secara khusus, kami mencadangkan algoritma yang mampu menangkap korelasi antara banyak aliran data berterusan dengan cara yang sangat cekap dan tepat. Algoritma dan teknik kami berlaku untuk kedua-dua persekitaran aliran data segerak dan tak segerak. Kami menangkap korelasi antara pelbagai aliran menggunakan teknik terkenal Singular Value Decomposition SVD. Hubungan antara item data, dan teknik SVD khususnya, telah berulang kali digunakan dalam masalah perlombongan data non-aliran non-line, misalnya peramalan, perkiraan menjawab pertanyaan, dan pengurangan data. Kami mencadangkan metodologi berdasarkan kombinasi pengurangan dimensi dan pensampelan untuk menjadikan teknik SVD sesuai untuk konteks aliran data. Teknik kami adalah anggaran, ketepatan perdagangan dengan prestasi, dan kami secara analitik mengukur pertukaran ini. Kami menyajikan melalui penilaian eksperimental, menggunakan kedua-dua set data nyata dan sintetik, dari pelaksanaan prototaip teknik kami, menyelidiki pengaruh pelbagai parameter dalam ketepatan pengiraan keseluruhan. Hasil kajian kami menunjukkan, bahawa korelasi antara beberapa aliran data dapat dikenal pasti dengan sangat efisien dan tepat. Algoritma yang dicadangkan di sini, disajikan sebagai alat generik, dengan banyak aplikasi mengenai masalah perlombongan aliran data. [[EENNDD]] aliran data; penguraian nilai tunggal; pengiraan anggaran"], [{"string": "Data filtering for automatic classification of rocks from reflectance spectra The ability to identify the mineral composition of rocks and soils is an important tool for the exploration of geological sites . For instance , NASA intends to design robots that are sufficiently autonomous to perform this task on planetary missions . Spectrometer readings provide one important source of data for identifying sites with minerals of interest . Reflectance spectrometers measure intensities of light reflected from surfaces over a range of wavelengths . Spectral intensity patterns may in some cases be sufficiently distinctive for proper identification of minerals or classes of minerals . For some mineral classes , carbonates for example , specific short spectral intervals are known to carry a distinctive signature . Finding similar distinctive spectral ranges for other mineral classes is not an easy problem . We propose and evaluate data-driven techniques that automatically search for spectral ranges optimized for specific minerals . In one set of studies , we partition the whole interval of wavelengths available in our data into sub-intervals , or bins , and use a genetic algorithm to evaluate a candidate selection of subintervals . As alternatives to this computationally expensive search technique , we present an entropy-based heuristic that gives higher scores for wavelengths more likely to distinguish between classes , as well as other greedy search procedures . Results are presented for four different classes , showing reasonable improvements in identifying some , but not all , of the mineral classes tested .", "keywords": ["design methodology", "applications"], "combined": "Data filtering for automatic classification of rocks from reflectance spectra The ability to identify the mineral composition of rocks and soils is an important tool for the exploration of geological sites . For instance , NASA intends to design robots that are sufficiently autonomous to perform this task on planetary missions . Spectrometer readings provide one important source of data for identifying sites with minerals of interest . Reflectance spectrometers measure intensities of light reflected from surfaces over a range of wavelengths . Spectral intensity patterns may in some cases be sufficiently distinctive for proper identification of minerals or classes of minerals . For some mineral classes , carbonates for example , specific short spectral intervals are known to carry a distinctive signature . Finding similar distinctive spectral ranges for other mineral classes is not an easy problem . We propose and evaluate data-driven techniques that automatically search for spectral ranges optimized for specific minerals . In one set of studies , we partition the whole interval of wavelengths available in our data into sub-intervals , or bins , and use a genetic algorithm to evaluate a candidate selection of subintervals . As alternatives to this computationally expensive search technique , we present an entropy-based heuristic that gives higher scores for wavelengths more likely to distinguish between classes , as well as other greedy search procedures . Results are presented for four different classes , showing reasonable improvements in identifying some , but not all , of the mineral classes tested . [[EENNDD]] design methodology; applications"}, "Penyaringan data untuk pengkelasan batu secara automatik dari spektrum pantulan Keupayaan untuk mengenal pasti komposisi mineral batuan dan tanah adalah alat penting untuk penerokaan kawasan geologi. Sebagai contoh, NASA bermaksud merancang robot yang cukup autonomi untuk melaksanakan tugas ini dalam misi planet. Pembacaan spektrometer menyediakan satu sumber data penting untuk mengenal pasti laman web dengan mineral yang diminati. Spektrometer pantulan mengukur keamatan cahaya yang dipantulkan dari permukaan pada jarak panjang gelombang. Corak intensiti spektral dalam beberapa kes mungkin cukup berbeza untuk mengenal pasti mineral atau kelas mineral yang betul. Untuk beberapa kelas mineral, karbonat misalnya, selang spektrum pendek tertentu diketahui membawa tanda khas. Mencari julat spektrum khas yang serupa untuk kelas mineral lain bukanlah masalah yang mudah. Kami mencadangkan dan menilai teknik berdasarkan data yang secara automatik mencari julat spektrum yang dioptimumkan untuk mineral tertentu. Dalam satu set kajian, kami membahagikan keseluruhan selang panjang gelombang yang terdapat dalam data kami ke dalam sub selang, atau tong sampah, dan menggunakan algoritma genetik untuk menilai pemilihan calon subinterval. Sebagai alternatif kepada teknik pencarian yang mahal ini, kami menyajikan heuristik berdasarkan entropi yang memberikan skor yang lebih tinggi untuk panjang gelombang lebih cenderung untuk membezakan antara kelas, serta prosedur carian rakus yang lain. Hasil dibentangkan untuk empat kelas yang berbeza, menunjukkan peningkatan yang wajar dalam mengenal pasti beberapa, tetapi tidak semua, kelas mineral yang diuji. [[EENNDD]] metodologi reka bentuk; aplikasi"], [{"string": "Discovering word senses from text Inventories of manually compiled dictionaries usually serve as a source for word senses . However , they often include many rare senses while missing corpus\\/domain-specific senses . We present a clustering algorithm called CBC Clustering By Committee that automatically discovers word senses from text . It initially discovers a set of tight clusters called committees that are well scattered in the similarity space . The centroid of the members of a committee is used as the feature vector of the cluster . We proceed by assigning words to their most similar clusters . After assigning an element to a cluster , we remove their overlapping features from the element . This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses . Each cluster that a word belongs to represents one of its senses . We also present an evaluation methodology for automatically measuring the precision and recall of discovered senses .", "keywords": ["machine learning", "word sense discovery", "evaluation"], "combined": "Discovering word senses from text Inventories of manually compiled dictionaries usually serve as a source for word senses . However , they often include many rare senses while missing corpus\\/domain-specific senses . We present a clustering algorithm called CBC Clustering By Committee that automatically discovers word senses from text . It initially discovers a set of tight clusters called committees that are well scattered in the similarity space . The centroid of the members of a committee is used as the feature vector of the cluster . We proceed by assigning words to their most similar clusters . After assigning an element to a cluster , we remove their overlapping features from the element . This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses . Each cluster that a word belongs to represents one of its senses . We also present an evaluation methodology for automatically measuring the precision and recall of discovered senses . [[EENNDD]] machine learning; word sense discovery; evaluation"}, "Mencari deria kata dari teks Persediaan kamus yang disusun secara manual biasanya berfungsi sebagai sumber deria kata. Walau bagaimanapun, mereka sering merangkumi banyak deria yang jarang berlaku ketika kehilangan indra khusus korpus \\ / domain. Kami menyajikan algoritma kluster yang disebut CBC Clustering By Committee yang secara automatik menjumpai deria kata dari teks. Pada mulanya ia menemui sekumpulan kelompok ketat yang disebut jawatankuasa yang tersebar dengan baik di ruang persamaan. Pusat anggota jawatankuasa digunakan sebagai vektor ciri kluster. Kami meneruskan dengan memberikan kata-kata kepada kelompok mereka yang paling serupa. Setelah menetapkan elemen ke kluster, kami membuang ciri yang bertindih dari elemen tersebut. Ini membolehkan CBC menemui deria kata yang lebih jarang dan untuk mengelakkan penemuan deria pendua. Setiap kelompok yang dimiliki oleh kata mewakili salah satu inderanya. Kami juga menyajikan metodologi penilaian untuk mengukur ketepatan dan penarikan semula deria yang ditemui secara automatik. [[EENNDD]] pembelajaran mesin; penemuan deria kata; penilaian"], [{"string": "On effective classification of strings with wavelets In recent years , the technological advances in mapping genes have made it increasingly easy to store and use a wide variety of biological data . Such data are usually in the form of very long strings for which it is difficult to determine the most relevant features for a classification task . For example , a typical DNA string may be millions of characters long , and there may be thousands of such strings in a database . In many cases , the classification behavior of the data may be hidden in the compositional behavior of certain segments of the string which can not be easily determined apriori . Another problem which complicates the classification task is that in some cases the classification behavior is reflected in global behavior of the string , whereas in others it is reflected in local patterns . Given the enormous variation in the behavior of the strings over different data sets , it is useful to develop an approach which is sensitive to both the global and local behavior of the strings for the purpose of classification . For this purpose , we will exploit the multi-resolution property of wavelet decomposition in order to create a scheme which can mine classification characteristics at different levels of granularity . The resulting scheme turns out to be very effective in practice on a wide range of problems .", "keywords": ["deduction", "representations"], "combined": "On effective classification of strings with wavelets In recent years , the technological advances in mapping genes have made it increasingly easy to store and use a wide variety of biological data . Such data are usually in the form of very long strings for which it is difficult to determine the most relevant features for a classification task . For example , a typical DNA string may be millions of characters long , and there may be thousands of such strings in a database . In many cases , the classification behavior of the data may be hidden in the compositional behavior of certain segments of the string which can not be easily determined apriori . Another problem which complicates the classification task is that in some cases the classification behavior is reflected in global behavior of the string , whereas in others it is reflected in local patterns . Given the enormous variation in the behavior of the strings over different data sets , it is useful to develop an approach which is sensitive to both the global and local behavior of the strings for the purpose of classification . For this purpose , we will exploit the multi-resolution property of wavelet decomposition in order to create a scheme which can mine classification characteristics at different levels of granularity . The resulting scheme turns out to be very effective in practice on a wide range of problems . [[EENNDD]] deduction; representations"}, "Mengenai pengkelasan tali dengan gelombang yang berkesan Dalam beberapa tahun kebelakangan ini, kemajuan teknologi dalam pemetaan gen menjadikannya semakin mudah untuk menyimpan dan menggunakan pelbagai jenis data biologi. Data sedemikian biasanya dalam bentuk rentetan yang sangat panjang dan sukar untuk menentukan ciri yang paling relevan untuk tugas klasifikasi. Contohnya, rentetan DNA khas mungkin berjuta-juta watak, dan mungkin terdapat ribuan rentetan seperti itu dalam pangkalan data. Dalam banyak kes, tingkah laku klasifikasi data mungkin tersembunyi dalam tingkah laku penggabungan segmen rentetan tertentu yang tidak dapat ditentukan dengan mudah apriori. Masalah lain yang merumitkan tugas klasifikasi adalah bahawa dalam beberapa kes tingkah laku klasifikasi tercermin dalam perilaku rentetan global, sedangkan pada yang lain hal itu tercermin dalam pola lokal. Memandangkan variasi yang sangat besar dalam tingkah laku rentetan terhadap set data yang berbeza, adalah berguna untuk mengembangkan pendekatan yang peka terhadap tingkah laku global dan tempatan tali untuk tujuan klasifikasi. Untuk tujuan ini, kami akan mengeksploitasi harta pelbagai resolusi penguraian wavelet untuk membuat skema yang dapat menambang ciri klasifikasi pada tahap butiran yang berbeza. Skema yang dihasilkan ternyata sangat berkesan dalam praktik pada pelbagai masalah. [[EENNDD]] pemotongan; perwakilan"], [{"string": "Measuring and extracting proximity in networks Measuring distance or some other form of proximity between objects is a standard data mining tool . Connection subgraphs were recently proposed as a way to demonstrate proximity between nodes in networks . We propose a new way of measuring and extracting proximity in networks called `` cycle free effective conductance '' CFEC . Our proximity measure can handle more than two endpoints , directed edges , is statistically well-behaved , and produces an effectiveness score for the computed subgraphs . We provide an efficien talgorithm . Also , we report experimental results and show examples for three large network data sets : a telecommunications calling graph , the IMDB actors graph , and an academic co-authorship network .", "keywords": ["random walks", "proximity", "cycle-free escape probability", "connection subgraph", "escape probability", "proximity subgraph"], "combined": "Measuring and extracting proximity in networks Measuring distance or some other form of proximity between objects is a standard data mining tool . Connection subgraphs were recently proposed as a way to demonstrate proximity between nodes in networks . We propose a new way of measuring and extracting proximity in networks called `` cycle free effective conductance '' CFEC . Our proximity measure can handle more than two endpoints , directed edges , is statistically well-behaved , and produces an effectiveness score for the computed subgraphs . We provide an efficien talgorithm . Also , we report experimental results and show examples for three large network data sets : a telecommunications calling graph , the IMDB actors graph , and an academic co-authorship network . [[EENNDD]] random walks; proximity; cycle-free escape probability; connection subgraph; escape probability; proximity subgraph"}, "Mengukur dan mengekstrak jarak dalam rangkaian Mengukur jarak atau bentuk jarak lain antara objek adalah alat perlombongan data standard. Subgraf sambungan baru-baru ini dicadangkan sebagai cara untuk menunjukkan kedekatan antara nod dalam rangkaian. Kami mencadangkan kaedah baru untuk mengukur dan mengekstrak jarak dalam rangkaian yang disebut CFEC \"konduktiviti bebas bebas kitaran\". Ukuran jarak kami dapat menangani lebih dari dua titik akhir, tepi yang diarahkan, berkelakuan baik secara statistik, dan menghasilkan skor keberkesanan untuk subgraf yang dikira. Kami menyediakan talgoritma yang berkesan. Juga, kami melaporkan hasil eksperimen dan menunjukkan contoh untuk tiga kumpulan data rangkaian besar: grafik panggilan telekomunikasi, grafik pelakon IMDB, dan rangkaian pengarang bersama akademik. [[EENNDD]] jalan rawak; jarak dekat; kebarangkalian melepaskan kitaran; subgraf sambungan; kebarangkalian melarikan diri; subgraf jarak"], [{"string": "Predicting prostate cancer recurrence via maximizing the concordance index In order to effectively use machine learning algorithms , e.g. , neural networks , for the analysis of survival data , the correct treatment of censored data is crucial . The concordance index CI is a typical metric for quantifying the predictive ability of a survival model . We propose a new algorithm that directly uses the CI as the objective function to train a model , which predicts whether an event will eventually occur or not . Directly optimizing the CI allows the model to make complete use of the information from both censored and non-censored observations . In particular , we approximate the CI via a differentiable function so that gradient-based methods can be used to train the model . We applied the new algorithm to predict the eventual recurrence of prostate cancer following radical prostatectomy . Compared with the traditional Cox proportional hazards model and several other algorithms based on neural networks and support vector machines , our algorithm achieves a significant improvement in being able to identify high-risk and low-risk groups of patients .", "keywords": ["prostate cancer recurrence", "concordance index", "survival analysis", "neural networks", "nomogram"], "combined": "Predicting prostate cancer recurrence via maximizing the concordance index In order to effectively use machine learning algorithms , e.g. , neural networks , for the analysis of survival data , the correct treatment of censored data is crucial . The concordance index CI is a typical metric for quantifying the predictive ability of a survival model . We propose a new algorithm that directly uses the CI as the objective function to train a model , which predicts whether an event will eventually occur or not . Directly optimizing the CI allows the model to make complete use of the information from both censored and non-censored observations . In particular , we approximate the CI via a differentiable function so that gradient-based methods can be used to train the model . We applied the new algorithm to predict the eventual recurrence of prostate cancer following radical prostatectomy . Compared with the traditional Cox proportional hazards model and several other algorithms based on neural networks and support vector machines , our algorithm achieves a significant improvement in being able to identify high-risk and low-risk groups of patients . [[EENNDD]] prostate cancer recurrence; concordance index; survival analysis; neural networks; nomogram"}, "Meramalkan kanker prostat berulang dengan memaksimumkan indeks kesesuaian Untuk menggunakan algoritma pembelajaran mesin dengan berkesan, mis. , rangkaian neural, untuk analisis data survival, perlakuan yang tepat terhadap data yang disensor adalah penting. Indeks konkordans CI adalah metrik khas untuk mengukur kemampuan ramalan model survival. Kami mencadangkan algoritma baru yang secara langsung menggunakan CI sebagai fungsi objektif untuk melatih model, yang meramalkan sama ada suatu peristiwa akhirnya akan berlaku atau tidak. Mengoptimumkan CI secara langsung membolehkan model menggunakan sepenuhnya maklumat dari pemerhatian yang disensor dan yang tidak disensor. Khususnya, kami menghitung CI melalui fungsi yang dapat dibezakan sehingga kaedah berdasarkan kecerunan dapat digunakan untuk melatih model. Kami menggunakan algoritma baru untuk meramalkan kanker prostat berulang selepas prostatektomi radikal. Berbanding dengan model bahaya proporsional Cox tradisional dan beberapa algoritma lain berdasarkan rangkaian neural dan mesin vektor sokongan, algoritma kami mencapai peningkatan yang signifikan dalam dapat mengenal pasti kumpulan pesakit berisiko tinggi dan berisiko rendah. [[EENNDD]] kambuhan barah prostat; indeks kesesuaian; analisis survival; rangkaian saraf; nomogram"], [{"string": "Semi-supervised time series classification The problem of time series classification has attracted great interest in the last decade . However current research assumes the existence of large amounts of labeled training data . In reality , such data may be very difficult or expensive to obtain . For example , it may require the time and expertise of cardiologists , space launch technicians , or other domain specialists . As in many other domains , there are often copious amounts of unlabeled data available . For example , the PhysioBank archive contains gigabytes of ECG data . In this work we propose a semi-supervised technique for building time series classifiers . While such algorithms are well known in text domains , we will show that special considerations must be made to make them both efficient and effective for the time series domain . We evaluate our work with a comprehensive set of experiments on diverse data sources including electrocardiograms , handwritten documents , and video datasets . The experimental results demonstrate that our approach requires only a handful of labeled examples to construct accurate classifiers .", "keywords": ["time series", "classification", "semi-supervised learning"], "combined": "Semi-supervised time series classification The problem of time series classification has attracted great interest in the last decade . However current research assumes the existence of large amounts of labeled training data . In reality , such data may be very difficult or expensive to obtain . For example , it may require the time and expertise of cardiologists , space launch technicians , or other domain specialists . As in many other domains , there are often copious amounts of unlabeled data available . For example , the PhysioBank archive contains gigabytes of ECG data . In this work we propose a semi-supervised technique for building time series classifiers . While such algorithms are well known in text domains , we will show that special considerations must be made to make them both efficient and effective for the time series domain . We evaluate our work with a comprehensive set of experiments on diverse data sources including electrocardiograms , handwritten documents , and video datasets . The experimental results demonstrate that our approach requires only a handful of labeled examples to construct accurate classifiers . [[EENNDD]] time series; classification; semi-supervised learning"}, "Pengelasan siri masa separa diselia Masalah klasifikasi siri masa telah menarik minat ramai dalam dekad yang lalu. Walau bagaimanapun penyelidikan semasa mengandaikan adanya sejumlah besar data latihan berlabel. Pada hakikatnya, data tersebut mungkin sangat sukar atau mahal untuk diperoleh. Sebagai contoh, mungkin memerlukan masa dan kepakaran ahli kardiologi, juruteknik pelancaran ruang, atau pakar domain lain. Seperti di banyak domain lain, terdapat banyak data tidak berlabel yang tersedia. Sebagai contoh, arkib PhysioBank mengandungi gigabait data ECG. Dalam karya ini, kami mencadangkan teknik separa penyeliaan untuk membina pengkelasan siri masa. Walaupun algoritma seperti itu terkenal dalam domain teks, kami akan menunjukkan bahawa pertimbangan khas mesti dibuat untuk menjadikannya efisien dan berkesan untuk domain siri masa. Kami menilai kerja kami dengan satu set eksperimen yang komprehensif mengenai pelbagai sumber data termasuk elektrokardiogram, dokumen tulisan tangan, dan set data video. Hasil eksperimen menunjukkan bahawa pendekatan kami hanya memerlukan sedikit contoh berlabel untuk membina pengkelasan yang tepat. [[EENNDD]] siri masa; pengelasan; pembelajaran separa penyeliaan"], [{"string": "Classification of software behaviors for failure detection : a discriminative pattern mining approach Software is a ubiquitous component of our daily life . We often depend on the correct working of software systems . Due to the difficulty and complexity of software systems , bugs and anomalies are prevalent . Bugs have caused billions of dollars loss , in addition to privacy and security threats . In this work , we address software reliability issues by proposing a novel method to classify software behaviors based on past history or runs . With the technique , it is possible to generalize past known errors and mistakes to capture failures and anomalies . Our technique first mines a set of discriminative features capturing repetitive series of events from program execution traces . It then performs feature selection to select the best features for classification . These features are then used to train a classifier to detect failures . Experiments and case studies on traces of several benchmark software systems and a real-life concurrency bug from MySQL server show the utility of the technique in capturing failures and anomalies . On average , our pattern-based classification technique outperforms the baseline approach by 24.68 % in accuracy .", "keywords": ["iterative patterns", "closed unique patterns", "sequential database", "failure detection", "pattern-based classification", "software behaviors"], "combined": "Classification of software behaviors for failure detection : a discriminative pattern mining approach Software is a ubiquitous component of our daily life . We often depend on the correct working of software systems . Due to the difficulty and complexity of software systems , bugs and anomalies are prevalent . Bugs have caused billions of dollars loss , in addition to privacy and security threats . In this work , we address software reliability issues by proposing a novel method to classify software behaviors based on past history or runs . With the technique , it is possible to generalize past known errors and mistakes to capture failures and anomalies . Our technique first mines a set of discriminative features capturing repetitive series of events from program execution traces . It then performs feature selection to select the best features for classification . These features are then used to train a classifier to detect failures . Experiments and case studies on traces of several benchmark software systems and a real-life concurrency bug from MySQL server show the utility of the technique in capturing failures and anomalies . On average , our pattern-based classification technique outperforms the baseline approach by 24.68 % in accuracy . [[EENNDD]] iterative patterns; closed unique patterns; sequential database; failure detection; pattern-based classification; software behaviors"}, "Klasifikasi tingkah laku perisian untuk pengesanan kegagalan: pendekatan perlombongan corak diskriminatif Perisian adalah komponen di mana-mana kehidupan seharian kita. Kita sering bergantung pada sistem perisian yang betul. Oleh kerana kesukaran dan kerumitan sistem perisian, pepijat dan anomali berlaku. Bug telah menyebabkan kerugian berbilion-bilion dolar, selain ancaman privasi dan keselamatan. Dalam karya ini, kami menangani masalah kebolehpercayaan perisian dengan mencadangkan kaedah baru untuk mengklasifikasikan tingkah laku perisian berdasarkan sejarah masa lalu atau berjalan. Dengan teknik ini, adalah mungkin untuk menggeneralisasi kesalahan dan kesalahan yang telah diketahui sebelumnya untuk menangkap kegagalan dan anomali. Teknik kami pertama kali menggunakan sekumpulan ciri diskriminatif yang menangkap siri peristiwa berulang dari jejak pelaksanaan program. Ia kemudian melakukan pemilihan ciri untuk memilih ciri terbaik untuk klasifikasi. Ciri-ciri ini kemudian digunakan untuk melatih pengklasifikasi untuk mengesan kegagalan. Eksperimen dan kajian kes mengenai jejak beberapa sistem perisian penanda aras dan bug serentak kehidupan nyata dari pelayan MySQL menunjukkan kegunaan teknik dalam menangkap kegagalan dan anomali. Rata-rata, teknik klasifikasi berasaskan corak kami mengatasi pendekatan asas dengan ketepatan 24.68%. [[EENNDD]] corak berulang; corak unik tertutup; pangkalan data berurutan; pengesanan kegagalan; pengkelasan berdasarkan corak; tingkah laku perisian"], [{"string": "Pragmatic text mining : minimizing human effort to quantify many issues in call logs We discuss our experiences in analyzing customer-support issues from the unstructured free-text fields of technical-support call logs . The identification of frequent issues and their accurate quantification is essential in order to track aggregate costs broken down by issue type , to appropriately target engineering resources , and to provide the best diagnosis , support and documentation for most common issues . We present a new set of techniques for doing this efficiently on an industrial scale , without requiring manual coding of calls in the call center . Our approach involves 1 a new text clustering method to identify common and emerging issues ; 2 a method to rapidly train large numbers of categorizers in a practical , interactive manner ; and 3 a method to accurately quantify categories , even in the face of inaccurate classifications and training sets that necessarily can not match the class distribution of each new month 's data . We present our methodology and a tool we developed and deployed that uses these methods for tracking ongoing support issues and discovering emerging issues at HP .", "keywords": ["text classification", "text mining", "applications", "quantification", "decision support", "log processing", "supervised machine learning"], "combined": "Pragmatic text mining : minimizing human effort to quantify many issues in call logs We discuss our experiences in analyzing customer-support issues from the unstructured free-text fields of technical-support call logs . The identification of frequent issues and their accurate quantification is essential in order to track aggregate costs broken down by issue type , to appropriately target engineering resources , and to provide the best diagnosis , support and documentation for most common issues . We present a new set of techniques for doing this efficiently on an industrial scale , without requiring manual coding of calls in the call center . Our approach involves 1 a new text clustering method to identify common and emerging issues ; 2 a method to rapidly train large numbers of categorizers in a practical , interactive manner ; and 3 a method to accurately quantify categories , even in the face of inaccurate classifications and training sets that necessarily can not match the class distribution of each new month 's data . We present our methodology and a tool we developed and deployed that uses these methods for tracking ongoing support issues and discovering emerging issues at HP . [[EENNDD]] text classification; text mining; applications; quantification; decision support; log processing; supervised machine learning"}, "Perlombongan teks pragmatik: meminimumkan usaha manusia untuk mengukur banyak isu dalam log panggilan Kami membincangkan pengalaman kami dalam menganalisis isu sokongan pelanggan dari bidang teks bebas yang tidak tersusun dari log panggilan sokongan teknikal. Pengenalpastian masalah yang kerap dan pengukurannya yang tepat sangat penting untuk mengesan kos agregat yang dipecahkan mengikut jenis masalah, untuk menargetkan sumber kejuruteraan dengan tepat, dan memberikan diagnosis, sokongan dan dokumentasi terbaik untuk masalah yang paling umum. Kami menyajikan satu set teknik baru untuk melakukan ini dengan cekap pada skala industri, tanpa memerlukan pengekodan manual panggilan di pusat panggilan. Pendekatan kami melibatkan 1 kaedah pengelompokan teks baru untuk mengenal pasti masalah umum dan timbul; 2 kaedah untuk melatih sebilangan besar pengkategoratori secara praktikal dan interaktif; dan 3 kaedah untuk mengkuantifikasi kategori secara tepat, walaupun dalam klasifikasi dan set latihan yang tidak tepat yang semestinya tidak dapat menandingi taburan kelas setiap data bulan baru. Kami menyajikan metodologi kami dan alat yang kami kembangkan dan gunakan yang menggunakan kaedah ini untuk mengesan masalah sokongan yang sedang berjalan dan menemui masalah yang muncul di HP. [[EENNDD]] pengelasan teks; perlombongan teks; permohonan; pengukuran; sokongan keputusan; pemprosesan log; pembelajaran mesin yang diselia"], [{"string": "Discovering significant OPSM subspace clusters in massive gene expression data Order-preserving submatrixes OPSMs have been accepted as a biologically meaningful subspace cluster model , capturing the general tendency of gene expressions across a subset of conditions . In an OPSM , the expression levels of all genes induce the same linear ordering of the conditions . OPSM mining is reducible to a special case of the sequential pattern mining problem , in which a pattern and its supporting sequences uniquely specify an OPSM cluster . Those small twig clusters , specified by long patterns with naturally low support , incur explosive computational costs and would be completely pruned off by most existing methods for massive datasets containing thousands of conditions and hundreds of thousands of genes , which are common in today 's gene expression analysis . However , it is in particular interest of biologists to reveal such small groups of genes that are tightly coregulated under many conditions , and some pathways or processes might require only two genes to act in concert . In this paper , we introduce the KiWi mining framework for massive datasets , that exploits two parameters k and w to provide a biased testing on a bounded number of candidates , substantially reducing the search space and problem scale , targeting on highly promising seeds that lead to significant clusters and twig clusters . Extensive biological and computational evaluations on real datasets demonstrate that KiWi can effectively mine biologically meaningful OPSM subspace clusters with good efficiency and scalability .", "keywords": ["gene expression data", "scalability", "order-preserving submatrix", "subspace clustering", "twig cluster"], "combined": "Discovering significant OPSM subspace clusters in massive gene expression data Order-preserving submatrixes OPSMs have been accepted as a biologically meaningful subspace cluster model , capturing the general tendency of gene expressions across a subset of conditions . In an OPSM , the expression levels of all genes induce the same linear ordering of the conditions . OPSM mining is reducible to a special case of the sequential pattern mining problem , in which a pattern and its supporting sequences uniquely specify an OPSM cluster . Those small twig clusters , specified by long patterns with naturally low support , incur explosive computational costs and would be completely pruned off by most existing methods for massive datasets containing thousands of conditions and hundreds of thousands of genes , which are common in today 's gene expression analysis . However , it is in particular interest of biologists to reveal such small groups of genes that are tightly coregulated under many conditions , and some pathways or processes might require only two genes to act in concert . In this paper , we introduce the KiWi mining framework for massive datasets , that exploits two parameters k and w to provide a biased testing on a bounded number of candidates , substantially reducing the search space and problem scale , targeting on highly promising seeds that lead to significant clusters and twig clusters . Extensive biological and computational evaluations on real datasets demonstrate that KiWi can effectively mine biologically meaningful OPSM subspace clusters with good efficiency and scalability . [[EENNDD]] gene expression data; scalability; order-preserving submatrix; subspace clustering; twig cluster"}, "Menemui kelompok ruang bawah tanah OPSM yang signifikan dalam data ekspresi gen besar-besaran Substrat pemeliharaan pesanan OPSM telah diterima sebagai model kelompok ruang yang bermakna secara biologi, menangkap kecenderungan umum ekspresi gen melintasi subset keadaan. Dalam OPSM, tahap ekspresi semua gen mendorong susunan linear keadaan yang sama. Perlombongan OPSM dapat dikurangkan menjadi kes khas dari masalah perlombongan pola berurutan, di mana corak dan urutan pendukungnya secara unik menentukan gugus OPSM. Kelompok ranting kecil itu, ditentukan oleh corak panjang dengan sokongan rendah secara semula jadi, menanggung kos komputasi letupan dan akan dipangkas sepenuhnya oleh kebanyakan kaedah yang ada untuk set data besar yang mengandungi ribuan keadaan dan beratus-ratus ribu gen, yang biasa terdapat pada gen masa kini analisis ekspresi. Walau bagaimanapun, adalah kepentingan khusus ahli biologi untuk mendedahkan sekumpulan gen kecil seperti itu yang dikawal ketat dalam banyak keadaan, dan beberapa jalan atau proses mungkin hanya memerlukan dua gen untuk bertindak bersama. Dalam makalah ini, kami memperkenalkan kerangka penambangan KiWi untuk kumpulan data besar, yang mengeksploitasi dua parameter k dan w untuk memberikan pengujian berat sebelah pada jumlah calon yang dibatasi, mengurangkan ruang pencarian dan skala masalah, dengan menargetkan benih yang sangat menjanjikan yang mengarah ke kelompok kluster dan ranting ketara. Penilaian komprehensif biologi dan komputasi pada set data sebenar menunjukkan bahawa KiWi dapat menambang kelompok ruang OPSM yang bermakna secara biologi dengan kecekapan dan skalabiliti yang baik. [[EENNDD]] data ekspresi gen; skalabiliti; submatrix pemeliharaan pesanan; pengelompokan ruang bawah tanah; gugusan ranting"], [{"string": "Empirical comparisons of various voting methods in bagging Finding effective methods for developing an ensemble of models has been an active research area of large-scale data mining in recent years . Models learned from data are often subject to some degree of uncertainty , for a variety of resoans . In classification , ensembles of models provide a useful means of averaging out error introduced by individual classifiers , hence reducing the generalization error of prediction . The plurality voting method is often chosen for bagging , because of its simplicity of implementation . However , the plurality approach to model reconciliation is ad-hoc . There are many other voting methods to choose from , including the anti-plurality method , the plurality method with elimination , the Borda count method , and Condorcet 's method of pairwise comparisons . Any of these could lead to a better method for reconciliation . In this paper , we analyze the use of these voting methods in model reconciliation . We present empirical results comparing performance of these voting methods when applied in bagging . These results include some surprises , and among other things suggest that 1 plurality is not always the best voting method ; 2 the number of classes can affect the performance of voting methods ; and 3 the degree of dataset noise can affect the performance of voting methods . While it is premature to make final judgments about specific voting methods , the results of this work raise interesting questions , and they open the door to the application of voting theory in classification theory .", "keywords": ["voting", "ensemble classification", "learning", "model reconciliation"], "combined": "Empirical comparisons of various voting methods in bagging Finding effective methods for developing an ensemble of models has been an active research area of large-scale data mining in recent years . Models learned from data are often subject to some degree of uncertainty , for a variety of resoans . In classification , ensembles of models provide a useful means of averaging out error introduced by individual classifiers , hence reducing the generalization error of prediction . The plurality voting method is often chosen for bagging , because of its simplicity of implementation . However , the plurality approach to model reconciliation is ad-hoc . There are many other voting methods to choose from , including the anti-plurality method , the plurality method with elimination , the Borda count method , and Condorcet 's method of pairwise comparisons . Any of these could lead to a better method for reconciliation . In this paper , we analyze the use of these voting methods in model reconciliation . We present empirical results comparing performance of these voting methods when applied in bagging . These results include some surprises , and among other things suggest that 1 plurality is not always the best voting method ; 2 the number of classes can affect the performance of voting methods ; and 3 the degree of dataset noise can affect the performance of voting methods . While it is premature to make final judgments about specific voting methods , the results of this work raise interesting questions , and they open the door to the application of voting theory in classification theory . [[EENNDD]] voting; ensemble classification; learning; model reconciliation"}, "Perbandingan empirik pelbagai kaedah pengundian dalam mengantongi. Mencari kaedah yang berkesan untuk mengembangkan kumpulan model telah menjadi kawasan penyelidikan aktif perlombongan data berskala besar dalam beberapa tahun terakhir. Model yang dipelajari dari data sering mengalami tahap ketidakpastian, untuk pelbagai resonan. Dalam klasifikasi, ensemble model memberikan kaedah berguna untuk mengatasi kesalahan yang diperkenalkan oleh pengelasan individu, sehingga mengurangkan kesilapan ramalan ramalan. Kaedah pemilihan pluraliti sering dipilih untuk penundaan, kerana kesederhanaan pelaksanaannya. Walau bagaimanapun, pendekatan pluraliti untuk pendamaian model adalah ad-hoc. Terdapat banyak kaedah pengundian lain untuk dipilih, termasuk kaedah anti-jamak, kaedah pluraliti dengan penghapusan, kaedah penghitungan Borda, dan kaedah perbandingan pasangan berpasangan dengan Condorcet. Mana-mana ini boleh membawa kepada kaedah yang lebih baik untuk pendamaian. Dalam makalah ini, kami menganalisis penggunaan kaedah pengundian ini dalam rekonsiliasi model. Kami menyajikan hasil empirik yang membandingkan prestasi kaedah pengundian ini ketika diaplikasikan secara bertahap. Hasil ini merangkumi beberapa kejutan, dan antara lain menunjukkan bahawa 1 pluraliti tidak selalu merupakan kaedah pengundian terbaik; 2 bilangan kelas boleh mempengaruhi prestasi kaedah mengundi; dan 3 tahap kebisingan set data boleh mempengaruhi prestasi kaedah pengundian. Walaupun terlalu awal untuk membuat penilaian akhir mengenai kaedah pengundian tertentu, hasil karya ini menimbulkan pertanyaan menarik, dan mereka membuka pintu untuk penerapan teori pemungutan suara dalam teori klasifikasi. [[EENNDD]] pengundian; klasifikasi ensemble; belajar; pendamaian model"], [{"string": "An efficient algorithm for a class of fused lasso problems The fused Lasso penalty enforces sparsity in both the coefficients and their successive differences , which is desirable for applications with features ordered in some meaningful way . The resulting problem is , however , challenging to solve , as the fused Lasso penalty is both non-smooth and non-separable . Existing algorithms have high computational complexity and do not scale to large-size problems . In this paper , we propose an Efficient Fused Lasso Algorithm EFLA for optimizing this class of problems . One key building block in the proposed EFLA is the Fused Lasso Signal Approximator FLSA . To efficiently solve FLSA , we propose to reformulate it as the problem of finding an `` appropriate '' subgradient of the fused penalty at the minimizer , and develop a Subgradient Finding Algorithm SFA . We further design a restart technique to accelerate the convergence of SFA , by exploiting the special `` structures '' of both the original and the reformulated FLSA problems . Our empirical evaluations show that , both SFA and EFLA significantly outperform existing solvers . We also demonstrate several applications of the fused Lasso .", "keywords": ["l1 regularization", "restart", "fused lasso", "subgradient"], "combined": "An efficient algorithm for a class of fused lasso problems The fused Lasso penalty enforces sparsity in both the coefficients and their successive differences , which is desirable for applications with features ordered in some meaningful way . The resulting problem is , however , challenging to solve , as the fused Lasso penalty is both non-smooth and non-separable . Existing algorithms have high computational complexity and do not scale to large-size problems . In this paper , we propose an Efficient Fused Lasso Algorithm EFLA for optimizing this class of problems . One key building block in the proposed EFLA is the Fused Lasso Signal Approximator FLSA . To efficiently solve FLSA , we propose to reformulate it as the problem of finding an `` appropriate '' subgradient of the fused penalty at the minimizer , and develop a Subgradient Finding Algorithm SFA . We further design a restart technique to accelerate the convergence of SFA , by exploiting the special `` structures '' of both the original and the reformulated FLSA problems . Our empirical evaluations show that , both SFA and EFLA significantly outperform existing solvers . We also demonstrate several applications of the fused Lasso . [[EENNDD]] l1 regularization; restart; fused lasso; subgradient"}, "Algoritma yang cekap untuk kelas masalah lasso bersatu. Hukuman Lasso yang disatukan menguatkan kelangkaan dalam kedua-dua pekali dan perbezaannya berturut-turut, yang diharapkan untuk aplikasi dengan ciri yang disusun dengan cara yang bermakna. Masalah yang dihasilkan, bagaimanapun, sukar untuk diselesaikan, kerana penalti Lasso yang menyatu tidak lancar dan tidak dapat dipisahkan. Algoritma yang ada mempunyai kerumitan komputasi yang tinggi dan tidak mengikut skala masalah besar. Dalam makalah ini, kami mencadangkan EFLA Algoritma Lasso Fused Efisien untuk mengoptimumkan masalah ini. Salah satu blok utama dalam EFLA yang dicadangkan adalah FSA Lasso Signal Approximator FLSA. Untuk menyelesaikan FLSA dengan cekap, kami mengusulkan untuk merumuskannya semula sebagai masalah mencari subgradien penalti yang menyatu \"minimizer\" yang sesuai, dan mengembangkan Algoritma Penemuan Subgradien SFA. Kami seterusnya merancang teknik restart untuk mempercepat penumpuan SFA, dengan memanfaatkan \"struktur\" khas dari masalah FLSA yang asli dan yang dirumuskan semula. Penilaian empirikal kami menunjukkan bahawa, kedua-dua SFA dan EFLA secara signifikan mengatasi penyelesai yang ada. Kami juga menunjukkan beberapa aplikasi Lasso yang menyatu. [[EENNDD]] l1 regularisasi; mula semula; lasso bersatu; subgradien"], [{"string": "Online discovery and maintenance of time series motifs The detection of repeated subsequences , time series motifs , is a problem which has been shown to have great utility for several higher-level data mining algorithms , including classification , clustering , segmentation , forecasting , and rule discovery . In recent years there has been significant research effort spent on efficiently discovering these motifs in static offline databases . However , for many domains , the inherent streaming nature of time series demands online discovery and maintenance of time series motifs . In this paper , we develop the first online motif discovery algorithm which monitors and maintains motifs exactly in real time over the most recent history of a stream . Our algorithm has a worst-case update time which is linear to the window size and is extendible to maintain more complex pattern structures . In contrast , the current offline algorithms either need significant update time or require very costly pre-processing steps which online algorithms simply can not afford . Our core ideas allow useful extensions of our algorithm to deal with arbitrary data rates and discovering multidimensional motifs . We demonstrate the utility of our algorithms with a variety of case studies in the domains of robotics , acoustic monitoring and online compression .", "keywords": ["motifs", "information search and retrieval", "online algorithms", "time series"], "combined": "Online discovery and maintenance of time series motifs The detection of repeated subsequences , time series motifs , is a problem which has been shown to have great utility for several higher-level data mining algorithms , including classification , clustering , segmentation , forecasting , and rule discovery . In recent years there has been significant research effort spent on efficiently discovering these motifs in static offline databases . However , for many domains , the inherent streaming nature of time series demands online discovery and maintenance of time series motifs . In this paper , we develop the first online motif discovery algorithm which monitors and maintains motifs exactly in real time over the most recent history of a stream . Our algorithm has a worst-case update time which is linear to the window size and is extendible to maintain more complex pattern structures . In contrast , the current offline algorithms either need significant update time or require very costly pre-processing steps which online algorithms simply can not afford . Our core ideas allow useful extensions of our algorithm to deal with arbitrary data rates and discovering multidimensional motifs . We demonstrate the utility of our algorithms with a variety of case studies in the domains of robotics , acoustic monitoring and online compression . [[EENNDD]] motifs; information search and retrieval; online algorithms; time series"}, "Penemuan dalam talian dan penyelenggaraan motif siri masa Pengesanan berulang berulang, motif siri masa, adalah masalah yang telah terbukti mempunyai kegunaan yang hebat untuk beberapa algoritma perlombongan data peringkat tinggi, termasuk klasifikasi, pengelompokan, segmentasi, ramalan, dan penemuan peraturan . Dalam tahun-tahun kebelakangan ini, terdapat banyak usaha penyelidikan yang dilakukan untuk mencari motif ini dengan berkesan dalam pangkalan data luar talian yang statik. Walau bagaimanapun, bagi banyak domain, sifat streaming siri masa yang melekat menuntut penemuan dalam talian dan pemeliharaan motif siri masa. Dalam makalah ini, kami mengembangkan algoritma penemuan motif dalam talian pertama yang memantau dan mengekalkan motif secara tepat dalam masa nyata sepanjang sejarah aliran terkini. Algoritma kami mempunyai masa kemas kini terburuk yang sejajar dengan saiz tetingkap dan boleh diperpanjang untuk mengekalkan struktur corak yang lebih kompleks. Sebaliknya, algoritma luar talian semasa memerlukan masa kemas kini yang ketara atau memerlukan langkah pra-pemprosesan yang sangat mahal yang tidak mampu dilakukan oleh algoritma dalam talian. Idea teras kami membolehkan peluasan algoritma kami yang berguna untuk menangani kadar data sewenang-wenangnya dan menemui motif multidimensi. Kami menunjukkan kegunaan algoritma kami dengan pelbagai kajian kes dalam domain robotik, pemantauan akustik dan pemampatan dalam talian. [[EENNDD]] motif; carian dan pengambilan maklumat; algoritma dalam talian; siri masa"], [{"string": "Identifying biologically relevant genes via multiple heterogeneous data sources Selection of genes that are differentially expressed and critical to a particular biological process has been a major challenge in post-array analysis . Recent development in bioinformatics has made various data sources available such as mRNA and miRNA expression profiles , biological pathway and gene annotation , etc. . Efficient and effective integration of multiple data sources helps enrich our knowledge about the involved samples and genes for selecting genes bearing significant biological relevance . In this work , we studied a novel problem of multi-source gene selection : given multiple heterogeneous data sources or data sets , select genes from expression profiles by integrating information from various data sources . We investigated how to effectively employ information contained in multiple data sources to extract an intrinsic global geometric pattern and use it in covariance analysis for gene selection . We designed and conducted experiments to systematically compare the proposed approach with representative methods in terms of statistical and biological significance , and showed the efficacy and potential of the proposed approach with promising findings .", "keywords": ["bioinformatics", "information integration", "gene selection"], "combined": "Identifying biologically relevant genes via multiple heterogeneous data sources Selection of genes that are differentially expressed and critical to a particular biological process has been a major challenge in post-array analysis . Recent development in bioinformatics has made various data sources available such as mRNA and miRNA expression profiles , biological pathway and gene annotation , etc. . Efficient and effective integration of multiple data sources helps enrich our knowledge about the involved samples and genes for selecting genes bearing significant biological relevance . In this work , we studied a novel problem of multi-source gene selection : given multiple heterogeneous data sources or data sets , select genes from expression profiles by integrating information from various data sources . We investigated how to effectively employ information contained in multiple data sources to extract an intrinsic global geometric pattern and use it in covariance analysis for gene selection . We designed and conducted experiments to systematically compare the proposed approach with representative methods in terms of statistical and biological significance , and showed the efficacy and potential of the proposed approach with promising findings . [[EENNDD]] bioinformatics; information integration; gene selection"}, "Mengenal pasti gen yang relevan secara biologi melalui pelbagai sumber data heterogen Pemilihan gen yang dinyatakan secara berbeza dan kritikal terhadap proses biologi tertentu telah menjadi cabaran utama dalam analisis pasca-array. Perkembangan terkini dalam bioinformatik telah menyediakan pelbagai sumber data seperti profil ekspresi mRNA dan miRNA, jalur biologi dan anotasi gen, dll. Penyatuan pelbagai sumber data dengan berkesan dan berkesan membantu memperkayakan pengetahuan kita mengenai sampel dan gen yang terlibat untuk memilih gen yang mempunyai kaitan dengan biologi yang signifikan. Dalam karya ini, kami mengkaji masalah baru pemilihan gen pelbagai sumber: memandangkan pelbagai sumber data atau set data heterogen, pilih gen dari profil ekspresi dengan mengintegrasikan maklumat dari pelbagai sumber data. Kami menyiasat bagaimana menggunakan maklumat yang terdapat dalam pelbagai sumber data secara berkesan untuk mengekstrak corak geometri global yang intrinsik dan menggunakannya dalam analisis kovarians untuk pemilihan gen. Kami merancang dan menjalankan eksperimen untuk membandingkan pendekatan yang dicadangkan secara sistematik dengan kaedah perwakilan dari segi kepentingan statistik dan biologi, dan menunjukkan keberkesanan dan potensi pendekatan yang dicadangkan dengan penemuan yang menjanjikan. [[EENNDD]] bioinformatik; penyatuan maklumat; pemilihan gen"], [{"string": "Multi-level organization and summarization of the discovered rules", "keywords": ["decision support"], "combined": "Multi-level organization and summarization of the discovered rules [[EENNDD]] decision support"}, "Organisasi pelbagai peringkat dan ringkasan peraturan keputusan [[EENNDD]] yang ditemui menyokong sokongan"], [{"string": "Automatic labeling of multinomial topic models Multinomial distributions over words are frequently used to model topics in text collections . A common , major challenge in applying all such topic models to any text mining problem is to label a multinomial topic model accurately so that a user can interpret the discovered topic . So far , such labels have been generated manually in a subjective way . In this paper , we propose probabilistic approaches to automatically labeling multinomial topic models in an objective way . We cast this labeling problem as an optimization problem involving minimizing Kullback-Leibler divergence between word distributions and maximizing mutual information between a label and a topic model . Experiments with user study have been done on two text data sets with different genres . The results show that the proposed labeling methods are quite effective to generate labels that are meaningful and useful for interpreting the discovered topic models . Our methods are general and can be applied to labeling topics learned through all kinds of topic models such as PLSA , LDA , and their variations .", "keywords": ["multinomial distribution", "topic model labeling", "information search and retrieval", "statistical topic models"], "combined": "Automatic labeling of multinomial topic models Multinomial distributions over words are frequently used to model topics in text collections . A common , major challenge in applying all such topic models to any text mining problem is to label a multinomial topic model accurately so that a user can interpret the discovered topic . So far , such labels have been generated manually in a subjective way . In this paper , we propose probabilistic approaches to automatically labeling multinomial topic models in an objective way . We cast this labeling problem as an optimization problem involving minimizing Kullback-Leibler divergence between word distributions and maximizing mutual information between a label and a topic model . Experiments with user study have been done on two text data sets with different genres . The results show that the proposed labeling methods are quite effective to generate labels that are meaningful and useful for interpreting the discovered topic models . Our methods are general and can be applied to labeling topics learned through all kinds of topic models such as PLSA , LDA , and their variations . [[EENNDD]] multinomial distribution; topic model labeling; information search and retrieval; statistical topic models"}, "Pelabelan automatik model topik multinomial Pengagihan multinomial ke atas perkataan sering digunakan untuk memodelkan topik dalam koleksi teks. Satu cabaran yang umum dan utama dalam menerapkan semua model topik seperti itu pada masalah penambangan teks adalah melabel model topik multinomial dengan tepat sehingga pengguna dapat menafsirkan topik yang ditemui. Setakat ini, label seperti itu dihasilkan secara manual secara subjektif. Dalam makalah ini, kami mencadangkan pendekatan probabilistik untuk melabel model topik multinomial secara objektif secara objektif. Kami meletakkan masalah pelabelan ini sebagai masalah pengoptimuman yang melibatkan meminimumkan perbezaan Kullback-Leibler antara pengedaran kata dan memaksimumkan maklumat bersama antara label dan model topik. Eksperimen dengan kajian pengguna telah dilakukan pada dua set data teks dengan genre yang berbeza. Hasil kajian menunjukkan bahawa kaedah pelabelan yang dicadangkan cukup berkesan untuk menghasilkan label yang bermakna dan berguna untuk menafsirkan model topik yang ditemui. Kaedah kami adalah umum dan dapat diterapkan pada pelabelan topik yang dipelajari melalui semua jenis model topik seperti PLSA, LDA, dan variasinya. [[EENNDD]] pengedaran multinomial; pelabelan model topik; carian dan pengambilan maklumat; model topik statistik"], [{"string": "iSAX : indexing and mining terabyte sized time series Current research in indexing and mining time series data has produced many interesting algorithms and representations . However , the algorithms and the size of data considered have generally not been representative of the increasingly massive datasets encountered in science , engineering , and business domains . In this work , we show how a novel multi-resolution symbolic representation can be used to index datasets which are several orders of magnitude larger than anything else considered in the literature . Our approach allows both fast exact search and ultra fast approximate search . We show how to exploit the combination of both types of search as sub-routines in data mining algorithms , allowing for the exact mining of truly massive real world datasets , containing millions of time series .", "keywords": ["time series", "representations", "indexing"], "combined": "iSAX : indexing and mining terabyte sized time series Current research in indexing and mining time series data has produced many interesting algorithms and representations . However , the algorithms and the size of data considered have generally not been representative of the increasingly massive datasets encountered in science , engineering , and business domains . In this work , we show how a novel multi-resolution symbolic representation can be used to index datasets which are several orders of magnitude larger than anything else considered in the literature . Our approach allows both fast exact search and ultra fast approximate search . We show how to exploit the combination of both types of search as sub-routines in data mining algorithms , allowing for the exact mining of truly massive real world datasets , containing millions of time series . [[EENNDD]] time series; representations; indexing"}, "iSAX: mengindeks dan melombong siri masa bersaiz terabyte Penyelidikan semasa dalam pengindeksan dan data siri masa perlombongan telah menghasilkan banyak algoritma dan perwakilan yang menarik. Walau bagaimanapun, algoritma dan ukuran data yang dipertimbangkan umumnya tidak mewakili kumpulan data yang semakin besar yang dihadapi dalam bidang sains, kejuruteraan, dan perniagaan. Dalam karya ini, kami menunjukkan bagaimana representasi simbolik multi-resolusi baru dapat digunakan untuk mengindeks set data yang beberapa urutan besarnya lebih besar daripada yang lain yang dipertimbangkan dalam literatur. Pendekatan kami membolehkan carian tepat tepat dan carian anggaran ultra cepat. Kami menunjukkan bagaimana memanfaatkan kombinasi kedua-dua jenis carian sebagai sub-rutin dalam algoritma perlombongan data, yang memungkinkan untuk perlombongan data kumpulan dunia nyata yang tepat, yang mengandungi berjuta-juta siri masa. [[EENNDD]] siri masa; perwakilan; pengindeksan"], [{"string": "Training linear SVMs in linear time Linear Support Vector Machines SVMs have become one of the most prominent machine learning techniques for high-dimensional sparse data commonly encountered in applications like text classification , word-sense disambiguation , and drug design . These applications involve a large number of examples n as well as a large number of features N , while each example has only s N non-zero features . This paper presents a Cutting Plane Algorithm for training linear SVMs that provably has training time 0 s , n for classification problems and o sn log n for ordinal regression problems . The algorithm is based on an alternative , but equivalent formulation of the SVM optimization problem . Empirically , the Cutting-Plane Algorithm is several orders of magnitude faster than decomposition methods like svm light for large datasets .", "keywords": ["learning", "training algorithms", "ordinal regression", "large-scale problems", "support vector machines", "roc-area"], "combined": "Training linear SVMs in linear time Linear Support Vector Machines SVMs have become one of the most prominent machine learning techniques for high-dimensional sparse data commonly encountered in applications like text classification , word-sense disambiguation , and drug design . These applications involve a large number of examples n as well as a large number of features N , while each example has only s N non-zero features . This paper presents a Cutting Plane Algorithm for training linear SVMs that provably has training time 0 s , n for classification problems and o sn log n for ordinal regression problems . The algorithm is based on an alternative , but equivalent formulation of the SVM optimization problem . Empirically , the Cutting-Plane Algorithm is several orders of magnitude faster than decomposition methods like svm light for large datasets . [[EENNDD]] learning; training algorithms; ordinal regression; large-scale problems; support vector machines; roc-area"}, "Melatih SVM linear dalam masa linear Mesin Vektor Sokongan Linear SVM telah menjadi salah satu teknik pembelajaran mesin yang paling menonjol untuk data jarang dimensi tinggi yang biasa ditemui dalam aplikasi seperti klasifikasi teks, disambiguasi kata-kata, dan reka bentuk ubat. Aplikasi ini melibatkan sebilangan besar contoh n dan sebilangan besar ciri N, sementara setiap contoh hanya mempunyai ciri N bukan sifar. Makalah ini mengemukakan Algoritma Cutting Plane untuk melatih SVM linear yang terbukti mempunyai masa latihan 0 s, n untuk masalah klasifikasi dan o log log n untuk masalah regresi ordinal. Algoritma ini berdasarkan penggubalan alternatif, tetapi setara dengan masalah pengoptimuman SVM. Secara empirikal, Algoritma Cutting-Plane adalah beberapa urutan magnitud lebih cepat daripada kaedah penguraian seperti cahaya svm untuk set data yang besar. [[EENNDD]] pembelajaran; algoritma latihan; regresi ordinal; masalah berskala besar; mesin vektor sokongan; roc-area"], [{"string": "Balanced allocation with succinct representation Motivated by applications in guaranteed delivery in computational advertising , we consider the general problem of balanced allocation in a bipartite supply-demand setting . Our formulation captures the notion of deviation from being balanced by a convex penalty function . While this formulation admits a convex programming solution , we strive for more robust and scalable algorithms . For the case of L1 penalty functions we obtain a simple combinatorial algorithm based on min-cost flow in graphs and show how to precompute a linear amount of information such that the allocation along any edge can be approximated in constant time . We then extend our combinatorial solution to any convex function by solving a convex cost flow . These scalable methods may have applications in other contexts stipulating balanced allocation . We study the performance of our algorithms on large real-world graphs and show that they are efficient , scalable , and robust in practice .", "keywords": ["maximum flow", "balanced allocation", "convex flow"], "combined": "Balanced allocation with succinct representation Motivated by applications in guaranteed delivery in computational advertising , we consider the general problem of balanced allocation in a bipartite supply-demand setting . Our formulation captures the notion of deviation from being balanced by a convex penalty function . While this formulation admits a convex programming solution , we strive for more robust and scalable algorithms . For the case of L1 penalty functions we obtain a simple combinatorial algorithm based on min-cost flow in graphs and show how to precompute a linear amount of information such that the allocation along any edge can be approximated in constant time . We then extend our combinatorial solution to any convex function by solving a convex cost flow . These scalable methods may have applications in other contexts stipulating balanced allocation . We study the performance of our algorithms on large real-world graphs and show that they are efficient , scalable , and robust in practice . [[EENNDD]] maximum flow; balanced allocation; convex flow"}, "Peruntukan seimbang dengan representasi ringkas Dimotivasi oleh aplikasi dalam penyampaian terjamin dalam iklan komputasi, kami mempertimbangkan masalah umum peruntukan seimbang dalam pengaturan penawaran-permintaan bipartit. Rumusan kami menangkap konsep penyimpangan daripada seimbang dengan fungsi penalti cembung. Walaupun rumusan ini mengakui penyelesaian pengaturcaraan cembung, kami berusaha untuk algoritma yang lebih mantap dan berskala. Untuk fungsi fungsi penalti L1, kami memperoleh algoritma kombinatori sederhana berdasarkan aliran kos min dalam grafik dan menunjukkan bagaimana mengira jumlah maklumat linear sehingga peruntukan sepanjang tepi dapat dihampiri dalam masa yang tetap. Kami kemudian memperluaskan penyelesaian gabungan kami untuk sebarang fungsi cembung dengan menyelesaikan aliran kos cembung. Kaedah berskala ini mungkin memiliki aplikasi dalam konteks lain yang menetapkan peruntukan seimbang. Kami mengkaji prestasi algoritma kami pada grafik dunia nyata yang besar dan menunjukkan bahawa praktiknya cekap, berskala, dan mantap. [[EENNDD]] aliran maksimum; peruntukan seimbang; aliran cembung"], [{"string": "Training structural svms with kernels using sampled cuts Discriminative training for structured outputs has found increasing applications in areas such as natural language processing , bioinformatics , information retrieval , and computer vision . Focusing on large-margin methods , the most general in terms of loss function and model structure training algorithms known to date are based on cutting-plane approaches . While these algorithms are very efficient for linear models , their training complexity becomes quadratic in the number of examples when kernels are used . To overcome this bottleneck , we propose new training algorithms that use approximate cutting planes and random sampling to enable efficient training with kernels . We prove that these algorithms have improved time complexity while providing approximation guarantees . In empirical evaluations , our algorithms produced solutions with training and test error rates close to those of exact solvers . Even on binary classification problems where highly optimized conventional training methods exist e.g. SVM-light , our methods are about an order of magnitude faster than conventional training methods on large datasets , while remaining competitive in speed on datasets of medium size .", "keywords": ["large-scale problems", "learning", "support vector machines", "kernels"], "combined": "Training structural svms with kernels using sampled cuts Discriminative training for structured outputs has found increasing applications in areas such as natural language processing , bioinformatics , information retrieval , and computer vision . Focusing on large-margin methods , the most general in terms of loss function and model structure training algorithms known to date are based on cutting-plane approaches . While these algorithms are very efficient for linear models , their training complexity becomes quadratic in the number of examples when kernels are used . To overcome this bottleneck , we propose new training algorithms that use approximate cutting planes and random sampling to enable efficient training with kernels . We prove that these algorithms have improved time complexity while providing approximation guarantees . In empirical evaluations , our algorithms produced solutions with training and test error rates close to those of exact solvers . Even on binary classification problems where highly optimized conventional training methods exist e.g. SVM-light , our methods are about an order of magnitude faster than conventional training methods on large datasets , while remaining competitive in speed on datasets of medium size . [[EENNDD]] large-scale problems; learning; support vector machines; kernels"}, "Melatih svms struktur dengan kernel menggunakan potongan sampel. Latihan diskriminatif untuk output berstruktur telah mendapati peningkatan aplikasi di bidang seperti pemprosesan bahasa semula jadi, bioinformatik, pencarian maklumat, dan penglihatan komputer. Berfokus pada kaedah margin besar, yang paling umum dari segi fungsi kerugian dan algoritma latihan struktur model yang diketahui sehingga kini adalah berdasarkan pendekatan pemotongan. Walaupun algoritma ini sangat berkesan untuk model linier, kerumitan latihannya menjadi kuadratik dalam jumlah contoh ketika kernel digunakan. Untuk mengatasi masalah ini, kami mencadangkan algoritma latihan baru yang menggunakan pesawat pemotongan anggaran dan persampelan rawak untuk membolehkan latihan yang cekap dengan kernel. Kami membuktikan bahawa algoritma ini telah meningkatkan kerumitan masa sambil memberikan jaminan penghampiran. Dalam penilaian empirikal, algoritma kami menghasilkan penyelesaian dengan kadar kesalahan latihan dan ujian yang hampir dengan penyelesaian yang tepat. Walaupun terdapat masalah klasifikasi binari di mana kaedah latihan konvensional yang dioptimumkan, mis. SVM-light, kaedah kami adalah mengenai urutan magnitud lebih cepat daripada kaedah latihan konvensional pada set data yang besar, sementara tetap kompetitif dalam kelajuan pada set data bersaiz sederhana. [[EENNDD]] masalah berskala besar; belajar; mesin vektor sokongan; kernel"], [{"string": "Critical event prediction for proactive management in large-scale computer clusters As the complexity of distributed computing systems increases , systems management tasks require significantly higher levels of automation ; examples include diagnosis and prediction based on real-time streams of computer events , setting alarms , and performing continuous monitoring . The core of autonomic computing , a recently proposed initiative towards next-generation IT-systems capable of ` self-healing ' , is the ability to analyze data in real-time and to predict potential problems . The goal is to avoid catastrophic failures through prompt execution of remedial actions . This paper describes an attempt to build a proactive prediction and control system for large clusters . We collected event logs containing various system reliability , availability and serviceability RAS events , and system activity reports SARs from a 350-node cluster system for a period of one year . The ` raw ' system health measurements contain a great deal of redundant event data , which is either repetitive in nature or misaligned with respect to time . We applied a filtering technique and modeled the data into a set of primary and derived variables . These variables used probabilistic networks for establishing event correlations through prediction algorithms . We also evaluated the role of time-series methods , rule-based classification algorithms and Bayesian network models in event prediction . Based on historical data , our results suggest that it is feasible to predict system performance parameters SARs with a high degree of accuracy using time-series models . Rule-based classification techniques can be used to extract machine-event signatures to predict critical events with up to 70 % accuracy .", "keywords": ["critical event prediction", "learning", "system event log", "database applications", "large-scale clusters"], "combined": "Critical event prediction for proactive management in large-scale computer clusters As the complexity of distributed computing systems increases , systems management tasks require significantly higher levels of automation ; examples include diagnosis and prediction based on real-time streams of computer events , setting alarms , and performing continuous monitoring . The core of autonomic computing , a recently proposed initiative towards next-generation IT-systems capable of ` self-healing ' , is the ability to analyze data in real-time and to predict potential problems . The goal is to avoid catastrophic failures through prompt execution of remedial actions . This paper describes an attempt to build a proactive prediction and control system for large clusters . We collected event logs containing various system reliability , availability and serviceability RAS events , and system activity reports SARs from a 350-node cluster system for a period of one year . The ` raw ' system health measurements contain a great deal of redundant event data , which is either repetitive in nature or misaligned with respect to time . We applied a filtering technique and modeled the data into a set of primary and derived variables . These variables used probabilistic networks for establishing event correlations through prediction algorithms . We also evaluated the role of time-series methods , rule-based classification algorithms and Bayesian network models in event prediction . Based on historical data , our results suggest that it is feasible to predict system performance parameters SARs with a high degree of accuracy using time-series models . Rule-based classification techniques can be used to extract machine-event signatures to predict critical events with up to 70 % accuracy . [[EENNDD]] critical event prediction; learning; system event log; database applications; large-scale clusters"}, "Ramalan peristiwa kritikal untuk pengurusan proaktif dalam kelompok komputer berskala besar Oleh kerana kerumitan sistem pengkomputeran yang diedarkan meningkat, tugas pengurusan sistem memerlukan tahap automasi yang jauh lebih tinggi; contohnya meliputi diagnosis dan ramalan berdasarkan aliran peristiwa komputer masa nyata, menetapkan penggera, dan melakukan pemantauan berterusan. Inti pengkomputeran autonomi, inisiatif yang baru-baru ini dicadangkan ke arah sistem IT generasi seterusnya yang mampu \"penyembuhan diri\", adalah kemampuan untuk menganalisis data dalam masa nyata dan untuk meramalkan kemungkinan masalah. Tujuannya adalah untuk mengelakkan kegagalan bencana melalui pelaksanaan tindakan pemulihan yang segera. Makalah ini menerangkan percubaan untuk membina sistem ramalan dan kawalan proaktif untuk kelompok besar. Kami mengumpulkan log peristiwa yang mengandungi pelbagai kebolehpercayaan sistem, ketersediaan dan keseragaman RAS peristiwa, dan aktiviti aktiviti melaporkan SAR dari sistem kluster 350-node untuk jangka masa satu tahun. Pengukuran kesihatan sistem \"mentah\" mengandungi banyak data kejadian berlebihan, yang sama ada berulang-ulang atau tidak sesuai dengan masa. Kami menerapkan teknik penyaringan dan memodelkan data ke dalam sekumpulan pemboleh ubah primer dan turunan. Pemboleh ubah ini menggunakan rangkaian probabilistik untuk mewujudkan korelasi peristiwa melalui algoritma ramalan. Kami juga menilai peranan kaedah siri masa, algoritma klasifikasi berdasarkan peraturan dan model rangkaian Bayesian dalam ramalan peristiwa. Berdasarkan data sejarah, hasil kami menunjukkan bahawa layak untuk memprediksi parameter prestasi sistem SAR dengan tahap ketepatan yang tinggi menggunakan model siri masa. Teknik klasifikasi berdasarkan peraturan dapat digunakan untuk mengekstrak tandatangan acara mesin untuk meramalkan peristiwa kritikal dengan ketepatan hingga 70%. [[EENNDD]] ramalan peristiwa kritikal; belajar; log acara sistem; aplikasi pangkalan data; kelompok besar"], [{"string": "Data mining in metric space : an empirical analysis of supervised learning performance criteria Many criteria can be used to evaluate the performance of supervised learning . Different criteria are appropriate in different settings , and it is not always clear which criteria to use . A further complication is that learning methods that perform well on one criterion may not perform well on other criteria . For example , SVMs and boosting are designed to optimize accuracy , whereas neural nets typically optimize squared error or cross entropy . We conducted an empirical study using a variety of learning methods SVMs , neural nets , k-nearest neighbor , bagged and boosted trees , and boosted stumps to compare nine boolean classification performance metrics : Accuracy , Lift , F-Score , Area under the ROC Curve , Average Precision , Precision\\/Recall Break-Even Point , Squared Error , Cross Entropy , and Probability Calibration . Multidimensional scaling MDS shows that these metrics span a low dimensional manifold . The three metrics that are appropriate when predictions are interpreted as probabilities : squared error , cross entropy , and calibration , lay in one part of metric space far away from metrics that depend on the relative order of the predicted values : ROC area , average precision , break-even point , and lift . In between them fall two metrics that depend on comparing predictions to a threshold : accuracy and F-score . As expected , maximum margin methods such as SVMs and boosted trees have excellent performance on metrics like accuracy , but perform poorly on probability metrics such as squared error . What was not expected was that the margin methods have excellent performance on ordering metrics such as ROC area and average precision . We introduce a new metric , SAR , that combines squared error , accuracy , and ROC area into one metric . MDS and correlation analysis shows that SAR is centrally located and correlates well with other metrics , suggesting that it is a good general purpose metric to use when more specific criteria are not known .", "keywords": ["roc", "precision", "lift", "cross entropy", "supervised learning", "recall", "metrics", "performance evaluation"], "combined": "Data mining in metric space : an empirical analysis of supervised learning performance criteria Many criteria can be used to evaluate the performance of supervised learning . Different criteria are appropriate in different settings , and it is not always clear which criteria to use . A further complication is that learning methods that perform well on one criterion may not perform well on other criteria . For example , SVMs and boosting are designed to optimize accuracy , whereas neural nets typically optimize squared error or cross entropy . We conducted an empirical study using a variety of learning methods SVMs , neural nets , k-nearest neighbor , bagged and boosted trees , and boosted stumps to compare nine boolean classification performance metrics : Accuracy , Lift , F-Score , Area under the ROC Curve , Average Precision , Precision\\/Recall Break-Even Point , Squared Error , Cross Entropy , and Probability Calibration . Multidimensional scaling MDS shows that these metrics span a low dimensional manifold . The three metrics that are appropriate when predictions are interpreted as probabilities : squared error , cross entropy , and calibration , lay in one part of metric space far away from metrics that depend on the relative order of the predicted values : ROC area , average precision , break-even point , and lift . In between them fall two metrics that depend on comparing predictions to a threshold : accuracy and F-score . As expected , maximum margin methods such as SVMs and boosted trees have excellent performance on metrics like accuracy , but perform poorly on probability metrics such as squared error . What was not expected was that the margin methods have excellent performance on ordering metrics such as ROC area and average precision . We introduce a new metric , SAR , that combines squared error , accuracy , and ROC area into one metric . MDS and correlation analysis shows that SAR is centrally located and correlates well with other metrics , suggesting that it is a good general purpose metric to use when more specific criteria are not known . [[EENNDD]] roc; precision; lift; cross entropy; supervised learning; recall; metrics; performance evaluation"}, "Perlombongan data dalam ruang metrik: analisis empirikal kriteria prestasi pembelajaran yang diselia. Banyak kriteria boleh digunakan untuk menilai prestasi pembelajaran yang diselia. Kriteria yang berbeza sesuai dalam tetapan yang berbeza, dan tidak selalu jelas kriteria mana yang harus digunakan. Komplikasi selanjutnya adalah bahawa kaedah pembelajaran yang berprestasi baik pada satu kriteria mungkin tidak menunjukkan prestasi yang baik pada kriteria lain. Sebagai contoh, SVM dan peningkatan dirancang untuk mengoptimumkan ketepatan, sedangkan jaringan saraf biasanya mengoptimumkan ralat kuasa dua atau entropi silang. Kami melakukan kajian empirik menggunakan pelbagai kaedah pembelajaran SVM, jaring neural, jiran terdekat, pohon yang diikat dan dorong, dan meningkatkan tunggul untuk membandingkan sembilan metrik prestasi klasifikasi boolean: Accuracy, Lift, F-Score, Area under the ROC Curve , Ketepatan Purata, Ketepatan \\ / Ingat Titik Pecahan, Kesalahan Kuadrat, Entropi Lintang, dan Kalibrasi Kebarangkalian. MDS penskalaan multidimensi menunjukkan bahawa metrik ini merangkumi manifold dimensi rendah. Tiga metrik yang sesuai apabila ramalan ditafsirkan sebagai kebarangkalian: ralat kuasa dua, entropi silang, dan penentukuran, terletak di satu bahagian ruang metrik yang jauh dari metrik yang bergantung pada susunan relatif nilai yang diramalkan: luas ROC, ketepatan rata-rata, titik pulang modal, dan angkat. Di antara keduanya jatuh dua metrik yang bergantung pada membandingkan ramalan dengan ambang: ketepatan dan skor-F. Seperti yang dijangkakan, kaedah margin maksimum seperti SVM dan pohon yang ditingkatkan mempunyai prestasi yang sangat baik pada metrik seperti ketepatan, tetapi berprestasi buruk pada metrik kebarangkalian seperti ralat kuasa dua. Apa yang tidak diharapkan ialah kaedah margin mempunyai prestasi yang sangat baik dalam memerintahkan metrik seperti kawasan ROC dan ketepatan rata-rata. Kami memperkenalkan metrik baru, SAR, yang menggabungkan ralat kuasa dua, ketepatan, dan luas ROC menjadi satu metrik. Analisis MDS dan korelasi menunjukkan bahawa SAR terletak di pusat dan berkorelasi dengan baik dengan metrik lain, menunjukkan bahawa metrik tujuan umum yang baik digunakan apabila kriteria yang lebih spesifik tidak diketahui. [[EENNDD]] roc; ketepatan; lif; entropi silang; pembelajaran yang diselia; ingat semula; sukatan; penilaian prestasi"], [{"string": "Efficient discovery of error-tolerant frequent itemsets in high dimensions We present a generalization of frequent itemsets allowing for the notion of errors in the itemset definition . We motivate the problem and present an efficient algorithm that identifies error-tolerant frequent clusters of items in transactional data customer-purchase data , web browsing data , text , etc. . The algorithm exploits sparseness of the underlying data to find large groups of items that are correlated over database records rows . The notion of transaction coverage allows us to extend the algorithm and view it as a fast clustering algorithm for discovering segments of similar transactions in binary sparse data . We evaluate the new algorithm on three real-world applications : clustering high-dimensional data , query selectivity estimation and collaborative filtering . Results show that the algorithm consistently uncovers structure in large sparse databases that other traditional clustering algorithms fail to find .", "keywords": ["collaborative filtering", "query selectivity estimation", "error-tolerant frequent itemset", "high dimensions"], "combined": "Efficient discovery of error-tolerant frequent itemsets in high dimensions We present a generalization of frequent itemsets allowing for the notion of errors in the itemset definition . We motivate the problem and present an efficient algorithm that identifies error-tolerant frequent clusters of items in transactional data customer-purchase data , web browsing data , text , etc. . The algorithm exploits sparseness of the underlying data to find large groups of items that are correlated over database records rows . The notion of transaction coverage allows us to extend the algorithm and view it as a fast clustering algorithm for discovering segments of similar transactions in binary sparse data . We evaluate the new algorithm on three real-world applications : clustering high-dimensional data , query selectivity estimation and collaborative filtering . Results show that the algorithm consistently uncovers structure in large sparse databases that other traditional clustering algorithms fail to find . [[EENNDD]] collaborative filtering; query selectivity estimation; error-tolerant frequent itemset; high dimensions"}, "Penemuan cekap itemets toleransi ralat dalam dimensi yang tinggi Kami membentangkan generalisasi kumpulan item yang kerap yang memungkinkan pengertian kesilapan dalam definisi itemet. Kami memotivasi masalah dan menyajikan algoritma yang cekap yang mengenal pasti kumpulan item yang sering bertoleransi ralat dalam data pembelian pelanggan data transaksi, data penyemak imbas web, teks, dll. Algoritma mengeksploitasi jarang data yang mendasari untuk mencari kumpulan besar item yang berkorelasi dengan baris rekod pangkalan data. Pengertian liputan transaksi membolehkan kita memperluas algoritma dan melihatnya sebagai algoritma pengelompokan cepat untuk menemui segmen transaksi serupa dalam data jarang binari. Kami menilai algoritma baru pada tiga aplikasi dunia nyata: pengelompokan data dimensi tinggi, anggaran selektiviti pertanyaan dan penapisan kolaboratif. Hasil menunjukkan bahawa algoritma secara konsisten menemui struktur dalam pangkalan data jarang yang tidak dijumpai oleh algoritma pengelompokan tradisional lain. [[EENNDD]] penapisan kolaboratif; anggaran selektiviti pertanyaan; itemet yang kerap bertolak ansur; dimensi tinggi"], [{"string": "Discovering similar patterns in time series", "keywords": ["knowledge discovery", "time series"], "combined": "Discovering similar patterns in time series [[EENNDD]] knowledge discovery; time series"}, "Menemui corak yang serupa dalam penemuan pengetahuan siri masa [[EENNDD]]; siri masa"], [{"string": "Bursty and hierarchical structure in streams A fundamental problem in text data mining is to extract meaningful structure from document streams that arrive continuously over time . E-mail and news articles are two natural examples of such streams , each characterized by topics that appear , grow in intensity for a period of time , and then fade away . The published literature in a particular research field can be seen to exhibit similar phenomena over a much longer time scale . Underlying much of the text mining work in this area is the following intuitive premise -- that the appearance of a topic in a document stream is signaled by a `` burst of activity , '' with certain features rising sharply in frequency as the topic emerges . The goal of the present work is to develop a formal approach for modeling such `` bursts , '' in such a way that they can be robustly and efficiently identified , and can provide an organizational framework for analyzing the underlying content . The approach is based on modeling the stream using an infinite-state automaton , in which bursts appear naturally as state transitions ; in some ways , it can be viewed as drawing an analogy with models from queueing theory for bursty network traffic . The resulting algorithms are highly efficient , and yield a nested representation of the set of bursts that imposes a hierarchical structure on the overall stream . Experiments with e-mail and research paper archives suggest that the resulting structures have a natural meaning in terms of the content that gave rise to them .", "keywords": ["probabilistic algorithms"], "combined": "Bursty and hierarchical structure in streams A fundamental problem in text data mining is to extract meaningful structure from document streams that arrive continuously over time . E-mail and news articles are two natural examples of such streams , each characterized by topics that appear , grow in intensity for a period of time , and then fade away . The published literature in a particular research field can be seen to exhibit similar phenomena over a much longer time scale . Underlying much of the text mining work in this area is the following intuitive premise -- that the appearance of a topic in a document stream is signaled by a `` burst of activity , '' with certain features rising sharply in frequency as the topic emerges . The goal of the present work is to develop a formal approach for modeling such `` bursts , '' in such a way that they can be robustly and efficiently identified , and can provide an organizational framework for analyzing the underlying content . The approach is based on modeling the stream using an infinite-state automaton , in which bursts appear naturally as state transitions ; in some ways , it can be viewed as drawing an analogy with models from queueing theory for bursty network traffic . The resulting algorithms are highly efficient , and yield a nested representation of the set of bursts that imposes a hierarchical structure on the overall stream . Experiments with e-mail and research paper archives suggest that the resulting structures have a natural meaning in terms of the content that gave rise to them . [[EENNDD]] probabilistic algorithms"}, "Struktur pecah dan hierarki dalam aliran Masalah mendasar dalam perlombongan data teks adalah untuk mengekstrak struktur yang bermakna dari aliran dokumen yang tiba secara berterusan dari masa ke masa. E-mel dan artikel berita adalah dua contoh semula jadi aliran tersebut, masing-masing dicirikan oleh topik yang muncul, bertambah intensif untuk jangka waktu tertentu, dan kemudian hilang. Literatur yang diterbitkan dalam bidang penyelidikan tertentu dapat dilihat menunjukkan fenomena serupa dalam skala masa yang lebih lama. Yang mendasari sebahagian besar pekerjaan penambangan teks di kawasan ini adalah premis intuitif berikut - bahawa penampilan topik dalam aliran dokumen ditandakan oleh \"ledakan aktiviti,\" dengan ciri-ciri tertentu meningkat dengan frekuensi mendadak ketika topik itu muncul . Tujuan kerja ini adalah untuk mengembangkan pendekatan formal untuk memodelkan \"ledakan,\" sedemikian rupa sehingga mereka dapat dikenal pasti dengan kuat dan efisien, dan dapat menyediakan kerangka organisasi untuk menganalisis konten yang mendasari. Pendekatan ini didasarkan pada pemodelan aliran menggunakan automaton keadaan tak terbatas, di mana pecah muncul secara semula jadi sebagai peralihan keadaan; dalam beberapa cara, ia dapat dilihat sebagai menggambar analogi dengan model dari teori antrian untuk lalu lintas rangkaian yang padat. Algoritma yang dihasilkan sangat efisien, dan menghasilkan representasi bersarang dari set pecah yang memaksakan struktur hierarki pada aliran keseluruhan. Eksperimen dengan e-mel dan arkib kertas penyelidikan menunjukkan bahawa struktur yang dihasilkan mempunyai makna semula jadi dari segi kandungan yang menimbulkannya. [[EENNDD]] algoritma probabilistik"], [{"string": "Visualization support for a user-centered KDD process Viewing knowledge discovery as a user-centered process that requires an effective collaboration between the user and the discovery system , our work aims to support an active role of the user in that process by developing synergistic visualization tools integrated in our discovery system D2MS . These tools provide an ability of visualizing the entire process of knowledge discovery in order to help the user with data preprocessing , selecting mining algorithms and parameters , evaluating and comparing discovered models , and taking control of the whole discover process . Our case-studies with two medical datasets on meningitis and stomach cancer show that , with visualization tools in D2MS , the user gains better insight in each step of the knowledge discovery process as well the relationship between data and discovered knowledge .", "keywords": ["the user's active role", "knowledge discovery process", "representations", "model selection", "data and knowledge visualization"], "combined": "Visualization support for a user-centered KDD process Viewing knowledge discovery as a user-centered process that requires an effective collaboration between the user and the discovery system , our work aims to support an active role of the user in that process by developing synergistic visualization tools integrated in our discovery system D2MS . These tools provide an ability of visualizing the entire process of knowledge discovery in order to help the user with data preprocessing , selecting mining algorithms and parameters , evaluating and comparing discovered models , and taking control of the whole discover process . Our case-studies with two medical datasets on meningitis and stomach cancer show that , with visualization tools in D2MS , the user gains better insight in each step of the knowledge discovery process as well the relationship between data and discovered knowledge . [[EENNDD]] the user's active role; knowledge discovery process; representations; model selection; data and knowledge visualization"}, "Sokongan visualisasi untuk proses KDD yang berpusat pada pengguna Melihat penemuan pengetahuan sebagai proses yang berpusatkan pengguna yang memerlukan kerjasama yang berkesan antara pengguna dan sistem penemuan, kerja kami bertujuan untuk menyokong peranan aktif pengguna dalam proses itu dengan mengembangkan alat visualisasi sinergis bersepadu dalam sistem penemuan kami D2MS. Alat-alat ini memberikan kemampuan untuk memvisualisasikan keseluruhan proses penemuan pengetahuan untuk membantu pengguna dengan proses pra-data, memilih algoritma dan parameter perlombongan, menilai dan membandingkan model yang ditemui, dan mengendalikan keseluruhan proses penemuan. Kajian kes kami dengan dua set data perubatan mengenai meningitis dan barah perut menunjukkan bahawa, dengan alat visualisasi di D2MS, pengguna memperoleh wawasan yang lebih baik dalam setiap langkah proses penemuan pengetahuan serta hubungan antara data dan pengetahuan yang ditemui. [[EENNDD]] peranan aktif pengguna; proses penemuan pengetahuan; perwakilan; pemilihan model; visualisasi data dan pengetahuan"], [{"string": "Identifying early buyers from purchase data Market research has shown that consumers exhibit a variety of different purchasing behaviors ; specifically , some tend to purchase products earlier than other consumers . Identifying such early buyers can help personalize marketing strategies , potentially improving their effectiveness . In this paper , we present a non-parametric approach to the problem of identifying early buyers from purchase data . Our formulation takes as inputs the detailed purchase information of each consumer , with which we construct a weighted directed graph whose nodes correspond to consumers and whose edges correspond to purchases consumers have in common ; the edge weights indicate how frequently consumers purchase products earlier than other consumers . Identifying early buyers corresponds to the problem of finding a subset of nodes in the graph with maximum difference between the weights of the outgoing and incoming edges . This problem is a variation of the maximum cut problem in a directed graph . We provide an approximation algorithm based on semidefinite programming SDP relaxations pioneered by Goemans and Williamson , and analyze its performance . We apply the algorithm to real purchase data from Amazon.com , providing new insights into consumer behaviors .", "keywords": ["semidefinite programming", "social network", "consumer behavior", "early buyers"], "combined": "Identifying early buyers from purchase data Market research has shown that consumers exhibit a variety of different purchasing behaviors ; specifically , some tend to purchase products earlier than other consumers . Identifying such early buyers can help personalize marketing strategies , potentially improving their effectiveness . In this paper , we present a non-parametric approach to the problem of identifying early buyers from purchase data . Our formulation takes as inputs the detailed purchase information of each consumer , with which we construct a weighted directed graph whose nodes correspond to consumers and whose edges correspond to purchases consumers have in common ; the edge weights indicate how frequently consumers purchase products earlier than other consumers . Identifying early buyers corresponds to the problem of finding a subset of nodes in the graph with maximum difference between the weights of the outgoing and incoming edges . This problem is a variation of the maximum cut problem in a directed graph . We provide an approximation algorithm based on semidefinite programming SDP relaxations pioneered by Goemans and Williamson , and analyze its performance . We apply the algorithm to real purchase data from Amazon.com , providing new insights into consumer behaviors . [[EENNDD]] semidefinite programming; social network; consumer behavior; early buyers"}, "Mengenal pasti pembeli awal dari data pembelian Penyelidikan pasaran menunjukkan bahawa pengguna menunjukkan pelbagai tingkah laku pembelian yang berbeza; secara khusus, ada yang cenderung membeli produk lebih awal daripada pengguna lain. Mengenal pasti pembeli awal seperti itu dapat membantu memperibadikan strategi pemasaran, berpotensi meningkatkan keberkesanannya. Dalam makalah ini, kami menyajikan pendekatan non-parametrik untuk masalah mengenal pasti pembeli awal dari data pembelian. Rumusan kami mengambil sebagai input maklumat pembelian terperinci bagi setiap pengguna, dengan mana kami membuat graf terarah berwajaran yang nodenya sesuai dengan pengguna dan yang tepi sesuai dengan pembelian yang sama persamaan dengan pengguna; bobot tepi menunjukkan berapa kerap pengguna membeli produk lebih awal daripada pengguna lain. Mengenalpasti pembeli awal sesuai dengan masalah mencari subset nod dalam grafik dengan perbezaan maksimum antara berat tepi keluar dan keluar. Masalah ini adalah variasi dari masalah pemotongan maksimum dalam graf yang diarahkan. Kami menyediakan algoritma penghampiran berdasarkan kelonggaran SDP pengaturcaraan semidefinite yang dipelopori oleh Goemans dan Williamson, dan menganalisis prestasinya. Kami menerapkan algoritma untuk data pembelian sebenar dari Amazon.com, memberikan pandangan baru mengenai tingkah laku pengguna. [[EENNDD]] pengaturcaraan semidefinite; rangkaian sosial; tingkah laku pengguna; pembeli awal"], [{"string": "Exploiting vulnerability to secure user privacy on a social networking site As one 's social network expands , a user 's privacy protection goes beyond his privacy settings and becomes a social networking problem . In this research , we aim to address some critical issues related to privacy protection : Would the highest privacy settings guarantee a secure protection ? Given the open nature of social networking sites , is it possible to manage one 's privacy protection ? With the diversity of one 's social media friends , how can one figure out an effective approach to balance between vulnerability and privacy ? We present a novel way to define a vulnerable friend from an individual user 's perspective is dependent on whether or not the user 's friends ' privacy settings protect the friend and the individual 's network of friends which includes the user . As a single vulnerable friend in a user 's social network might place all friends at risk , we resort to experiments and observe how much security an individual user can improve by unfriending a vulnerable friend . We also show how privacy weakens if newly accepted friends are unguarded or unprotected . This work provides a large-scale evaluation of new security and privacy indexes using a Facebook dataset . We present and discuss a new perspective for reasoning about social networking security . When a user accepts a new friend , the user should ensure that the new friend is not an increased security risk with the potential of negatively impacting the entire friend network . Additionally , by leveraging the indexes proposed and employing new strategies for unfriending vulnerable friends , it is possible to further improve security and privacy without changing the social networking site 's existing architecture .", "keywords": ["privacy", "social network", "vulnerability"], "combined": "Exploiting vulnerability to secure user privacy on a social networking site As one 's social network expands , a user 's privacy protection goes beyond his privacy settings and becomes a social networking problem . In this research , we aim to address some critical issues related to privacy protection : Would the highest privacy settings guarantee a secure protection ? Given the open nature of social networking sites , is it possible to manage one 's privacy protection ? With the diversity of one 's social media friends , how can one figure out an effective approach to balance between vulnerability and privacy ? We present a novel way to define a vulnerable friend from an individual user 's perspective is dependent on whether or not the user 's friends ' privacy settings protect the friend and the individual 's network of friends which includes the user . As a single vulnerable friend in a user 's social network might place all friends at risk , we resort to experiments and observe how much security an individual user can improve by unfriending a vulnerable friend . We also show how privacy weakens if newly accepted friends are unguarded or unprotected . This work provides a large-scale evaluation of new security and privacy indexes using a Facebook dataset . We present and discuss a new perspective for reasoning about social networking security . When a user accepts a new friend , the user should ensure that the new friend is not an increased security risk with the potential of negatively impacting the entire friend network . Additionally , by leveraging the indexes proposed and employing new strategies for unfriending vulnerable friends , it is possible to further improve security and privacy without changing the social networking site 's existing architecture . [[EENNDD]] privacy; social network; vulnerability"}, "Mengeksploitasi kerentanan untuk menjamin privasi pengguna di laman jaringan sosial Ketika jaringan sosial seseorang berkembang, perlindungan privasi pengguna melampaui tetapan privasinya dan menjadi masalah rangkaian sosial. Dalam penyelidikan ini, kami bertujuan untuk mengatasi beberapa masalah penting yang berkaitan dengan perlindungan privasi: Adakah tetapan privasi tertinggi menjamin perlindungan yang selamat? Memandangkan sifat terbuka laman rangkaian sosial, adakah mungkin untuk mengatur perlindungan privasi seseorang? Dengan kepelbagaian rakan media sosial seseorang, bagaimana seseorang dapat mengetahui pendekatan yang berkesan untuk menyeimbangkan antara kerentanan dan privasi? Kami menyajikan cara baru untuk mendefinisikan teman yang rentan dari perspektif pengguna individu adalah bergantung pada sama ada tetapan privasi rakan pengguna melindungi rakan dan rangkaian rakan individu yang merangkumi pengguna. Sebagai rakan tunggal yang rentan di rangkaian sosial pengguna mungkin membahayakan semua rakan, kami menggunakan eksperimen dan memerhatikan seberapa banyak keselamatan yang dapat ditingkatkan oleh pengguna individu dengan tidak berteman dengan rakan yang rentan. Kami juga menunjukkan bagaimana privasi melemahkan jika rakan yang baru diterima tidak dijaga atau tidak dilindungi. Karya ini memberikan penilaian besar-besaran terhadap indeks keselamatan dan privasi baru menggunakan set data Facebook. Kami membentangkan dan membincangkan perspektif baru untuk membuat pertimbangan mengenai keselamatan rangkaian sosial. Apabila pengguna menerima rakan baru, pengguna harus memastikan bahawa rakan baru itu tidak meningkatkan risiko keselamatan dengan potensi memberi kesan negatif kepada seluruh rangkaian rakan. Selain itu, dengan memanfaatkan indeks yang diusulkan dan menggunakan strategi baru untuk tidak berteman dengan teman yang rentan, adalah mungkin untuk meningkatkan keselamatan dan privasi tanpa mengubah arsitektur laman web sosial yang ada. [[EENNDD]] privasi; rangkaian sosial; kerentanan"], [{"string": "Mining statistically important equivalence classes and delta-discriminative emerging patterns The support-confidence framework is the most common measure used in itemset mining algorithms , for its antimonotonicity that effectively simplifies the search lattice . This computational convenience brings both quality and statistical flaws to the results as observed by many previous studies . In this paper , we introduce a novel algorithm that produces itemsets with ranked statistical merits under sophisticated test statistics such as chi-square , risk ratio , odds ratio , etc. . Our algorithm is based on the concept of equivalence classes . An equivalence class is a set of frequent itemsets that always occur together in the same set of transactions . Therefore , itemsets within an equivalence class all share the same level of statistical significance regardless of the variety of test statistics . As an equivalence class can be uniquely determined and concisely represented by a closed pattern and a set of generators , we just mine closed patterns and generators , taking a simultaneous depth-first search scheme . This parallel approach has not been exploited by any prior work . We evaluate our algorithm on two aspects . In general , we compare to LCM and FPclose which are the best algorithms tailored for mining only closed patterns . In particular , we compare to epMiner which is the most recent algorithm for mining a type of relative risk patterns , known as minimal emerging patterns . Experimental results show that our algorithm is faster than all of them , sometimes even multiple orders of magnitude faster . These statistically ranked patterns and the efficiency have a high potential for real-life applications , especially in biomedical and financial fields where classical test statistics are of dominant interest .", "keywords": ["equivalence classes", "itemsets with ranked statistical merit"], "combined": "Mining statistically important equivalence classes and delta-discriminative emerging patterns The support-confidence framework is the most common measure used in itemset mining algorithms , for its antimonotonicity that effectively simplifies the search lattice . This computational convenience brings both quality and statistical flaws to the results as observed by many previous studies . In this paper , we introduce a novel algorithm that produces itemsets with ranked statistical merits under sophisticated test statistics such as chi-square , risk ratio , odds ratio , etc. . Our algorithm is based on the concept of equivalence classes . An equivalence class is a set of frequent itemsets that always occur together in the same set of transactions . Therefore , itemsets within an equivalence class all share the same level of statistical significance regardless of the variety of test statistics . As an equivalence class can be uniquely determined and concisely represented by a closed pattern and a set of generators , we just mine closed patterns and generators , taking a simultaneous depth-first search scheme . This parallel approach has not been exploited by any prior work . We evaluate our algorithm on two aspects . In general , we compare to LCM and FPclose which are the best algorithms tailored for mining only closed patterns . In particular , we compare to epMiner which is the most recent algorithm for mining a type of relative risk patterns , known as minimal emerging patterns . Experimental results show that our algorithm is faster than all of them , sometimes even multiple orders of magnitude faster . These statistically ranked patterns and the efficiency have a high potential for real-life applications , especially in biomedical and financial fields where classical test statistics are of dominant interest . [[EENNDD]] equivalence classes; itemsets with ranked statistical merit"}, "Perlombongan kelas kesetaraan yang penting secara statistik dan corak muncul diskriminatif delta Kerangka keyakinan-sokongan adalah ukuran yang paling biasa digunakan dalam algoritma perlombongan itemet, kerana antimonotoniknya yang secara efektif mempermudah kisi pencarian. Kemudahan pengkomputeran ini membawa kekurangan kualiti dan statistik terhadap hasil seperti yang diperhatikan oleh banyak kajian sebelumnya. Dalam makalah ini, kami memperkenalkan algoritma novel yang menghasilkan set item dengan kelebihan statistik peringkat berdasarkan statistik ujian canggih seperti chi-square, ratio ratio, odds ratio, dll. Algoritma kami berdasarkan konsep kelas kesetaraan. Kelas kesetaraan adalah sekumpulan kumpulan barang yang selalu berlaku bersama dalam satu set urus niaga yang sama. Oleh itu, kumpulan item dalam kelas kesetaraan semuanya mempunyai tahap kepentingan statistik yang sama tanpa mengira pelbagai statistik ujian. Oleh kerana kelas kesetaraan dapat ditentukan secara unik dan diwakili secara ringkas oleh corak tertutup dan sekumpulan penjana, kami hanya melantik corak dan penjana tertutup, dengan menggunakan skema carian pertama yang mendalam. Pendekatan selari ini belum dimanfaatkan oleh karya sebelumnya. Kami menilai algoritma kami pada dua aspek. Secara umum, kami membandingkan dengan LCM dan FPclose yang merupakan algoritma terbaik yang disesuaikan untuk perlombongan corak tertutup sahaja. Secara khusus, kami membandingkan dengan epMiner yang merupakan algoritma terbaru untuk melombong jenis corak risiko relatif, yang dikenali sebagai pola muncul minimum. Hasil eksperimen menunjukkan bahawa algoritma kami lebih cepat daripada semua itu, kadang-kadang bahkan banyak pesanan berukuran lebih cepat. Corak dan kecekapan yang diberi peringkat statistik ini berpotensi tinggi untuk aplikasi kehidupan nyata, terutama dalam bidang bioperubatan dan kewangan di mana statistik ujian klasik mempunyai kepentingan dominan. [[EENNDD]] kelas kesetaraan; set item dengan merit statistik yang diperingkat"], [{"string": "Using relational knowledge discovery to prevent securities fraud We describe an application of relational knowledge discovery to a key regulatory mission of the National Association of Securities Dealers NASD . NASD is the world 's largest private-sector securities regulator , with responsibility for preventing and discovering misconduct among securities brokers . Our goal was to help focus NASD 's limited regulatory resources on the brokers who are most likely to engage in securities violations . Using statistical relational learning algorithms , we developed models that rank brokers with respect to the probability that they would commit a serious violation of securities regulations in the near future . Our models incorporate organizational relationships among brokers e.g. , past coworker , which domain experts consider important but have not been easily used before now . The learned models were subjected to an extensive evaluation using more than 18 months of data unseen by the model developers and comprising over two person weeks of effort by NASD staff . Model predictions were found to correlate highly with the subjective evaluations of experienced NASD examiners . Furthermore , in all performance measures , our models performed as well as or better than the handcrafted rules that are currently in use at NASD .", "keywords": ["fraud detection", "learning", "statistical relational learning", "relational probability trees"], "combined": "Using relational knowledge discovery to prevent securities fraud We describe an application of relational knowledge discovery to a key regulatory mission of the National Association of Securities Dealers NASD . NASD is the world 's largest private-sector securities regulator , with responsibility for preventing and discovering misconduct among securities brokers . Our goal was to help focus NASD 's limited regulatory resources on the brokers who are most likely to engage in securities violations . Using statistical relational learning algorithms , we developed models that rank brokers with respect to the probability that they would commit a serious violation of securities regulations in the near future . Our models incorporate organizational relationships among brokers e.g. , past coworker , which domain experts consider important but have not been easily used before now . The learned models were subjected to an extensive evaluation using more than 18 months of data unseen by the model developers and comprising over two person weeks of effort by NASD staff . Model predictions were found to correlate highly with the subjective evaluations of experienced NASD examiners . Furthermore , in all performance measures , our models performed as well as or better than the handcrafted rules that are currently in use at NASD . [[EENNDD]] fraud detection; learning; statistical relational learning; relational probability trees"}, "Menggunakan penemuan pengetahuan relasional untuk mencegah penipuan sekuriti Kami menerangkan aplikasi penemuan pengetahuan hubungan ke misi pengawalseliaan utama Persatuan Nasional Penjual Sekuriti NASD. NASD adalah pengawal selia sektor swasta terbesar di dunia, dengan tanggungjawab untuk mencegah dan menemui salah laku di kalangan broker sekuriti. Tujuan kami adalah untuk membantu memusatkan sumber peraturan NASD yang terhad pada broker yang kemungkinan besar terlibat dalam pelanggaran sekuriti. Dengan menggunakan algoritma pembelajaran relasional statistik, kami mengembangkan model yang memberi peringkat kepada broker berkenaan dengan kebarangkalian bahawa mereka akan melakukan pelanggaran serius terhadap peraturan sekuriti dalam waktu terdekat. Model kami menggabungkan hubungan organisasi antara broker mis. , rakan sekerja masa lalu, yang dianggap penting oleh pakar domain tetapi belum mudah digunakan sebelum ini. Model yang dipelajari telah menjalani penilaian yang luas dengan menggunakan data lebih dari 18 bulan yang tidak dapat dilihat oleh pemaju model dan terdiri daripada usaha selama dua minggu oleh staf NASD. Ramalan model didapati sangat berkaitan dengan penilaian subjektif pemeriksa NASD yang berpengalaman. Selanjutnya, dalam semua ukuran prestasi, model kami tampil baik atau lebih baik daripada peraturan buatan tangan yang saat ini digunakan di NASD. [[EENNDD]] pengesanan penipuan; belajar; pembelajaran perhubungan statistik; pokok kebarangkalian hubungan"], [{"string": "Cross-training : learning probabilistic mappings between topics Classification is a well-established operation in text mining . Given a set of labels A and a set DA of training documents tagged with these labels , a classifier learns to assign labels to unlabeled test documents . Suppose we also had available a different set of labels B , together with a set of documents DB marked with labels from B. If A and B have some semantic overlap , can the availability of DB help us build a better classifier for A , and vice versa ? We answer this question in the affirmative by proposing cross-training : a new approach to semi-supervised learning in presence of multiple label sets . We give distributional and discriminative algorithms for cross-training and show , through extensive experiments , that cross-training can discover and exploit probabilistic relations between two taxonomies for more accurate classification .", "keywords": ["semi-supervised multi-task learning", "document classification", "learning", "design methodology", "em", "support vector machines"], "combined": "Cross-training : learning probabilistic mappings between topics Classification is a well-established operation in text mining . Given a set of labels A and a set DA of training documents tagged with these labels , a classifier learns to assign labels to unlabeled test documents . Suppose we also had available a different set of labels B , together with a set of documents DB marked with labels from B. If A and B have some semantic overlap , can the availability of DB help us build a better classifier for A , and vice versa ? We answer this question in the affirmative by proposing cross-training : a new approach to semi-supervised learning in presence of multiple label sets . We give distributional and discriminative algorithms for cross-training and show , through extensive experiments , that cross-training can discover and exploit probabilistic relations between two taxonomies for more accurate classification . [[EENNDD]] semi-supervised multi-task learning; document classification; learning; design methodology; em; support vector machines"}, "Latihan silang: pemetaan probabilistik pembelajaran antara topik Klasifikasi adalah operasi yang mapan dalam perlombongan teks. Memandangkan satu set label A dan satu set dokumen latihan yang diberi label dengan label ini, pengkelas belajar menetapkan label pada dokumen ujian yang tidak berlabel. Anggaplah kami juga mempunyai sekumpulan label B yang berbeza, bersama dengan sekumpulan dokumen DB yang ditandai dengan label dari B. Sekiranya A dan B mempunyai pertindihan semantik, adakah ketersediaan DB dapat membantu kami membina pengkelasan yang lebih baik untuk A, dan naib sebaliknya? Kami menjawab soalan ini secara afirmatif dengan mencadangkan latihan silang: pendekatan baru untuk pembelajaran separa penyeliaan dengan adanya beberapa set label. Kami memberikan algoritma distribusi dan diskriminatif untuk latihan silang dan menunjukkan, melalui eksperimen yang luas, bahawa latihan silang dapat menemui dan memanfaatkan hubungan probabilistik antara dua taksonomi untuk klasifikasi yang lebih tepat. [[EENNDD]] pembelajaran pelbagai tugas separa penyeliaan; pengelasan dokumen; belajar; metodologi reka bentuk; em; mesin vektor sokongan"], [{"string": "High-precision phrase-based document classification on a modern scale We present a document classification system that employs lazy learning from labeled phrases , and argue that the system can be highly effective whenever the following property holds : most of information on document labels is captured in phrases . We call this property near sufficiency . Our research contribution is twofold : a we quantify the near sufficiency property using the Information Bottleneck principle and show that it is easy to check on a given dataset ; b we reveal that in all practical cases -- from small-scale to very large-scale -- manual labeling of phrases is feasible : the natural language constrains the number of common phrases composed of a vocabulary to grow linearly with the size of the vocabulary . Both these contributions provide firm foundation to applicability of the phrase-based classification PBC framework to a variety of large-scale tasks . We deployed the PBC system on the task of job title classification , as a part of LinkedIn 's data standardization effort . The system significantly outperforms its predecessor both in terms of precision and coverage . It is currently being used in LinkedIn 's ad targeting product , and more applications are being developed . We argue that PBC excels in high explainability of the classification results , as well as in low development and low maintenance costs . We benchmark PBC against existing high-precision document classification algorithms and conclude that it is most useful in multilabel classification .", "keywords": ["multilabel text classification", "high-precision classification", "large-scale classification"], "combined": "High-precision phrase-based document classification on a modern scale We present a document classification system that employs lazy learning from labeled phrases , and argue that the system can be highly effective whenever the following property holds : most of information on document labels is captured in phrases . We call this property near sufficiency . Our research contribution is twofold : a we quantify the near sufficiency property using the Information Bottleneck principle and show that it is easy to check on a given dataset ; b we reveal that in all practical cases -- from small-scale to very large-scale -- manual labeling of phrases is feasible : the natural language constrains the number of common phrases composed of a vocabulary to grow linearly with the size of the vocabulary . Both these contributions provide firm foundation to applicability of the phrase-based classification PBC framework to a variety of large-scale tasks . We deployed the PBC system on the task of job title classification , as a part of LinkedIn 's data standardization effort . The system significantly outperforms its predecessor both in terms of precision and coverage . It is currently being used in LinkedIn 's ad targeting product , and more applications are being developed . We argue that PBC excels in high explainability of the classification results , as well as in low development and low maintenance costs . We benchmark PBC against existing high-precision document classification algorithms and conclude that it is most useful in multilabel classification . [[EENNDD]] multilabel text classification; high-precision classification; large-scale classification"}, "Klasifikasi dokumen berasaskan frasa berketepatan tinggi pada skala moden Kami menyajikan sistem klasifikasi dokumen yang menggunakan pembelajaran malas dari frasa berlabel, dan berpendapat bahawa sistem ini boleh menjadi sangat berkesan setiap kali harta tanah berikut: kebanyakan maklumat pada label dokumen ditangkap di frasa. Kami memanggil harta ini hampir mencukupi. Sumbangan penyelidikan kami dua kali ganda: a kami mengukur harta tanah yang mencukupi dengan menggunakan prinsip Information Bottleneck dan menunjukkan bahawa mudah untuk memeriksa set data tertentu; b kami mendedahkan bahawa dalam semua kes praktikal - dari skala kecil hingga skala besar - pelabelan manual frasa dapat dilaksanakan: bahasa semula jadi mengehadkan bilangan frasa umum yang terdiri daripada kosa kata untuk berkembang secara linear dengan ukuran kosa kata . Kedua-dua sumbangan ini memberikan asas yang kuat untuk penerapan kerangka kerja PBC klasifikasi berdasarkan frasa untuk pelbagai tugas berskala besar. Kami menggunakan sistem PBC untuk tugas klasifikasi tajuk pekerjaan, sebagai bagian dari usaha penyeragaman data LinkedIn. Sistem ini mengatasi pendahulunya dengan ketara dari segi ketepatan dan liputan. Ia sedang digunakan dalam produk penargetan iklan LinkedIn, dan lebih banyak aplikasi sedang dikembangkan. Kami berpendapat bahawa PBC unggul dalam penjelasan yang tinggi dari hasil klasifikasi, serta dalam pembangunan dan kos penyelenggaraan yang rendah. Kami menanda aras PBC terhadap algoritma klasifikasi dokumen berketepatan tinggi yang ada dan menyimpulkan bahawa ia sangat berguna dalam klasifikasi multilabel. [[EENNDD]] klasifikasi teks pelbagai label; pengelasan berketepatan tinggi; pengkelasan skala besar"], [{"string": "Latent aspect rating analysis on review text data : a rating regression approach In this paper , we define and study a new opinionated text data analysis problem called Latent Aspect Rating Analysis LARA , which aims at analyzing opinions expressed about an entity in an online review at the level of topical aspects to discover each individual reviewer 's latent opinion on each aspect as well as the relative emphasis on different aspects when forming the overall judgment of the entity . We propose a novel probabilistic rating regression model to solve this new text mining problem in a general way . Empirical experiments on a hotel review data set show that the proposed latent rating regression model can effectively solve the problem of LARA , and that the detailed analysis of opinions at the level of topical aspects enabled by the proposed model can support a wide range of application tasks , such as aspect opinion summarization , entity ranking based on aspect ratings , and analysis of reviewers rating behavior .", "keywords": ["information search and retrieval"], "combined": "Latent aspect rating analysis on review text data : a rating regression approach In this paper , we define and study a new opinionated text data analysis problem called Latent Aspect Rating Analysis LARA , which aims at analyzing opinions expressed about an entity in an online review at the level of topical aspects to discover each individual reviewer 's latent opinion on each aspect as well as the relative emphasis on different aspects when forming the overall judgment of the entity . We propose a novel probabilistic rating regression model to solve this new text mining problem in a general way . Empirical experiments on a hotel review data set show that the proposed latent rating regression model can effectively solve the problem of LARA , and that the detailed analysis of opinions at the level of topical aspects enabled by the proposed model can support a wide range of application tasks , such as aspect opinion summarization , entity ranking based on aspect ratings , and analysis of reviewers rating behavior . [[EENNDD]] information search and retrieval"}, "Analisis penilaian aspek laten pada data teks kajian: pendekatan regresi penilaian Dalam makalah ini, kami menentukan dan mengkaji masalah analisis data teks pendapat baru yang disebut Latent Aspect Rating Analysis LARA, yang bertujuan menganalisis pendapat yang dinyatakan mengenai entiti dalam tinjauan dalam talian di tahap aspek topikal untuk mengetahui pendapat terpendam setiap pengulas mengenai setiap aspek serta penekanan relatif terhadap aspek yang berbeza ketika membentuk penilaian keseluruhan entiti. Kami mencadangkan model regresi penarafan probabilistik baru untuk menyelesaikan masalah perlombongan teks baru ini secara umum. Eksperimen empirikal pada set data tinjauan hotel menunjukkan bahawa model regresi penilaian laten yang dicadangkan dapat menyelesaikan masalah LARA dengan berkesan, dan bahawa analisis terperinci pendapat pada tahap aspek topikal yang diaktifkan oleh model yang dicadangkan dapat menyokong pelbagai tugas aplikasi , seperti ringkasan pendapat aspek, peringkat entiti berdasarkan penilaian aspek, dan analisis tingkah laku penilaian pengulas. [[EENNDD]] carian dan pengambilan maklumat"], [{"string": "Structured entity identification and document categorization : two tasks with one joint model Traditionally , research in identifying structured entities in documents has proceeded independently of document categorization research . In this paper , we observe that these two tasks have much to gain from each other . Apart from direct references to entities in a database , such as names of person entities , documents often also contain words that are correlated with discriminative entity attributes , such age-group and income-level of persons . This happens naturally in many enterprise domains such as CRM , Banking , etc. . Then , entity identification , which is typically vulnerable against noise and incompleteness in direct references to entities in documents , can benefit from document categorization with respect to such attributes . In return , entity identification enables documents to be categorized according to different label-sets arising from entity attributes without requiring any supervision . In this paper , we propose a probabilistic generative model for joint entity identification and document categorization . We show how the parameters of the model can be estimated using an EM algorithm in an unsupervised fashion . Using extensive experiments over real and semi-synthetic data , we demonstrate that the two tasks can benefit immensely from each other when performed jointly using the proposed model .", "keywords": ["document categorization", "probabilistic generative model", "entity identification"], "combined": "Structured entity identification and document categorization : two tasks with one joint model Traditionally , research in identifying structured entities in documents has proceeded independently of document categorization research . In this paper , we observe that these two tasks have much to gain from each other . Apart from direct references to entities in a database , such as names of person entities , documents often also contain words that are correlated with discriminative entity attributes , such age-group and income-level of persons . This happens naturally in many enterprise domains such as CRM , Banking , etc. . Then , entity identification , which is typically vulnerable against noise and incompleteness in direct references to entities in documents , can benefit from document categorization with respect to such attributes . In return , entity identification enables documents to be categorized according to different label-sets arising from entity attributes without requiring any supervision . In this paper , we propose a probabilistic generative model for joint entity identification and document categorization . We show how the parameters of the model can be estimated using an EM algorithm in an unsupervised fashion . Using extensive experiments over real and semi-synthetic data , we demonstrate that the two tasks can benefit immensely from each other when performed jointly using the proposed model . [[EENNDD]] document categorization; probabilistic generative model; entity identification"}, "Pengenalan entiti berstruktur dan pengkategorian dokumen: dua tugas dengan satu model bersama Secara tradisinya, penyelidikan dalam mengenal pasti entiti berstruktur dalam dokumen telah berjalan secara bebas daripada penyelidikan pengkategorian dokumen. Dalam makalah ini, kita melihat bahawa kedua-dua tugas ini mempunyai banyak keuntungan antara satu sama lain. Selain daripada rujukan langsung ke entiti dalam pangkalan data, seperti nama entiti orang, dokumen sering juga mengandungi kata-kata yang berkorelasi dengan atribut entiti diskriminatif, seperti kumpulan umur dan tahap pendapatan orang. Ini berlaku secara semula jadi di banyak domain perusahaan seperti CRM, Banking, dll. Kemudian, pengenalan entiti, yang biasanya rentan terhadap kebisingan dan ketidaklengkapan dalam rujukan langsung ke entiti dalam dokumen, dapat memanfaatkan pengkategorian dokumen berkenaan dengan atribut tersebut. Sebagai balasannya, pengenalan entiti membolehkan dokumen dikategorikan mengikut kumpulan label yang berbeza yang timbul dari atribut entiti tanpa memerlukan pengawasan. Dalam makalah ini, kami mencadangkan model generatif probabilistik untuk pengenalan entiti bersama dan pengkategorian dokumen. Kami menunjukkan bagaimana parameter model dapat diperkirakan menggunakan algoritma EM secara tidak diawasi. Dengan menggunakan eksperimen yang meluas terhadap data sebenar dan separa sintetik, kami menunjukkan bahawa kedua tugas tersebut dapat memperoleh manfaat yang sangat besar antara satu sama lain apabila dilakukan bersama menggunakan model yang dicadangkan. [[EENNDD]] pengkategorian dokumen; model generatif probabilistik; pengenalan entiti"], [{"string": "Global distance-based segmentation of trajectories This work introduces distance-based criteria for segmentation of object trajectories . Segmentation leads to simplification of the original objects into smaller , less complex primitives that are better suited for storage and retrieval purposes . Previous work on trajectory segmentation attacked the problem locally , segmenting separately each trajectory of the database . Therefore , they did not directly optimize the inter-object separability , which is necessary for mining operations such as searching , clustering , and classification on large databases . In this paper we analyze the trajectory segmentation problem from a global perspective , utilizing data aware distance-based optimization techniques , which optimize pairwise distance estimates hence leading to more efficient object pruning . We first derive exact solutions of the distance-based formulation . Due to the intractable complexity of the exact solution , we present anapproximate , greedy solution that exploits forward searching of locally optimal solutions . Since the greedy solution also imposes a prohibitive computational cost , we also put forward more light weight variance-based segmentation techniques , which intelligently `` relax '' the pairwise distance only in the areas that affect the least the mining operation .", "keywords": ["dna visualization", "data simplification"], "combined": "Global distance-based segmentation of trajectories This work introduces distance-based criteria for segmentation of object trajectories . Segmentation leads to simplification of the original objects into smaller , less complex primitives that are better suited for storage and retrieval purposes . Previous work on trajectory segmentation attacked the problem locally , segmenting separately each trajectory of the database . Therefore , they did not directly optimize the inter-object separability , which is necessary for mining operations such as searching , clustering , and classification on large databases . In this paper we analyze the trajectory segmentation problem from a global perspective , utilizing data aware distance-based optimization techniques , which optimize pairwise distance estimates hence leading to more efficient object pruning . We first derive exact solutions of the distance-based formulation . Due to the intractable complexity of the exact solution , we present anapproximate , greedy solution that exploits forward searching of locally optimal solutions . Since the greedy solution also imposes a prohibitive computational cost , we also put forward more light weight variance-based segmentation techniques , which intelligently `` relax '' the pairwise distance only in the areas that affect the least the mining operation . [[EENNDD]] dna visualization; data simplification"}, "Segmentasi lintasan berdasarkan jarak global Karya ini memperkenalkan kriteria berdasarkan jarak untuk pemisahan lintasan objek. Segmentasi membawa kepada penyederhanaan objek asal menjadi primitif yang lebih kecil dan kurang kompleks yang lebih sesuai untuk tujuan penyimpanan dan pengambilan. Kerja sebelumnya mengenai segmentasi lintasan menyerang masalah secara tempatan, menyegmentasikan secara terpisah setiap lintasan pangkalan data. Oleh itu, mereka tidak secara langsung mengoptimumkan keterasingan antara objek, yang diperlukan untuk operasi perlombongan seperti pencarian, pengelompokan, dan klasifikasi pada pangkalan data yang besar. Dalam makalah ini kami menganalisis masalah segmentasi lintasan dari perspektif global, menggunakan teknik pengoptimuman berdasarkan jarak sadar data, yang mengoptimumkan perkiraan jarak berpasangan sehingga menyebabkan pemangkasan objek yang lebih efisien. Kami mula-mula memperoleh penyelesaian tepat dari rumusan berdasarkan jarak. Oleh kerana kerumitan penyelesaian yang tepat, kami menyajikan penyelesaian tamak yang kurang tepat yang mengeksploitasi mencari penyelesaian optimum tempatan. Oleh kerana penyelesaian tamak juga membebankan kos komputasi yang melarang, kami juga mengemukakan lebih banyak teknik segmentasi berdasarkan varians ringan, yang secara cerdas \"mengendurkan\" jarak berpasangan hanya di daerah yang paling sedikit mempengaruhi operasi penambangan. [[EENNDD]] visualisasi dna; penyederhanaan data"], [{"string": "Using structure indices for efficient approximation of network properties Statistics on networks have become vital to the study of relational data drawn from areas such as bibliometrics , fraud detection , bioinformatics , and the Internet . Calculating many of the most important measures - such as betweenness centrality , closeness centrality , and graph diameter-requires identifying short paths in these networks . However , finding these short paths can be intractable for even moderate-size networks . We introduce the concept of a network structure index NSI , a composition of 1 a set of annotations on every node in the network and 2 a function that uses the annotations to estimate graph distance between pairs of nodes . We present several varieties of NSIs , examine their time and space complexity , and analyze their performance on synthetic and real data sets . We show that creating an NSI for a given network enables extremely efficient and accurate estimation of a wide variety of network statistics on that network .", "keywords": ["centrality", "knowledge discovery in graphs", "network structure index", "social network analysis"], "combined": "Using structure indices for efficient approximation of network properties Statistics on networks have become vital to the study of relational data drawn from areas such as bibliometrics , fraud detection , bioinformatics , and the Internet . Calculating many of the most important measures - such as betweenness centrality , closeness centrality , and graph diameter-requires identifying short paths in these networks . However , finding these short paths can be intractable for even moderate-size networks . We introduce the concept of a network structure index NSI , a composition of 1 a set of annotations on every node in the network and 2 a function that uses the annotations to estimate graph distance between pairs of nodes . We present several varieties of NSIs , examine their time and space complexity , and analyze their performance on synthetic and real data sets . We show that creating an NSI for a given network enables extremely efficient and accurate estimation of a wide variety of network statistics on that network . [[EENNDD]] centrality; knowledge discovery in graphs; network structure index; social network analysis"}, "Menggunakan indeks struktur untuk menghampiri sifat rangkaian yang cekap Statistik rangkaian telah menjadi penting untuk kajian data hubungan yang diambil dari bidang seperti bibliometrik, pengesanan penipuan, bioinformatik, dan Internet. Mengira banyak langkah yang paling penting - seperti pusat jarak antara, pusat jarak dekat, dan diameter graf-memerlukan mengenal pasti jalan pendek dalam rangkaian ini. Walau bagaimanapun, mencari jalan pendek ini boleh menjadi sukar untuk rangkaian bersaiz sederhana. Kami memperkenalkan konsep indeks struktur rangkaian NSI, komposisi 1 set anotasi pada setiap simpul dalam rangkaian dan 2 fungsi yang menggunakan anotasi untuk menganggarkan jarak grafik antara pasangan nod. Kami membentangkan beberapa jenis NSI, memeriksa kerumitan masa dan ruang mereka, dan menganalisis prestasinya pada set data sintetik dan sebenar. Kami menunjukkan bahawa membuat NSI untuk rangkaian tertentu memungkinkan anggaran yang sangat efisien dan tepat dari pelbagai statistik rangkaian pada rangkaian tersebut. [[EENNDD]] pusat; penemuan pengetahuan dalam grafik; indeks struktur rangkaian; analisis rangkaian sosial"], [{"string": "A sequential dual method for large scale multi-class linear svms Efficient training of direct multi-class formulations of linear Support Vector Machines is very useful in applications such as text classification with a huge number examples as well as features . This paper presents a fast dual method for this training . The main idea is to sequentially traverse through the training set and optimize the dual variables associated with one example at a time . The speed of training is enhanced further by shrinking and cooling heuristics . Experiments indicate that our method is much faster than state of the art solvers such as bundle , cutting plane and exponentiated gradient methods .", "keywords": ["multi-class", "text classification", "support vector machines"], "combined": "A sequential dual method for large scale multi-class linear svms Efficient training of direct multi-class formulations of linear Support Vector Machines is very useful in applications such as text classification with a huge number examples as well as features . This paper presents a fast dual method for this training . The main idea is to sequentially traverse through the training set and optimize the dual variables associated with one example at a time . The speed of training is enhanced further by shrinking and cooling heuristics . Experiments indicate that our method is much faster than state of the art solvers such as bundle , cutting plane and exponentiated gradient methods . [[EENNDD]] multi-class; text classification; support vector machines"}, "Kaedah dwi urutan untuk svms linear berskala besar berskala besar Latihan yang cekap untuk formulasi langsung pelbagai kelas Mesin Vektor Sokongan linear sangat berguna dalam aplikasi seperti klasifikasi teks dengan sebilangan besar contoh serta ciri. Makalah ini mengemukakan kaedah dwi cepat untuk latihan ini. Idea utama adalah melintasi rangkaian latihan secara berurutan dan mengoptimumkan pemboleh ubah berganda yang berkaitan dengan satu contoh pada satu masa. Kelajuan latihan ditingkatkan lebih jauh dengan mengecilkan dan menyejukkan heuristik. Eksperimen menunjukkan bahawa kaedah kami jauh lebih pantas daripada kaedah pemecah canggih seperti kaedah bundle, cutting plane dan exponentiated gradient. [[EENNDD]] berbilang kelas; pengelasan teks; mesin vektor sokongan"], [{"string": "PVA : a self-adaptive personal view agent system In this paper , we present PVA , an adaptive personal view information agent system to track , learn and manage , user 's interests in Internet documents . When user 's interests change , PVA , in not only the contents , but also in the structure of user profile , is modified to adapt to the changes . Experimental results show that modulating the structure of user profile does increase the accuracy of personalization systems .", "keywords": ["personal view", "systems and software", "personalization", "user interfaces", "www", "machine learning"], "combined": "PVA : a self-adaptive personal view agent system In this paper , we present PVA , an adaptive personal view information agent system to track , learn and manage , user 's interests in Internet documents . When user 's interests change , PVA , in not only the contents , but also in the structure of user profile , is modified to adapt to the changes . Experimental results show that modulating the structure of user profile does increase the accuracy of personalization systems . [[EENNDD]] personal view; systems and software; personalization; user interfaces; www; machine learning"}, "PVA: sistem ejen pandangan peribadi penyesuaian diri Dalam makalah ini, kami membentangkan PVA, sistem ejen maklumat pandangan peribadi adaptif untuk mengesan, belajar dan mengurus, kepentingan pengguna dalam dokumen Internet. Apabila minat pengguna berubah, PVA, bukan hanya pada isi, tetapi juga dalam struktur profil pengguna, diubah untuk menyesuaikan diri dengan perubahan tersebut. Hasil eksperimen menunjukkan bahawa memodulasi struktur profil pengguna meningkatkan ketepatan sistem pemperibadian. [[EENNDD]] pandangan peribadi; sistem dan perisian; pemperibadian; antara muka pengguna; www; pembelajaran mesin"], [{"string": "New ensemble methods for evolving data streams Advanced analysis of data streams is quickly becoming a key area of data mining research as the number of applications demanding such processing increases . Online mining when such data streams evolve over time , that is when concepts drift or change completely , is becoming one of the core issues . When tackling non-stationary concepts , ensembles of classifiers have several advantages over single classifier methods : they are easy to scale and parallelize , they can adapt to change quickly by pruning under-performing parts of the ensemble , and they therefore usually also generate more accurate concept descriptions . This paper proposes a new experimental data stream framework for studying concept drift , and two new variants of Bagging : ADWIN Bagging and Adaptive-Size Hoeffding Tree ASHT Bagging . Using the new experimental framework , an evaluation study on synthetic and real-world datasets comprising up to ten million examples shows that the new ensemble methods perform very well compared to several known methods .", "keywords": ["data streams", "decision trees", "ensemble methods", "concept drift"], "combined": "New ensemble methods for evolving data streams Advanced analysis of data streams is quickly becoming a key area of data mining research as the number of applications demanding such processing increases . Online mining when such data streams evolve over time , that is when concepts drift or change completely , is becoming one of the core issues . When tackling non-stationary concepts , ensembles of classifiers have several advantages over single classifier methods : they are easy to scale and parallelize , they can adapt to change quickly by pruning under-performing parts of the ensemble , and they therefore usually also generate more accurate concept descriptions . This paper proposes a new experimental data stream framework for studying concept drift , and two new variants of Bagging : ADWIN Bagging and Adaptive-Size Hoeffding Tree ASHT Bagging . Using the new experimental framework , an evaluation study on synthetic and real-world datasets comprising up to ten million examples shows that the new ensemble methods perform very well compared to several known methods . [[EENNDD]] data streams; decision trees; ensemble methods; concept drift"}, "Kaedah ensemble baru untuk evolusi aliran data Analisis aliran data lanjutan dengan cepat menjadi bidang utama penyelidikan perlombongan data kerana jumlah aplikasi yang menuntut pemprosesan tersebut meningkat. Perlombongan dalam talian apabila aliran data sedemikian berkembang dari masa ke masa, iaitu ketika konsep melayang atau berubah sepenuhnya, menjadi salah satu isu teras. Semasa menangani konsep tidak bergerak, ensembel pengkelasan mempunyai beberapa kelebihan berbanding kaedah pengkelasan tunggal: mereka mudah ditimbang dan sejajar, mereka dapat menyesuaikan diri dengan perubahan dengan cepat dengan memotong bahagian ensemble yang tidak berfungsi dengan baik, dan oleh itu mereka biasanya juga menghasilkan lebih tepat penerangan konsep. Makalah ini mencadangkan kerangka aliran data eksperimental baru untuk mengkaji konsep konsep, dan dua varian baru Bagging: ADWIN Bagging dan Adaptive-Size Hoeffding Tree ASHT Bagging. Dengan menggunakan kerangka eksperimen baru, kajian penilaian mengenai kumpulan data sintetik dan dunia nyata yang merangkumi hingga sepuluh juta contoh menunjukkan bahawa kaedah ensemble baru menunjukkan prestasi yang sangat baik berbanding dengan beberapa kaedah yang diketahui. [[EENNDD]] aliran data; pokok keputusan; kaedah ensemble; drift konsep"], [{"string": "Distributed cooperative mining for information consortia We consider the situation where a number of agents are distributed and each of them collects a data sequence generated according to an unknown probability distribution . Here each of the distributions is specified by common parameters and individual parameters e.g. , a normal distribution with an identical mean and a different variance . Here we introduce a notion of an information consortium , which is a framework where the agents can not show raw data to one another , but they like to enjoy significant information gain for estimating the respective distributions . Such an information consortium has recently received much interest in a broad range of areas including financial risk management , ubiquitous network mining , etc. . In this paper we are concerned with the following three issues : 1 how to design a collaborative strategy for agents to estimate the respective distributions in the information consortium , 2 characterizing when each agent has a benefit in terms of information gain for estimating its distribution or information loss for predicting future data , and 3 charracterizing how much benefit each agent obtains . In this paper we yield a statistical formulation of information consortia and solve all of the above three problems for a general form of probability distributions . Specifically we propose a basic strategy for cooperative estimation and derive a necessary and sufficient condition for each agent to have a significant benefit .", "keywords": ["estimation", "distributed mining", "database applications", "collective mining", "statistical model", "information consortium"], "combined": "Distributed cooperative mining for information consortia We consider the situation where a number of agents are distributed and each of them collects a data sequence generated according to an unknown probability distribution . Here each of the distributions is specified by common parameters and individual parameters e.g. , a normal distribution with an identical mean and a different variance . Here we introduce a notion of an information consortium , which is a framework where the agents can not show raw data to one another , but they like to enjoy significant information gain for estimating the respective distributions . Such an information consortium has recently received much interest in a broad range of areas including financial risk management , ubiquitous network mining , etc. . In this paper we are concerned with the following three issues : 1 how to design a collaborative strategy for agents to estimate the respective distributions in the information consortium , 2 characterizing when each agent has a benefit in terms of information gain for estimating its distribution or information loss for predicting future data , and 3 charracterizing how much benefit each agent obtains . In this paper we yield a statistical formulation of information consortia and solve all of the above three problems for a general form of probability distributions . Specifically we propose a basic strategy for cooperative estimation and derive a necessary and sufficient condition for each agent to have a significant benefit . [[EENNDD]] estimation; distributed mining; database applications; collective mining; statistical model; information consortium"}, "Perlombongan koperasi yang diedarkan untuk konsortia maklumat Kami mempertimbangkan keadaan di mana sebilangan ejen diedarkan dan masing-masing mengumpulkan urutan data yang dihasilkan mengikut sebaran kebarangkalian yang tidak diketahui. Di sini setiap pengedaran ditentukan oleh parameter umum dan parameter individu mis. , taburan normal dengan min yang sama dan varians yang berbeza. Di sini kami memperkenalkan konsep konsortium maklumat, yang merupakan kerangka di mana ejen tidak dapat menunjukkan data mentah antara satu sama lain, tetapi mereka suka menikmati perolehan maklumat yang signifikan untuk menganggarkan pengedaran masing-masing. Konsortium maklumat seperti ini baru-baru ini mendapat banyak minat dalam pelbagai bidang termasuk pengurusan risiko kewangan, perlombongan rangkaian di mana-mana, dll. Dalam makalah ini kita memperhatikan tiga masalah berikut: 1 bagaimana merancang strategi kolaborasi untuk ejen untuk menganggarkan pengedaran masing-masing dalam konsortium maklumat, 2 mencirikan apabila setiap ejen mempunyai manfaat dari segi perolehan maklumat untuk menganggarkan penyebaran atau maklumatnya kerugian kerana meramalkan data masa depan, dan 3 membebankan berapa banyak faedah yang diperoleh setiap ejen. Dalam makalah ini kami menghasilkan rumusan statistik dari konsortia maklumat dan menyelesaikan semua tiga masalah di atas untuk bentuk umum taburan kebarangkalian. Secara khusus kami mencadangkan strategi dasar untuk perkiraan koperasi dan memperoleh syarat yang diperlukan dan mencukupi untuk setiap ejen mendapat manfaat yang signifikan. [[EENNDD]] anggaran; perlombongan diedarkan; aplikasi pangkalan data; perlombongan kolektif; model statistik; konsortium maklumat"], [{"string": "Generating non-redundant association rules", "keywords": ["learning"], "combined": "Generating non-redundant association rules [[EENNDD]] learning"}, "Menjana peraturan pergaulan yang tidak berlebihan [[EENNDD]] pembelajaran"], [{"string": "Interpretable nonnegative matrix decompositions A matrix decomposition expresses a matrix as a product of at least two factor matrices . Equivalently , it expresses each column of the input matrix as a linear combination of the columns in the first factor matrix . The interpretability of the decompositions is a key issue in many data-analysis tasks . We propose two new matrix-decomposition problems : the nonnegative CX and nonnegative CUR problems , that give naturally interpretable factors . They extend the recently-proposed column and column-row based decompositions , and are aimed to be used with nonnegative matrices . Our decompositions represent the input matrix as a nonnegative linear combination of a subset of its columns or columns and rows . We present two algorithms to solve these problems and provide an extensive experimental evaluation where we assess the quality of our algorithms ' results as well as the intuitiveness of nonnegative CX and CUR decompositions . We show that our algorithms return intuitive answers with smaller reconstruction errors than the previously-proposed methods for column and column-row decompositions .", "keywords": ["matrix decompositions", "local search", "alternating least squares", "column-row decompositions"], "combined": "Interpretable nonnegative matrix decompositions A matrix decomposition expresses a matrix as a product of at least two factor matrices . Equivalently , it expresses each column of the input matrix as a linear combination of the columns in the first factor matrix . The interpretability of the decompositions is a key issue in many data-analysis tasks . We propose two new matrix-decomposition problems : the nonnegative CX and nonnegative CUR problems , that give naturally interpretable factors . They extend the recently-proposed column and column-row based decompositions , and are aimed to be used with nonnegative matrices . Our decompositions represent the input matrix as a nonnegative linear combination of a subset of its columns or columns and rows . We present two algorithms to solve these problems and provide an extensive experimental evaluation where we assess the quality of our algorithms ' results as well as the intuitiveness of nonnegative CX and CUR decompositions . We show that our algorithms return intuitive answers with smaller reconstruction errors than the previously-proposed methods for column and column-row decompositions . [[EENNDD]] matrix decompositions; local search; alternating least squares; column-row decompositions"}, "Penguraian matriks nonnegatif yang dapat ditafsirkan Penguraian matriks menyatakan matriks sebagai produk sekurang-kurangnya dua matriks faktor. Sama, ia menyatakan setiap lajur matriks input sebagai gabungan lajur lajur dalam matriks faktor pertama. Kebolehtafsiran penguraian adalah isu utama dalam banyak tugas analisis data. Kami mencadangkan dua masalah penguraian matriks baru: masalah CX nonnegatif dan masalah CUR nonnegatif, yang memberikan faktor yang dapat ditafsirkan secara semula jadi. Mereka memperluas dekomposisi berdasarkan lajur dan lajur baris yang baru dicadangkan, dan bertujuan untuk digunakan dengan matriks bukan negatif. Penguraian kami mewakili matriks input sebagai kombinasi linear bukan negatif dari subkumpulan lajur atau lajur dan barisnya. Kami mengemukakan dua algoritma untuk menyelesaikan masalah ini dan memberikan penilaian eksperimental yang luas di mana kami menilai kualiti hasil algoritma kami serta intuitif penguraian CX dan CUR yang tidak negatif. Kami menunjukkan bahawa algoritma kami mengembalikan jawapan intuitif dengan ralat pembinaan semula yang lebih kecil daripada kaedah yang dicadangkan sebelumnya untuk penyahkod lajur dan lajur. [[EENNDD]] penguraian matriks; carian tempatan; petak seli berselang seli; penguraian baris lajur"], [{"string": "Clustering pair-wise dissimilarity data into partially ordered sets Ontologies represent data relationships as hierarchies of possibly overlapping classes . Ontologies are closely related to clustering hierarchies , and in this article we explore this relationship in depth . In particular , we examine the space of ontologies that can be generated by pairwise dissimilarity matrices . We demonstrate that classical clustering algorithms , which take dissimilarity matrices as inputs , do not incorporate all available information . In fact , only special types of dissimilarity matrices can be exactly preserved by previous clustering methods . We model ontologies as a partially ordered set poset over the subset relation . In this paper , we propose a new clustering algorithm , that generates a partially ordered set of clusters from a dissimilarity matrix .", "keywords": ["pocluster", "dissimilarity", "clustering", "poset"], "combined": "Clustering pair-wise dissimilarity data into partially ordered sets Ontologies represent data relationships as hierarchies of possibly overlapping classes . Ontologies are closely related to clustering hierarchies , and in this article we explore this relationship in depth . In particular , we examine the space of ontologies that can be generated by pairwise dissimilarity matrices . We demonstrate that classical clustering algorithms , which take dissimilarity matrices as inputs , do not incorporate all available information . In fact , only special types of dissimilarity matrices can be exactly preserved by previous clustering methods . We model ontologies as a partially ordered set poset over the subset relation . In this paper , we propose a new clustering algorithm , that generates a partially ordered set of clusters from a dissimilarity matrix . [[EENNDD]] pocluster; dissimilarity; clustering; poset"}, "Menggabungkan data ketidaksamaan pasangan ke dalam kumpulan yang disusun secara separa Ontologi mewakili hubungan data sebagai hierarki kelas yang mungkin bertindih. Ontologi berkait rapat dengan pengelompokan hierarki, dan dalam artikel ini kita meneroka hubungan ini secara mendalam. Khususnya, kami mengkaji ruang ontologi yang dapat dihasilkan oleh matriks ketidaksamaan berpasangan. Kami menunjukkan bahawa algoritma pengelompokan klasik, yang mengambil matriks ketidaksamaan sebagai input, tidak memasukkan semua maklumat yang ada. Sebenarnya, hanya jenis matriks perbezaan yang khusus dapat dijaga dengan tepat dengan kaedah pengelompokan sebelumnya. Kami memodelkan ontologi sebagai poset yang disusun sebahagian daripada hubungan subset. Dalam makalah ini, kami mencadangkan algoritma pengelompokan baru, yang menghasilkan sekumpulan kelompok yang disusun secara separa dari matriks perbezaan. [[EENNDD]] pocluster; ketidaksamaan; pengelompokan; poset"], [{"string": "Streaming feature selection using alpha-investing In Streaming Feature Selection SFS , new features are sequentially considered for addition to a predictive model . When the space of potential features is large , SFS offers many advantages over traditional feature selection methods , which assume that all features are known in advance . Features can be generated dynamically , focusing the search for new features on promising subspaces , and overfitting can be controlled by dynamically adjusting the threshold for adding features to the model . We describe \u03b1-investing , an adaptive complexity penalty method for SFS which dynamically adjusts the threshold on the error reduction required for adding a new feature . \u03b1-investing gives false discovery rate-style guarantees against overfitting . It differs from standard penalty methods such as AIC , BIC or RIC , which always drastically over - or under-fit in the limit of infinite numbers of non-predictive features . Empirical results show that SFS is competitive with much more compute-intensive feature selection methods such as stepwise regression , and allows feature selection on problems with over a million potential features .", "keywords": ["feature selection", "multiple regression", "false discovery rate", "classification"], "combined": "Streaming feature selection using alpha-investing In Streaming Feature Selection SFS , new features are sequentially considered for addition to a predictive model . When the space of potential features is large , SFS offers many advantages over traditional feature selection methods , which assume that all features are known in advance . Features can be generated dynamically , focusing the search for new features on promising subspaces , and overfitting can be controlled by dynamically adjusting the threshold for adding features to the model . We describe \u03b1-investing , an adaptive complexity penalty method for SFS which dynamically adjusts the threshold on the error reduction required for adding a new feature . \u03b1-investing gives false discovery rate-style guarantees against overfitting . It differs from standard penalty methods such as AIC , BIC or RIC , which always drastically over - or under-fit in the limit of infinite numbers of non-predictive features . Empirical results show that SFS is competitive with much more compute-intensive feature selection methods such as stepwise regression , and allows feature selection on problems with over a million potential features . [[EENNDD]] feature selection; multiple regression; false discovery rate; classification"}, "Pemilihan fitur streaming menggunakan pelaburan alfa Dalam Pemilihan Fitur Streaming SFS, ciri-ciri baru dipertimbangkan secara berurutan sebagai tambahan kepada model ramalan. Apabila ruang ciri-ciri berpotensi besar, SFS menawarkan banyak kelebihan berbanding kaedah pemilihan ciri tradisional, yang menganggap bahawa semua ciri sudah diketahui sebelumnya. Ciri-ciri dapat dihasilkan secara dinamik, memfokuskan pencarian fitur baru pada ruang bawah tanah yang menjanjikan, dan kelebihan dapat dikendalikan dengan menyesuaikan ambang dinamik untuk menambahkan fitur pada model. Kami menerangkan \u03b1-pelaburan, kaedah penalaran kerumitan adaptif untuk SFS yang secara dinamik menyesuaikan ambang pengurangan ralat yang diperlukan untuk menambahkan ciri baru. \u03b1-pelaburan memberikan jaminan gaya penemuan palsu terhadap overfitting. Ia berbeza dengan kaedah penalti standard seperti AIC, BIC atau RIC, yang selalu secara drastik - atau kurang sesuai dengan had bilangan ciri bukan ramalan. Hasil empirikal menunjukkan bahawa SFS berdaya saing dengan kaedah pemilihan ciri intensif komputasi seperti regresi bertahap, dan memungkinkan pemilihan ciri pada masalah dengan lebih dari satu juta potensi ciri. [[EENNDD]] pemilihan ciri; regresi berganda; kadar penemuan palsu; pengelasan"], [{"string": "Analyzing patterns of user content generation in online social networks Various online social networks OSNs have been developed rapidly on the Internet . Researchers have analyzed different properties of such OSNs , mainly focusing on the formation and evolution of the networks as well as the information propagation over the networks . In knowledge-sharing OSNs , such as blogs and question answering systems , issues on how users participate in the network and how users `` generate\\/contribute '' knowledge are vital to the sustained and healthy growth of the networks . However , related discussions have not been reported in the research literature . In this work , we empirically study workloads from three popular knowledge-sharing OSNs , including a blog system , a social bookmark sharing network , and a question answering social network to examine these properties . Our analysis consistently shows that 1 users ' posting behavior in these networks exhibits strong daily and weekly patterns , but the user active time in these OSNs does not follow exponential distributions ; 2 the user posting behavior in these OSNs follows stretched exponential distributions instead of power-law distributions , indicating the influence of a small number of core users can not dominate the network ; 3 the distributions of user contributions on high-quality and effort-consuming contents in these OSNs have smaller stretch factors for the stretched exponential distribution . Our study provides insights into user activity patterns and lays out an analytical foundation for further understanding various properties of these OSNs .", "keywords": ["social networks", "user/machine systems", "stretched exponential distribution", "user generated content"], "combined": "Analyzing patterns of user content generation in online social networks Various online social networks OSNs have been developed rapidly on the Internet . Researchers have analyzed different properties of such OSNs , mainly focusing on the formation and evolution of the networks as well as the information propagation over the networks . In knowledge-sharing OSNs , such as blogs and question answering systems , issues on how users participate in the network and how users `` generate\\/contribute '' knowledge are vital to the sustained and healthy growth of the networks . However , related discussions have not been reported in the research literature . In this work , we empirically study workloads from three popular knowledge-sharing OSNs , including a blog system , a social bookmark sharing network , and a question answering social network to examine these properties . Our analysis consistently shows that 1 users ' posting behavior in these networks exhibits strong daily and weekly patterns , but the user active time in these OSNs does not follow exponential distributions ; 2 the user posting behavior in these OSNs follows stretched exponential distributions instead of power-law distributions , indicating the influence of a small number of core users can not dominate the network ; 3 the distributions of user contributions on high-quality and effort-consuming contents in these OSNs have smaller stretch factors for the stretched exponential distribution . Our study provides insights into user activity patterns and lays out an analytical foundation for further understanding various properties of these OSNs . [[EENNDD]] social networks; user/machine systems; stretched exponential distribution; user generated content"}, "Menganalisis corak penghasilan kandungan pengguna dalam rangkaian sosial dalam talian Pelbagai OSN rangkaian sosial dalam talian telah dikembangkan dengan pesat di Internet. Para penyelidik telah menganalisis sifat-sifat yang berbeza dari OSN tersebut, terutama memfokuskan pada pembentukan dan evolusi rangkaian serta penyebaran maklumat melalui rangkaian. Dalam OSN perkongsian pengetahuan, seperti blog dan sistem menjawab soalan, isu mengenai bagaimana pengguna berpartisipasi dalam rangkaian dan bagaimana pengguna \"menjana \\ / menyumbang\" pengetahuan sangat penting untuk pertumbuhan rangkaian yang berterusan dan sihat. Walau bagaimanapun, perbincangan yang berkaitan belum dilaporkan dalam literatur penyelidikan. Dalam karya ini, kami secara empirik mempelajari beban kerja dari tiga OSN perkongsian pengetahuan yang popular, termasuk sistem blog, rangkaian perkongsian penanda buku sosial, dan rangkaian menjawab soalan sosial untuk memeriksa sifat-sifat ini. Analisis kami secara konsisten menunjukkan bahawa tingkah laku pengeposan 1 pengguna di rangkaian ini menunjukkan corak harian dan mingguan yang kuat, tetapi masa aktif pengguna dalam OSN ini tidak mengikuti pengedaran eksponensial; 2 tingkah laku pengeposan pengguna dalam OSN ini mengikuti pengedaran eksponensial yang diregangkan dan bukannya pengedaran undang-undang kuasa, yang menunjukkan pengaruh sebilangan kecil pengguna teras tidak dapat menguasai rangkaian; 3 pengagihan sumbangan pengguna pada kandungan berkualiti tinggi dan memakan tenaga dalam OSN ini mempunyai faktor regangan yang lebih kecil untuk pengedaran eksponensial regangan. Kajian kami memberikan pandangan mengenai corak aktiviti pengguna dan meletakkan asas analisis untuk lebih memahami pelbagai sifat OSN ini. [[EENNDD]] rangkaian sosial; sistem pengguna / mesin; pengedaran eksponensial terbentang; kandungan yang dihasilkan pengguna"], [{"string": "Very sparse random projections There has been considerable interest in random projections , an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space . Let A in Rn x D be our n points in D dimensions . The method multiplies A by a random matrix R in RD x k , reducing the D dimensions down to just k for speeding up the computation . R typically consists of entries of standard normal N 0,1 . It is well known that random projections preserve pairwise distances in the expectation . Achlioptas proposed sparse random projections by replacing the N 0,1 entries in R with entries in -1,0,1 with probabilities 1\\/6 , 2\\/3 , 1\\/6 , achieving a threefold speedup in processing time . We recommend using R of entries in -1,0,1 with probabilities 1\\/2 \u221a D , 1-1 \u221a D , 1\\/2 \u221a D for achieving a significant \u221a D-fold speedup , with little loss in accuracy .", "keywords": ["rates of convergence", "random projections", "sampling"], "combined": "Very sparse random projections There has been considerable interest in random projections , an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space . Let A in Rn x D be our n points in D dimensions . The method multiplies A by a random matrix R in RD x k , reducing the D dimensions down to just k for speeding up the computation . R typically consists of entries of standard normal N 0,1 . It is well known that random projections preserve pairwise distances in the expectation . Achlioptas proposed sparse random projections by replacing the N 0,1 entries in R with entries in -1,0,1 with probabilities 1\\/6 , 2\\/3 , 1\\/6 , achieving a threefold speedup in processing time . We recommend using R of entries in -1,0,1 with probabilities 1\\/2 \u221a D , 1-1 \u221a D , 1\\/2 \u221a D for achieving a significant \u221a D-fold speedup , with little loss in accuracy . [[EENNDD]] rates of convergence; random projections; sampling"}, "Unjuran rawak sangat jarang Terdapat minat yang besar dalam unjuran rawak, algoritma anggaran untuk menganggar jarak antara pasangan titik dalam ruang vektor dimensi tinggi. Biarkan A dalam Rn x D menjadi titik n kami dalam dimensi D. Kaedah mengalikan A dengan matriks rawak R dalam RD x k, mengurangkan dimensi D ke k hanya untuk mempercepat pengiraan. R biasanya terdiri daripada catatan N 0,1 normal biasa. Telah diketahui bahawa unjuran rawak mengekalkan jarak berpasangan mengikut jangkaan. Achlioptas mencadangkan unjuran rawak jarang dengan menggantikan entri N 0,1 di R dengan entri di -1,0,1 dengan kebarangkalian 1 \\ / 6, 2 \\ / 3, 1 \\ / 6, mencapai peningkatan tiga kali ganda dalam masa pemprosesan. Kami mengesyorkan untuk menggunakan entri R dalam -1,0,1 dengan kebarangkalian 1 \\ / 2 \u221a D, 1-1 \u221a D, 1 \\ / 2 \u221a D untuk mencapai peningkatan \u221a D-kali ganda yang ketara, dengan ketepatan kehilangan yang sedikit. [[EENNDD]] kadar penumpuan; unjuran rawak; persampelan"], [{"string": "Query chains : learning to rank from implicit feedback This paper presents a novel approach for using clickthrough data to learn ranked retrieval functions for web search results . We observe that users searching the web often perform a sequence , or chain , of queries with a similar information need . Using query chains , we generate new types of preference judgments from search engine logs , thus taking advantage of user intelligence in reformulating queries . To validate our method we perform a controlled user study comparing generated preference judgments to explicit relevance judgments . We also implemented a real-world search engine to test our approach , using a modified ranking SVM to learn an improved ranking function from preference data . Our results demonstrate significant improvements in the ranking given by the search engine . The learned rankings outperform both a static ranking function , as well as one trained without considering query chains .", "keywords": ["clickthrough data", "search engines", "information search and retrieval", "implicit feedback", "support vector machines", "machine learning"], "combined": "Query chains : learning to rank from implicit feedback This paper presents a novel approach for using clickthrough data to learn ranked retrieval functions for web search results . We observe that users searching the web often perform a sequence , or chain , of queries with a similar information need . Using query chains , we generate new types of preference judgments from search engine logs , thus taking advantage of user intelligence in reformulating queries . To validate our method we perform a controlled user study comparing generated preference judgments to explicit relevance judgments . We also implemented a real-world search engine to test our approach , using a modified ranking SVM to learn an improved ranking function from preference data . Our results demonstrate significant improvements in the ranking given by the search engine . The learned rankings outperform both a static ranking function , as well as one trained without considering query chains . [[EENNDD]] clickthrough data; search engines; information search and retrieval; implicit feedback; support vector machines; machine learning"}, "Rangkaian pertanyaan: belajar memberi peringkat dari maklum balas tersirat Makalah ini menyajikan pendekatan baru untuk menggunakan data klik untuk mengetahui fungsi pengambilan peringkat untuk hasil carian web. Kami melihat bahawa pengguna yang mencari di web sering melakukan urutan, atau rantai, pertanyaan dengan keperluan maklumat yang serupa. Dengan menggunakan rantai pertanyaan, kami menghasilkan jenis penilaian pilihan baru dari log enjin carian, sehingga memanfaatkan kecerdasan pengguna dalam merumuskan semula pertanyaan. Untuk mengesahkan kaedah kami, kami melakukan kajian pengguna terkawal membandingkan penilaian preferensi yang dihasilkan dengan penilaian relevan eksplisit. Kami juga menerapkan mesin carian dunia nyata untuk menguji pendekatan kami, menggunakan SVM peringkat yang diubah untuk mempelajari fungsi peringkat yang lebih baik dari data pilihan. Hasil kami menunjukkan peningkatan yang signifikan dalam peringkat yang diberikan oleh mesin pencari. Peringkat yang dipelajari mengungguli fungsi peringkat statik, dan juga yang terlatih tanpa mempertimbangkan rantai pertanyaan. [[EENNDD]] data klik lalu; enjin carian; carian dan pengambilan maklumat; maklum balas tersirat; mesin vektor sokongan; pembelajaran mesin"], [{"string": "Connecting the dots between news articles The process of extracting useful knowledge from large datasets has become one of the most pressing problems in today 's society . The problem spans entire sectors , from scientists to intelligence analysts and web users , all of whom are constantly struggling to keep up with the larger and larger amounts of content published every day . With this much data , it is often easy to miss the big picture . In this paper , we investigate methods for automatically connecting the dots -- providing a structured , easy way to navigate within a new topic and discover hidden connections . We focus on the news domain : given two news articles , our system automatically finds a coherent chain linking them together . For example , it can recover the chain of events starting with the decline of home prices January 2007 , and ending with the ongoing health-care debate . We formalize the characteristics of a good chain and provide an efficient algorithm with theoretical guarantees to connect two fixed endpoints . We incorporate user feedback into our framework , allowing the stories to be refined and personalized . Finally , we evaluate our algorithm over real news data . Our user studies demonstrate the algorithm 's effectiveness in helping users understanding the news .", "keywords": ["news", "coherence", "learning"], "combined": "Connecting the dots between news articles The process of extracting useful knowledge from large datasets has become one of the most pressing problems in today 's society . The problem spans entire sectors , from scientists to intelligence analysts and web users , all of whom are constantly struggling to keep up with the larger and larger amounts of content published every day . With this much data , it is often easy to miss the big picture . In this paper , we investigate methods for automatically connecting the dots -- providing a structured , easy way to navigate within a new topic and discover hidden connections . We focus on the news domain : given two news articles , our system automatically finds a coherent chain linking them together . For example , it can recover the chain of events starting with the decline of home prices January 2007 , and ending with the ongoing health-care debate . We formalize the characteristics of a good chain and provide an efficient algorithm with theoretical guarantees to connect two fixed endpoints . We incorporate user feedback into our framework , allowing the stories to be refined and personalized . Finally , we evaluate our algorithm over real news data . Our user studies demonstrate the algorithm 's effectiveness in helping users understanding the news . [[EENNDD]] news; coherence; learning"}, "Menghubungkan titik antara artikel berita Proses mengekstrak pengetahuan berguna dari kumpulan data yang besar telah menjadi salah satu masalah yang paling mendesak dalam masyarakat hari ini. Masalahnya merangkumi seluruh sektor, dari saintis hingga penganalisis perisikan dan pengguna web, yang semuanya selalu berjuang untuk mengikuti jumlah kandungan yang diterbitkan setiap hari. Dengan sebilangan besar data ini, seringkali mudah untuk kehilangan gambaran besar. Dalam makalah ini, kami menyiasat kaedah untuk menghubungkan titik secara automatik - menyediakan cara terstruktur dan mudah untuk menavigasi dalam topik baru dan menemui sambungan tersembunyi. Kami memberi tumpuan kepada domain berita: memandangkan dua artikel berita, sistem kami secara automatik menemui rantai yang koheren yang menghubungkannya bersama. Sebagai contoh, ia dapat memulihkan rangkaian peristiwa bermula dengan penurunan harga rumah Januari 2007, dan berakhir dengan perbahasan perawatan kesihatan yang sedang berlangsung. Kami memformalkan ciri-ciri rantai yang baik dan memberikan algoritma yang cekap dengan jaminan teori untuk menghubungkan dua titik akhir yang tetap. Kami memasukkan maklum balas pengguna ke dalam kerangka kerja kami, yang membolehkan cerita disempurnakan dan diperibadikan. Akhirnya, kami menilai algoritma kami terhadap data berita sebenar. Kajian pengguna kami menunjukkan keberkesanan algoritma dalam membantu pengguna memahami berita. [[EENNDD]] berita; koheren; belajar"], [{"string": "Dense itemsets Frequent itemset mining has been the subject of a lot of work in data mining research ever since association rules were introduced . In this paper we address a problem with frequent itemsets : that they only count rows where all their attributes are present , and do not allow for any noise . We show that generalizing the concept of frequency while preserving the performance of mining algorithms is nontrivial , and introduce a generalization of frequent itemsets , dense itemsets . Dense itemsets do not require all attributes to be present at the same time ; instead , the itemset needs to define a sufficiently large submatrix that exceeds a given density threshold of attributes present . We consider the problem of computing all dense itemsets in a database . We give a levelwise algorithm for this problem , and also study the top - $ k $ variations , i.e. , finding the k densest sets with a given support , or the k best-supported sets with a given density . These algorithms select the other parameter automatically , which simplifies mining dense itemsets in an explorative way . We show that the concept captures natural facets of data sets , and give extensive empirical results on the performance of the algorithms . Combining the concept of dense itemsets with set cover ideas , we also show that dense itemsets can be used to obtain succinct descriptions of large datasets . We also discuss some variations of dense itemsets .", "keywords": ["frequent itemsets", "error tolerance"], "combined": "Dense itemsets Frequent itemset mining has been the subject of a lot of work in data mining research ever since association rules were introduced . In this paper we address a problem with frequent itemsets : that they only count rows where all their attributes are present , and do not allow for any noise . We show that generalizing the concept of frequency while preserving the performance of mining algorithms is nontrivial , and introduce a generalization of frequent itemsets , dense itemsets . Dense itemsets do not require all attributes to be present at the same time ; instead , the itemset needs to define a sufficiently large submatrix that exceeds a given density threshold of attributes present . We consider the problem of computing all dense itemsets in a database . We give a levelwise algorithm for this problem , and also study the top - $ k $ variations , i.e. , finding the k densest sets with a given support , or the k best-supported sets with a given density . These algorithms select the other parameter automatically , which simplifies mining dense itemsets in an explorative way . We show that the concept captures natural facets of data sets , and give extensive empirical results on the performance of the algorithms . Combining the concept of dense itemsets with set cover ideas , we also show that dense itemsets can be used to obtain succinct descriptions of large datasets . We also discuss some variations of dense itemsets . [[EENNDD]] frequent itemsets; error tolerance"}, "Itemet padat Perlombongan itemet yang kerap telah menjadi subjek banyak pekerjaan dalam penyelidikan perlombongan data sejak peraturan persatuan diperkenalkan. Dalam makalah ini kami menangani masalah dengan set item yang kerap: bahawa mereka hanya menghitung baris di mana semua atributnya ada, dan tidak membenarkan kebisingan. Kami menunjukkan bahawa menggeneralisasikan konsep frekuensi sambil mengekalkan prestasi algoritma perlombongan adalah tidak biasa, dan memperkenalkan generalisasi kumpulan barang kerap, set barang padat. Set item padat tidak memerlukan semua atribut hadir pada masa yang sama; sebaliknya, itemet perlu menentukan submatrix yang cukup besar yang melebihi ambang ketumpatan atribut yang ada. Kami mempertimbangkan masalah pengkomputeran semua set item padat dalam pangkalan data. Kami memberikan algoritma tahap demi tahap untuk masalah ini, dan juga mengkaji variasi teratas - $ k $, iaitu mencari set k paling padat dengan sokongan tertentu, atau set k paling disokong dengan kepadatan tertentu. Algoritma ini memilih parameter lain secara automatik, yang memudahkan perlombongan item padat dengan cara eksploratif. Kami menunjukkan bahawa konsep menangkap aspek semula jadi set data, dan memberikan hasil empirikal yang luas pada prestasi algoritma. Menggabungkan konsep set item padat dengan idea penutup set, kami juga menunjukkan bahawa set barang padat dapat digunakan untuk mendapatkan keterangan ringkas dari set data besar. Kami juga membincangkan beberapa variasi set item padat. [[EENNDD]] kumpulan item yang kerap; toleransi kesalahan"], [{"string": "Discovering informative content blocks from Web documents In this paper , we propose a new approach to discover informative contents from a set of tabular documents or Web pages of a Web site . Our system , InfoDiscoverer , first partitions a page into several content blocks according to HTML tag TABLE in a Web page . Based on the occurrence of the features terms in the set of pages , it calculates entropy value of each feature . According to the entropy value of each feature in a content block , the entropy value of the block is defined . By analyzing the information measure , we propose a method to dynamically select the entropy-threshold that partitions blocks into either informative or redundant . Informative content blocks are distinguished parts of the page , whereas redundant content blocks are common parts . Based on the answer set generated from 13 manually tagged news Web sites with a total of 26,518 Web pages , experiments show that both recall and precision rates are greater than 0.956 . That is , using the approach , informative blocks news articles of these sites can be automatically separated from semantically redundant contents such as advertisements , banners , navigation panels , news categories , etc. . By adopting InfoDiscoverer as the preprocessor of information retrieval and extraction applications , the retrieval and extracting precision will be increased , and the indexing size and extracting complexity will also be reduced .", "keywords": ["information extraction", "entropy", "information retrieval", "informative content discovery"], "combined": "Discovering informative content blocks from Web documents In this paper , we propose a new approach to discover informative contents from a set of tabular documents or Web pages of a Web site . Our system , InfoDiscoverer , first partitions a page into several content blocks according to HTML tag TABLE in a Web page . Based on the occurrence of the features terms in the set of pages , it calculates entropy value of each feature . According to the entropy value of each feature in a content block , the entropy value of the block is defined . By analyzing the information measure , we propose a method to dynamically select the entropy-threshold that partitions blocks into either informative or redundant . Informative content blocks are distinguished parts of the page , whereas redundant content blocks are common parts . Based on the answer set generated from 13 manually tagged news Web sites with a total of 26,518 Web pages , experiments show that both recall and precision rates are greater than 0.956 . That is , using the approach , informative blocks news articles of these sites can be automatically separated from semantically redundant contents such as advertisements , banners , navigation panels , news categories , etc. . By adopting InfoDiscoverer as the preprocessor of information retrieval and extraction applications , the retrieval and extracting precision will be increased , and the indexing size and extracting complexity will also be reduced . [[EENNDD]] information extraction; entropy; information retrieval; informative content discovery"}, "Menemui blok kandungan berinformasi dari dokumen Web Dalam makalah ini, kami mengusulkan pendekatan baru untuk menemukan kandungan berinformasi dari sekumpulan dokumen tabel atau halaman Web dari laman web. Sistem kami, InfoDiscoverer, membahagikan halaman pertama ke dalam beberapa blok kandungan mengikut label HTML TABLE di halaman Web. Berdasarkan berlakunya istilah ciri dalam kumpulan halaman, ia mengira nilai entropi setiap ciri. Mengikut nilai entropi setiap ciri dalam blok kandungan, nilai entropi blok ditentukan. Dengan menganalisis ukuran maklumat, kami mencadangkan kaedah untuk memilih secara dinamis ambang entropi yang disekat oleh partisi menjadi maklumat atau berlebihan. Blok kandungan berinformasi adalah bahagian halaman yang dibezakan, sedangkan blok kandungan berlebihan adalah bahagian yang umum. Berdasarkan set jawapan yang dihasilkan dari 13 laman web berita yang ditandai secara manual dengan jumlah keseluruhan 26.518 halaman Web, eksperimen menunjukkan bahawa kedua-dua kadar penarikan dan ketepatan lebih besar daripada 0,956. Maksudnya, dengan menggunakan pendekatan, artikel berita blok maklumat dari laman web ini dapat dipisahkan secara automatik dari isi semantik yang berlebihan seperti iklan, sepanduk, panel navigasi, kategori berita, dll. Dengan mengadopsi InfoDiscoverer sebagai praproses aplikasi pengambilan dan pengekstrakan maklumat, pengambilan dan ketepatan pengekstrakan akan ditingkatkan, dan ukuran pengindeksan dan kerumitan pengekstrakan juga akan berkurang. [[EENNDD]] pengekstrakan maklumat; entropi; pengambilan maklumat; penemuan kandungan yang bermaklumat"], [{"string": "Clustering spatial data using random walks Discovering significant patterns that exist implicitly in huge spatial databases is an important computational task . A common approach to this problem is to use cluster analysis . We propose a novel approach to clustering , based on the deterministic analysis of random walks on a weighted graph generated from the data . Our approach can decompose the data into arbitrarily shaped clusters of different sizes and densities , overcoming noise and outliers that may blur the natural decomposition of the data . The method requires only O n log n time , and one of its variants needs only constant space .", "keywords": ["probabilistic algorithms"], "combined": "Clustering spatial data using random walks Discovering significant patterns that exist implicitly in huge spatial databases is an important computational task . A common approach to this problem is to use cluster analysis . We propose a novel approach to clustering , based on the deterministic analysis of random walks on a weighted graph generated from the data . Our approach can decompose the data into arbitrarily shaped clusters of different sizes and densities , overcoming noise and outliers that may blur the natural decomposition of the data . The method requires only O n log n time , and one of its variants needs only constant space . [[EENNDD]] probabilistic algorithms"}, "Mengumpulkan data spasial menggunakan jalan rawak Mencari corak penting yang wujud secara tersirat dalam pangkalan data spasial yang besar adalah tugas komputasi yang penting. Pendekatan umum untuk masalah ini adalah menggunakan analisis kluster. Kami mencadangkan pendekatan baru untuk pengelompokan, berdasarkan analisis deterministik jalan rawak pada grafik berwajaran yang dihasilkan dari data. Pendekatan kami dapat menguraikan data menjadi kelompok berbentuk sewenang-wenang dengan berbagai ukuran dan kepadatan, mengatasi kebisingan dan outliers yang dapat mengaburkan penguraian semula jadi data. Kaedah ini hanya memerlukan masa O n log n, dan salah satu variannya hanya memerlukan ruang tetap. [[EENNDD]] algoritma probabilistik"], [{"string": "Can we learn a template-independent wrapper for news article extraction from a single training site ? Automatic news extraction from news pages is important in many Web applications such as news aggregation . However , the existing news extraction methods based on template-level wrapper induction have three serious limitations . First , the existing methods can not correctly extract pages belonging to an unseen template . Second , it is costly to maintain up-to-date wrappers for a large amount of news websites , because any change of a template may invalidate the corresponding wrapper . Last , the existing methods can merely extract unformatted plain texts , and thus are not user friendly . In this paper , we tackle the problem of template-independent Web news extraction in a user-friendly way . We formalize Web news extraction as a machine learning problem and learn a template-independent wrapper using a very small number of labeled news pages from a single site . Novel features dedicated to news titles and bodies are developed . Correlations between news titles and news bodies are exploited . Our template-independent wrapper can extract news pages from different sites regardless of templates . Moreover , our approach can extract not only texts , but also images and animates within the news bodies and the extracted news articles are in the same visual style as in the original pages . In our experiments , a wrapper learned from 40 pages from a single news site achieved an accuracy of 98.1 % on 3,973 news pages from 12 news sites .", "keywords": ["data extraction", "web mining", "miscellaneous", "classification"], "combined": "Can we learn a template-independent wrapper for news article extraction from a single training site ? Automatic news extraction from news pages is important in many Web applications such as news aggregation . However , the existing news extraction methods based on template-level wrapper induction have three serious limitations . First , the existing methods can not correctly extract pages belonging to an unseen template . Second , it is costly to maintain up-to-date wrappers for a large amount of news websites , because any change of a template may invalidate the corresponding wrapper . Last , the existing methods can merely extract unformatted plain texts , and thus are not user friendly . In this paper , we tackle the problem of template-independent Web news extraction in a user-friendly way . We formalize Web news extraction as a machine learning problem and learn a template-independent wrapper using a very small number of labeled news pages from a single site . Novel features dedicated to news titles and bodies are developed . Correlations between news titles and news bodies are exploited . Our template-independent wrapper can extract news pages from different sites regardless of templates . Moreover , our approach can extract not only texts , but also images and animates within the news bodies and the extracted news articles are in the same visual style as in the original pages . In our experiments , a wrapper learned from 40 pages from a single news site achieved an accuracy of 98.1 % on 3,973 news pages from 12 news sites . [[EENNDD]] data extraction; web mining; miscellaneous; classification"}, "Bolehkah kita mempelajari pembungkus bebas templat untuk pengekstrakan artikel berita dari satu laman latihan? Pengambilan berita secara automatik dari halaman berita penting dalam banyak aplikasi Web seperti penggabungan berita. Walau bagaimanapun, kaedah pengekstrakan berita yang ada berdasarkan induksi pembungkus peringkat templat mempunyai tiga batasan serius. Pertama, kaedah yang ada tidak dapat mengekstrak halaman milik templat yang tidak kelihatan dengan betul. Kedua, mahal untuk mengekalkan pembungkus terkini untuk sebilangan besar laman web berita, kerana sebarang perubahan templat boleh membatalkan pembungkus yang sesuai. Terakhir, kaedah yang ada hanya dapat mengekstrak teks biasa yang tidak diformat, dan oleh itu tidak mesra pengguna. Dalam makalah ini, kami menangani masalah pengekstrakan berita Web bebas templat dengan cara yang mesra pengguna. Kami memformalkan pengekstrakan berita Web sebagai masalah pembelajaran mesin dan mempelajari pembungkus bebas templat menggunakan sebilangan kecil halaman berita berlabel dari satu laman web. Ciri-ciri novel yang khusus untuk tajuk berita dan badan dikembangkan. Hubungan antara tajuk berita dan badan berita dieksploitasi. Pembungkus bebas templat kami dapat mengekstrak halaman berita dari laman web yang berbeza tanpa mengira templat. Lebih-lebih lagi, pendekatan kami dapat mengekstrak bukan hanya teks, tetapi juga gambar dan animasi dalam badan berita dan artikel berita yang diekstrak dalam gaya visual yang sama seperti di halaman asalnya. Dalam eksperimen kami, pembungkus yang dipelajari dari 40 halaman dari satu laman berita mencapai ketepatan 98.1% pada 3,973 halaman berita dari 12 laman berita. [[EENNDD]] pengekstrakan data; perlombongan web; pelbagai; pengelasan"], [{"string": "Detecting time series motifs under uniform scaling Time series motifs are approximately repeated patterns foundwithin the data . Such motifs have utility for many data mining algorithms , including rule-discovery , novelty-detection , summarization and clustering . Since the formalization of the problem and the introduction of efficient linear time algorithms , motif discovery has been successfully applied tomany domains , including medicine , motion capture , robotics and meteorology . In this work we show that most previous applications of time series motifs have been severely limited by the definition 's brittleness to even slight changes of uniform scaling , the speed at which the patterns develop . We introduce a new algorithm that allows discovery of time series motifs with invariance to uniform scaling , and show that it produces objectively superior results in several important domains . Apart from being more general than all other motifdiscovery algorithms , a further contribution of our work isthat it is simpler than previous approaches , in particular we have drastically reduced the number of parameters that need to be specified .", "keywords": ["motifs", "time series", "uniform scaling", "random projection"], "combined": "Detecting time series motifs under uniform scaling Time series motifs are approximately repeated patterns foundwithin the data . Such motifs have utility for many data mining algorithms , including rule-discovery , novelty-detection , summarization and clustering . Since the formalization of the problem and the introduction of efficient linear time algorithms , motif discovery has been successfully applied tomany domains , including medicine , motion capture , robotics and meteorology . In this work we show that most previous applications of time series motifs have been severely limited by the definition 's brittleness to even slight changes of uniform scaling , the speed at which the patterns develop . We introduce a new algorithm that allows discovery of time series motifs with invariance to uniform scaling , and show that it produces objectively superior results in several important domains . Apart from being more general than all other motifdiscovery algorithms , a further contribution of our work isthat it is simpler than previous approaches , in particular we have drastically reduced the number of parameters that need to be specified . [[EENNDD]] motifs; time series; uniform scaling; random projection"}, "Mengesan motif siri masa di bawah skala seragam Motif siri masa adalah corak berulang yang terdapat dalam data. Motif semacam itu mempunyai kegunaan untuk banyak algoritma perlombongan data, termasuk penemuan peraturan, pengesanan baru, ringkasan dan pengelompokan. Sejak formalisasi masalah dan pengenalan algoritma waktu linier yang efisien, penemuan motif telah berjaya diterapkan domain tomany, termasuk perubatan, tangkapan gerakan, robotik dan meteorologi. Dalam karya ini kami menunjukkan bahawa kebanyakan aplikasi motif siri masa sebelumnya telah sangat dibatasi oleh kerapuhan definisi hingga sedikit perubahan skala yang seragam, kecepatan perkembangan corak. Kami memperkenalkan algoritma baru yang memungkinkan penemuan motif siri masa dengan invarians ke penskalaan seragam, dan menunjukkan bahawa ia menghasilkan hasil yang unggul secara objektif dalam beberapa domain penting. Selain lebih umum daripada semua algoritma penemuan motif lain, sumbangan lebih jauh dari kerja kami adalah lebih mudah daripada pendekatan sebelumnya, khususnya kami telah mengurangkan jumlah parameter yang perlu ditentukan secara drastik. [[EENNDD]] motif; siri masa; penskalaan seragam; unjuran rawak"], [{"string": "Angle-based outlier detection in high-dimensional data Detecting outliers in a large set of data objects is a major data mining task aiming at finding different mechanisms responsible for different groups of objects in a data set . All existing approaches , however , are based on an assessment of distances sometimes indirectly by assuming certain distributions in the full-dimensional Euclidean data space . In high-dimensional data , these approaches are bound to deteriorate due to the notorious `` curse of dimensionality '' . In this paper , we propose a novel approach named ABOD Angle-Based Outlier Detection and some variants assessing the variance in the angles between the difference vectors of a point to the other points . This way , the effects of the `` curse of dimensionality '' are alleviated compared to purely distance-based approaches . A main advantage of our new approach is that our method does not rely on any parameter selection influencing the quality of the achieved ranking . In a thorough experimental evaluation , we compare ABOD to the well-established distance-based method LOF for various artificial and a real world data set and show ABOD to perform especially well on high-dimensional data .", "keywords": ["angle-based", "high-dimensional", "outlier detection"], "combined": "Angle-based outlier detection in high-dimensional data Detecting outliers in a large set of data objects is a major data mining task aiming at finding different mechanisms responsible for different groups of objects in a data set . All existing approaches , however , are based on an assessment of distances sometimes indirectly by assuming certain distributions in the full-dimensional Euclidean data space . In high-dimensional data , these approaches are bound to deteriorate due to the notorious `` curse of dimensionality '' . In this paper , we propose a novel approach named ABOD Angle-Based Outlier Detection and some variants assessing the variance in the angles between the difference vectors of a point to the other points . This way , the effects of the `` curse of dimensionality '' are alleviated compared to purely distance-based approaches . A main advantage of our new approach is that our method does not rely on any parameter selection influencing the quality of the achieved ranking . In a thorough experimental evaluation , we compare ABOD to the well-established distance-based method LOF for various artificial and a real world data set and show ABOD to perform especially well on high-dimensional data . [[EENNDD]] angle-based; high-dimensional; outlier detection"}, "Pengesanan outlier berasaskan sudut dalam data dimensi tinggi Mengesan outliers dalam sekumpulan besar objek data adalah tugas perlombongan data utama yang bertujuan mencari mekanisme berbeza yang bertanggungjawab untuk kumpulan objek yang berlainan dalam satu set data. Semua pendekatan yang ada, bagaimanapun, didasarkan pada penilaian jarak kadang-kadang secara tidak langsung dengan menganggap pengedaran tertentu di ruang data Euclidean dimensi penuh. Dalam data dimensi tinggi, pendekatan ini pasti akan merosot kerana \"kutukan dimensi\" yang terkenal. Dalam makalah ini, kami mengusulkan pendekatan baru bernama ABOD Angle-Based Outlier Detection dan beberapa varian yang menilai variasi sudut antara vektor perbezaan titik ke titik lain. Dengan cara ini, kesan \"kutukan dimensi\" dikurangkan dibandingkan dengan pendekatan berdasarkan jarak semata-mata. Kelebihan utama pendekatan baru kami ialah kaedah kami tidak bergantung pada pemilihan parameter yang mempengaruhi kualiti peringkat yang dicapai. Dalam penilaian eksperimental yang menyeluruh, kami membandingkan ABOD dengan kaedah LOF berdasarkan jarak jauh yang mapan untuk pelbagai set data buatan dan dunia nyata dan menunjukkan ABOD berkinerja sangat baik pada data dimensi tinggi. [[EENNDD]] berasaskan sudut; dimensi tinggi; pengesanan luar"], [{"string": "Weighted Association Rule Mining using weighted support and significance framework We address the issues of discovering significant binary relationships in transaction datasets in a weighted setting . Traditional model of association rule mining is adapted to handle weighted association rule mining problems where each item is allowed to have a weight . The goal is to steer the mining focus to those significant relationships involving items with significant weights rather than being flooded in the combinatornal explosion of insignificant relationships . We identify the challenge of using weights in the iterative process of generating large itemsets . The problem of invalidation of the `` downward closure property '' in the weighted setting is solved by using an improved model of weighted support measurements and exploiting a `` weighted downward closure property '' . A new algorithm called WARM Weighted Association Rule Mining is developed based on the improved model . The algorithm is both scalable and efficient in discovering significant relationships in weighted settings as illustrated by experiments performed on simulated datasets .", "keywords": ["weighted association rule mining", "significant relationship", "weighted support", "warm algorithm", "weighted downward closure property"], "combined": "Weighted Association Rule Mining using weighted support and significance framework We address the issues of discovering significant binary relationships in transaction datasets in a weighted setting . Traditional model of association rule mining is adapted to handle weighted association rule mining problems where each item is allowed to have a weight . The goal is to steer the mining focus to those significant relationships involving items with significant weights rather than being flooded in the combinatornal explosion of insignificant relationships . We identify the challenge of using weights in the iterative process of generating large itemsets . The problem of invalidation of the `` downward closure property '' in the weighted setting is solved by using an improved model of weighted support measurements and exploiting a `` weighted downward closure property '' . A new algorithm called WARM Weighted Association Rule Mining is developed based on the improved model . The algorithm is both scalable and efficient in discovering significant relationships in weighted settings as illustrated by experiments performed on simulated datasets . [[EENNDD]] weighted association rule mining; significant relationship; weighted support; warm algorithm; weighted downward closure property"}, "Perlombongan Peraturan Pertimbangan Berat menggunakan kerangka sokongan dan kepentingan yang berwajaran Kami menangani isu-isu menemui hubungan binari yang signifikan dalam set data transaksi dalam keadaan berwajaran. Model tradisional perlombongan peraturan persatuan disesuaikan untuk menangani masalah perlombongan peraturan persatuan berwajaran di mana setiap item dibenarkan mempunyai berat badan. Tujuannya adalah untuk mengarahkan fokus perlombongan ke hubungan penting yang melibatkan barang-barang dengan bobot yang signifikan daripada dibanjiri dalam ledakan gabungan hubungan yang tidak signifikan. Kami mengenal pasti cabaran menggunakan bobot dalam proses berulang untuk menghasilkan item yang besar. Masalah pembatalan \"harta penutupan ke bawah\" dalam pengaturan tertimbang diselesaikan dengan menggunakan model pengukuran sokongan tertimbang yang lebih baik dan memanfaatkan \"harta penutupan turun\". Algoritma baru yang dipanggil WARM Weighted Association Rule Mining dikembangkan berdasarkan model yang diperbaiki. Algoritma ini berskala dan efisien dalam mencari hubungan yang signifikan dalam tetapan berwajaran seperti yang digambarkan oleh eksperimen yang dilakukan pada set data simulasi. [[EENNDD]] perlombongan peraturan persatuan berwajaran; hubungan yang signifikan; sokongan berwajaran; algoritma hangat; harta penutupan turun berwajaran"], [{"string": "MIME : a framework for interactive visual pattern mining We present a framework for interactive visual pattern mining . Our system enables the user to browse through the data and patterns easily and intuitively , using a toolbox consisting of interestingness measures , mining algorithms and post-processing algorithms to assist in identifying interesting patterns . By mining interactively , we enable the user to combine their subjective interestingness measure and background knowledge with a wide variety of objective measures to easily and quickly mine the most important and interesting patterns . Basically , we enable the user to become an essential part of the mining algorithm . Our demo currently applies to mining interesting itemsets and association rules , and its extension to episodes and decision trees is ongoing .", "keywords": ["mime", "pattern exploration", "interactive visual mining"], "combined": "MIME : a framework for interactive visual pattern mining We present a framework for interactive visual pattern mining . Our system enables the user to browse through the data and patterns easily and intuitively , using a toolbox consisting of interestingness measures , mining algorithms and post-processing algorithms to assist in identifying interesting patterns . By mining interactively , we enable the user to combine their subjective interestingness measure and background knowledge with a wide variety of objective measures to easily and quickly mine the most important and interesting patterns . Basically , we enable the user to become an essential part of the mining algorithm . Our demo currently applies to mining interesting itemsets and association rules , and its extension to episodes and decision trees is ongoing . [[EENNDD]] mime; pattern exploration; interactive visual mining"}, "MIME: kerangka kerja untuk perlombongan corak visual interaktif Kami membentangkan kerangka untuk perlombongan corak visual interaktif. Sistem kami membolehkan pengguna melayari data dan corak dengan mudah dan intuitif, menggunakan kotak alat yang terdiri daripada langkah-langkah menarik, algoritma perlombongan dan algoritma pasca pemprosesan untuk membantu mengenal pasti corak yang menarik. Dengan melombong secara interaktif, kami membolehkan pengguna menggabungkan ukuran minat subjektif dan pengetahuan latar belakang mereka dengan pelbagai langkah objektif untuk menambang corak yang paling penting dan menarik dengan mudah dan pantas. Pada asasnya, kami membolehkan pengguna menjadi bahagian penting dalam algoritma perlombongan. Demo kami pada masa ini berlaku untuk melombong set item menarik dan peraturan persatuan, dan pelanjutannya ke episod dan pohon keputusan sedang berlangsung. [[EENNDD]] mime; penerokaan corak; perlombongan visual interaktif"], [{"string": "Mining asynchronous periodic patterns in time series data", "keywords": ["segment-based approach", "asynchronous periodic pattern", "database applications"], "combined": "Mining asynchronous periodic patterns in time series data [[EENNDD]] segment-based approach; asynchronous periodic pattern; database applications"}, "Perlombongan pola berkala tak segerak dalam data siri masa [[EENNDD]] pendekatan berasaskan segmen; corak berkala tak segerak; aplikasi pangkalan data"], [{"string": "Efficient anomaly monitoring over moving object trajectory streams Lately there exist increasing demands for online abnormality monitoring over trajectory streams , which are obtained from moving object tracking devices . This problem is challenging due to the requirement of high speed data processing within limited space cost . In this paper , we present a novel framework for monitoring anomalies over continuous trajectory streams . First , we illustrate the importance of distance-based anomaly monitoring over moving object trajectories . Then , we utilize the local continuity characteristics of trajectories to build local clusters upon trajectory streams and monitor anomalies via efficient pruning strategies . Finally , we propose a piecewise metric index structure to reschedule the joining order of local clusters to further reduce the time cost . Our extensive experiments demonstrate the effectiveness and efficiency of our methods .", "keywords": ["temporal data", "similarity search", "outlier detection"], "combined": "Efficient anomaly monitoring over moving object trajectory streams Lately there exist increasing demands for online abnormality monitoring over trajectory streams , which are obtained from moving object tracking devices . This problem is challenging due to the requirement of high speed data processing within limited space cost . In this paper , we present a novel framework for monitoring anomalies over continuous trajectory streams . First , we illustrate the importance of distance-based anomaly monitoring over moving object trajectories . Then , we utilize the local continuity characteristics of trajectories to build local clusters upon trajectory streams and monitor anomalies via efficient pruning strategies . Finally , we propose a piecewise metric index structure to reschedule the joining order of local clusters to further reduce the time cost . Our extensive experiments demonstrate the effectiveness and efficiency of our methods . [[EENNDD]] temporal data; similarity search; outlier detection"}, "Pemantauan anomali yang cekap terhadap aliran lintasan objek bergerak Akhir-akhir ini terdapat permintaan yang meningkat untuk pemantauan keabnormalan dalam talian terhadap aliran lintasan, yang diperoleh dari peranti pengesanan objek bergerak. Masalah ini mencabar kerana keperluan pemprosesan data berkelajuan tinggi dengan kos ruang yang terhad. Dalam makalah ini, kami memaparkan kerangka baru untuk memantau anomali terhadap aliran lintasan berterusan. Pertama, kami menggambarkan pentingnya pemantauan anomali berdasarkan jarak berbanding lintasan objek yang bergerak. Kemudian, kami menggunakan ciri lanjutan lokal lintasan untuk membina kelompok lokal di aliran lintasan dan memantau anomali melalui strategi pemangkasan yang cekap. Akhirnya, kami mencadangkan struktur indeks metrik sepotong untuk menjadualkan semula susunan gabungan kluster tempatan untuk mengurangkan kos masa. Eksperimen kami yang luas menunjukkan keberkesanan dan kecekapan kaedah kami. [[EENNDD]] data temporal; carian kesamaan; pengesanan luar"], [{"string": "Composition attacks and auxiliary information in data privacy Privacy is an increasingly important aspect of data publishing . Reasoning about privacy , however , is fraught with pitfalls . One of the most significant is the auxiliary information also called external knowledge , background knowledge , or side information that an adversary gleans from other channels such as the web , public records , or domain knowledge . This paper explores how one can reason about privacy in the face of rich , realistic sources of auxiliary information . Specifically , we investigate the effectiveness of current anonymization schemes in preserving privacy when multiple organizations independently release anonymized data about overlapping populations . 1 . We investigate composition attacks , in which an adversary uses independent anonymized releases to breach privacy . We explain why recently proposed models of limited auxiliary information fail to capture composition attacks . Our experiments demonstrate that even a simple instance of a composition attack can breach privacy in practice , for a large class of currently proposed techniques . The class includes k-anonymity and several recent variants . 2 . On a more positive note , certain randomization-based notions of privacy such as differential privacy provably resist composition attacks and , in fact , the use of arbitrary side information . This resistance enables `` stand-alone '' design of anonymization schemes , without the need for explicitly keeping track of other releases . We provide a precise formulation of this property , and prove that an important class of relaxations of differential privacy also satisfy the property . This significantly enlarges the class of protocols known to enable modular design .", "keywords": ["general", "privacy", "adversarial attacks", "anonymization"], "combined": "Composition attacks and auxiliary information in data privacy Privacy is an increasingly important aspect of data publishing . Reasoning about privacy , however , is fraught with pitfalls . One of the most significant is the auxiliary information also called external knowledge , background knowledge , or side information that an adversary gleans from other channels such as the web , public records , or domain knowledge . This paper explores how one can reason about privacy in the face of rich , realistic sources of auxiliary information . Specifically , we investigate the effectiveness of current anonymization schemes in preserving privacy when multiple organizations independently release anonymized data about overlapping populations . 1 . We investigate composition attacks , in which an adversary uses independent anonymized releases to breach privacy . We explain why recently proposed models of limited auxiliary information fail to capture composition attacks . Our experiments demonstrate that even a simple instance of a composition attack can breach privacy in practice , for a large class of currently proposed techniques . The class includes k-anonymity and several recent variants . 2 . On a more positive note , certain randomization-based notions of privacy such as differential privacy provably resist composition attacks and , in fact , the use of arbitrary side information . This resistance enables `` stand-alone '' design of anonymization schemes , without the need for explicitly keeping track of other releases . We provide a precise formulation of this property , and prove that an important class of relaxations of differential privacy also satisfy the property . This significantly enlarges the class of protocols known to enable modular design . [[EENNDD]] general; privacy; adversarial attacks; anonymization"}, "Serangan komposisi dan maklumat tambahan dalam privasi data Privasi adalah aspek yang semakin penting dalam penerbitan data. Penalaran mengenai privasi, bagaimanapun, penuh dengan perangkap. Salah satu yang paling penting adalah maklumat tambahan yang juga disebut pengetahuan luaran, pengetahuan latar belakang, atau maklumat sampingan yang disebarkan oleh musuh dari saluran lain seperti web, rekod awam, atau pengetahuan domain. Makalah ini menerangkan bagaimana seseorang dapat memberi alasan mengenai privasi dalam menghadapi sumber maklumat tambahan yang kaya dan realistik. Secara khusus, kami menyiasat keberkesanan skema anonimisasi semasa dalam menjaga privasi apabila banyak organisasi melepaskan data tanpa nama mengenai populasi yang bertindih. 1. Kami menyiasat serangan komposisi, di mana musuh menggunakan siaran bebas tanpa nama untuk melanggar privasi. Kami menjelaskan mengapa model maklumat tambahan yang dicadangkan baru-baru ini gagal menangkap serangan komposisi. Eksperimen kami menunjukkan bahawa walaupun contoh serangan komposisi yang sederhana dapat melanggar privasi dalam praktiknya, untuk kelas teknik yang kini dicadangkan. Kelas ini merangkumi k-anonimiti dan beberapa varian terkini. 2. Pada pandangan yang lebih positif, pengertian privasi berdasarkan rawak seperti privasi pembezaan terbukti menentang serangan komposisi dan, sebenarnya, penggunaan maklumat sampingan sewenang-wenangnya. Rintangan ini membolehkan reka bentuk skema anonimisasi \"berdiri sendiri\", tanpa perlu secara jelas mengesan pelepasan lain. Kami memberikan rumusan tepat mengenai harta ini, dan membuktikan bahawa kelas penting dari kelonggaran privasi juga dapat memenuhi harta tanah ini. Ini memperbesar kelas protokol yang diketahui membolehkan reka bentuk modular. [[EENNDD]] umum; privasi; serangan musuh; tanpa nama"], [{"string": "BoostCluster : boosting clustering by pairwise constraints Data clustering is an important task in many disciplines . A large number of studies have attempted to improve clustering by using the side information that is often encoded as pairwise constraints . However , these studies focus on designing special clustering algorithms that can effectively exploit the pairwise constraints . We present a boosting framework for data clustering , termed as BoostCluster , that is able to iteratively improve the accuracy of any given clustering algorithm by exploiting the pairwise constraints . The key challenge in designing a boosting framework for data clustering is how to influence an arbitrary clustering algorithm with the side information since clustering algorithms by definition are unsupervised . The proposed framework addresses this problem by dynamically generating new data representations at each iteration that are , on the one hand , adapted to the clustering results at previous iterations by the given algorithm , and on the other hand consistent with the given side information . Our empirical study shows that the proposed boosting framework is effective in improving the performance of a number of popular clustering algorithms K-means , partitional SingleLink , spectral clustering , and its performance is comparable to the state-of-the-art algorithms for data clustering with side information .", "keywords": ["boosting", "pairwise constraints", "data clustering", "semi-supervised learning"], "combined": "BoostCluster : boosting clustering by pairwise constraints Data clustering is an important task in many disciplines . A large number of studies have attempted to improve clustering by using the side information that is often encoded as pairwise constraints . However , these studies focus on designing special clustering algorithms that can effectively exploit the pairwise constraints . We present a boosting framework for data clustering , termed as BoostCluster , that is able to iteratively improve the accuracy of any given clustering algorithm by exploiting the pairwise constraints . The key challenge in designing a boosting framework for data clustering is how to influence an arbitrary clustering algorithm with the side information since clustering algorithms by definition are unsupervised . The proposed framework addresses this problem by dynamically generating new data representations at each iteration that are , on the one hand , adapted to the clustering results at previous iterations by the given algorithm , and on the other hand consistent with the given side information . Our empirical study shows that the proposed boosting framework is effective in improving the performance of a number of popular clustering algorithms K-means , partitional SingleLink , spectral clustering , and its performance is comparable to the state-of-the-art algorithms for data clustering with side information . [[EENNDD]] boosting; pairwise constraints; data clustering; semi-supervised learning"}, "BoostCluster: meningkatkan pengelompokan dengan kekangan berpasangan Pengelompokan data adalah tugas penting dalam banyak disiplin ilmu. Sebilangan besar kajian telah berusaha memperbaiki pengelompokan dengan menggunakan maklumat sampingan yang sering dikodkan sebagai batasan berpasangan. Walau bagaimanapun, kajian ini menumpukan pada merancang algoritma pengelompokan khas yang dapat memanfaatkan batasan berpasangan secara berkesan. Kami menyajikan kerangka penambahbaikan untuk pengelompokan data, disebut sebagai BoostCluster, yang secara iteratif dapat meningkatkan ketepatan algoritma pengelompokan tertentu dengan memanfaatkan batasan berpasangan. Cabaran utama dalam merancang kerangka peningkatan untuk pengelompokan data adalah bagaimana mempengaruhi algoritma pengelompokan sewenang-wenangnya dengan informasi sampingan kerana algoritma pengelompokan secara definisi tidak diawasi. Kerangka kerja yang dicadangkan menangani masalah ini dengan menghasilkan perwakilan data baru secara dinamis pada setiap lelaran yang, di satu pihak, disesuaikan dengan hasil pengelompokan pada lelaran sebelumnya oleh algoritma yang diberikan, dan di sisi lain selaras dengan maklumat sampingan yang diberikan. Kajian empirikal kami menunjukkan bahawa kerangka penambahbaikan yang dicadangkan berkesan dalam meningkatkan prestasi sejumlah algoritma kluster popular K-means, partikel SingleLink, pengelompokan spektrum, dan prestasinya setanding dengan algoritma canggih untuk pengelompokan data dengan maklumat sampingan. [[EENNDD]] meningkatkan; kekangan berpasangan; pengelompokan data; pembelajaran separa penyeliaan"], [{"string": "Towards systematic design of distance functions for data mining applications Distance function computation is a key subtask in many data mining algorithms and applications . The most effective form of the distance function can only be expressed in the context of a particular data domain . It is also often a challenging and non-trivial task to find the most effective form of the distance function . For example , in the text domain , distance function design has been considered such an important and complex issue that it has been the focus of intensive research over three decades . The final design of distance functions in this domain has been reached only by detailed empirical testing and consensus over the quality of results provided by the different variations . With the increasing ability to collect data in an automated way , the number of new kinds of data continues to increase rapidly . This makes it increasingly difficult to undertake such efforts for each and every new data type . The most important aspect of distance function design is that since a human is the end-user for any application , the design must satisfy the user requirements with regard to effectiveness . This creates the need for a systematic framework to design distance functions which are sensitive to the particular characteristics of the data domain . In this paper , we discuss such a framework . The goal is to create distance functions in an automated waywhile minimizing the work required from the user . We will show that this framework creates distance functions which are significantly more effective than popularly used functions such as the Euclidean metric .", "keywords": ["distance functions", "user interaction"], "combined": "Towards systematic design of distance functions for data mining applications Distance function computation is a key subtask in many data mining algorithms and applications . The most effective form of the distance function can only be expressed in the context of a particular data domain . It is also often a challenging and non-trivial task to find the most effective form of the distance function . For example , in the text domain , distance function design has been considered such an important and complex issue that it has been the focus of intensive research over three decades . The final design of distance functions in this domain has been reached only by detailed empirical testing and consensus over the quality of results provided by the different variations . With the increasing ability to collect data in an automated way , the number of new kinds of data continues to increase rapidly . This makes it increasingly difficult to undertake such efforts for each and every new data type . The most important aspect of distance function design is that since a human is the end-user for any application , the design must satisfy the user requirements with regard to effectiveness . This creates the need for a systematic framework to design distance functions which are sensitive to the particular characteristics of the data domain . In this paper , we discuss such a framework . The goal is to create distance functions in an automated waywhile minimizing the work required from the user . We will show that this framework creates distance functions which are significantly more effective than popularly used functions such as the Euclidean metric . [[EENNDD]] distance functions; user interaction"}, "Ke arah reka bentuk fungsi jarak jauh yang sistematik untuk aplikasi perlombongan data Pengiraan fungsi jarak adalah subtugas utama dalam banyak algoritma dan aplikasi perlombongan data. Bentuk fungsi jarak yang paling berkesan hanya dapat dinyatakan dalam konteks domain data tertentu. Ia juga sering menjadi tugas yang mencabar dan tidak remeh untuk mencari bentuk fungsi jarak yang paling berkesan. Sebagai contoh, dalam domain teks, reka bentuk fungsi jarak telah dianggap sebagai masalah penting dan kompleks sehingga menjadi tumpuan penyelidikan intensif selama tiga dekad. Reka bentuk akhir fungsi jarak dalam domain ini hanya dapat dicapai dengan pengujian empirikal terperinci dan konsensus mengenai kualiti hasil yang diberikan oleh variasi yang berbeza. Dengan peningkatan kemampuan untuk mengumpulkan data secara automatik, jumlah jenis data baru terus meningkat dengan cepat. Ini menjadikannya semakin sukar untuk melakukan usaha tersebut untuk setiap jenis data baru. Aspek yang paling penting dalam reka bentuk fungsi jarak adalah kerana manusia adalah pengguna akhir untuk sebarang aplikasi, reka bentuk harus memenuhi kehendak pengguna berkenaan dengan keberkesanan. Ini mewujudkan keperluan untuk rangka kerja sistematik untuk merancang fungsi jarak yang sensitif terhadap ciri-ciri tertentu dari domain data. Dalam makalah ini, kita membincangkan kerangka tersebut. Tujuannya adalah untuk membuat fungsi jarak dengan cara automatik sambil meminimumkan kerja yang diperlukan dari pengguna. Kami akan menunjukkan bahawa kerangka ini mewujudkan fungsi jarak yang jauh lebih berkesan daripada fungsi yang sering digunakan seperti metrik Euclidean. [[EENNDD]] fungsi jarak; interaksi pengguna"], [{"string": "Generation of synthetic data sets for evaluating the accuracy of knowledge discovery systems Information Discovery and Analysis Systems IDAS are designed to correlate multiple sources of data and use data mining techniques to identify potential significant events . Application domains for IDAS are numerous and include the emerging area of homeland security . Developing test cases for an IDAS requires background data sets into which hypothetical future scenarios can be overlaid . The IDAS can then be measured in terms of false positive and false negative error rates . Obtaining the test data sets can be an obstacle due to both privacy issues and also the time and cost associated with collecting a diverse set of data sources . In this paper , we give an overview of the design and architecture of an IDAS Data Set Generator IDSG that enables a fast and comprehensive test of an IDAS . The IDSG generates data using statistical and rule-based algorithms and also semantic graphs that represent interdependencies between attributes . A credit card transaction application is used to illustrate the approach .", "keywords": ["testing tools", "data mining", "data generation", "information discovery"], "combined": "Generation of synthetic data sets for evaluating the accuracy of knowledge discovery systems Information Discovery and Analysis Systems IDAS are designed to correlate multiple sources of data and use data mining techniques to identify potential significant events . Application domains for IDAS are numerous and include the emerging area of homeland security . Developing test cases for an IDAS requires background data sets into which hypothetical future scenarios can be overlaid . The IDAS can then be measured in terms of false positive and false negative error rates . Obtaining the test data sets can be an obstacle due to both privacy issues and also the time and cost associated with collecting a diverse set of data sources . In this paper , we give an overview of the design and architecture of an IDAS Data Set Generator IDSG that enables a fast and comprehensive test of an IDAS . The IDSG generates data using statistical and rule-based algorithms and also semantic graphs that represent interdependencies between attributes . A credit card transaction application is used to illustrate the approach . [[EENNDD]] testing tools; data mining; data generation; information discovery"}, "Penjanaan set data sintetik untuk menilai ketepatan sistem penemuan pengetahuan Sistem Penemuan dan Analisis Maklumat IDAS dirancang untuk menghubungkan pelbagai sumber data dan menggunakan teknik perlombongan data untuk mengenal pasti peristiwa penting yang berpotensi. Domain aplikasi untuk IDAS banyak dan merangkumi kawasan keselamatan tanah air yang baru muncul. Membangunkan kes ujian untuk IDAS memerlukan kumpulan data latar belakang yang mana senario masa depan hipotesis dapat dilapisi. IDAS kemudian dapat diukur dari segi kadar kesalahan positif palsu dan negatif palsu. Mendapatkan set data ujian boleh menjadi halangan kerana kedua-dua masalah privasi dan juga masa dan kos yang berkaitan dengan pengumpulan pelbagai kumpulan sumber data. Dalam makalah ini, kami memberikan gambaran umum mengenai reka bentuk dan seni bina IDAS Data Set Generator IDSG yang memungkinkan ujian IDAS yang cepat dan komprehensif. IDSG menghasilkan data menggunakan algoritma berdasarkan statistik dan peraturan dan juga grafik semantik yang mewakili saling bergantung antara atribut. Aplikasi transaksi kad kredit digunakan untuk menggambarkan pendekatan. [[EENNDD]] alat ujian; perlombongan data; penjanaan data; penemuan maklumat"], [{"string": "Detecting privacy leaks using corpus-based association rules Detecting inferences in documents is critical for ensuring privacy when sharing information . In this paper , we propose a refined and practical model of inference detection using a reference corpus . Our model is inspired by association rule mining : inferences are based on word co-occurrences . Using the model and taking the Web as the reference corpus , we can find inferences and measure their strength through web-mining algorithms that leverage search engines such as Google or Yahoo ! . Our model also includes the important case of private corpora , to model inference detection in enterprise settings in which there is a large private document repository . We find inferences in private corpora by using analogs of our Web-mining algorithms , relying on an index for the corpus rather than a Web search engine . We present results from two experiments . The first experiment demonstrates the performance of our techniques in identifying all the keywords that allow for inference of a particular topic e.g. `` HIV '' with confidence above a certain threshold . The second experiment uses the public Enron e-mail dataset . We postulate a sensitive topic and use the Enron corpus and the Web together to find inferences for the topic . These experiments demonstrate that our techniques are practical , and that our model of inference based on word co-occurrence is well-suited to efficient inference detection .", "keywords": ["search engine", "inference control", "inference detection", "association rule mining", "web mining"], "combined": "Detecting privacy leaks using corpus-based association rules Detecting inferences in documents is critical for ensuring privacy when sharing information . In this paper , we propose a refined and practical model of inference detection using a reference corpus . Our model is inspired by association rule mining : inferences are based on word co-occurrences . Using the model and taking the Web as the reference corpus , we can find inferences and measure their strength through web-mining algorithms that leverage search engines such as Google or Yahoo ! . Our model also includes the important case of private corpora , to model inference detection in enterprise settings in which there is a large private document repository . We find inferences in private corpora by using analogs of our Web-mining algorithms , relying on an index for the corpus rather than a Web search engine . We present results from two experiments . The first experiment demonstrates the performance of our techniques in identifying all the keywords that allow for inference of a particular topic e.g. `` HIV '' with confidence above a certain threshold . The second experiment uses the public Enron e-mail dataset . We postulate a sensitive topic and use the Enron corpus and the Web together to find inferences for the topic . These experiments demonstrate that our techniques are practical , and that our model of inference based on word co-occurrence is well-suited to efficient inference detection . [[EENNDD]] search engine; inference control; inference detection; association rule mining; web mining"}, "Mengesan kebocoran privasi menggunakan peraturan persatuan berdasarkan korpus Mengesan kesimpulan dalam dokumen sangat penting untuk memastikan privasi ketika berkongsi maklumat. Dalam makalah ini, kami mencadangkan model pengesanan inferensi yang halus dan praktikal menggunakan corpus rujukan. Model kami diilhamkan oleh perlombongan peraturan persatuan: kesimpulan berdasarkan pada kata-kata bersama. Dengan menggunakan model dan menjadikan Web sebagai korpus rujukan, kita dapat mencari kesimpulan dan mengukur kekuatannya melalui algoritma perlombongan web yang memanfaatkan enjin carian seperti Google atau Yahoo! . Model kami juga merangkumi kes penting korporat swasta, untuk memodelkan pengesanan inferensi dalam tetapan perusahaan di mana terdapat repositori dokumen peribadi yang besar. Kami menjumpai kesimpulan dalam korporat swasta dengan menggunakan analog algoritma perlombongan Web kami, bergantung pada indeks untuk korpus dan bukannya mesin carian Web. Kami membentangkan hasil dari dua eksperimen. Percubaan pertama menunjukkan prestasi teknik kami dalam mengenal pasti semua kata kunci yang memungkinkan untuk membuat kesimpulan mengenai topik tertentu mis. \"HIV\" dengan keyakinan melebihi ambang tertentu. Eksperimen kedua menggunakan set data e-mel Enron awam. Kami mengemukakan topik yang sensitif dan menggunakan Enron corpus dan Web bersama untuk mencari kesimpulan untuk topik tersebut. Eksperimen ini menunjukkan bahawa teknik kami praktikal, dan bahawa model inferens kami berdasarkan kejadian bersama sesuai untuk pengesanan inferensi yang cekap. [[EENNDD]] enjin carian; kawalan inferens; pengesanan inferens; perlombongan peraturan persatuan; perlombongan web"], [{"string": "From frequent itemsets to semantically meaningful visual patterns Data mining techniques that are successful in transaction and text data may not be simply applied to image data that contain high-dimensional features and have spatial structures . It is not a trivial task to discover meaningful visual patterns in image databases , because the content variations and spatial dependency in the visual data greatly challenge most existing methods . This paper presents a novel approach to coping with these difficulties for mining meaningful visual patterns . Specifically , the novelty of this work lies in the following new contributions : 1 a principled solution to the discovery of meaningful itemsets based on frequent itemset mining ; 2 a self-supervised clustering scheme of the high-dimensional visual features by feeding back discovered patterns to tune the similarity measure through metric learning ; and 3 a pattern summarization method that deals with the measurement noises brought by the image data . The experimental results in the real images show that our method can discover semantically meaningful patterns efficiently and effectively .", "keywords": ["image data mining", "pattern summarization", "self-supervised clustering", "meaningful itemset mining"], "combined": "From frequent itemsets to semantically meaningful visual patterns Data mining techniques that are successful in transaction and text data may not be simply applied to image data that contain high-dimensional features and have spatial structures . It is not a trivial task to discover meaningful visual patterns in image databases , because the content variations and spatial dependency in the visual data greatly challenge most existing methods . This paper presents a novel approach to coping with these difficulties for mining meaningful visual patterns . Specifically , the novelty of this work lies in the following new contributions : 1 a principled solution to the discovery of meaningful itemsets based on frequent itemset mining ; 2 a self-supervised clustering scheme of the high-dimensional visual features by feeding back discovered patterns to tune the similarity measure through metric learning ; and 3 a pattern summarization method that deals with the measurement noises brought by the image data . The experimental results in the real images show that our method can discover semantically meaningful patterns efficiently and effectively . [[EENNDD]] image data mining; pattern summarization; self-supervised clustering; meaningful itemset mining"}, "Dari kumpulan item yang kerap hingga corak visual yang bermakna secara semantik Teknik perlombongan data yang berjaya dalam transaksi dan data teks mungkin tidak hanya diterapkan pada data gambar yang mengandungi ciri dimensi tinggi dan mempunyai struktur spasial. Bukan tugas yang remeh untuk menemukan corak visual yang bermakna dalam pangkalan data gambar, kerana variasi kandungan dan ketergantungan spasial dalam data visual sangat mencabar kebanyakan kaedah yang ada. Makalah ini menyajikan pendekatan baru untuk mengatasi kesulitan-kesulitan ini untuk mencari corak visual yang bermakna. Secara khusus, kebaruan karya ini terletak pada sumbangan baru berikut: 1 penyelesaian berprinsip untuk penemuan set barang yang bermakna berdasarkan perlombongan itemet yang kerap; 2 skema pengelompokan kendiri dari ciri visual dimensi tinggi dengan memberi makan corak yang ditemui untuk menyesuaikan ukuran kesamaan melalui pembelajaran metrik; dan 3 kaedah ringkasan corak yang menangani bunyi pengukuran yang dibawa oleh data gambar. Hasil percubaan dalam gambar sebenar menunjukkan bahawa kaedah kami dapat menemui corak makna semantik dengan cekap dan berkesan. [[EENNDD]] perlombongan data imej; ringkasan corak; pengelompokan yang diselia sendiri; perlombongan itemet yang bermakna"], [{"string": "The predictive power of online chatter An increasing fraction of the global discourse is migrating online in the form of blogs , bulletin boards , web pages , wikis , editorials , and a dizzying array of new collaborative technologies . The migration has now proceeded to the point that topics reflecting certain individual products are sufficiently popular to allow targeted online tracking of the ebb and flow of chatter around these topics . Based on an analysis of around half a million sales rank values for 2,340 books over a period of four months , and correlating postings in blogs , media , and web pages , we are able to draw several interesting conclusions . First , carefully hand-crafted queries produce matching postings whose volume predicts sales ranks . Second , these queries can be automatically generated in many cases . And third , even though sales rank motion might be difficult to predict in general , algorithmic predictors can use online postings to successfully predict spikes in sales rank .", "keywords": ["sales rank", "information search and retrieval", "prediction", "blogs", "time-series analysis"], "combined": "The predictive power of online chatter An increasing fraction of the global discourse is migrating online in the form of blogs , bulletin boards , web pages , wikis , editorials , and a dizzying array of new collaborative technologies . The migration has now proceeded to the point that topics reflecting certain individual products are sufficiently popular to allow targeted online tracking of the ebb and flow of chatter around these topics . Based on an analysis of around half a million sales rank values for 2,340 books over a period of four months , and correlating postings in blogs , media , and web pages , we are able to draw several interesting conclusions . First , carefully hand-crafted queries produce matching postings whose volume predicts sales ranks . Second , these queries can be automatically generated in many cases . And third , even though sales rank motion might be difficult to predict in general , algorithmic predictors can use online postings to successfully predict spikes in sales rank . [[EENNDD]] sales rank; information search and retrieval; prediction; blogs; time-series analysis"}, "Kekuatan ramalan perbincangan dalam talian Sebilangan besar wacana global semakin berpindah dalam talian dalam bentuk blog, papan buletin, laman web, wiki, editorial, dan pelbagai teknologi kolaboratif baru yang memusingkan. Penghijrahan kini telah berjalan ke tahap bahawa topik yang menggambarkan produk individu tertentu cukup popular untuk membolehkan penjejakan surut dalam talian yang disasarkan dan aliran perbincangan mengenai topik ini. Berdasarkan analisis sekitar setengah juta nilai kedudukan penjualan untuk 2.340 buku dalam jangka masa empat bulan, dan mengaitkan pengeposan di blog, media, dan laman web, kami dapat membuat beberapa kesimpulan yang menarik. Pertama, pertanyaan buatan tangan dengan teliti menghasilkan catatan yang sepadan yang volumnya meramalkan kedudukan penjualan. Kedua, pertanyaan ini dapat dihasilkan secara automatik dalam banyak kes. Dan ketiga, walaupun pergerakan peringkat penjualan mungkin sukar untuk diramalkan secara umum, peramal algoritma dapat menggunakan catatan dalam talian untuk berjaya meramalkan lonjakan peringkat penjualan. [[EENNDD]] kedudukan jualan; carian dan pengambilan maklumat; ramalan; blog; analisis siri masa"], [{"string": "A maximum entropy web recommendation system : combining collaborative and content features Web users display their preferences implicitly by navigating through a sequence of pages or by providing numeric ratings to some items . Web usage mining techniques are used to extract useful knowledge about user interests from such data . The discovered user models are then used for a variety of applications such as personalized recommendations . Web site content or semantic features of objects provide another source of knowledge for deciphering users ' needs or interests . We propose a novel Web recommendation system in which collaborative features such as navigation or rating data as well as the content features accessed by the users are seamlessly integrated under the maximum entropy principle . Both the discovered user patterns and the semantic relationships among Web objects are represented as sets of constraints that are integrated to fit the model . In the case of content features , we use a new approach based on Latent Dirichlet Allocation LDA to discover the hidden semantic relationships among items and derive constraints used in the model . Experiments on real Web site usage data sets show that this approach can achieve better recommendation accuracy , when compared to systems using only usage information . The integration of semantic information also allows for better interpretation of the generated recommendations .", "keywords": ["recommendation", "web usage mining", "maximum entropy", "user profiling"], "combined": "A maximum entropy web recommendation system : combining collaborative and content features Web users display their preferences implicitly by navigating through a sequence of pages or by providing numeric ratings to some items . Web usage mining techniques are used to extract useful knowledge about user interests from such data . The discovered user models are then used for a variety of applications such as personalized recommendations . Web site content or semantic features of objects provide another source of knowledge for deciphering users ' needs or interests . We propose a novel Web recommendation system in which collaborative features such as navigation or rating data as well as the content features accessed by the users are seamlessly integrated under the maximum entropy principle . Both the discovered user patterns and the semantic relationships among Web objects are represented as sets of constraints that are integrated to fit the model . In the case of content features , we use a new approach based on Latent Dirichlet Allocation LDA to discover the hidden semantic relationships among items and derive constraints used in the model . Experiments on real Web site usage data sets show that this approach can achieve better recommendation accuracy , when compared to systems using only usage information . The integration of semantic information also allows for better interpretation of the generated recommendations . [[EENNDD]] recommendation; web usage mining; maximum entropy; user profiling"}, "Sistem cadangan web entropi maksimum: menggabungkan ciri-ciri kolaboratif dan kandungan Pengguna Web menunjukkan pilihan mereka secara tersirat dengan menavigasi urutan halaman atau dengan memberikan penilaian berangka ke beberapa item. Teknik perlombongan penggunaan web digunakan untuk mengekstrak pengetahuan berguna mengenai minat pengguna dari data tersebut. Model pengguna yang ditemui kemudian digunakan untuk pelbagai aplikasi seperti cadangan yang diperibadikan. Kandungan laman web atau ciri semantik objek menyediakan sumber pengetahuan lain untuk menguraikan keperluan atau minat pengguna. Kami mencadangkan sistem cadangan Web baru di mana ciri kolaborasi seperti navigasi atau data penilaian serta ciri kandungan yang diakses oleh pengguna disatukan dengan lancar di bawah prinsip entropi maksimum. Kedua-dua corak pengguna yang ditemui dan hubungan semantik antara objek Web diwakili sebagai sekatan kekangan yang disatukan agar sesuai dengan model. Dalam kes ciri kandungan, kami menggunakan pendekatan baru berdasarkan Latent Dirichlet Allocation LDA untuk mengetahui hubungan semantik tersembunyi di antara item dan memperoleh kekangan yang digunakan dalam model. Eksperimen pada set data penggunaan laman web nyata menunjukkan bahawa pendekatan ini dapat mencapai ketepatan cadangan yang lebih baik, jika dibandingkan dengan sistem yang hanya menggunakan informasi penggunaan. Penggabungan maklumat semantik juga memungkinkan penafsiran yang lebih baik terhadap cadangan yang dihasilkan. [[EENNDD]] cadangan; perlombongan penggunaan web; entropi maksimum; profil pengguna"], [{"string": "Approximating a collection of frequent sets One of the most well-studied problems in data mining is computing the collection of frequent item sets in large transactional databases . One obstacle for the applicability of frequent-set mining is that the size of the output collection can be far too large to be carefully examined and understood by the users . Even restricting the output to the border of the frequent item-set collection does not help much in alleviating the problem . In this paper we address the issue of overwhelmingly large output size by introducing and studying the following problem : What are the k sets that best approximate a collection of frequent item sets ? Our measure of approximating a collection of sets by k sets is defined to be the size of the collection covered by the the k sets , i.e. , the part of the collection that is included in one of the k sets . We also specify a bound on the number of extra sets that are allowed to be covered . We examine different problem variants for which we demonstrate the hardness of the corresponding problems and we provide simple polynomial-time approximation algorithms . We give empirical evidence showing that the approximation methods work well in practice .", "keywords": ["nonnumerical algorithms and problems", "foundations of data mining", "mining frequent itemsets"], "combined": "Approximating a collection of frequent sets One of the most well-studied problems in data mining is computing the collection of frequent item sets in large transactional databases . One obstacle for the applicability of frequent-set mining is that the size of the output collection can be far too large to be carefully examined and understood by the users . Even restricting the output to the border of the frequent item-set collection does not help much in alleviating the problem . In this paper we address the issue of overwhelmingly large output size by introducing and studying the following problem : What are the k sets that best approximate a collection of frequent item sets ? Our measure of approximating a collection of sets by k sets is defined to be the size of the collection covered by the the k sets , i.e. , the part of the collection that is included in one of the k sets . We also specify a bound on the number of extra sets that are allowed to be covered . We examine different problem variants for which we demonstrate the hardness of the corresponding problems and we provide simple polynomial-time approximation algorithms . We give empirical evidence showing that the approximation methods work well in practice . [[EENNDD]] nonnumerical algorithms and problems; foundations of data mining; mining frequent itemsets"}, "Mengira koleksi kumpulan yang kerap Salah satu masalah yang paling banyak dikaji dalam perlombongan data adalah mengira pengumpulan set item yang kerap dalam pangkalan data transaksi yang besar. Satu halangan untuk penerapan penambangan yang kerap dilakukan adalah ukuran koleksi output yang terlalu besar untuk diperiksa dan difahami oleh pengguna dengan teliti. Bahkan menyekat output ke sempadan koleksi kumpulan item yang kerap tidak banyak membantu mengatasi masalah tersebut. Dalam makalah ini kita membahas masalah ukuran keluaran yang sangat besar dengan memperkenalkan dan mengkaji masalah berikut: Apakah set k yang paling sesuai dengan koleksi kumpulan item yang kerap? Ukuran kami untuk menghampiri koleksi set dengan set k didefinisikan sebagai ukuran koleksi yang diliputi oleh set k, iaitu bahagian koleksi yang termasuk dalam salah satu set k. Kami juga menentukan had jumlah set tambahan yang dibenarkan untuk dilindungi. Kami memeriksa varian masalah yang berbeza yang kami menunjukkan kekerasan masalah yang sesuai dan kami menyediakan algoritma penghampiran masa polinomial sederhana. Kami memberikan bukti empirikal yang menunjukkan bahawa kaedah pendekatan berfungsi dengan baik dalam praktik. [[EENNDD]] algoritma dan masalah bukan angka; asas perlombongan data; melombong barang yang kerap"], [{"string": "Cryptographically private support vector machines We propose private protocols implementing the Kernel Adatron and Kernel Perceptron learning algorithms , give private classification protocols and private polynomial kernel computation protocols . The new protocols return their outputs - either the kernel value , the classifier or the classifications - in encrypted form so that they can be decrypted only by a common agreement by the protocol participants . We show how to use the encrypted classifications to privately estimate many properties of the data and the classifier . The new SVM classifiers are the first to be proven private according to the standard cryptographic definitions .", "keywords": ["kernel methods", "privacy preserving data mining"], "combined": "Cryptographically private support vector machines We propose private protocols implementing the Kernel Adatron and Kernel Perceptron learning algorithms , give private classification protocols and private polynomial kernel computation protocols . The new protocols return their outputs - either the kernel value , the classifier or the classifications - in encrypted form so that they can be decrypted only by a common agreement by the protocol participants . We show how to use the encrypted classifications to privately estimate many properties of the data and the classifier . The new SVM classifiers are the first to be proven private according to the standard cryptographic definitions . [[EENNDD]] kernel methods; privacy preserving data mining"}, "Mesin vektor sokongan kriptografi peribadi Kami mencadangkan protokol swasta yang menerapkan algoritma pembelajaran Kernel Adatron dan Kernel Perceptron, memberikan protokol klasifikasi peribadi dan protokol pengiraan kernel polinomial swasta. Protokol baru mengembalikan outputnya - sama ada nilai kernel, pengklasifikasi atau klasifikasi - dalam bentuk yang dienkripsi sehingga mereka dapat didekripsi hanya dengan persetujuan bersama oleh para peserta protokol. Kami menunjukkan cara menggunakan klasifikasi yang dienkripsi untuk secara peribadi menganggarkan banyak sifat data dan pengklasifikasi. Pengelasan SVM baru adalah yang pertama terbukti peribadi mengikut definisi kriptografi standard. [[EENNDD]] kaedah kernel; privasi memelihara perlombongan data"], [{"string": "Similarity measure based on partial information of time series Similarity measure of time series is an important subroutine in many KDD applications . Previous similarity models mainly focus on the prominent series behaviors by considering the whole information of time series . In this paper , we address the problem : which portion of information is more suitable for similarity measure for the data collected from a certain field . We propose a model for the retrieval and representation of the partial information in time series data , and a methodology for evaluating the similarity measurements based on partial information . The methodology is to retrieve various portions of information from the raw data and represent it in a concise form , then cluster the time series using the partial information and evaluate the similarity measurements through comparing the results with a standard classification . Experiments on data set from stock market give some interesting observations and justify the usefulness of our approach .", "keywords": ["time series", "partial information", "similarity measure"], "combined": "Similarity measure based on partial information of time series Similarity measure of time series is an important subroutine in many KDD applications . Previous similarity models mainly focus on the prominent series behaviors by considering the whole information of time series . In this paper , we address the problem : which portion of information is more suitable for similarity measure for the data collected from a certain field . We propose a model for the retrieval and representation of the partial information in time series data , and a methodology for evaluating the similarity measurements based on partial information . The methodology is to retrieve various portions of information from the raw data and represent it in a concise form , then cluster the time series using the partial information and evaluate the similarity measurements through comparing the results with a standard classification . Experiments on data set from stock market give some interesting observations and justify the usefulness of our approach . [[EENNDD]] time series; partial information; similarity measure"}, "Ukuran kesamaan berdasarkan sebahagian maklumat siri masa Ukuran kesamaan siri masa adalah subrutin penting dalam banyak aplikasi KDD. Model kesamaan sebelumnya terutamanya memfokus pada tingkah laku siri yang menonjol dengan mempertimbangkan keseluruhan maklumat siri masa. Dalam makalah ini, kami mengatasi masalah: bahagian maklumat mana yang lebih sesuai untuk ukuran kesamaan untuk data yang dikumpulkan dari bidang tertentu. Kami mencadangkan model untuk pengambilan dan perwakilan maklumat separa dalam data siri masa, dan metodologi untuk menilai pengukuran kesamaan berdasarkan maklumat separa. Metodologi adalah untuk mendapatkan pelbagai bahagian maklumat dari data mentah dan mewakilkannya dalam bentuk ringkas, kemudian mengelompokkan siri masa dengan menggunakan maklumat separa dan menilai pengukuran kesamaan dengan membandingkan hasilnya dengan klasifikasi standard. Eksperimen pada set data dari pasaran saham memberikan beberapa pemerhatian yang menarik dan membenarkan kegunaan pendekatan kami. [[EENNDD]] siri masa; maklumat separa; ukuran kesamaan"], [{"string": "Microscopic evolution of social networks We present a detailed study of network evolution by analyzing four large online social networks with full temporal information about node and edge arrivals . For the first time at such a large scale , we study individual node arrival and edge creation processes that collectively lead to macroscopic properties of networks . Using a methodology based on the maximum-likelihood principle , we investigate a wide variety of network formation strategies , and show that edge locality plays a critical role in evolution of networks . Our findings supplement earlier network models based on the inherently non-local preferential attachment . Based on our observations , we develop a complete model of network evolution , where nodes arrive at a prespecified rate and select their lifetimes . Each node then independently initiates edges according to a `` gap '' process , selecting a destination for each edge according to a simple triangle-closing model free of any parameters . We show analytically that the combination of the gap distribution with the node lifetime leads to a power law out-degree distribution that accurately reflects the true network in all four cases . Finally , we give model parameter settings that allow automatic evolution and generation of realistic synthetic networks of arbitrary scale .", "keywords": ["maximum likelihood", "triadic closure", "network evolution", "social networks", "graph generators", "transitivity"], "combined": "Microscopic evolution of social networks We present a detailed study of network evolution by analyzing four large online social networks with full temporal information about node and edge arrivals . For the first time at such a large scale , we study individual node arrival and edge creation processes that collectively lead to macroscopic properties of networks . Using a methodology based on the maximum-likelihood principle , we investigate a wide variety of network formation strategies , and show that edge locality plays a critical role in evolution of networks . Our findings supplement earlier network models based on the inherently non-local preferential attachment . Based on our observations , we develop a complete model of network evolution , where nodes arrive at a prespecified rate and select their lifetimes . Each node then independently initiates edges according to a `` gap '' process , selecting a destination for each edge according to a simple triangle-closing model free of any parameters . We show analytically that the combination of the gap distribution with the node lifetime leads to a power law out-degree distribution that accurately reflects the true network in all four cases . Finally , we give model parameter settings that allow automatic evolution and generation of realistic synthetic networks of arbitrary scale . [[EENNDD]] maximum likelihood; triadic closure; network evolution; social networks; graph generators; transitivity"}, "Evolusi mikroskopik rangkaian sosial Kami menyajikan kajian terperinci evolusi rangkaian dengan menganalisis empat rangkaian sosial dalam talian yang besar dengan maklumat temporal penuh mengenai kedatangan nod dan pinggir. Buat pertama kalinya pada skala besar, kami mengkaji proses ketibaan nod individu dan proses pembuatan tepi yang secara kolektif membawa kepada sifat makroskopik rangkaian. Dengan menggunakan metodologi berdasarkan prinsip kemungkinan maksimum, kami menyelidiki berbagai strategi pembentukan rangkaian, dan menunjukkan bahawa lokaliti tepi memainkan peranan penting dalam evolusi jaringan. Penemuan kami melengkapkan model rangkaian sebelumnya berdasarkan lampiran keutamaan bukan tempatan. Berdasarkan pemerhatian kami, kami mengembangkan model evolusi rangkaian yang lengkap, di mana node tiba pada kadar yang ditentukan dan memilih jangka hayatnya. Setiap simpul kemudian secara bebas memulakan tepi mengikut proses \"jurang\", memilih tujuan untuk setiap tepi mengikut model penutup segitiga sederhana tanpa sebarang parameter. Kami menunjukkan secara analitikal bahawa gabungan pengagihan jurang dengan jangka hayat node membawa kepada pembahagian tahap undang-undang kuasa yang secara tepat menggambarkan rangkaian sebenar dalam keempat-empat kes tersebut. Akhirnya, kami memberikan tetapan parameter model yang membolehkan evolusi automatik dan penjanaan rangkaian sintetik realistik skala sewenang-wenangnya. [[EENNDD]] kemungkinan maksimum; penutupan triad; evolusi rangkaian; rangkaian sosial; penjana grafik; transitiviti"], [{"string": "Nestedness and segmented nestedness Consider each row of a 0-1 dataset as the subset of the columns for which the row has an 1 . Then a dataset is nested , if for all pairs of rows one row is either a superset or subset of the other . The concept of nestedness has its origins in ecology , where approximate versions of it has been used to model the species distribution in different locations . We argue that nestedness and its extensions are interesting properties of datasets , and that they can be applied also to domains other than ecology . We first define natural measures of nestedness and study their properties . We then define the concept of k-nestedness : a dataset is almost k-nested if the set of columns can be partitioned to k parts so that each part is almost nested . We consider the algorithmic problems of computing how far a dataset is from being k-nested , and for finding a good partition of the columns into k parts . The algorithms are based on spectral partitioning , and scale to moderately large datasets . We apply the methods to real data from ecology and from other applications , and demonstrate the usefulness of the concept .", "keywords": ["nestedness", "0-1 matrices", "presence/absence data", "nonnumerical algorithms and problems"], "combined": "Nestedness and segmented nestedness Consider each row of a 0-1 dataset as the subset of the columns for which the row has an 1 . Then a dataset is nested , if for all pairs of rows one row is either a superset or subset of the other . The concept of nestedness has its origins in ecology , where approximate versions of it has been used to model the species distribution in different locations . We argue that nestedness and its extensions are interesting properties of datasets , and that they can be applied also to domains other than ecology . We first define natural measures of nestedness and study their properties . We then define the concept of k-nestedness : a dataset is almost k-nested if the set of columns can be partitioned to k parts so that each part is almost nested . We consider the algorithmic problems of computing how far a dataset is from being k-nested , and for finding a good partition of the columns into k parts . The algorithms are based on spectral partitioning , and scale to moderately large datasets . We apply the methods to real data from ecology and from other applications , and demonstrate the usefulness of the concept . [[EENNDD]] nestedness; 0-1 matrices; presence/absence data; nonnumerical algorithms and problems"}, "Nestedness dan segmented nestedness Pertimbangkan setiap baris set data 0-1 sebagai subset lajur yang barisnya mempunyai 1. Kemudian dataset disarang, jika untuk semua pasangan baris satu baris adalah superset atau subset yang lain. Konsep bersarang berasal dari ekologi, di mana versi perkiraannya telah digunakan untuk memodelkan penyebaran spesies di lokasi yang berbeza. Kami berpendapat bahawa bersarang dan peluasannya adalah sifat kumpulan data yang menarik, dan ia juga dapat digunakan untuk domain selain ekologi. Kami pertama kali menentukan ukuran semula jadi yang bersarang dan mengkaji sifatnya. Kami kemudian menentukan konsep k-nestedness: set data hampir bersarang k jika kumpulan lajur dapat dipartisi ke bahagian k sehingga setiap bahagian hampir bersarang. Kami mempertimbangkan masalah algoritma pengkomputeran sejauh mana set data dari k-nested, dan untuk mencari partisi lajur yang baik menjadi bahagian k. Algoritma berdasarkan partisi spektrum, dan skala hingga dataset yang sederhana besar. Kami menerapkan kaedah untuk data sebenar dari ekologi dan dari aplikasi lain, dan menunjukkan kegunaan konsep tersebut. [[EENNDD]] bersarang; Matriks 0-1; data kehadiran / ketiadaan; algoritma dan masalah bukan berangka"], [{"string": "TreeDT : gene mapping by tree disequilibrium test We introduce and evaluate TreeDT , a novel gene mapping method which is based on discovering and assessing tree-like patterns in genetic marker data . Gene mapping aims at discovering a statistical connection from a particular disease or trait to a narrow region in the genome . In a typical case-control setting , data consists of genetic markers typed for a set of disease-associated chromosomes and a set of control chromosomes . A computer scientist would view this data as a set of strings . TreeDT extracts , essentially in the form of substrings and prefix trees , information about the historical recombinations in the population . This information is used to locate fragments potentially inherited from a common diseased founder , and to map the disease gene into the most likely such fragment . The method measures for each chromosomal location the disequilibrium of the prefix tree of marker strings starting from the location , to assess the distribution of disease-associated chromosomes . We evaluate experimentally the performance of TreeDT on realistic , simulated data sets , and comparisons to state of the art methods TDT , HPM show that TreeDT is very competitive .", "keywords": ["permutation tests", "gene mapping", "prefix trees"], "combined": "TreeDT : gene mapping by tree disequilibrium test We introduce and evaluate TreeDT , a novel gene mapping method which is based on discovering and assessing tree-like patterns in genetic marker data . Gene mapping aims at discovering a statistical connection from a particular disease or trait to a narrow region in the genome . In a typical case-control setting , data consists of genetic markers typed for a set of disease-associated chromosomes and a set of control chromosomes . A computer scientist would view this data as a set of strings . TreeDT extracts , essentially in the form of substrings and prefix trees , information about the historical recombinations in the population . This information is used to locate fragments potentially inherited from a common diseased founder , and to map the disease gene into the most likely such fragment . The method measures for each chromosomal location the disequilibrium of the prefix tree of marker strings starting from the location , to assess the distribution of disease-associated chromosomes . We evaluate experimentally the performance of TreeDT on realistic , simulated data sets , and comparisons to state of the art methods TDT , HPM show that TreeDT is very competitive . [[EENNDD]] permutation tests; gene mapping; prefix trees"}, "TreeDT: pemetaan gen dengan ujian keseimbangan pohon Kami memperkenalkan dan menilai TreeDT, kaedah pemetaan gen baru yang berdasarkan pada penemuan dan penilaian corak seperti pohon dalam data penanda genetik. Pemetaan gen bertujuan untuk mencari hubungan statistik dari penyakit atau sifat tertentu ke kawasan sempit dalam genom. Dalam tetapan kawalan kes khas, data terdiri daripada penanda genetik yang ditaip untuk satu set kromosom yang berkaitan dengan penyakit dan satu set kromosom kawalan. Seorang saintis komputer akan melihat data ini sebagai rangkaian rentetan. TreeDT ekstrak, pada dasarnya dalam bentuk substring dan awalan pokok, maklumat mengenai penggabungan sejarah penduduk. Maklumat ini digunakan untuk mencari serpihan yang berpotensi diwarisi dari pengasas penyakit biasa, dan untuk memetakan gen penyakit menjadi serpihan yang paling mungkin. Kaedah mengukur untuk setiap lokasi kromosom ketidakseimbangan pohon awalan tali penanda bermula dari lokasi, untuk menilai pengedaran kromosom yang berkaitan dengan penyakit. Kami menilai secara eksperimen prestasi TreeDT pada set data realistik, simulasi, dan perbandingan dengan kaedah canggih TDT, HPM menunjukkan bahawa TreeDT sangat kompetitif. [[EENNDD]] ujian permutasi; pemetaan gen; pokok awalan"]]