[[{"string": "Efficient search in large textual collections with redundancy Current web search engines focus on searching only themost recentsnapshot of the web. In some cases, however, it would be desirableto search over collections that include many different crawls andversions of each page. One important example of such a collectionis the Internet Archive, though there are many others. Sincethe data size of such an archive is multiple times that of a singlesnapshot, this presents us with significant performance challenges.Current engines use various techniques for index compression andoptimized query execution, but these techniques do not exploit thesignificant similarities between different versions of a page, or betweendifferent pages.In this paper, we propose a general framework for indexing andquery processing of archival collections and, more generally, anycollections with a sufficient amount of redundancy. Our approachresults in significant reductions in index size and query processingcosts on such collections, and it is orthogonal to and can be combinedwith the existing techniques. It also supports highly efficientupdates, both locally and over a network. Within this framework,we describe and evaluate different implementations that trade offindex size versus CPU cost and other factors, and discuss applicationsranging from archival web search to local search of web sites,email archives, or file systems. We present experimental resultsbased on search engine query log and a large collection consistingof multiple crawls.", "keywords": ["content analysis and indexing", "search engines", "query execution", "inverted index", "redundancy elimination", "index compression"], "combined": "Efficient search in large textual collections with redundancy Current web search engines focus on searching only themost recentsnapshot of the web. In some cases, however, it would be desirableto search over collections that include many different crawls andversions of each page. One important example of such a collectionis the Internet Archive, though there are many others. Sincethe data size of such an archive is multiple times that of a singlesnapshot, this presents us with significant performance challenges.Current engines use various techniques for index compression andoptimized query execution, but these techniques do not exploit thesignificant similarities between different versions of a page, or betweendifferent pages.In this paper, we propose a general framework for indexing andquery processing of archival collections and, more generally, anycollections with a sufficient amount of redundancy. Our approachresults in significant reductions in index size and query processingcosts on such collections, and it is orthogonal to and can be combinedwith the existing techniques. It also supports highly efficientupdates, both locally and over a network. Within this framework,we describe and evaluate different implementations that trade offindex size versus CPU cost and other factors, and discuss applicationsranging from archival web search to local search of web sites,email archives, or file systems. We present experimental resultsbased on search engine query log and a large collection consistingof multiple crawls. [[EENNDD]] content analysis and indexing; search engines; query execution; inverted index; redundancy elimination; index compression"}, "Carian yang cekap dalam koleksi teks besar dengan kelebihan Mesin pencari laman web semasa hanya menumpukan hanya mencari gambar terbaru di web. Dalam beberapa kes, bagaimanapun, adalah lebih baik mencari lebih banyak koleksi yang merangkumi banyak perayapan dan versi yang berbeza dari setiap halaman. Salah satu contoh penting koleksi seperti Arkib Internet, walaupun terdapat banyak yang lain. Sejujurnya, ukuran data arkib ini berkali-kali daripada gambar tunggal, ini memberi kita cabaran prestasi yang signifikan. Mesin semasa menggunakan pelbagai teknik untuk pemampatan indeks dan pelaksanaan pertanyaan yang dioptimumkan, tetapi teknik ini tidak memanfaatkan persamaan yang signifikan antara versi halaman yang berbeza, atau antara halaman yang berbeza. Dalam makalah ini, kami mencadangkan kerangka umum untuk pengindeksan dan pemprosesan pertanyaan koleksi arkib dan, lebih umum, koleksi apa pun dengan jumlah kelebihan yang mencukupi. Pendekatan kami menghasilkan pengurangan yang signifikan dalam ukuran indeks dan kos pemprosesan pertanyaan pada koleksi tersebut, dan ini adalah ortogonal dan dapat digabungkan dengan teknik yang ada. Ia juga menyokong kemas kini yang sangat berkesan, baik secara tempatan dan juga melalui rangkaian. Dalam kerangka ini, kami menerangkan dan menilai pelaksanaan yang berbeza yang memperdagangkan ukuran offindex berbanding kos CPU dan faktor-faktor lain, dan membincangkan aplikasi mulai dari carian web arkib hingga pencarian laman web tempatan, arkib e-mel, atau sistem fail. Kami membentangkan hasil eksperimen berdasarkan log pertanyaan enjin carian dan koleksi besar yang terdiri daripada banyak perayapan. [[EENNDD]] analisis kandungan dan pengindeksan; enjin carian; pelaksanaan pertanyaan; indeks terbalik; penghapusan redundansi; pemampatan indeks"], [{"string": "On integrating catalogs An abstract is not available.", "keywords": ["catalog integration", "classification", "web portals", "categorization", "web marketplaces"], "combined": "On integrating catalogs An abstract is not available. [[EENNDD]] catalog integration; classification; web portals; categorization; web marketplaces"}, "Mengintegrasikan katalog Abstrak tidak tersedia. [[EENNDD]] penyatuan katalog; pengelasan; portal web; pengkategorian; pasar web"], [{"string": "Probabilistic question answering on the web No contact information provided yet.", "keywords": ["information retrieval", "search engines", "natural language processing", "miscellaneous", "question answering", "answer selection", "answer extraction", "query modulation"], "combined": "Probabilistic question answering on the web No contact information provided yet. [[EENNDD]] information retrieval; search engines; natural language processing; miscellaneous; question answering; answer selection; answer extraction; query modulation"}, "Kemungkinan menjawab soalan di web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengambilan maklumat; enjin carian; pemprosesan bahasa semula jadi; pelbagai; menjawab soalan; pemilihan jawapan; pengambilan jawapan; modulasi pertanyaan"], [{"string": "How much can behavioral targeting help online advertising? Behavioral Targeting (BT) is a technique used by online advertisers to increase the effectiveness of their campaigns, and is playing an increasingly important role in the online advertising market. However, it is underexplored in academia when looking at how much BT can truly help online advertising in commercial search engines. To answer this question, in this paper we provide an empirical study on the click-through log of advertisements collected from a commercial search engine. From the comprehensively experiment results on the sponsored search log of the commercial search engine over a period of seven days, we can draw three important conclusions: (1) Users who clicked the same ad will truly have similar behaviors on the Web; (2) Click-Through Rate (CTR) of an ad can be averagely improved as high as 670% by properly segmenting users for behavioral targeted advertising in a sponsored search; (3) Using the short term user behaviors to represent users is more effective than using the long term user behaviors for BT. The statistical t-test verifies that all conclusions drawn in the paper are statistically significant. To the best of our knowledge, this work is the first empirical study for BT on the click-through log of real world ads.", "keywords": ["model validation and analysis", "user segmentation", "behavioral targeting", "click-through rate .", "online advertising"], "combined": "How much can behavioral targeting help online advertising? Behavioral Targeting (BT) is a technique used by online advertisers to increase the effectiveness of their campaigns, and is playing an increasingly important role in the online advertising market. However, it is underexplored in academia when looking at how much BT can truly help online advertising in commercial search engines. To answer this question, in this paper we provide an empirical study on the click-through log of advertisements collected from a commercial search engine. From the comprehensively experiment results on the sponsored search log of the commercial search engine over a period of seven days, we can draw three important conclusions: (1) Users who clicked the same ad will truly have similar behaviors on the Web; (2) Click-Through Rate (CTR) of an ad can be averagely improved as high as 670% by properly segmenting users for behavioral targeted advertising in a sponsored search; (3) Using the short term user behaviors to represent users is more effective than using the long term user behaviors for BT. The statistical t-test verifies that all conclusions drawn in the paper are statistically significant. To the best of our knowledge, this work is the first empirical study for BT on the click-through log of real world ads. [[EENNDD]] model validation and analysis; user segmentation; behavioral targeting; click-through rate .; online advertising"}, "Sejauh mana penyasaran tingkah laku dapat membantu pengiklanan dalam talian? Behavioral Targeting (BT) adalah teknik yang digunakan oleh pengiklan dalam talian untuk meningkatkan keberkesanan kempen mereka, dan memainkan peranan yang semakin penting dalam pasaran iklan dalam talian. Walau bagaimanapun, ia belum diterokai dalam akademik ketika melihat seberapa banyak BT dapat benar-benar membantu pengiklanan dalam talian di mesin carian komersial. Untuk menjawab persoalan ini, dalam makalah ini kami memberikan kajian empirikal mengenai log iklan klik-tayang yang dikumpulkan dari mesin carian komersial. Dari hasil eksperimen yang komprehensif pada log carian tajaan mesin carian komersial selama tujuh hari, kami dapat membuat tiga kesimpulan penting: (1) Pengguna yang mengklik iklan yang sama akan benar-benar mempunyai tingkah laku yang serupa di Web; (2) Kadar Klik-Tayang (RKT) iklan dapat rata-rata ditingkatkan setinggi 670% dengan menyegmentasikan pengguna dengan tepat untuk periklanan yang disasarkan dalam carian yang ditaja; (3) Menggunakan tingkah laku pengguna jangka pendek untuk mewakili pengguna lebih berkesan daripada menggunakan tingkah laku pengguna jangka panjang untuk BT. Ujian-t statistik mengesahkan bahawa semua kesimpulan yang diambil dalam makalah ini signifikan secara statistik. Sepengetahuan kami, karya ini adalah kajian empirikal pertama untuk BT pada log klik-tayang iklan dunia nyata. [[EENNDD]] pengesahan dan analisis model; segmentasi pengguna; penyasaran tingkah laku; kadar klik-tayang .; iklan dalam talian"], [{"string": "Optimizing budget allocation among channels and influencers Brands and agencies use marketing as a tool to influence customers. One of the major decisions in a marketing plan deals with the allocation of a given budget among media channels in order to maximize the impact on a set of potential customers. A similar situation occurs in a social network, where a marketing budget needs to be distributed among a set of potential influencers in a way that provides high-impact.", "keywords": ["general", "approximation algorithms", "influence models", "budget allocation"], "combined": "Optimizing budget allocation among channels and influencers Brands and agencies use marketing as a tool to influence customers. One of the major decisions in a marketing plan deals with the allocation of a given budget among media channels in order to maximize the impact on a set of potential customers. A similar situation occurs in a social network, where a marketing budget needs to be distributed among a set of potential influencers in a way that provides high-impact. [[EENNDD]] general; approximation algorithms; influence models; budget allocation"}, "Mengoptimumkan peruntukan belanjawan di antara saluran dan pemberi pengaruh Jenama dan agensi menggunakan pemasaran sebagai alat untuk mempengaruhi pelanggan. Salah satu keputusan utama dalam rancangan pemasaran berkaitan dengan peruntukan anggaran tertentu di antara saluran media untuk memaksimumkan dampak pada sekumpulan calon pelanggan. Situasi serupa berlaku dalam rangkaian sosial, di mana anggaran pemasaran perlu dibahagikan di antara sekumpulan berpengaruh berpotensi dengan cara yang memberikan impak tinggi. [[EENNDD]] umum; algoritma penghampiran; mempengaruhi model; peruntukan belanjawan"], [{"string": "Automatically filling form-based web interfaces with free text inputs On the web of today the most prevalent solution for users to interact with data-intensive applications is the use of form-based interfaces composed by several data input fields, such as text boxes, radio buttons, pull-down lists, check boxes, etc. Although these interfaces are popular and effective, in many cases, free text interfaces are preferred over form-based ones. In this paper we discuss the proposal and the implementation of a novel IR-based method for using data rich free text to interact with form-based interfaces. Our solution takes a free text as input, extracts implicitly data values from it and fills appropriate fields using them. For this task, we rely on values of previous submissions for each field, which are freely obtained from the usage of form-based interfaces", "keywords": ["general", "web applications", "form filling", "data extraction"], "combined": "Automatically filling form-based web interfaces with free text inputs On the web of today the most prevalent solution for users to interact with data-intensive applications is the use of form-based interfaces composed by several data input fields, such as text boxes, radio buttons, pull-down lists, check boxes, etc. Although these interfaces are popular and effective, in many cases, free text interfaces are preferred over form-based ones. In this paper we discuss the proposal and the implementation of a novel IR-based method for using data rich free text to interact with form-based interfaces. Our solution takes a free text as input, extracts implicitly data values from it and fills appropriate fields using them. For this task, we rely on values of previous submissions for each field, which are freely obtained from the usage of form-based interfaces [[EENNDD]] general; web applications; form filling; data extraction"}, "Mengisi antara muka web berasaskan bentuk secara automatik dengan input teks percuma Di laman web hari ini, penyelesaian paling lazim bagi pengguna untuk berinteraksi dengan aplikasi intensif data adalah penggunaan antara muka berasaskan bentuk yang disusun oleh beberapa bidang input data, seperti kotak teks, radio butang, senarai tarik-turun, kotak centang, dll. Walaupun antara muka ini popular dan berkesan, dalam banyak kes, antara muka teks percuma lebih disukai daripada yang berasaskan bentuk. Dalam makalah ini kita membincangkan proposal dan pelaksanaan metode berbasis IR baru untuk menggunakan teks bebas kaya data untuk berinteraksi dengan antara muka berasaskan bentuk. Penyelesaian kami mengambil teks percuma sebagai input, mengekstrak nilai data secara implisit daripadanya dan mengisi bidang yang sesuai menggunakannya. Untuk tugas ini, kami bergantung pada nilai penyerahan sebelumnya untuk setiap bidang, yang diperoleh secara bebas dari penggunaan antara muka berasaskan bentuk [[EENNDD]] umum; aplikasi web; pengisian borang; pengekstrakan data"], [{"string": "Selective early request termination for busy internet services No contact information provided yet.", "keywords": ["on-line information services", "internet services", "load shedding", "request termination"], "combined": "Selective early request termination for busy internet services No contact information provided yet. [[EENNDD]] on-line information services; internet services; load shedding; request termination"}, "Penamatan permintaan awal yang terpilih untuk perkhidmatan internet yang sibuk Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perkhidmatan maklumat dalam talian; perkhidmatan internet; penumpahan beban; meminta penamatan"], [{"string": "Towards intent-driven bidterm suggestion In online advertising, pervasive in commercial search engines, advertisers typically bid on few terms, and the scarcity of data makes ad matching difficult. Suggesting additional bidterms can significantly improve ad clickability and conversion rates. In this paper, we present a large-scale bidterm suggestion system that models an advertiser's intent and finds new bidterms consistent with that intent. Preliminary experiments show that our system significantly increases the coverage of a state of the art production system used at Yahoo while maintaining comparable precision.", "keywords": ["information search and retrieval", "sponsored search"], "combined": "Towards intent-driven bidterm suggestion In online advertising, pervasive in commercial search engines, advertisers typically bid on few terms, and the scarcity of data makes ad matching difficult. Suggesting additional bidterms can significantly improve ad clickability and conversion rates. In this paper, we present a large-scale bidterm suggestion system that models an advertiser's intent and finds new bidterms consistent with that intent. Preliminary experiments show that our system significantly increases the coverage of a state of the art production system used at Yahoo while maintaining comparable precision. [[EENNDD]] information search and retrieval; sponsored search"}, "Menuju cadangan bidterm yang berorientasikan niat Dalam pengiklanan dalam talian, meluas di mesin carian komersial, pengiklan biasanya menawar beberapa syarat, dan kekurangan data menjadikan pencocokan iklan menjadi sukar. Mencadangkan bidaan tambahan dapat meningkatkan kadar klik dan penukaran iklan dengan ketara. Dalam makalah ini, kami menyajikan sistem cadangan bidterm skala besar yang memodelkan maksud pengiklan dan menemukan bidterma baru yang sesuai dengan maksud tersebut. Eksperimen awal menunjukkan bahawa sistem kami meningkatkan liputan sistem pengeluaran canggih yang digunakan di Yahoo dengan ketara sambil mengekalkan ketepatan yang setanding. [[EENNDD]] carian dan pengambilan maklumat; carian tajaan"], [{"string": "Tag-based social interest discovery The success and popularity of social network systems, such as del.icio.us, Facebook, MySpace, and YouTube, have generated many interesting and challenging problems to the research community. Among others, discovering social interests shared by groups of users is very important because it helps to connect people with common interests and encourages people to contribute and share more contents. The main challenge to solving this problem comes from the difficulty of detecting and representing the interest of the users. The existing approaches are all based on the online connections of users and so unable to identify the common interest of users who have no online connections.", "keywords": ["del.icio.us", "social networks", "performance evaluation", "isid", "tag"], "combined": "Tag-based social interest discovery The success and popularity of social network systems, such as del.icio.us, Facebook, MySpace, and YouTube, have generated many interesting and challenging problems to the research community. Among others, discovering social interests shared by groups of users is very important because it helps to connect people with common interests and encourages people to contribute and share more contents. The main challenge to solving this problem comes from the difficulty of detecting and representing the interest of the users. The existing approaches are all based on the online connections of users and so unable to identify the common interest of users who have no online connections. [[EENNDD]] del.icio.us; social networks; performance evaluation; isid; tag"}, "Penemuan minat sosial berdasarkan tag Kejayaan dan populariti sistem rangkaian sosial, seperti del.icio.us, Facebook, MySpace, dan YouTube, telah menimbulkan banyak masalah menarik dan mencabar kepada komuniti penyelidik. Antara lain, menemui minat sosial yang dikongsi oleh kumpulan pengguna sangat penting kerana dapat membantu menghubungkan orang dengan minat yang sama dan mendorong orang untuk menyumbang dan berkongsi lebih banyak isi. Cabaran utama untuk menyelesaikan masalah ini datang dari kesukaran untuk mengesan dan mewakili minat pengguna. Pendekatan yang ada semuanya berdasarkan pada sambungan pengguna dalam talian dan oleh itu tidak dapat mengenal pasti kepentingan bersama pengguna yang tidak mempunyai hubungan dalam talian. [[EENNDD]] del.icio.us; rangkaian sosial; penilaian prestasi; isid; teg"], [{"string": "Position paper: a comparison of two modelling paradigms in the Semantic Web No contact information provided yet.", "keywords": ["general", "representation", "philosophical foundations", "knowledge representation formalisms and methods", "modelling", "semantic web"], "combined": "Position paper: a comparison of two modelling paradigms in the Semantic Web No contact information provided yet. [[EENNDD]] general; representation; philosophical foundations; knowledge representation formalisms and methods; modelling; semantic web"}, "Kertas kedudukan: perbandingan dua paradigma pemodelan dalam Semantik Web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] umum; perwakilan; asas falsafah; formalisme dan kaedah perwakilan pengetahuan; pemodelan; web semantik"], [{"string": "Learning to detect phishing emails Each month, more attacks are launched with the aim of making web users believe that they are communicating with a trusted entity for the purpose of stealing account information, logon credentials, and identity information in general. This attack method, commonly known as \"phishing,\" is most commonly initiated by sending out emails with links to spoofed websites that harvest information. We present a method for detecting these attacks, which in its most general form is an application of machine learning on a feature set designed to highlight user-targeted deception in electronic communication. This method is applicable, with slight modification, to detection of phishing websites, or the emails used to direct victims to these sites. We evaluate this method on a set of approximately 860 such phishing emails, and 6950 non-phishing emails, and correctly identify over 96% of the phishing emails while only mis-classifying on the order of 0.1% of the legitimate emails. We conclude with thoughts on the future for such techniques to specifically identify deception, specifically with respect to the evolutionary nature of the attacks and information available.", "keywords": ["learning", "spam", "semantic attacks", "phishing", "filtering", "email"], "combined": "Learning to detect phishing emails Each month, more attacks are launched with the aim of making web users believe that they are communicating with a trusted entity for the purpose of stealing account information, logon credentials, and identity information in general. This attack method, commonly known as \"phishing,\" is most commonly initiated by sending out emails with links to spoofed websites that harvest information. We present a method for detecting these attacks, which in its most general form is an application of machine learning on a feature set designed to highlight user-targeted deception in electronic communication. This method is applicable, with slight modification, to detection of phishing websites, or the emails used to direct victims to these sites. We evaluate this method on a set of approximately 860 such phishing emails, and 6950 non-phishing emails, and correctly identify over 96% of the phishing emails while only mis-classifying on the order of 0.1% of the legitimate emails. We conclude with thoughts on the future for such techniques to specifically identify deception, specifically with respect to the evolutionary nature of the attacks and information available. [[EENNDD]] learning; spam; semantic attacks; phishing; filtering; email"}, "Belajar untuk mengesan e-mel pancingan data Setiap bulan, lebih banyak serangan dilancarkan dengan tujuan untuk membuat pengguna web percaya bahawa mereka berkomunikasi dengan entiti yang dipercayai untuk tujuan mencuri maklumat akaun, bukti log masuk, dan maklumat identiti secara umum. Kaedah serangan ini, yang biasanya dikenal sebagai \"pancingan data\", biasanya dimulai dengan mengirim e-mel dengan pautan ke laman web palsu yang mengumpulkan maklumat. Kami menyajikan metode untuk mengesan serangan ini, yang dalam bentuk yang paling umum adalah aplikasi pembelajaran mesin pada set fitur yang dirancang untuk menyoroti penipuan yang disasarkan pengguna dalam komunikasi elektronik. Kaedah ini dapat digunakan, dengan sedikit pengubahsuaian, untuk mengesan laman web phishing, atau e-mel yang digunakan untuk mengarahkan mangsa ke laman web ini. Kami menilai kaedah ini pada sekumpulan lebih kurang 860 e-mel phishing, dan 6950 e-mel bukan pancingan data, dan mengenal pasti lebih daripada 96% e-mel pancingan data, sementara hanya mengklasifikasikan salah mengikut urutan 0.1% daripada e-mel yang sah. Kami menyimpulkan dengan pemikiran tentang masa depan untuk teknik tersebut untuk mengenal pasti penipuan, khususnya berkenaan dengan sifat evolusi serangan dan maklumat yang ada. [[EENNDD]] pembelajaran; spam; serangan semantik; pancingan data; tapisan; e-mel"], [{"string": "Thresher: automating the unwrapping of semantic content from the World Wide Web No contact information provided yet.", "keywords": ["tree edit distance", "on-line information services", "user interfaces", "semantic web", "haystack", "rdf", "wrapper induction"], "combined": "Thresher: automating the unwrapping of semantic content from the World Wide Web No contact information provided yet. [[EENNDD]] tree edit distance; on-line information services; user interfaces; semantic web; haystack; rdf; wrapper induction"}, "Thresher: mengautomasikan pembungkusan kandungan semantik dari World Wide Web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] jarak edit pokok; perkhidmatan maklumat dalam talian; antara muka pengguna; web semantik; rumput kering; rdf; aruhan pembalut"], [{"string": "Homepage live: automatic block tracing for web personalization The emergence of personalized homepage services, e.g. personalized Google Homepage and Microsoft Windows Live, has enabled Web users to select Web contents of interest and to aggregate them in a single Web page. The web contents are often predefined content blocks provided by the service providers. However, it involves intensive manual efforts to define the content blocks and maintain the information in it. In this paper, we propose a novel personalized homepage system, called .Homepage Live., to allow end users to use drag-and-drop actions to collect their favorite Web content blocks from existing Web pages and organize them in a single page. Moreover, Homepage Live automatically traces the changes of blocks with the evolvement of the container pages by measuring the tree edit distance of the selected blocks. By exploiting the immutable elements of Web pages, the tracing algorithm performance is significantly improved. The experimental results demonstrate the effectiveness and efficiency of our algorithm.", "keywords": ["tree edit distance", "adaptive user interfaces", "tracing", "web blocks", "tree pruning"], "combined": "Homepage live: automatic block tracing for web personalization The emergence of personalized homepage services, e.g. personalized Google Homepage and Microsoft Windows Live, has enabled Web users to select Web contents of interest and to aggregate them in a single Web page. The web contents are often predefined content blocks provided by the service providers. However, it involves intensive manual efforts to define the content blocks and maintain the information in it. In this paper, we propose a novel personalized homepage system, called .Homepage Live., to allow end users to use drag-and-drop actions to collect their favorite Web content blocks from existing Web pages and organize them in a single page. Moreover, Homepage Live automatically traces the changes of blocks with the evolvement of the container pages by measuring the tree edit distance of the selected blocks. By exploiting the immutable elements of Web pages, the tracing algorithm performance is significantly improved. The experimental results demonstrate the effectiveness and efficiency of our algorithm. [[EENNDD]] tree edit distance; adaptive user interfaces; tracing; web blocks; tree pruning"}, "Halaman utama secara langsung: penjejakan blok automatik untuk pemperibadian web Kemunculan perkhidmatan laman utama yang diperibadikan, mis. Laman Web Google yang diperibadikan dan Microsoft Windows Live, telah membolehkan pengguna Web memilih kandungan Web yang menarik dan mengumpulkannya dalam satu laman Web. Kandungan web selalunya merupakan blok kandungan yang telah ditentukan oleh penyedia perkhidmatan. Walau bagaimanapun, ia memerlukan usaha manual yang intensif untuk menentukan blok kandungan dan mengekalkan maklumat di dalamnya. Dalam makalah ini, kami mengusulkan sistem homepage khusus novel, yang disebut .Homepage Live., Untuk membolehkan pengguna akhir menggunakan tindakan drag-and-drop untuk mengumpulkan blok kandungan Web kegemaran mereka dari halaman Web yang ada dan menyusunnya dalam satu halaman. Lebih-lebih lagi, Homepage Live secara automatik mengesan perubahan blok dengan perkembangan halaman kontena dengan mengukur jarak edit pokok dari blok yang dipilih. Dengan mengeksploitasi elemen laman Web yang tidak berubah, prestasi algoritma penjejakan bertambah baik. Hasil eksperimen menunjukkan keberkesanan dan kecekapan algoritma kami. [[EENNDD]] jarak edit pokok; antara muka pengguna yang adaptif; mengesan; blok web; pemangkasan pokok"], [{"string": "Open user profiles for adaptive news systems: help or harm? Over the last five years, a range of projects have focused on progressively more elaborated techniques for adaptive news delivery. However, the adaptation process in these systems has become more complicated and thus less transparent to the users. In this paper, we concentrate on the application of open user models in adding transparency and controllability to adaptive news systems. We present a personalized news system, YourNews, which allows users to view and edit their interest profiles, and report a user study on the system. Our results confirm that users prefer transparency and control in their systems, and generate more trust to such systems. However, similar to previous studies, our study demonstrate that this ability to edit user profiles may also harm the system.s performance and has to be used with caution.", "keywords": ["control", "user profile", "open user model", "news personalization", "trust"], "combined": "Open user profiles for adaptive news systems: help or harm? Over the last five years, a range of projects have focused on progressively more elaborated techniques for adaptive news delivery. However, the adaptation process in these systems has become more complicated and thus less transparent to the users. In this paper, we concentrate on the application of open user models in adding transparency and controllability to adaptive news systems. We present a personalized news system, YourNews, which allows users to view and edit their interest profiles, and report a user study on the system. Our results confirm that users prefer transparency and control in their systems, and generate more trust to such systems. However, similar to previous studies, our study demonstrate that this ability to edit user profiles may also harm the system.s performance and has to be used with caution. [[EENNDD]] control; user profile; open user model; news personalization; trust"}, "Buka profil pengguna untuk sistem berita adaptif: bantuan atau bahaya? Selama lima tahun kebelakangan ini, sebilangan projek telah menumpukan pada teknik yang lebih terperinci untuk penyampaian berita yang lebih baik. Walau bagaimanapun, proses penyesuaian dalam sistem ini menjadi lebih rumit dan oleh itu kurang telus kepada pengguna. Dalam makalah ini, kami menumpukan perhatian pada penerapan model pengguna terbuka dalam menambahkan ketelusan dan kebolehkendalian pada sistem berita adaptif. Kami menyajikan sistem berita yang diperibadikan, YourNews, yang memungkinkan pengguna melihat dan mengedit profil minat mereka, dan melaporkan kajian pengguna mengenai sistem tersebut. Hasil kami mengesahkan bahawa pengguna lebih suka ketelusan dan kawalan dalam sistem mereka, dan menghasilkan lebih banyak kepercayaan terhadap sistem tersebut. Walau bagaimanapun, sama seperti kajian sebelumnya, kajian kami menunjukkan bahawa kemampuan mengedit profil pengguna ini juga boleh membahayakan prestasi sistem dan harus digunakan dengan berhati-hati. [[EENNDD]] kawalan; profil pengguna; model pengguna terbuka; pemperibadian berita; kepercayaan"], [{"string": "Efficient Web form entry on PDAs An abstract is not available.", "keywords": ["html", "forms", "web", "pda", "wap", "wireless access", "mobile computing", "portable devices"], "combined": "Efficient Web form entry on PDAs An abstract is not available. [[EENNDD]] html; forms; web; pda; wap; wireless access; mobile computing; portable devices"}, "Kemasukan borang Web yang cekap pada PDA Abstrak tidak tersedia. [[EENNDD]] html; borang; laman web; pda; wap; akses tanpa wayar; pengkomputeran mudah alih; peranti mudah alih"], [{"string": "Detecting group review spam It is well-known that many online reviews are not written by genuine users of products, but by spammers who write fake reviews to promote or demote some target products. Although some existing works have been done to detect fake reviews and individual spammers, to our knowledge, no work has been done on detecting spammer groups. This paper focuses on this task and proposes an effective technique to detect such groups.", "keywords": ["adversarial data mining", "review spam", "spammer group detection"], "combined": "Detecting group review spam It is well-known that many online reviews are not written by genuine users of products, but by spammers who write fake reviews to promote or demote some target products. Although some existing works have been done to detect fake reviews and individual spammers, to our knowledge, no work has been done on detecting spammer groups. This paper focuses on this task and proposes an effective technique to detect such groups. [[EENNDD]] adversarial data mining; review spam; spammer group detection"}, "Mengesan spam ulasan kumpulan Sudah diketahui umum bahawa banyak ulasan dalam talian tidak ditulis oleh pengguna produk yang tulen, tetapi oleh spammer yang menulis ulasan palsu untuk mempromosikan atau menurunkan beberapa produk sasaran. Walaupun beberapa kerja yang ada telah dilakukan untuk mengesan ulasan palsu dan spammer individu, sepanjang pengetahuan kami, tidak ada pekerjaan yang dilakukan untuk mengesan kumpulan spammer. Makalah ini memfokuskan tugas ini dan mencadangkan teknik yang berkesan untuk mengesan kumpulan tersebut. [[EENNDD]] perlombongan data lawan; semak spam; pengesanan kumpulan spammer"], [{"string": "CoSi: context-sensitive keyword query interpretation on RDF databases The demo will present CoSi, a system that enables context-sensitive interpretation of keyword queries on RDF databases. The techniques for representing, managing and exploiting query history are central to achieving this objective. The demonstration will show the effectiveness of our approach for capturing a user's querying context from their query history. Further, it will show how context is utilized to influence the interpretation of a new query. The demonstration is based on DBPedia, the RDF representation of Wikipedia.", "keywords": ["query history", "keyword query interpretation"], "combined": "CoSi: context-sensitive keyword query interpretation on RDF databases The demo will present CoSi, a system that enables context-sensitive interpretation of keyword queries on RDF databases. The techniques for representing, managing and exploiting query history are central to achieving this objective. The demonstration will show the effectiveness of our approach for capturing a user's querying context from their query history. Further, it will show how context is utilized to influence the interpretation of a new query. The demonstration is based on DBPedia, the RDF representation of Wikipedia. [[EENNDD]] query history; keyword query interpretation"}, "CoSi: tafsiran pertanyaan kata kunci sensitif konteks pada pangkalan data RDF Demo akan membentangkan CoSi, sistem yang membolehkan penafsiran sensitif konteks mengenai kata kunci pada pangkalan data RDF. Teknik untuk mewakili, mengurus dan mengeksploitasi sejarah pertanyaan adalah penting untuk mencapai objektif ini. Demonstrasi akan menunjukkan keberkesanan pendekatan kami untuk menangkap konteks pertanyaan pengguna dari sejarah pertanyaan mereka. Selanjutnya, ia akan menunjukkan bagaimana konteks digunakan untuk mempengaruhi penafsiran pertanyaan baru. Demonstrasi itu berdasarkan DBPedia, representasi RDF dari Wikipedia. [[EENNDD]] sejarah pertanyaan; tafsiran pertanyaan kata kunci"], [{"string": "QoS computation and policing in dynamic web service selection No contact information provided yet.", "keywords": ["ranking of qos", "web services", "qos", "extensible qos model"], "combined": "QoS computation and policing in dynamic web service selection No contact information provided yet. [[EENNDD]] ranking of qos; web services; qos; extensible qos model"}, "Pengiraan dan kepolisian QoS dalam pemilihan perkhidmatan web yang dinamik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kedudukan qos; perkhidmatan web; qos; model qos yang boleh dipanjangkan"], [{"string": "User-centric Web crawling No contact information provided yet.", "keywords": ["user-centric", "web crawling", "web page refreshing"], "combined": "User-centric Web crawling No contact information provided yet. [[EENNDD]] user-centric; web crawling; web page refreshing"}, "Perayapan Web yang berpusatkan pengguna Belum ada maklumat hubungan yang diberikan. [[EENNDD]] berpusatkan pengguna; merangkak web; laman web menyegarkan"], [{"string": "DemIL: an online interaction language between citizen and government No contact information provided yet.", "keywords": ["interface", "e-government", "e-democracy", "user interfaces", "interaction"], "combined": "DemIL: an online interaction language between citizen and government No contact information provided yet. [[EENNDD]] interface; e-government; e-democracy; user interfaces; interaction"}, "DemIL: bahasa interaksi dalam talian antara rakyat dan kerajaan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] antara muka; e-kerajaan; e-demokrasi; antara muka pengguna; interaksi"], [{"string": "Query topic detection for reformulation In this paper, we show that most multiple term queries include more than one topic and users usually reformulate their queries by topics instead of terms. In order to provide empirical evidence on user's reformulation behavior and to help search engines better handle the query reformulation problem, we focus on detecting internal topics in the original query and analyzing users. reformulation to those topics. Particularly, we utilize the Interaction Information (II) to measure the degree of one sub-query being a topic based on the local search results. The experimental results on query log show that: most users reformulate query at the topical level; and our proposed II-based algorithm is a good method to detect topics from original queries.", "keywords": ["topic", "query reformulation", "information search and retrieval", "interaction information"], "combined": "Query topic detection for reformulation In this paper, we show that most multiple term queries include more than one topic and users usually reformulate their queries by topics instead of terms. In order to provide empirical evidence on user's reformulation behavior and to help search engines better handle the query reformulation problem, we focus on detecting internal topics in the original query and analyzing users. reformulation to those topics. Particularly, we utilize the Interaction Information (II) to measure the degree of one sub-query being a topic based on the local search results. The experimental results on query log show that: most users reformulate query at the topical level; and our proposed II-based algorithm is a good method to detect topics from original queries. [[EENNDD]] topic; query reformulation; information search and retrieval; interaction information"}, "Pengesanan topik pertanyaan untuk penyusunan semula Dalam makalah ini, kami menunjukkan bahawa kebanyakan pertanyaan jangka panjang merangkumi lebih dari satu topik dan pengguna biasanya merumuskan semula pertanyaan mereka berdasarkan topik dan bukannya istilah. Untuk memberikan bukti empirikal mengenai tingkah laku penyusunan semula pengguna dan untuk membantu mesin pencari menangani masalah penyusunan semula pertanyaan dengan lebih baik, kami fokus untuk mengesan topik dalaman dalam pertanyaan asal dan menganalisis pengguna. penyusunan semula topik-topik tersebut. Terutama, kami menggunakan Interaksi Maklumat (II) untuk mengukur tahap satu sub-pertanyaan menjadi topik berdasarkan hasil carian tempatan. Hasil eksperimen pada log pertanyaan menunjukkan bahawa: kebanyakan pengguna menyusun semula pertanyaan pada tahap topikal; dan algoritma berasaskan II yang dicadangkan kami adalah kaedah yang baik untuk mengesan topik dari pertanyaan asal. [[EENNDD]] topik; penyusunan semula pertanyaan; pencarian dan pengambilan maklumat; maklumat interaksi"], [{"string": "Optimizing web search using social annotations This paper explores the use of social annotations to improve websearch. Nowadays, many services, e.g. del.icio.us, have been developed for web users to organize and share their favorite webpages on line by using social annotations. We observe that the social annotations can benefit web search in two aspects: 1) the annotations are usually good summaries of corresponding webpages; 2) the count of annotations indicates the popularity of webpages. Two novel algorithms are proposed to incorporate the above information into page ranking: 1) SocialSimRank (SSR)calculates the similarity between social annotations and webqueries; 2) SocialPageRank (SPR) captures the popularity of webpages. Preliminary experimental results show that SSR can find the latent semantic association between queries and annotations, while SPR successfully measures the quality (popularity) of a webpage from the web users' perspective. We further evaluate the proposed methods empirically with 50 manually constructed queries and 3000 auto-generated queries on a dataset crawledfrom delicious. Experiments show that both SSR and SPRbenefit web search significantly.", "keywords": ["social similarity", "social page rank", "web search", "information search and retrieval", "evaluation", "social annotation"], "combined": "Optimizing web search using social annotations This paper explores the use of social annotations to improve websearch. Nowadays, many services, e.g. del.icio.us, have been developed for web users to organize and share their favorite webpages on line by using social annotations. We observe that the social annotations can benefit web search in two aspects: 1) the annotations are usually good summaries of corresponding webpages; 2) the count of annotations indicates the popularity of webpages. Two novel algorithms are proposed to incorporate the above information into page ranking: 1) SocialSimRank (SSR)calculates the similarity between social annotations and webqueries; 2) SocialPageRank (SPR) captures the popularity of webpages. Preliminary experimental results show that SSR can find the latent semantic association between queries and annotations, while SPR successfully measures the quality (popularity) of a webpage from the web users' perspective. We further evaluate the proposed methods empirically with 50 manually constructed queries and 3000 auto-generated queries on a dataset crawledfrom delicious. Experiments show that both SSR and SPRbenefit web search significantly. [[EENNDD]] social similarity; social page rank; web search; information search and retrieval; evaluation; social annotation"}, "Mengoptimumkan carian web menggunakan anotasi sosial Makalah ini meneroka penggunaan anotasi sosial untuk meningkatkan carian web. Pada masa kini, banyak perkhidmatan, mis. del.icio.us, telah dikembangkan untuk pengguna web untuk mengatur dan berkongsi laman web kegemaran mereka secara dalam talian dengan menggunakan anotasi sosial. Kami melihat bahawa anotasi sosial dapat menguntungkan carian web dalam dua aspek: 1) anotasi biasanya merupakan ringkasan yang baik dari laman web yang sesuai; 2) jumlah anotasi menunjukkan populariti halaman web. Dua algoritma novel dicadangkan untuk memasukkan maklumat di atas ke dalam kedudukan halaman: 1) SocialSimRank (SSR) mengira persamaan antara anotasi sosial dan pertanyaan web; 2) SocialPageRank (SPR) menangkap populariti laman web. Hasil eksperimen awal menunjukkan bahawa SSR dapat menemukan hubungan semantik terpendam antara pertanyaan dan anotasi, sementara SPR berjaya mengukur kualiti (populariti) laman web dari perspektif pengguna web. Kami selanjutnya menilai kaedah yang dicadangkan secara empirik dengan 50 pertanyaan yang dibina secara manual dan 3000 pertanyaan yang dijana secara automatik pada set data yang dijumpai daripada yang lazat. Eksperimen menunjukkan bahawa kedua-dua SSR dan SPR menguntungkan carian web dengan ketara. [[EENNDD]] persamaan sosial; kedudukan laman sosial; carian sesawang; carian dan pengambilan maklumat; penilaian; anotasi sosial"], [{"string": "A query algebra for xml p2p databases No contact information provided yet.", "keywords": ["peer data management systems", "data description languages", "query algebras", "xml"], "combined": "A query algebra for xml p2p databases No contact information provided yet. [[EENNDD]] peer data management systems; data description languages; query algebras; xml"}, "Algebra pertanyaan untuk pangkalan data xml p2p Belum ada maklumat hubungan yang diberikan. [[EENNDD]] sistem pengurusan data rakan sebaya; bahasa penerangan data; pertanyaan algebras; xml"], [{"string": "Semantic navigation on the web of data: specification of routes, web fragments and actions The massive semantic data sources linked in the Web of Data give new meaning to old features like navigation; introduce new challenges like semantic specification of Web fragments; and make it possible to specify actions relying on semantic data. In this paper we introduce a declarative language to face these challenges. Based on navigational features, it is designed to specify fragments of the Web of Data and actions to be performed based on these data. We implement it in a centralized fashion, and show its power and performance. Finally, we explore the same ideas in a distributed setting, showing their feasibility, potentialities and challenges.", "keywords": ["web of data", "systems and software", "linked data", "navigation", "semantic web"], "combined": "Semantic navigation on the web of data: specification of routes, web fragments and actions The massive semantic data sources linked in the Web of Data give new meaning to old features like navigation; introduce new challenges like semantic specification of Web fragments; and make it possible to specify actions relying on semantic data. In this paper we introduce a declarative language to face these challenges. Based on navigational features, it is designed to specify fragments of the Web of Data and actions to be performed based on these data. We implement it in a centralized fashion, and show its power and performance. Finally, we explore the same ideas in a distributed setting, showing their feasibility, potentialities and challenges. [[EENNDD]] web of data; systems and software; linked data; navigation; semantic web"}, "Navigasi semantik di web data: spesifikasi laluan, serpihan web dan tindakan Sumber data semantik besar yang dihubungkan dalam Web Data memberi makna baru kepada ciri lama seperti navigasi; memperkenalkan cabaran baru seperti spesifikasi semantik fragmen Web; dan memungkinkan untuk menentukan tindakan yang bergantung pada data semantik. Dalam makalah ini kami memperkenalkan bahasa deklaratif untuk menghadapi cabaran ini. Berdasarkan fitur navigasi, dirancang untuk menentukan fragmen dari Web Data dan tindakan yang akan dilakukan berdasarkan data ini. Kami menerapkannya secara terpusat, dan menunjukkan kehebatan dan prestasinya. Akhirnya, kami meneroka idea yang sama dalam suasana yang diedarkan, menunjukkan kemungkinan, potensi dan cabaran mereka. [[EENNDD]] web data; sistem dan perisian; data yang dipautkan; pelayaran; web semantik"], [{"string": "Cross-domain sentiment classification via spectral feature alignment Sentiment classification aims to automatically predict sentiment polarity (e.g., positive or negative) of users publishing sentiment data (e.g., reviews, blogs). Although traditional classification algorithms can be used to train sentiment classifiers from manually labeled text data, the labeling work can be time-consuming and expensive. Meanwhile, users often use some different words when they express sentiment in different domains. If we directly apply a classifier trained in one domain to other domains, the performance will be very low due to the differences between these domains. In this work, we develop a general solution to sentiment classification when we do not have any labels in a target domain but have some labeled data in a different domain, regarded as source domain. In this cross-domain sentiment classification setting, to bridge the gap between the domains, we propose a spectral feature alignment (SFA) algorithm to align domain-specific words from different domains into unified clusters, with the help of domain-independent words as a bridge. In this way, the clusters can be used to reduce the gap between domain-specific words of the two domains, which can be used to train sentiment classifiers in the target domain accurately. Compared to previous approaches, SFA can discover a robust representation for cross-domain data by fully exploiting the relationship between the domain-specific and domain-independent words via simultaneously co-clustering them in a common latent space. We perform extensive experiments on two real world datasets, and demonstrate that SFA significantly outperforms previous approaches to cross-domain sentiment classification.", "keywords": ["transfer learning", "feature alignment", "learning", "database applications", "domain adaptation", "sentiment classification", "opinion mining"], "combined": "Cross-domain sentiment classification via spectral feature alignment Sentiment classification aims to automatically predict sentiment polarity (e.g., positive or negative) of users publishing sentiment data (e.g., reviews, blogs). Although traditional classification algorithms can be used to train sentiment classifiers from manually labeled text data, the labeling work can be time-consuming and expensive. Meanwhile, users often use some different words when they express sentiment in different domains. If we directly apply a classifier trained in one domain to other domains, the performance will be very low due to the differences between these domains. In this work, we develop a general solution to sentiment classification when we do not have any labels in a target domain but have some labeled data in a different domain, regarded as source domain. In this cross-domain sentiment classification setting, to bridge the gap between the domains, we propose a spectral feature alignment (SFA) algorithm to align domain-specific words from different domains into unified clusters, with the help of domain-independent words as a bridge. In this way, the clusters can be used to reduce the gap between domain-specific words of the two domains, which can be used to train sentiment classifiers in the target domain accurately. Compared to previous approaches, SFA can discover a robust representation for cross-domain data by fully exploiting the relationship between the domain-specific and domain-independent words via simultaneously co-clustering them in a common latent space. We perform extensive experiments on two real world datasets, and demonstrate that SFA significantly outperforms previous approaches to cross-domain sentiment classification. [[EENNDD]] transfer learning; feature alignment; learning; database applications; domain adaptation; sentiment classification; opinion mining"}, "Klasifikasi sentimen merentas domain melalui penjajaran ciri spektrum Klasifikasi sentimen bertujuan untuk meramalkan polaritas sentimen secara automatik (mis., Positif atau negatif) pengguna yang menerbitkan data sentimen (mis., Ulasan, blog). Walaupun algoritma klasifikasi tradisional dapat digunakan untuk melatih pengklasifikasi sentimen dari data teks berlabel secara manual, kerja pelabelan boleh memakan masa dan mahal. Sementara itu, pengguna sering menggunakan beberapa perkataan yang berbeza ketika mereka menyatakan sentimen dalam domain yang berbeza. Sekiranya kita secara langsung menerapkan pengelasan yang dilatih dalam satu domain ke domain lain, kinerjanya akan sangat rendah kerana perbezaan antara domain ini. Dalam karya ini, kami mengembangkan solusi umum untuk klasifikasi sentimen ketika kami tidak memiliki label dalam domain sasaran tetapi memiliki beberapa data berlabel dalam domain yang berbeda, dianggap sebagai domain sumber. Dalam tetapan klasifikasi sentimen merentas domain ini, untuk merapatkan jurang antara domain, kami mencadangkan algoritma penjajaran ciri spektrum (SFA) untuk menyelaraskan kata-kata khusus domain dari domain yang berbeza menjadi kelompok terpadu, dengan bantuan kata bebas domain sebagai jambatan. Dengan cara ini, kluster dapat digunakan untuk mengurangi jurang antara kata-kata khusus domain dari dua domain, yang dapat digunakan untuk melatih pengklasifikasi sentimen dalam domain sasaran dengan tepat. Berbanding dengan pendekatan sebelumnya, SFA dapat menemukan perwakilan yang kuat untuk data rentas domain dengan memanfaatkan sepenuhnya hubungan antara kata-kata khusus domain dan bebas domain dengan secara bersamaan menggabungkannya dalam ruang laten yang sama. Kami melakukan eksperimen yang meluas pada dua kumpulan data dunia nyata, dan menunjukkan bahawa SFA mengatasi pendekatan sebelumnya dengan klasifikasi sentimen lintas domain. [[EENNDD]] memindahkan pembelajaran; penjajaran ciri; belajar; aplikasi pangkalan data; penyesuaian domain; klasifikasi sentimen; perlombongan pendapat"], [{"string": "Automatic identification of user interest for personalized search No contact information provided yet.", "keywords": ["personalized search", "web search", "user search behavior", "user profile"], "combined": "Automatic identification of user interest for personalized search No contact information provided yet. [[EENNDD]] personalized search; web search; user search behavior; user profile"}, "Pengenalan automatik minat pengguna untuk carian yang diperibadikan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] carian diperibadikan; carian sesawang; tingkah laku carian pengguna; profil pengguna"], [{"string": "Invasive browser sniffing and countermeasures No contact information provided yet.", "keywords": ["browser cache", "personalization", "sniffing", "cascading style sheets", "phishing"], "combined": "Invasive browser sniffing and countermeasures No contact information provided yet. [[EENNDD]] browser cache; personalization; sniffing; cascading style sheets; phishing"}, "Penghidap dan pencegahan penyemak imbas invasif Belum ada maklumat hubungan yang diberikan. [[EENNDD]] cache penyemak imbas; pemperibadian; menghidu; Cascading Style Sheets; pancingan data"], [{"string": "Recommending questions using the mdl-based tree cut model The paper is concerned with the problem of question recommendation. Specifically, given a question as query, we are to retrieve and rank other questions according to their likelihood of being good recommendations of the queried question. A good recommendation provides alternative aspects around users' interest. We tackle the problem of question recommendation in two steps: first represent questions as graphs of topic terms, and then rank recommendations on the basis of the graphs. We formalize both steps as the tree-cutting problems and then employ the MDL (Minimum Description Length) for selecting the best cuts. Experiments have been conducted with the real questions posted at Yahoo! Answers. The questions are about two domains, 'travel' and 'computers &amp; internet'. Experimental results indicate that the use of the MDL-based tree cut model can significantly outperform the baseline methods of word-based VSM or phrase-based VSM. The results also show that the use of the MDL-based tree cut model is essential to our approach.", "keywords": ["question recommendation", "query suggestion", "minimum description length", "miscellaneous", "tree cut model"], "combined": "Recommending questions using the mdl-based tree cut model The paper is concerned with the problem of question recommendation. Specifically, given a question as query, we are to retrieve and rank other questions according to their likelihood of being good recommendations of the queried question. A good recommendation provides alternative aspects around users' interest. We tackle the problem of question recommendation in two steps: first represent questions as graphs of topic terms, and then rank recommendations on the basis of the graphs. We formalize both steps as the tree-cutting problems and then employ the MDL (Minimum Description Length) for selecting the best cuts. Experiments have been conducted with the real questions posted at Yahoo! Answers. The questions are about two domains, 'travel' and 'computers &amp; internet'. Experimental results indicate that the use of the MDL-based tree cut model can significantly outperform the baseline methods of word-based VSM or phrase-based VSM. The results also show that the use of the MDL-based tree cut model is essential to our approach. [[EENNDD]] question recommendation; query suggestion; minimum description length; miscellaneous; tree cut model"}, ""], [{"string": "Schemapath, a minimal extension to xml schema for conditional constraints No contact information provided yet.", "keywords": ["co-constraints", "schemapath", "xml", "schema languages"], "combined": "Schemapath, a minimal extension to xml schema for conditional constraints No contact information provided yet. [[EENNDD]] co-constraints; schemapath; xml; schema languages"}, "Schemapath, pelanjutan minimum untuk skema xml untuk kekangan bersyarat Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] kekangan bersama; schemapath; xml; bahasa skema"], [{"string": "Cataclysm: policing extreme overloads in internet applications No contact information provided yet.", "keywords": ["organization and design", "overload", "sentry", "internet application"], "combined": "Cataclysm: policing extreme overloads in internet applications No contact information provided yet. [[EENNDD]] organization and design; overload; sentry; internet application"}, "Cataclysm: menangani beban berlebihan dalam aplikasi internet Belum ada maklumat hubungan yang diberikan. [[EENNDD]] organisasi dan reka bentuk; beban berlebihan; penjaga; aplikasi internet"], [{"string": "Building term suggestion relational graphs from collective intelligence This paper proposes an effective approach to provide relevant search terms for conceptual Web search. 'Semantic Term Suggestion' function has been included so that users can find the most appropriate query term to what they really need. Conventional approaches for term suggestion involve extracting frequently occurring key terms from retrieved documents. They must deal with term extraction difficulties and interference from irrelevant documents. In this paper, we propose a semantic term suggestion function called Collective Intelligence based Term Suggestion (CITS). CITS provides a novel social-network based framework for relevant terms suggestion with a semantic graph of the search term without limiting to the specific query term. A visualization of semantic graph is presented to the users to help browsing search results from related terms in the semantic graph. The search results are ranked each time according to their relevance to the related terms in the entire query session. Comparing to two popular commercial search engines, a user study of 18 users on 50 search terms showed better user satisfactions and indicated the potential usefulness of proposed method in real-world search applications.", "keywords": ["re-ranking", "keyword expansion", "social network"], "combined": "Building term suggestion relational graphs from collective intelligence This paper proposes an effective approach to provide relevant search terms for conceptual Web search. 'Semantic Term Suggestion' function has been included so that users can find the most appropriate query term to what they really need. Conventional approaches for term suggestion involve extracting frequently occurring key terms from retrieved documents. They must deal with term extraction difficulties and interference from irrelevant documents. In this paper, we propose a semantic term suggestion function called Collective Intelligence based Term Suggestion (CITS). CITS provides a novel social-network based framework for relevant terms suggestion with a semantic graph of the search term without limiting to the specific query term. A visualization of semantic graph is presented to the users to help browsing search results from related terms in the semantic graph. The search results are ranked each time according to their relevance to the related terms in the entire query session. Comparing to two popular commercial search engines, a user study of 18 users on 50 search terms showed better user satisfactions and indicated the potential usefulness of proposed method in real-world search applications. [[EENNDD]] re-ranking; keyword expansion; social network"}, "Membina grafik relasi cadangan istilah dari kecerdasan kolektif Makalah ini mencadangkan pendekatan yang berkesan untuk menyediakan istilah carian yang relevan untuk carian Web konseptual. Fungsi 'Cadangan Istilah Semantik' telah disertakan sehingga pengguna dapat mencari istilah pertanyaan yang paling sesuai dengan apa yang sebenarnya mereka perlukan. Pendekatan konvensional untuk cadangan istilah melibatkan pengekstrakan istilah utama yang sering berlaku dari dokumen yang diambil. Mereka mesti menangani masalah pengekstrakan istilah dan gangguan dari dokumen yang tidak berkaitan. Dalam makalah ini, kami mencadangkan fungsi saran istilah semantik yang disebut Cadangan Istilah berdasarkan Kolektif Kecerdasan (CITS). CITS menyediakan kerangka berasaskan rangkaian sosial baru untuk cadangan istilah yang relevan dengan grafik semantik istilah carian tanpa mengehadkan istilah pertanyaan tertentu. Visualisasi grafik semantik disampaikan kepada pengguna untuk membantu melayari hasil carian dari istilah yang berkaitan dalam grafik semantik. Hasil carian diperingkat setiap kali mengikut kesesuaiannya dengan istilah yang berkaitan dalam keseluruhan sesi pertanyaan. Berbanding dengan dua enjin carian komersial yang popular, kajian pengguna terhadap 18 pengguna pada 50 istilah carian menunjukkan kepuasan pengguna yang lebih baik dan menunjukkan potensi kegunaan kaedah yang dicadangkan dalam aplikasi carian dunia nyata. [[EENNDD]] peringkat semula; pengembangan kata kunci; rangkaian sosial"], [{"string": "The distribution of pageRank follows a power-law only for particular values of the damping factor No contact information provided yet.", "keywords": ["pagerank distribution", "miscellaneous", "web graph"], "combined": "The distribution of pageRank follows a power-law only for particular values of the damping factor No contact information provided yet. [[EENNDD]] pagerank distribution; miscellaneous; web graph"}, "Pengedaran halamanRank mengikuti undang-undang kuasa hanya untuk nilai tertentu dari faktor redaman Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] pengedaran pagerank; pelbagai; grafik web"], [{"string": "Learning to rank relational objects and its application to web search Learning to rank is a new statistical learning technology on creating a ranking model for sorting objects. The technology has been successfully applied to web search, and is becoming one of the key machineries for building search engines. Existing approaches to learning to rank, however, did not consider the cases in which there exists relationship between the objects to be ranked, despite of the fact that such situations are very common in practice. For example, in web search, given a query certain relationships usually exist among the the retrieved documents, e.g., URL hierarchy, similarity, etc., and sometimes it is necessary to utilize the information in ranking of the documents. This paper addresses the issue and formulates it as a novel learning problem, referred to as, 'learning to rank relational objects'. In the new learning task, the ranking model is defined as a function of not only the contents (features) of objects but also the relations between objects. The paper further focuses on one setting of the learning problem in which the way of using relation information is predetermined. It formalizes the learning task as an optimization problem in the setting. The paper then proposes a new method to perform the optimization task, particularly an implementation based on SVM. Experimental results show that the proposed method outperforms the baseline methods for two ranking tasks (Pseudo Relevance Feedback and Topic Distillation) in web search, indicating that the proposed method can indeed make effective use of relation information and content information in ranking.", "keywords": ["relational ranking svm", "learning to rank relational objects", "topic distillation", "pseudo relevance feedback", "performance evaluation"], "combined": "Learning to rank relational objects and its application to web search Learning to rank is a new statistical learning technology on creating a ranking model for sorting objects. The technology has been successfully applied to web search, and is becoming one of the key machineries for building search engines. Existing approaches to learning to rank, however, did not consider the cases in which there exists relationship between the objects to be ranked, despite of the fact that such situations are very common in practice. For example, in web search, given a query certain relationships usually exist among the the retrieved documents, e.g., URL hierarchy, similarity, etc., and sometimes it is necessary to utilize the information in ranking of the documents. This paper addresses the issue and formulates it as a novel learning problem, referred to as, 'learning to rank relational objects'. In the new learning task, the ranking model is defined as a function of not only the contents (features) of objects but also the relations between objects. The paper further focuses on one setting of the learning problem in which the way of using relation information is predetermined. It formalizes the learning task as an optimization problem in the setting. The paper then proposes a new method to perform the optimization task, particularly an implementation based on SVM. Experimental results show that the proposed method outperforms the baseline methods for two ranking tasks (Pseudo Relevance Feedback and Topic Distillation) in web search, indicating that the proposed method can indeed make effective use of relation information and content information in ranking. [[EENNDD]] relational ranking svm; learning to rank relational objects; topic distillation; pseudo relevance feedback; performance evaluation"}, "Belajar untuk menentukan kedudukan objek perhubungan dan aplikasinya ke carian web Belajar memberi peringkat adalah teknologi pembelajaran statistik baru dalam membuat model peringkat untuk menyusun objek. Teknologi ini berjaya diterapkan pada carian web, dan menjadi salah satu mesin utama untuk membina mesin pencari. Pendekatan yang ada untuk belajar untuk memberi peringkat, bagaimanapun, tidak mempertimbangkan kes-kes di mana terdapat hubungan antara objek untuk diberi peringkat, walaupun pada kenyataannya situasi seperti itu sangat umum dalam praktik. Sebagai contoh, dalam carian web, diberikan pertanyaan hubungan tertentu biasanya wujud di antara dokumen yang diambil, misalnya, hierarki URL, kesamaan, dan lain-lain, dan kadang-kadang perlu menggunakan maklumat dalam peringkat dokumen. Makalah ini membahas masalah ini dan merumuskannya sebagai masalah pembelajaran novel, disebut sebagai, 'belajar untuk menilai objek hubungan'. Dalam tugas pembelajaran baru, model peringkat didefinisikan sebagai fungsi bukan hanya isi (ciri) objek tetapi juga hubungan antara objek. Makalah ini lebih memfokuskan pada satu tetapan masalah pembelajaran di mana cara menggunakan maklumat hubungan ditentukan sebelumnya. Ini memformalkan tugas pembelajaran sebagai masalah pengoptimuman dalam pengaturan. Makalah ini kemudian mengusulkan metode baru untuk melakukan tugas pengoptimuman, terutama pelaksanaan berdasarkan SVM. Hasil eksperimen menunjukkan bahawa kaedah yang dicadangkan melebihi kaedah asas untuk dua tugas peringkat (Pseudo Relevance Feedback dan Topic Distillation) dalam carian web, menunjukkan bahawa kaedah yang dicadangkan memang dapat memanfaatkan penggunaan maklumat hubungan dan maklumat kandungan secara efektif dalam peringkat. [[EENNDD]] hubungan relasi svm; belajar mengelaskan objek hubungan; penyulingan topik; maklum balas kesesuaian pseudo; penilaian prestasi"], [{"string": "Building adaptable and reusable XML applications with model transformations No contact information provided yet.", "keywords": ["model transformations", "xml processing", "transformation language", "xml", "mda"], "combined": "Building adaptable and reusable XML applications with model transformations No contact information provided yet. [[EENNDD]] model transformations; xml processing; transformation language; xml; mda"}, "Membangun aplikasi XML yang dapat disesuaikan dan boleh digunakan semula dengan transformasi model Belum ada maklumat hubungan yang diberikan. [[EENNDD]] transformasi model; pemprosesan xml; bahasa transformasi; xml; mda"], [{"string": "Placing search in context: the concept revisited An abstract is not available.", "keywords": ["context", "statistical natural language processing", "semantic processing", "invisible web", "search"], "combined": "Placing search in context: the concept revisited An abstract is not available. [[EENNDD]] context; statistical natural language processing; semantic processing; invisible web; search"}, "Meletakkan carian dalam konteks: konsep dikaji semula Abstrak tidak tersedia. [[EENNDD]] konteks; pemprosesan bahasa semula jadi statistik; pemprosesan semantik; web yang tidak kelihatan; cari"], [{"string": "Scaling link-based similarity search No contact information provided yet.", "keywords": ["link-analysis", "probabilistic algorithms", "information search and retrieval", "scalability", "similarity search", "fingerprint"], "combined": "Scaling link-based similarity search No contact information provided yet. [[EENNDD]] link-analysis; probabilistic algorithms; information search and retrieval; scalability; similarity search; fingerprint"}, "Mencari persamaan berdasarkan pautan penskalaan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] analisis pautan; algoritma probabilistik; pencarian dan pengambilan maklumat; skalabiliti; carian kesamaan; cap jari"], [{"string": "Highly efficient algorithms for structural clustering of large websites In this paper, we present a highly scalable algorithm for structurally clustering webpages for extraction. We show that, using only the URLs of the webpages and simple content features, it is possible to cluster webpages effectively and efficiently. At the heart of our techniques is a principled framework, based on the principles of information theory, that allows us to effectively leverage the URLs, and combine them with content and structural properties. Using an extensive evaluation over several large full websites, we demonstrate the effectiveness of our techniques, at a scale unattainable by previous techniques.", "keywords": ["information extraction", "minimum description length", "structural clustering"], "combined": "Highly efficient algorithms for structural clustering of large websites In this paper, we present a highly scalable algorithm for structurally clustering webpages for extraction. We show that, using only the URLs of the webpages and simple content features, it is possible to cluster webpages effectively and efficiently. At the heart of our techniques is a principled framework, based on the principles of information theory, that allows us to effectively leverage the URLs, and combine them with content and structural properties. Using an extensive evaluation over several large full websites, we demonstrate the effectiveness of our techniques, at a scale unattainable by previous techniques. [[EENNDD]] information extraction; minimum description length; structural clustering"}, "Algoritma yang sangat cekap untuk pengelompokan struktur laman web besar Dalam makalah ini, kami menyajikan algoritma yang sangat berskala untuk pengelompokan laman web secara struktur untuk pengekstrakan. Kami menunjukkan bahawa, hanya dengan menggunakan URL halaman web dan ciri kandungan yang mudah, adalah mungkin untuk mengelompokkan halaman web dengan berkesan dan cekap. Inti teknik kami adalah kerangka berprinsip, berdasarkan prinsip teori maklumat, yang memungkinkan kami memanfaatkan URL dengan berkesan, dan menggabungkannya dengan kandungan dan sifat struktur. Dengan menggunakan penilaian yang luas di beberapa laman web penuh yang besar, kami menunjukkan keberkesanan teknik kami, pada skala yang tidak dapat dicapai oleh teknik sebelumnya. [[EENNDD]] pengekstrakan maklumat; panjang keterangan minimum; pengelompokan struktur"], [{"string": "Translating XSLT programs to Efficient SQL queries No contact information provided yet.", "keywords": ["xslt", "virtual view", "translation", "sql", "xml", "query optimization"], "combined": "Translating XSLT programs to Efficient SQL queries No contact information provided yet. [[EENNDD]] xslt; virtual view; translation; sql; xml; query optimization"}, "Menterjemahkan program XSLT ke pertanyaan SQL yang cekap Belum ada maklumat hubungan yang diberikan. [[EENNDD]] xslt; paparan maya; terjemahan; sql; xml; pengoptimuman pertanyaan"], [{"string": "The recurrence dynamics of social tagging How often do tags recur? How hard is predicting tag recurrence? What tags are likely to recur? We try to answer these questions by analysing the RSDC08 dataset, in both individual and collective settings. Our findings provide useful insights for the development of tag suggestion techniques etc.", "keywords": ["social tagging", "folksonomy", "web 2.0", "content analysis and indexing"], "combined": "The recurrence dynamics of social tagging How often do tags recur? How hard is predicting tag recurrence? What tags are likely to recur? We try to answer these questions by analysing the RSDC08 dataset, in both individual and collective settings. Our findings provide useful insights for the development of tag suggestion techniques etc. [[EENNDD]] social tagging; folksonomy; web 2.0; content analysis and indexing"}, "Dinamika penandaan sosial berulang Berapa kerap teg berulang? Seberapa sukar untuk meramalkan pengulangan tag? Tag apa yang mungkin berulang? Kami cuba menjawab soalan-soalan ini dengan menganalisis set data RSDC08, dalam tetapan individu dan kolektif. Penemuan kami memberikan pandangan berguna untuk pengembangan teknik penandaan tag dll. [[EENNDD]] penandaan sosial; folksonomi; laman web 2.0; analisis kandungan dan pengindeksan"], [{"string": "Parallel crawlers No contact information provided yet.", "keywords": ["web crawler", "web spider", "parallelization"], "combined": "Parallel crawlers No contact information provided yet. [[EENNDD]] web crawler; web spider; parallelization"}, "Penjelajah selari Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perayap web; labah-labah jaring; selari"], [{"string": "Topic-sensitive PageRank No contact information provided yet.", "keywords": ["personalized search", "pagerank", "web graph", "link structure", "search in context", "search"], "combined": "Topic-sensitive PageRank No contact information provided yet. [[EENNDD]] personalized search; pagerank; web graph; link structure; search in context; search"}, "Peringkat Halaman sensitif topik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] carian diperibadikan; pagerank; grafik web; struktur pautan; cari mengikut konteks; cari"], [{"string": "Design and implementation of contextual information portals This paper presents a system for enabling offline web use to satisfy the information needs of disconnected communities. We describe the design, implementation, evaluation, and pilot deployment of an automated mechanism to construct Contextual Information Portals (CIPs). CIPs are large searchable information repositories of web pages tailored to the information needs of a target population. We combine an efficient classifier with a focused crawler to gather the web pages for the portal for any given topic. Given a set of topics of interest, our system constructs a CIP containing the most relevant pages from the web across these topics. Using several secondary school course syllabi, we demonstrate the effectiveness of our system for constructing CIPs for use as an education resource. We evaluate our system across several metrics: classification accuracy, crawl scalability, crawl accuracy and harvest rate. We describe the utility and usability of our system based on a preliminary deployment study at an after-school program in India, and also outline our ongoing larger-scale pilot deployment at five schools in Kenya.", "keywords": ["offline", "web portal", "computer uses in education", "document classification", "hypertext/hypermedia", "focused crawling"], "combined": "Design and implementation of contextual information portals This paper presents a system for enabling offline web use to satisfy the information needs of disconnected communities. We describe the design, implementation, evaluation, and pilot deployment of an automated mechanism to construct Contextual Information Portals (CIPs). CIPs are large searchable information repositories of web pages tailored to the information needs of a target population. We combine an efficient classifier with a focused crawler to gather the web pages for the portal for any given topic. Given a set of topics of interest, our system constructs a CIP containing the most relevant pages from the web across these topics. Using several secondary school course syllabi, we demonstrate the effectiveness of our system for constructing CIPs for use as an education resource. We evaluate our system across several metrics: classification accuracy, crawl scalability, crawl accuracy and harvest rate. We describe the utility and usability of our system based on a preliminary deployment study at an after-school program in India, and also outline our ongoing larger-scale pilot deployment at five schools in Kenya. [[EENNDD]] offline; web portal; computer uses in education; document classification; hypertext/hypermedia; focused crawling"}, "Reka bentuk dan pelaksanaan portal maklumat kontekstual Makalah ini menyajikan sistem untuk membolehkan penggunaan web luar talian untuk memenuhi keperluan maklumat masyarakat yang terputus. Kami menerangkan reka bentuk, pelaksanaan, penilaian, dan penerapan pilot mekanisme automatik untuk membangun Portal Informasi Kontekstual (CIP). CIP adalah repositori maklumat besar dari laman web yang disesuaikan dengan keperluan maklumat populasi sasaran. Kami menggabungkan pengklasifikasi yang cekap dengan crawler yang fokus untuk mengumpulkan halaman web untuk portal untuk topik tertentu. Memandangkan sekumpulan topik yang menarik, sistem kami membina CIP yang mengandungi halaman yang paling relevan dari web di seluruh topik ini. Dengan menggunakan beberapa sukatan pelajaran sekolah menengah, kami menunjukkan keberkesanan sistem kami untuk membina CIP untuk digunakan sebagai sumber pendidikan. Kami menilai sistem kami melalui beberapa metrik: ketepatan klasifikasi, skalabiliti perayapan, ketepatan perayapan dan kadar penuaian. Kami menerangkan kegunaan dan kegunaan sistem kami berdasarkan kajian penyebaran awal di program selepas sekolah di India, dan juga menggariskan penggunaan pilot yang berskala besar di lima sekolah di Kenya. [[EENNDD]] luar talian; portal web; penggunaan komputer dalam pendidikan; pengelasan dokumen; hiperteks / hipermedia; fokus merangkak"], [{"string": "Applying NavOptim to minimise navigational effort No contact information provided yet.", "keywords": ["efforts metrics", "navigation architecture"], "combined": "Applying NavOptim to minimise navigational effort No contact information provided yet. [[EENNDD]] efforts metrics; navigation architecture"}, "Menggunakan NavOptim untuk meminimumkan usaha pelayaran Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] metrik usaha; seni bina pelayaran"], [{"string": "Keyword extraction for social snippets Today, a huge amount of text is being generated for social purposes on social networking services on the Web. Unlike traditional documents, such text is usually extremely short and tends to be informal. Analysis of such text benefit many applications such as advertising, search, and content filtering. In this work, we study one traditional text mining task on such new form of text, that is extraction of meaningful keywords. We propose several intuitive yet useful features and experiment with various classification models. Evaluation is conducted on Facebook data. Performances of various features and models are reported and compared.", "keywords": ["social snippets", "keyword extraction", "miscellaneous", "online advertising"], "combined": "Keyword extraction for social snippets Today, a huge amount of text is being generated for social purposes on social networking services on the Web. Unlike traditional documents, such text is usually extremely short and tends to be informal. Analysis of such text benefit many applications such as advertising, search, and content filtering. In this work, we study one traditional text mining task on such new form of text, that is extraction of meaningful keywords. We propose several intuitive yet useful features and experiment with various classification models. Evaluation is conducted on Facebook data. Performances of various features and models are reported and compared. [[EENNDD]] social snippets; keyword extraction; miscellaneous; online advertising"}, "Pengekstrakan kata kunci untuk potongan sosial Hari ini, sejumlah besar teks dihasilkan untuk tujuan sosial pada perkhidmatan rangkaian sosial di Web. Tidak seperti dokumen tradisional, teks seperti itu biasanya sangat pendek dan cenderung tidak formal. Analisis teks seperti itu memberi banyak manfaat seperti pengiklanan, carian, dan penapisan kandungan. Dalam karya ini, kami mengkaji satu tugas perlombongan teks tradisional pada bentuk teks baru seperti pengekstrakan kata kunci yang bermakna. Kami mencadangkan beberapa ciri intuitif namun berguna dan bereksperimen dengan pelbagai model klasifikasi. Penilaian dilakukan pada data Facebook. Persembahan pelbagai ciri dan model dilaporkan dan dibandingkan. [[EENNDD]] coretan sosial; pengekstrakan kata kunci; pelbagai; iklan dalam talian"], [{"string": "Debugging standard document formats We present a tool for helping XML schema designers to obtain a high quality level for their specifications.", "keywords": ["schema", "format", "xml", "compatibility"], "combined": "Debugging standard document formats We present a tool for helping XML schema designers to obtain a high quality level for their specifications. [[EENNDD]] schema; format; xml; compatibility"}, "Debugging format dokumen standard Kami menyajikan alat untuk membantu pereka skema XML untuk mendapatkan tahap kualiti yang tinggi untuk spesifikasi mereka. [[EENNDD]] skema; format; xml; keserasian"], [{"string": "Learning multiple graphs for document recommendations The Web offers rich relational data with different semantics. In this paper, we address the problem of document recommendation in a digital library, where the documents in question are networked by citations and are associated with other entities by various relations. Due to the sparsity of a single graph and noise in graph construction, we propose a new method for combining multiple graphs to measure document similarities, where different factorization strategies are used based on the nature of different graphs. In particular, the new method seeks a single low-dimensional embedding of documents that captures their relative similarities in a latent space. Based on the obtained embedding, a new recommendation framework is developed using semi-supervised learning on graphs. In addition, we address the scalability issue and propose an incremental algorithm. The new incremental method significantly improves the efficiency by calculating the embedding for new incoming documents only. The new batch and incremental methods are evaluated on two real world datasets prepared from CiteSeer. Experiments demonstrate significant quality improvement for our batch method and significant efficiency improvement with tolerable quality loss for our incremental method.", "keywords": ["spectral clustering", "collaborative filtering", "optimization", "semi-supervised learning", "social network analysis", "recommender systems"], "combined": "Learning multiple graphs for document recommendations The Web offers rich relational data with different semantics. In this paper, we address the problem of document recommendation in a digital library, where the documents in question are networked by citations and are associated with other entities by various relations. Due to the sparsity of a single graph and noise in graph construction, we propose a new method for combining multiple graphs to measure document similarities, where different factorization strategies are used based on the nature of different graphs. In particular, the new method seeks a single low-dimensional embedding of documents that captures their relative similarities in a latent space. Based on the obtained embedding, a new recommendation framework is developed using semi-supervised learning on graphs. In addition, we address the scalability issue and propose an incremental algorithm. The new incremental method significantly improves the efficiency by calculating the embedding for new incoming documents only. The new batch and incremental methods are evaluated on two real world datasets prepared from CiteSeer. Experiments demonstrate significant quality improvement for our batch method and significant efficiency improvement with tolerable quality loss for our incremental method. [[EENNDD]] spectral clustering; collaborative filtering; optimization; semi-supervised learning; social network analysis; recommender systems"}, "Mempelajari pelbagai grafik untuk cadangan dokumen Web menawarkan data hubungan yang kaya dengan semantik yang berbeza. Dalam makalah ini, kami membahas masalah rekomendasi dokumen di perpustakaan digital, di mana dokumen-dokumen tersebut dihubungkan oleh kutipan dan dihubungkan dengan entiti lain oleh pelbagai hubungan. Oleh kerana kelangkaan satu grafik dan kebisingan dalam pembinaan grafik, kami mencadangkan kaedah baru untuk menggabungkan beberapa graf untuk mengukur kesamaan dokumen, di mana strategi pemfaktoran yang berbeza digunakan berdasarkan sifat grafik yang berbeza. Khususnya, kaedah baru mencari penyisipan dokumen rendah dimensi tunggal yang menangkap persamaan relatif mereka di ruang pendam. Berdasarkan penyisipan yang diperoleh, kerangka cadangan baru dikembangkan menggunakan pembelajaran separa diselia pada grafik. Sebagai tambahan, kami menangani masalah skalabilitas dan mencadangkan algoritma tambahan. Kaedah kenaikan baru secara signifikan meningkatkan kecekapan dengan mengira penyisipan untuk dokumen masuk baru sahaja. Kaedah kumpulan dan kenaikan baru dinilai pada dua set data dunia nyata yang disediakan dari CiteSeer. Eksperimen menunjukkan peningkatan kualiti yang signifikan untuk kaedah kumpulan kami dan peningkatan kecekapan yang signifikan dengan penurunan kualiti yang dapat diterima untuk kaedah kenaikan kami. [[EENNDD]] pengelompokan spektrum; penapisan kolaboratif; pengoptimuman; pembelajaran separa penyeliaan; analisis rangkaian sosial; sistem cadangan"], [{"string": "Dynamic learning-based mechanism design for dependent valued exchange economies Learning private information from multiple strategic agents poses challenge in many Internet applications. Sponsored search auctions, crowdsourcing, Amazon's mechanical turk, various online review forums are examples where we are interested in learning true values of the advertisers or true opinion of the reviewers. The common thread in these decision problems is that the optimal outcome depends on the private information of all the agents, while the decision of the outcome can be chosen only through reported information which may be manipulated by the strategic agents. The other important trait of these applications is their dynamic nature. The advertisers in an online auction or the users of mechanical turk arrive and depart, and when present, interact with the system repeatedly, giving the opportunity to learn their types. Dynamic mechanisms, which learn from the past interactions and make present decisions depending on the expected future evolution of the game, has been shown to improve performance over repeated versions of static mechanisms. In this paper, we will survey the past and current state-of-the-art dynamic mechanisms and analyze a new setting where the agents consist of buyers and sellers, known as exchange economies, and agents having value interdependency, which are relevant in applications illustrated through examples. We show that known results of dynamic mechanisms with independent value settings cannot guarantee certain desirable properties in this new significantly different setting. In the future work, we propose to analyze similar settings with dynamic types and population.", "keywords": ["nash equilibrium", "incentive compatibility", "individual rationality"], "combined": "Dynamic learning-based mechanism design for dependent valued exchange economies Learning private information from multiple strategic agents poses challenge in many Internet applications. Sponsored search auctions, crowdsourcing, Amazon's mechanical turk, various online review forums are examples where we are interested in learning true values of the advertisers or true opinion of the reviewers. The common thread in these decision problems is that the optimal outcome depends on the private information of all the agents, while the decision of the outcome can be chosen only through reported information which may be manipulated by the strategic agents. The other important trait of these applications is their dynamic nature. The advertisers in an online auction or the users of mechanical turk arrive and depart, and when present, interact with the system repeatedly, giving the opportunity to learn their types. Dynamic mechanisms, which learn from the past interactions and make present decisions depending on the expected future evolution of the game, has been shown to improve performance over repeated versions of static mechanisms. In this paper, we will survey the past and current state-of-the-art dynamic mechanisms and analyze a new setting where the agents consist of buyers and sellers, known as exchange economies, and agents having value interdependency, which are relevant in applications illustrated through examples. We show that known results of dynamic mechanisms with independent value settings cannot guarantee certain desirable properties in this new significantly different setting. In the future work, we propose to analyze similar settings with dynamic types and population. [[EENNDD]] nash equilibrium; incentive compatibility; individual rationality"}, "Reka bentuk mekanisme berasaskan pembelajaran yang dinamik untuk ekonomi pertukaran yang bergantung pada penilaian. Pembelajaran maklumat peribadi dari pelbagai ejen strategik menimbulkan cabaran dalam banyak aplikasi Internet. Lelongan carian yang ditaja, sumber khidmat ramai, mesin mekanik Amazon, pelbagai forum ulasan dalam talian adalah contoh di mana kami berminat untuk mempelajari nilai sebenar pengiklan atau pendapat sebenar pengulas. Titik umum dalam masalah keputusan ini adalah bahawa hasil yang optimum bergantung pada maklumat peribadi semua agen, sementara keputusan hasilnya dapat dipilih hanya melalui informasi yang dilaporkan yang dapat dimanipulasi oleh agen strategi. Sifat penting aplikasi ini adalah sifatnya yang dinamik. Pengiklan dalam lelongan dalam talian atau pengguna turk mekanikal tiba dan berlepas, dan ketika hadir, berinteraksi dengan sistem berulang kali, memberi peluang untuk mempelajari jenisnya. Mekanisme dinamik, yang belajar dari interaksi masa lalu dan membuat keputusan sekarang bergantung pada jangkaan evolusi permainan di masa depan, telah terbukti dapat meningkatkan prestasi berbanding versi mekanisme statik yang berulang. Dalam makalah ini, kami akan meninjau mekanisme dinamik terkini dan terkini dan menganalisis persekitaran baru di mana ejen terdiri daripada pembeli dan penjual, yang dikenali sebagai ekonomi pertukaran, dan ejen yang mempunyai saling ketergantungan nilai, yang relevan dalam aplikasi digambarkan melalui contoh. Kami menunjukkan bahawa hasil mekanisme dinamik yang diketahui dengan tetapan nilai bebas tidak dapat menjamin sifat-sifat tertentu yang diinginkan dalam tetapan baru yang berbeza ini. Dalam karya yang akan datang, kami mencadangkan untuk menganalisis tetapan yang serupa dengan jenis dan populasi dinamik. [[EENNDD]] keseimbangan nash; keserasian insentif; rasional individu"], [{"string": "Detecting Wikipedia vandalism with active learning and statistical language models This paper proposes an active learning approach using language model statistics to detect Wikipedia vandalism. Wikipedia is a popular and influential collaborative information system. The collaborative nature of authoring, as well as the high visibility of its content, have exposed Wikipedia articles to vandalism. Vandalism is defined as malicious editing intended to compromise the integrity of the content of articles. Extensive manual efforts are being made to combat vandalism and an automated approach to alleviate the laborious process is needed.", "keywords": ["classification", "active learning", "wikipedia", "statistical language models", "vandalism"], "combined": "Detecting Wikipedia vandalism with active learning and statistical language models This paper proposes an active learning approach using language model statistics to detect Wikipedia vandalism. Wikipedia is a popular and influential collaborative information system. The collaborative nature of authoring, as well as the high visibility of its content, have exposed Wikipedia articles to vandalism. Vandalism is defined as malicious editing intended to compromise the integrity of the content of articles. Extensive manual efforts are being made to combat vandalism and an automated approach to alleviate the laborious process is needed. [[EENNDD]] classification; active learning; wikipedia; statistical language models; vandalism"}, "Mengesan vandalisme Wikipedia dengan pembelajaran aktif dan model bahasa statistik Makalah ini mencadangkan pendekatan pembelajaran aktif menggunakan statistik model bahasa untuk mengesan vandalisme Wikipedia. Wikipedia adalah sistem maklumat kolaboratif yang popular dan berpengaruh. Sifat kolaboratif pengarang, serta keterlihatan tinggi kandungannya, telah mendedahkan artikel Wikipedia kepada vandalisme. Vandalisme didefinisikan sebagai penyuntingan berniat jahat yang bertujuan menjejaskan integriti kandungan artikel. Usaha manual yang luas sedang dilakukan untuk memerangi vandalisme dan pendekatan automatik untuk meringankan proses yang sukar dilakukan. [[EENNDD]] klasifikasi; pembelajaran aktif; wikipedia; model bahasa statistik; vandalisme"], [{"string": "The web of things The Web, similar to other successful man made systems is continuously evolving. With the miniaturization and increased performance of computing devices which are also being embedded in common physical objects, it is natural that the Web evolved to also include these - therefore the Web of Things. This tutorial provides an overview of the system vertical structure by identifying the relevant components, illustrating their functionality and showing existing tools and systems. The aim is to show how small devices can be connected to the Web at various levels of abstraction and transform them into \"first-class\" Web residents.", "keywords": ["systems and software", "web of things", "stream-mining", "sensor", "sensor network", "machine-learning", "semantic web", "text-mining", "web-mining"], "combined": "The web of things The Web, similar to other successful man made systems is continuously evolving. With the miniaturization and increased performance of computing devices which are also being embedded in common physical objects, it is natural that the Web evolved to also include these - therefore the Web of Things. This tutorial provides an overview of the system vertical structure by identifying the relevant components, illustrating their functionality and showing existing tools and systems. The aim is to show how small devices can be connected to the Web at various levels of abstraction and transform them into \"first-class\" Web residents. [[EENNDD]] systems and software; web of things; stream-mining; sensor; sensor network; machine-learning; semantic web; text-mining; web-mining"}, "Web perkara Web, serupa dengan sistem buatan manusia yang berjaya terus berkembang. Dengan miniaturisasi dan peningkatan prestasi peranti pengkomputeran yang juga disertakan dalam objek fizikal biasa, adalah wajar bahawa Web berkembang untuk menyertakannya - oleh itu Web of Things. Tutorial ini memberikan gambaran keseluruhan struktur menegak sistem dengan mengenal pasti komponen yang berkaitan, menggambarkan fungsi mereka dan menunjukkan alat dan sistem yang ada. Tujuannya adalah untuk menunjukkan bagaimana peranti kecil dapat dihubungkan ke Web pada pelbagai tahap pengambilan dan mengubahnya menjadi penghuni Web \"kelas pertama\". [[EENNDD]] sistem dan perisian; laman web perkara; perlombongan aliran; sensor; rangkaian sensor; pembelajaran mesin; web semantik; perlombongan teks; perlombongan web"], [{"string": "A study on the impact of product images on user clicks for online shopping In this paper we study the importance of image based features on the click-through rate (CTR) in the context of a large scale product search engine. Typically product search engines use text based features in their ranking function. We present a novel idea of using image based features, common in the photography literature, in addition to text based features. We used a stochastic gradient boosting based regression model to learn relationships between features and CTR. Our results indicate statistically significant correlations between the image features and CTR. We also see improvements to NDCG and mean standard regression.", "keywords": ["product images", "shopping search", "online shopping"], "combined": "A study on the impact of product images on user clicks for online shopping In this paper we study the importance of image based features on the click-through rate (CTR) in the context of a large scale product search engine. Typically product search engines use text based features in their ranking function. We present a novel idea of using image based features, common in the photography literature, in addition to text based features. We used a stochastic gradient boosting based regression model to learn relationships between features and CTR. Our results indicate statistically significant correlations between the image features and CTR. We also see improvements to NDCG and mean standard regression. [[EENNDD]] product images; shopping search; online shopping"}, "Kajian mengenai kesan gambar produk pada klik pengguna untuk membeli-belah dalam talian Dalam makalah ini kita mengkaji kepentingan ciri berdasarkan gambar pada kadar klik-tayang (CTR) dalam konteks mesin carian produk berskala besar. Biasanya mesin carian produk menggunakan ciri berdasarkan teks dalam fungsi peringkat mereka. Kami mengemukakan idea baru untuk menggunakan ciri berdasarkan gambar, yang biasa terdapat dalam literatur fotografi, selain ciri berdasarkan teks. Kami menggunakan model regresi berasaskan peningkatan gradien stokastik untuk mempelajari hubungan antara ciri dan CTR. Hasil kajian kami menunjukkan hubungan yang signifikan secara statistik antara ciri gambar dan CTR. Kami juga melihat peningkatan pada NDCG dan min regresi standard. [[EENNDD]] gambar produk; carian membeli-belah; membeli-belah dalam talian"], [{"string": "Scaling question answering to the Web An abstract is not available.", "keywords": ["content analysis and indexing"], "combined": "Scaling question answering to the Web An abstract is not available. [[EENNDD]] content analysis and indexing"}, "Penskalaan menjawab soalan ke Web Abstrak tidak tersedia. [[EENNDD]] analisis kandungan dan pengindeksan"], [{"string": "GigaHash: scalable minimal perfect hashing for billions of urls A minimal perfect function maps a static set of n keys on to the range of integers {0,1,2,...,n - 1}. We present a scalable high performance algorithm based on random graphs for constructing minimal perfect hash functions (MPHFs). For a set of n keys, our algorithm outputs a description of h in expected time O(n). The evaluation of h(x) requires three memory accesses for any key x and the description of h takes up 0.89n bytes (7.13n bits). This is the best (most space efficient) known result to date. Using a simple heuristic and Huffman coding, the space requirement is further reduced to 0.79n bytes (6.86n bits). We present a high performance architecture that is easy to parallelize and scales well to very large data sets encountered in internet search applications. Experimental results on a one billion URL dataset obtained from Live Search crawl data, show that the proposed algorithm (a)finds an MPHF for one billion URLs in less than 4 minutes, and (b) requires only 6.86 bits/key for the description of h.", "keywords": ["web search engine", "minimal perfect hashing", "space efficient hash table", "perfect hash function"], "combined": "GigaHash: scalable minimal perfect hashing for billions of urls A minimal perfect function maps a static set of n keys on to the range of integers {0,1,2,...,n - 1}. We present a scalable high performance algorithm based on random graphs for constructing minimal perfect hash functions (MPHFs). For a set of n keys, our algorithm outputs a description of h in expected time O(n). The evaluation of h(x) requires three memory accesses for any key x and the description of h takes up 0.89n bytes (7.13n bits). This is the best (most space efficient) known result to date. Using a simple heuristic and Huffman coding, the space requirement is further reduced to 0.79n bytes (6.86n bits). We present a high performance architecture that is easy to parallelize and scales well to very large data sets encountered in internet search applications. Experimental results on a one billion URL dataset obtained from Live Search crawl data, show that the proposed algorithm (a)finds an MPHF for one billion URLs in less than 4 minutes, and (b) requires only 6.86 bits/key for the description of h. [[EENNDD]] web search engine; minimal perfect hashing; space efficient hash table; perfect hash function"}, "GigaHash: hashing minimal perfect scalable untuk berbilion url. Fungsi minimal sempurna memetakan satu set kunci n statik ke julat bilangan bulat {0,1,2, ..., n - 1}. Kami menyajikan algoritma prestasi tinggi berskala berdasarkan grafik rawak untuk membina fungsi hash minimum minimum (MPHF). Untuk sekumpulan kekunci n, algoritma kami mengeluarkan penerangan mengenai h dalam jangkaan O (n). Penilaian h (x) memerlukan tiga akses memori untuk sebarang kunci x dan perihalan h mengambil 0,89n bait (7,13n bit). Ini adalah hasil terbaik (paling cekap ruang) sehingga kini. Dengan menggunakan pengkodan heuristik dan Huffman yang sederhana, keperluan ruang dikurangkan lagi menjadi 0.79n bait (6.86n bit). Kami menyajikan seni bina berprestasi tinggi yang mudah untuk diselaraskan dan disesuaikan dengan set data yang sangat besar yang terdapat dalam aplikasi carian internet. Hasil eksperimen pada set data satu bilion URL yang diperoleh dari data perayapan Carian Langsung, menunjukkan bahawa algoritma yang dicadangkan (a) menemui MPHF untuk satu bilion URL dalam masa kurang dari 4 minit, dan (b) hanya memerlukan 6.86 bit / kunci untuk keterangan mengenai h. [[EENNDD]] enjin carian web; hashing sempurna minimum; jadual hash cekap ruang; fungsi hash yang sempurna"], [{"string": "Multispace information visualization framework for the intercomparison of data sets retrieved from web services No contact information provided yet.", "keywords": ["intelligentbox", "graphical user interfaces", "web service", "worldbottle", "interaction styles", "visualization"], "combined": "Multispace information visualization framework for the intercomparison of data sets retrieved from web services No contact information provided yet. [[EENNDD]] intelligentbox; graphical user interfaces; web service; worldbottle; interaction styles; visualization"}, "Kerangka visualisasi maklumat pelbagai ruang untuk perbandingan antara set data yang diambil dari perkhidmatan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kotak pintar; antara muka pengguna grafik; perkhidmatan web; perihal dunia; gaya interaksi; visualisasi"], [{"string": "Sentence-level contextual opinion retrieval Existing opinion retrieval techniques do not provide context-dependent relevant results. Most of the approaches used by state-of-the-art techniques are based on frequency of query terms, such that all documents containing query terms are retrieved, regardless of contextual relevance to the intent of the human seeking the opinion. However, in a particular opinionated document, words could occur in different contexts, yet meet the frequency attached to a certain opinion threshold, thus explicitly creating a bias in overall opinion retrieved. In this paper we propose a sentence-level contextual model for opinion retrieval using grammatical tree derivations and approval voting mechanism. Model evaluation performed between our contextual model, BM25, and language model shows that the model can be effective for contextual opinion retrieval such as faceted opinion retrieval.", "keywords": ["information search and retrieval", "approval voting", "contextual opinion", "grammatical tree derivation", "sentence level"], "combined": "Sentence-level contextual opinion retrieval Existing opinion retrieval techniques do not provide context-dependent relevant results. Most of the approaches used by state-of-the-art techniques are based on frequency of query terms, such that all documents containing query terms are retrieved, regardless of contextual relevance to the intent of the human seeking the opinion. However, in a particular opinionated document, words could occur in different contexts, yet meet the frequency attached to a certain opinion threshold, thus explicitly creating a bias in overall opinion retrieved. In this paper we propose a sentence-level contextual model for opinion retrieval using grammatical tree derivations and approval voting mechanism. Model evaluation performed between our contextual model, BM25, and language model shows that the model can be effective for contextual opinion retrieval such as faceted opinion retrieval. [[EENNDD]] information search and retrieval; approval voting; contextual opinion; grammatical tree derivation; sentence level"}, "Pengambilan pendapat kontekstual peringkat ayat Teknik pengambilan pendapat yang ada tidak memberikan hasil yang relevan berdasarkan konteks. Sebilangan besar pendekatan yang digunakan oleh teknik canggih didasarkan pada kekerapan istilah pertanyaan, sehingga semua dokumen yang mengandungi istilah pertanyaan diambil, tanpa mengira relevan kontekstual dengan maksud manusia mencari pendapat. Namun, dalam dokumen tertentu, kata-kata dapat terjadi dalam konteks yang berbeda, namun memenuhi frekuensi yang melekat pada ambang pendapat tertentu, sehingga secara eksplisit menciptakan bias pada keseluruhan pendapat yang diambil. Dalam makalah ini kami mencadangkan model kontekstual peringkat kalimat untuk pengambilan pendapat menggunakan derivasi pokok tatabahasa dan mekanisme pengundian persetujuan. Penilaian model yang dilakukan antara model kontekstual kami, BM25, dan model bahasa menunjukkan bahawa model tersebut dapat menjadi efektif untuk pengambilan pendapat kontekstual seperti pengambilan pendapat dari aspek. [[EENNDD]] carian dan pengambilan maklumat; pengundian kelulusan; pendapat kontekstual; terbitan pokok tatabahasa; tahap ayat"], [{"string": "Detecting near-duplicates for web crawling Near-duplicate web documents are abundant. Two such documents differ from each other in a very small portion that displays advertisements, for example. Such differences are irrelevant for web search. So the quality of a web crawler increases if it can assess whether a newly crawled web page is a near-duplicate of a previously crawled web page or not. In the course of developing a near-duplicate detection system for a multi-billion page repository, we make two research contributions. First, we demonstrate that Charikar's fingerprinting technique is appropriate for this goal. Second, we present an algorithmic technique for identifying existing f-bit fingerprints that differ from a given fingerprint in at most k bit-positions, for small k. Our technique is useful for both online queries (single fingerprints) and all batch queries (multiple fingerprints). Experimental evaluation over real data confirms the practicality of our design.", "keywords": ["hamming distance", "sketch", "similarity", "web crawl", "near-duplicate", "fingerprint", "search", "web document"], "combined": "Detecting near-duplicates for web crawling Near-duplicate web documents are abundant. Two such documents differ from each other in a very small portion that displays advertisements, for example. Such differences are irrelevant for web search. So the quality of a web crawler increases if it can assess whether a newly crawled web page is a near-duplicate of a previously crawled web page or not. In the course of developing a near-duplicate detection system for a multi-billion page repository, we make two research contributions. First, we demonstrate that Charikar's fingerprinting technique is appropriate for this goal. Second, we present an algorithmic technique for identifying existing f-bit fingerprints that differ from a given fingerprint in at most k bit-positions, for small k. Our technique is useful for both online queries (single fingerprints) and all batch queries (multiple fingerprints). Experimental evaluation over real data confirms the practicality of our design. [[EENNDD]] hamming distance; sketch; similarity; web crawl; near-duplicate; fingerprint; search; web document"}, "Mengesan pendua hampir untuk merangkak web Dokumen web hampir pendua banyak. Dua dokumen seperti itu berbeza antara satu sama lain dalam bahagian yang sangat kecil yang memaparkan iklan, misalnya. Perbezaan seperti itu tidak relevan untuk carian web. Oleh itu, kualiti perayap web meningkat jika dapat menilai sama ada laman web yang baru dirangkak adalah hampir pendua dari laman web yang dirayapi sebelumnya atau tidak. Dalam usaha mengembangkan sistem pengesanan hampir pendua untuk repositori halaman berbilion-bilion, kami membuat dua sumbangan penyelidikan. Pertama, kami menunjukkan bahawa teknik cap jari Charikar sesuai untuk tujuan ini. Kedua, kami menyajikan teknik algoritma untuk mengenal pasti cap jari f-bit yang ada yang berbeza dengan cap jari tertentu pada kebanyakan kedudukan bit k, untuk k kecil. Teknik kami berguna untuk kedua-dua pertanyaan dalam talian (cap jari tunggal) dan semua pertanyaan kumpulan (beberapa cap jari). Penilaian eksperimental terhadap data sebenar mengesahkan kepraktisan reka bentuk kami. [[EENNDD]] jarak memukul; lakaran; persamaan; merangkak web; hampir pendua; cap jari; cari; dokumen web"], [{"string": "Improving web performance by client characterization driven server adaptation No contact information provided yet.", "keywords": ["client characterization", "client connectivity", "server adaptation", "network protocols"], "combined": "Improving web performance by client characterization driven server adaptation No contact information provided yet. [[EENNDD]] client characterization; client connectivity; server adaptation; network protocols"}, "Meningkatkan prestasi web dengan penyesuaian pelayan yang didorong oleh ciri pelanggan Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] pencirian pelanggan; kesambungan pelanggan; penyesuaian pelayan; protokol rangkaian"], [{"string": "A probabilistic approach to automated bidding in alternative auctions No contact information provided yet.", "keywords": ["miscellaneous"], "combined": "A probabilistic approach to automated bidding in alternative auctions No contact information provided yet. [[EENNDD]] miscellaneous"}, "Pendekatan probabilistik untuk pembidaan automatik dalam lelongan alternatif Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pelbagai"], [{"string": "Visual web mining No contact information provided yet.", "keywords": ["systems and software", "information visualization", "web usage mining", "frequent access patterns", "visual data exploration"], "combined": "Visual web mining No contact information provided yet. [[EENNDD]] systems and software; information visualization; web usage mining; frequent access patterns; visual data exploration"}, "Perlombongan web visual Belum ada maklumat hubungan yang diberikan. [[EENNDD]] sistem dan perisian; visualisasi maklumat; perlombongan penggunaan web; corak akses yang kerap; penerokaan data visual"], [{"string": "Web montage: a dynamic personalized start page No contact information provided yet.", "keywords": ["adaptive web sites", "adaptive user interfaces", "user modeling", "personalization"], "combined": "Web montage: a dynamic personalized start page No contact information provided yet. [[EENNDD]] adaptive web sites; adaptive user interfaces; user modeling; personalization"}, "Montaj web: halaman permulaan diperibadikan yang dinamik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] laman web penyesuaian; antara muka pengguna yang adaptif; pemodelan pengguna; pemperibadian"], [{"string": "Executing incoherency bounded continuous queries at web data aggregators No contact information provided yet.", "keywords": ["online decision making", "continuous queries", "markov model", "fidelity", "coherency"], "combined": "Executing incoherency bounded continuous queries at web data aggregators No contact information provided yet. [[EENNDD]] online decision making; continuous queries; markov model; fidelity; coherency"}, "Melaksanakan pertanyaan berterusan yang disekat oleh ketidakkekalan di agregator data web Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] membuat keputusan dalam talian; pertanyaan berterusan; model markov; kesetiaan; koheren"], [{"string": "Using urls and table layout for web classification tasks No contact information provided yet.", "keywords": ["news recommendation", "tree structures", "classification", "models and principles", "web applications"], "combined": "Using urls and table layout for web classification tasks No contact information provided yet. [[EENNDD]] news recommendation; tree structures; classification; models and principles; web applications"}, "Menggunakan url dan susun atur jadual untuk tugas klasifikasi web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] cadangan berita; struktur pokok; pengelasan; model dan prinsip; aplikasi web"], [{"string": "A general framework for adaptive and online detection of web attacks Detection of web attacks is an important issue in current defense-in-depth security framework. In this paper, we propose a novel general framework for adaptive and online detection of web attacks. The general framework can be based on any online clustering methods. A detection model based on the framework is able to learn online and deal with \"concept drift\" in web audit data streams. Str-DBSCAN that we extended DBSCAN to streaming data as well as StrAP are both used to validate the framework. The detection model based on the framework automatically labels the web audit data and adapts to normal behavior changes while identifies attacks through dynamical clustering of the streaming data. A very large size of real HTTP Log data collected in our institute is used to validate the framework and the model. The preliminary testing results demonstrated its effectiveness.", "keywords": ["anomaly detection", "intrusion detection", "security and protection", "clustering"], "combined": "A general framework for adaptive and online detection of web attacks Detection of web attacks is an important issue in current defense-in-depth security framework. In this paper, we propose a novel general framework for adaptive and online detection of web attacks. The general framework can be based on any online clustering methods. A detection model based on the framework is able to learn online and deal with \"concept drift\" in web audit data streams. Str-DBSCAN that we extended DBSCAN to streaming data as well as StrAP are both used to validate the framework. The detection model based on the framework automatically labels the web audit data and adapts to normal behavior changes while identifies attacks through dynamical clustering of the streaming data. A very large size of real HTTP Log data collected in our institute is used to validate the framework and the model. The preliminary testing results demonstrated its effectiveness. [[EENNDD]] anomaly detection; intrusion detection; security and protection; clustering"}, "Rangka kerja umum untuk pengesanan serangan web yang adaptif dan dalam talian Pengesanan serangan web adalah isu penting dalam kerangka keselamatan pertahanan mendalam. Dalam makalah ini, kami mencadangkan kerangka umum baru untuk pengesanan serangan web secara adaptif dan dalam talian. Kerangka umum boleh didasarkan pada kaedah pengelompokan dalam talian. Model pengesanan berdasarkan kerangka kerja dapat belajar dalam talian dan menangani \"konsep drift\" dalam aliran data audit web. Str-DBSCAN yang kami panjangkan DBSCAN ke streaming data dan juga StrAP digunakan untuk mengesahkan kerangka kerja. Model pengesanan berdasarkan kerangka secara otomatis melabel data audit web dan menyesuaikan diri dengan perubahan tingkah laku normal sambil mengidentifikasi serangan melalui pengelompokan data streaming secara dinamis. Ukuran data HTTP Log yang sangat besar yang dikumpulkan di institut kami digunakan untuk mengesahkan kerangka dan modelnya. Hasil ujian awal menunjukkan keberkesanannya. [[EENNDD]] pengesanan anomali; pengesanan pencerobohan; keselamatan dan perlindungan; pengelompokan"], [{"string": "Joint relevance and freshness learning from clickthroughs for news search In contrast to traditional Web search, where topical relevance is often the main selection criterion, news search is characterized by the increased importance of freshness. However, the estimation of relevance and freshness, and especially the relative importance of these two aspects, are highly specific to the query and the time when the query was issued. In this work, we propose a unified framework for modeling the topical relevance and freshness, as well as their relative importance, based on click logs. We use click statistics and content analysis techniques to define a set of temporal features, which predict the right mix of freshness and relevance for a given query. Experimental results on both historical click data and editorial judgments demonstrate the effectiveness of the proposed approach.", "keywords": ["learning to rank", "relevance and freshness modeling", "temporal features"], "combined": "Joint relevance and freshness learning from clickthroughs for news search In contrast to traditional Web search, where topical relevance is often the main selection criterion, news search is characterized by the increased importance of freshness. However, the estimation of relevance and freshness, and especially the relative importance of these two aspects, are highly specific to the query and the time when the query was issued. In this work, we propose a unified framework for modeling the topical relevance and freshness, as well as their relative importance, based on click logs. We use click statistics and content analysis techniques to define a set of temporal features, which predict the right mix of freshness and relevance for a given query. Experimental results on both historical click data and editorial judgments demonstrate the effectiveness of the proposed approach. [[EENNDD]] learning to rank; relevance and freshness modeling; temporal features"}, "Pembelajaran relevansi dan kesegaran bersama dari klik-tayang untuk pencarian berita Berbeza dengan carian Web tradisional, di mana relevansi topikal sering menjadi kriteria pemilihan utama, carian berita dicirikan oleh peningkatan kepentingan kesegaran. Walau bagaimanapun, anggaran kesesuaian dan kesegaran, dan terutama kepentingan relatif kedua aspek ini, sangat spesifik untuk pertanyaan dan masa ketika pertanyaan dikeluarkan. Dalam karya ini, kami mencadangkan kerangka kerja terpadu untuk memodelkan relevansi dan kesegaran topikal, serta kepentingan relatifnya, berdasarkan log klik. Kami menggunakan statistik klik dan teknik analisis kandungan untuk menentukan satu set ciri temporal, yang meramalkan gabungan kesegaran dan relevan yang tepat untuk pertanyaan tertentu. Hasil eksperimen pada data klik sejarah dan penilaian editorial menunjukkan keberkesanan pendekatan yang dicadangkan. [[EENNDD]] belajar berpangkat; pemodelan kesesuaian dan kesegaran; ciri temporal"], [{"string": "Randomization tests for distinguishing social influence and homophily effects Relational autocorrelation is ubiquitous in relational domains. This observed correlation between class labels of linked instances in a network (e.g., two friends are more likely to share political beliefs than two randomly selected people) can be due to the effects of two different social processes. If social influence effects are present, instances are likely to change their attributes to conform to their neighbor values. If homophily effects are present, instances are likely to link to other individuals with similar attribute values. Both these effects will result in autocorrelated attribute values. When analyzing static relational networks it is impossible to determine how much of the observed correlation is due each of these factors. However, the recent surge of interest in social networks has increased the availability of dynamic network data. In this paper, we present a randomization technique for temporal network data where the attributes and links change over time. Given data from two time steps, we measure the gain in correlation and assess whether a significant portion of this gain is due to influence and/or homophily. We demonstrate the efficacy of our method on semi-synthetic data and then apply the method to a real-world social networks dataset, showing the impact of both influence and homophily effects.", "keywords": ["homophily", "randomization", "miscellaneous", "social networks", "social influence"], "combined": "Randomization tests for distinguishing social influence and homophily effects Relational autocorrelation is ubiquitous in relational domains. This observed correlation between class labels of linked instances in a network (e.g., two friends are more likely to share political beliefs than two randomly selected people) can be due to the effects of two different social processes. If social influence effects are present, instances are likely to change their attributes to conform to their neighbor values. If homophily effects are present, instances are likely to link to other individuals with similar attribute values. Both these effects will result in autocorrelated attribute values. When analyzing static relational networks it is impossible to determine how much of the observed correlation is due each of these factors. However, the recent surge of interest in social networks has increased the availability of dynamic network data. In this paper, we present a randomization technique for temporal network data where the attributes and links change over time. Given data from two time steps, we measure the gain in correlation and assess whether a significant portion of this gain is due to influence and/or homophily. We demonstrate the efficacy of our method on semi-synthetic data and then apply the method to a real-world social networks dataset, showing the impact of both influence and homophily effects. [[EENNDD]] homophily; randomization; miscellaneous; social networks; social influence"}, "Ujian rawak untuk membezakan pengaruh sosial dan kesan homofili Autokorelasi hubungan ada di mana-mana dalam domain hubungan. Ini hubungan yang diamati antara label kelas kejadian yang dihubungkan dalam rangkaian (mis., Dua rakan lebih cenderung berkongsi kepercayaan politik daripada dua orang yang dipilih secara rawak) disebabkan oleh kesan dari dua proses sosial yang berbeza. Sekiranya terdapat kesan pengaruh sosial, keadaan cenderung mengubah sifatnya agar sesuai dengan nilai tetangga mereka. Sekiranya terdapat kesan homofilia, kejadian cenderung menghubungkan ke individu lain dengan nilai atribut yang serupa. Kedua-dua kesan ini akan menghasilkan nilai atribut yang berkorelasi secara automatik. Semasa menganalisis rangkaian hubungan statik, mustahil untuk menentukan berapa banyak korelasi yang diperhatikan disebabkan oleh setiap faktor ini. Walau bagaimanapun, lonjakan minat baru-baru ini dalam rangkaian sosial telah meningkatkan ketersediaan data rangkaian dinamik. Dalam makalah ini, kami menyajikan teknik pengacakan untuk data jaringan temporal di mana atribut dan pautan berubah dari masa ke masa. Memandangkan data dari dua langkah waktu, kami mengukur kenaikan dalam korelasi dan menilai apakah sebahagian besar keuntungan ini disebabkan oleh pengaruh dan / atau homofili. Kami menunjukkan keberkesanan kaedah kami pada data separa sintetik dan kemudian menerapkan kaedah tersebut ke set data rangkaian sosial dunia nyata, yang menunjukkan kesan pengaruh dan kesan homofili. [[EENNDD]] homofili; rawak; pelbagai; rangkaian sosial; pengaruh sosial"], [{"string": "Compiling XSLT 2.0 into XQuery 1.0 No contact information provided yet.", "keywords": ["xquery", "xslt", "software engineering", "xml", "web services"], "combined": "Compiling XSLT 2.0 into XQuery 1.0 No contact information provided yet. [[EENNDD]] xquery; xslt; software engineering; xml; web services"}, "Menyusun XSLT 2.0 ke XQuery 1.0 Belum ada maklumat hubungan yang diberikan. [[EENNDD]] xquery; xslt; Kejuruteraan perisian; xml; perkhidmatan web"], [{"string": "Efficient resource allocation and power saving in multi-tiered systems In this paper, we present Fastrack, a parameter-free algorithm for dynamic resource provisioning that uses simple statistics to promptly distill information about changes in workload burstiness. This information, coupled with the application's end-to-end response times and system bottleneck characteristics, guide resource allocation that shows to be very effective under a broad variety of burstiness profiles and bottleneck scenarios.", "keywords": ["multi-tiered systems", "burstiness", "resource allocation", "performance evaluation"], "combined": "Efficient resource allocation and power saving in multi-tiered systems In this paper, we present Fastrack, a parameter-free algorithm for dynamic resource provisioning that uses simple statistics to promptly distill information about changes in workload burstiness. This information, coupled with the application's end-to-end response times and system bottleneck characteristics, guide resource allocation that shows to be very effective under a broad variety of burstiness profiles and bottleneck scenarios. [[EENNDD]] multi-tiered systems; burstiness; resource allocation; performance evaluation"}, "Peruntukan sumber yang cekap dan penjimatan kuasa dalam sistem berjenjang Dalam makalah ini, kami menyajikan Fastrack, algoritma bebas parameter untuk penyediaan sumber dinamik yang menggunakan statistik sederhana untuk menyaring maklumat mengenai perubahan beban kerja dengan cepat. Maklumat ini, ditambah dengan masa tindak balas akhir-ke-akhir aplikasi dan ciri-ciri kemacetan sistem, membimbing peruntukan sumber yang terbukti sangat berkesan di bawah pelbagai profil kesukaran dan senario kemacetan. [[EENNDD]] sistem pelbagai peringkat; pecah; peruntukan sumber; penilaian prestasi"], [{"string": "Unsupervised query segmentation using only query logs We introduce an unsupervised query segmentation scheme that uses query logs as the only resource and can effectively capture the structural units in queries. We believe that Web search queries have a unique syntactic structure which is distinct from that of English or a bag-of-words model. The segments discovered by our scheme help understand this underlying grammatical structure. We apply a statistical model based on Hoeffding's Inequality to mine significant word n-grams from queries and subsequently use them for segmenting the queries. Evaluation against manually segmented queries shows that this technique can detect rare units that are missed by our Pointwise Mutual Information (PMI) baseline.", "keywords": ["query grammar", "query structure", "hoeffding's inequality", "unsupervised query segmentation"], "combined": "Unsupervised query segmentation using only query logs We introduce an unsupervised query segmentation scheme that uses query logs as the only resource and can effectively capture the structural units in queries. We believe that Web search queries have a unique syntactic structure which is distinct from that of English or a bag-of-words model. The segments discovered by our scheme help understand this underlying grammatical structure. We apply a statistical model based on Hoeffding's Inequality to mine significant word n-grams from queries and subsequently use them for segmenting the queries. Evaluation against manually segmented queries shows that this technique can detect rare units that are missed by our Pointwise Mutual Information (PMI) baseline. [[EENNDD]] query grammar; query structure; hoeffding's inequality; unsupervised query segmentation"}, "Segmentasi pertanyaan tanpa pengawasan hanya menggunakan log pertanyaan Kami memperkenalkan skema segmentasi pertanyaan tanpa pengawasan yang menggunakan log pertanyaan sebagai satu-satunya sumber dan dapat menangkap unit struktur dalam pertanyaan dengan berkesan. Kami percaya bahawa pertanyaan carian Web mempunyai struktur sintaksis yang unik yang berbeza dengan bahasa Inggeris atau model beg-of-word. Segmen yang ditemui oleh skema kami membantu memahami struktur tatabahasa yang mendasari ini. Kami mengaplikasikan model statistik berdasarkan Ketidakseimbangan Hoeffding untuk menambang n-gram kata penting dari pertanyaan dan seterusnya menggunakannya untuk menyegmentasikan pertanyaan. Penilaian terhadap pertanyaan yang disegmentasikan secara manual menunjukkan bahawa teknik ini dapat mengesan unit-unit jarang yang terlepas dari garis dasar Pointwise Mutual Information (PMI) kami. [[EENNDD]] tatabahasa pertanyaan; struktur pertanyaan; ketidaksamaan cacat; segmentasi pertanyaan yang tidak diselia"], [{"string": "The freshman handbook: a hint for the server placement of social networks There has been a recent unprecedented increase in the use of Online Social Networks (OSNs) to expand our social life, exchange information and share common interests. Many popular OSNs today attract hundreds of millions of users who share tremendous amount of data on it such as Facebook, Twitter, and Buzz. Given the huge business opportunities OSNs may bring, more and more new social applications has emerged on the Internet. For these newcomers in the social network business, one of the first key decisions to make is to where to deploy the computational resources to best accommodate future client requests. In this work, we aim at providing useful suggests to the new born social network providers (freshman) on the intelligent server placement, by exploring available public information from existing social network communities. In this work, we first propose three scalable server placement strategies for OSNs. Our solution can scalably select server locations among all the possible locations, at the same time reducing the cost for inter-user data sharing.", "keywords": ["social network"], "combined": "The freshman handbook: a hint for the server placement of social networks There has been a recent unprecedented increase in the use of Online Social Networks (OSNs) to expand our social life, exchange information and share common interests. Many popular OSNs today attract hundreds of millions of users who share tremendous amount of data on it such as Facebook, Twitter, and Buzz. Given the huge business opportunities OSNs may bring, more and more new social applications has emerged on the Internet. For these newcomers in the social network business, one of the first key decisions to make is to where to deploy the computational resources to best accommodate future client requests. In this work, we aim at providing useful suggests to the new born social network providers (freshman) on the intelligent server placement, by exploring available public information from existing social network communities. In this work, we first propose three scalable server placement strategies for OSNs. Our solution can scalably select server locations among all the possible locations, at the same time reducing the cost for inter-user data sharing. [[EENNDD]] social network"}, "Buku panduan pelajar baru: petunjuk untuk pelayan rangkaian sosial Terdapat peningkatan baru-baru ini dalam penggunaan Rangkaian Sosial Dalam Talian (OSN) untuk memperluas kehidupan sosial kita, bertukar maklumat dan berkongsi kepentingan bersama. Banyak OSN yang popular hari ini menarik ratusan juta pengguna yang berkongsi sejumlah besar data di dalamnya seperti Facebook, Twitter, dan Buzz. Memandangkan peluang perniagaan yang besar yang boleh dibawa oleh OSN, semakin banyak aplikasi sosial baru telah muncul di Internet. Bagi pendatang baru ini dalam perniagaan rangkaian sosial, salah satu keputusan utama yang harus dibuat adalah ke mana menggunakan sumber komputasi untuk menampung permintaan pelanggan masa depan dengan sebaik-baiknya. Dalam karya ini, kami bertujuan memberikan cadangan berguna kepada penyedia rangkaian sosial (mahasiswa baru) yang baru dilahirkan mengenai penempatan pelayan pintar, dengan meneroka maklumat awam yang tersedia dari komuniti rangkaian sosial yang ada. Dalam karya ini, pertama-tama kami mencadangkan tiga strategi penempatan pelayan berskala untuk OSN. Penyelesaian kami dapat memilih lokasi pelayan secara berskala di antara semua lokasi yang mungkin, sekaligus mengurangkan kos untuk perkongsian data antara pengguna. [[EENNDD]] rangkaian sosial"], [{"string": "Engineering server-driven consistency for large scale dynamic Web services An abstract is not available.", "keywords": ["scalability", "network protocols", "dynamic content", "volume lease", "web cache consistency"], "combined": "Engineering server-driven consistency for large scale dynamic Web services An abstract is not available. [[EENNDD]] scalability; network protocols; dynamic content; volume lease; web cache consistency"}, "Konsistensi berasaskan pelayan kejuruteraan untuk perkhidmatan Web dinamik skala besar Abstrak tidak tersedia. [[EENNDD]] skalabiliti; protokol rangkaian; kandungan dinamik; pajakan jumlah; konsistensi cache web"], [{"string": "Analyzing client interactivity in streaming media No contact information provided yet.", "keywords": ["general", "streaming media", "workload characterization"], "combined": "Analyzing client interactivity in streaming media No contact information provided yet. [[EENNDD]] general; streaming media; workload characterization"}, "Menganalisis interaktiviti klien dalam media penstriman Belum ada maklumat hubungan yang diberikan. [[EENNDD]] umum; media penstriman; pencirian beban kerja"], [{"string": "SCAD: collective discovery of attribute values Search engines today offer a rich user experience, no longer restricted to \"ten blue links\". For example, the query \"Canon EOS Digital Camera\" returns a photo of the digital camera, and a list of suitable merchants and prices. Similar results are offered in other domains like food, entertainment, travel, etc. All these experiences are fueled by the availability of structured data about the entities of interest.", "keywords": ["attribute discovery", "commerce search", "weak supervision", "integer linear program", "information search and retrieval", "natural language processing", "collective information extraction"], "combined": "SCAD: collective discovery of attribute values Search engines today offer a rich user experience, no longer restricted to \"ten blue links\". For example, the query \"Canon EOS Digital Camera\" returns a photo of the digital camera, and a list of suitable merchants and prices. Similar results are offered in other domains like food, entertainment, travel, etc. All these experiences are fueled by the availability of structured data about the entities of interest. [[EENNDD]] attribute discovery; commerce search; weak supervision; integer linear program; information search and retrieval; natural language processing; collective information extraction"}, "SCAD: penemuan kolektif nilai atribut Enjin carian hari ini menawarkan pengalaman pengguna yang kaya, tidak lagi terhad kepada \"sepuluh pautan biru\". Sebagai contoh, pertanyaan \"Canon EOS Digital Camera\" mengembalikan foto kamera digital, dan senarai peniaga dan harga yang sesuai. Hasil serupa ditawarkan di domain lain seperti makanan, hiburan, perjalanan, dan lain-lain. Semua pengalaman ini didorong oleh ketersediaan data berstruktur mengenai entiti yang diminati. [[EENNDD]] penemuan atribut; carian perdagangan; pengawasan yang lemah; program linear integer; pencarian dan pengambilan maklumat; pemprosesan bahasa semula jadi; pengekstrakan maklumat kolektif"], [{"string": "Law-governed peer-to-peer auctions No contact information provided yet.", "keywords": ["distributed enforcement", "online auctions", "law governed interaction", "software architectures", "distributed systems"], "combined": "Law-governed peer-to-peer auctions No contact information provided yet. [[EENNDD]] distributed enforcement; online auctions; law governed interaction; software architectures; distributed systems"}, "Lelong peer-to-peer yang diatur oleh undang-undang Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penguatkuasaan yang diedarkan; lelong dalam talian; undang-undang yang mengatur interaksi; seni bina perisian; sistem yang diedarkan"], [{"string": "Exploiting the deep web with DynaBot: matching, probing, and ranking No contact information provided yet.", "keywords": ["probing", "service class", "miscellaneous", "crawling", "deep web"], "combined": "Exploiting the deep web with DynaBot: matching, probing, and ranking No contact information provided yet. [[EENNDD]] probing; service class; miscellaneous; crawling; deep web"}, "Mengeksploitasi laman web dalam dengan DynaBot: pemadanan, penyiasatan, dan kedudukan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] menyiasat; kelas perkhidmatan; pelbagai; merangkak; web dalam"], [{"string": "The infocious web search engine: improving web searching through linguistic analysis No contact information provided yet.", "keywords": ["part-of-speech tagging", "indexing", "digital libraries", "web searching", "information retrieval", "content analysis and indexing", "information search and retrieval", "natural language processing", "web search engine", "phrase identification", "concept extraction", "language analysis", "linguistic analysis of web text", "word sense disambiguation", "crawling"], "combined": "The infocious web search engine: improving web searching through linguistic analysis No contact information provided yet. [[EENNDD]] part-of-speech tagging; indexing; digital libraries; web searching; information retrieval; content analysis and indexing; information search and retrieval; natural language processing; web search engine; phrase identification; concept extraction; language analysis; linguistic analysis of web text; word sense disambiguation; crawling"}, "Enjin carian web yang tidak dikenali: meningkatkan carian web melalui analisis linguistik Belum ada maklumat hubungan yang disediakan. [[EENNDD]] penandaan bahagian ucapan; pengindeksan; perpustakaan digital; carian web; pengambilan maklumat; analisis kandungan dan pengindeksan; pencarian dan pengambilan maklumat; pemprosesan bahasa semula jadi; enjin carian web; pengenalan frasa; pengekstrakan konsep; analisis bahasa; analisis linguistik teks web; disambiguasi rasa kata; merangkak"], [{"string": "Efficient diversification of search results using query logs We study the problem of diversifying search results by exploiting the knowledge mined from query logs. Our proposal exploits the presence of different \"specializations\" of queries in query logs to detect the submission of ambiguous/faceted queries, and manage them by diversifying the search results returned in order to cover the different possible interpretations of the query. We present an original formulation of the results diversification problem in terms of an objective function to be maximized that admits the finding of an optimal solution in linear time.", "keywords": ["search results diversification", "query log analysis"], "combined": "Efficient diversification of search results using query logs We study the problem of diversifying search results by exploiting the knowledge mined from query logs. Our proposal exploits the presence of different \"specializations\" of queries in query logs to detect the submission of ambiguous/faceted queries, and manage them by diversifying the search results returned in order to cover the different possible interpretations of the query. We present an original formulation of the results diversification problem in terms of an objective function to be maximized that admits the finding of an optimal solution in linear time. [[EENNDD]] search results diversification; query log analysis"}, "Kepelbagaian hasil carian yang cekap menggunakan log pertanyaan Kami mengkaji masalah mempelbagaikan hasil carian dengan memanfaatkan pengetahuan yang ditambang dari log pertanyaan. Cadangan kami mengeksploitasi kehadiran \"pengkhususan\" yang berbeza dalam log pertanyaan untuk mengesan penyerahan pertanyaan yang tidak jelas / bermasalah, dan menguruskannya dengan mempelbagaikan hasil carian yang dikembalikan untuk menutup kemungkinan penafsiran yang berbeza dari pertanyaan tersebut. Kami menyajikan rumusan asli dari masalah kepelbagaian hasil dari segi fungsi objektif yang akan dimaksimumkan yang mengakui penemuan penyelesaian yang optimum dalam waktu linear. [[EENNDD]] kepelbagaian hasil carian; analisis log pertanyaan"], [{"string": "Differences in the mechanics of information diffusion across topics: idioms, political hashtags, and complex contagion on twitter There is a widespread intuitive sense that different kinds of information spread differently on-line, but it has been difficult to evaluate this question quantitatively since it requires a setting where many different kinds of information spread in a shared environment. Here we study this issue on Twitter, analyzing the ways in which tokens known as hashtags spread on a network defined by the interactions among Twitter users. We find significant variation in the ways that widely-used hashtags on different topics spread.", "keywords": ["miscellaneous", "social media", "social contagion", "information diffusion"], "combined": "Differences in the mechanics of information diffusion across topics: idioms, political hashtags, and complex contagion on twitter There is a widespread intuitive sense that different kinds of information spread differently on-line, but it has been difficult to evaluate this question quantitatively since it requires a setting where many different kinds of information spread in a shared environment. Here we study this issue on Twitter, analyzing the ways in which tokens known as hashtags spread on a network defined by the interactions among Twitter users. We find significant variation in the ways that widely-used hashtags on different topics spread. [[EENNDD]] miscellaneous; social media; social contagion; information diffusion"}, "Perbezaan mekanisme penyebaran maklumat antara topik: simpulan bahasa, hashtag politik, dan penularan yang kompleks di twitter Terdapat pengertian intuitif yang meluas bahawa pelbagai jenis maklumat tersebar secara dalam talian, tetapi sukar untuk menilai soalan ini secara kuantitatif kerana memerlukan suasana di mana pelbagai jenis maklumat tersebar di persekitaran bersama. Di sini kami mengkaji masalah ini di Twitter, menganalisis cara-cara di mana token yang dikenali sebagai hashtag tersebar di rangkaian yang ditentukan oleh interaksi antara pengguna Twitter. Kami menemui variasi yang ketara dalam cara penyebaran hashtag yang digunakan secara meluas pada topik yang berbeza. [[EENNDD]] pelbagai; media sosial; penularan sosial; penyebaran maklumat"], [{"string": "Verifying genre-based clustering approach to content extraction No contact information provided yet.", "keywords": ["context", "accessibility", "html", "content extraction", "reformatting", "speech rendering", "electronic publishing", "website classification", "clustering"], "combined": "Verifying genre-based clustering approach to content extraction No contact information provided yet. [[EENNDD]] context; accessibility; html; content extraction; reformatting; speech rendering; electronic publishing; website classification; clustering"}, "Mengesahkan pendekatan pengelompokan berasaskan genre untuk pengekstrakan kandungan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] konteks; kebolehcapaian; html; pengekstrakan kandungan; memformat semula; penyampaian ucapan; penerbitan elektronik; pengelasan laman web; pengelompokan"], [{"string": "Site level noise removal for search engines No contact information provided yet.", "keywords": ["pagerank", "information search and retrieval", "spam", "link analysis", "noise reduction"], "combined": "Site level noise removal for search engines No contact information provided yet. [[EENNDD]] pagerank; information search and retrieval; spam; link analysis; noise reduction"}, "Penghapusan kebisingan tahap tapak untuk mesin pencari Belum ada maklumat hubungan yang diberikan [[EENNDD]] pagerank; pencarian dan pengambilan maklumat; spam; analisis pautan; pengurangan bunyi"], [{"string": "An XPath-based preference language for P3P No contact information provided yet.", "keywords": ["p3p", "preference", "xpath", "appel", "xpref", "privacy-aware data management", "hippocratic databases"], "combined": "An XPath-based preference language for P3P No contact information provided yet. [[EENNDD]] p3p; preference; xpath; appel; xpref; privacy-aware data management; hippocratic databases"}, "Bahasa pilihan berasaskan XPath untuk P3P Belum ada maklumat hubungan yang diberikan. [[EENNDD]] p3p; pilihan; xpath; appel; xpref; pengurusan data yang peka terhadap privasi; pangkalan data hipokratik"], [{"string": "On the bursty evolution of blogspace No contact information provided yet.", "keywords": ["miscellaneous"], "combined": "On the bursty evolution of blogspace No contact information provided yet. [[EENNDD]] miscellaneous"}, "Mengenai evolusi meluas ruang blog Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pelbagai"], [{"string": "Lock-free consistency control for web 2.0 applications Online collaboration and sharing is the central theme of many web-based services that create the so-called Web 2.0 phenomena. Using the Internet as a computing platform, many Web 2.0 applications set up mirror sites to provide large-scale availability and to achieve load balance. However, in the age of Web 2.0, where every user is also a writer and publisher, the deployment of mirror sites makes consistency maintenance a Web scale problem. Traditional concurrency control methods (e.g. two phase lock, serialization, etc.) are not up to the task for several reasons. First, large network latency between mirror sites will make two phase locking a throughput bottleneck. Second, locking will block a large portion of concurrent operations, which makes it impossible to provide large-scale availability. On the other hand, most Web 2.0 operations do not need strict serializability - it is not the intention of a user who is correcting a typo in a shared document to block another who is adding a comment, as long as consistency can still be achieved. Thus, in order to enable maximal online collaboration and sharing, we need a lock-free mechanism that can maintain consistency among mirror sites on the Web. In this paper, we propose a flexible and efficient method to achieve consistency maintenance in the Web 2.0 world. Our experiments show its good performance improvement compared with existing methods based on distributed lock.", "keywords": ["concurrency control", "consistency maintenance", "xml"], "combined": "Lock-free consistency control for web 2.0 applications Online collaboration and sharing is the central theme of many web-based services that create the so-called Web 2.0 phenomena. Using the Internet as a computing platform, many Web 2.0 applications set up mirror sites to provide large-scale availability and to achieve load balance. However, in the age of Web 2.0, where every user is also a writer and publisher, the deployment of mirror sites makes consistency maintenance a Web scale problem. Traditional concurrency control methods (e.g. two phase lock, serialization, etc.) are not up to the task for several reasons. First, large network latency between mirror sites will make two phase locking a throughput bottleneck. Second, locking will block a large portion of concurrent operations, which makes it impossible to provide large-scale availability. On the other hand, most Web 2.0 operations do not need strict serializability - it is not the intention of a user who is correcting a typo in a shared document to block another who is adding a comment, as long as consistency can still be achieved. Thus, in order to enable maximal online collaboration and sharing, we need a lock-free mechanism that can maintain consistency among mirror sites on the Web. In this paper, we propose a flexible and efficient method to achieve consistency maintenance in the Web 2.0 world. Our experiments show its good performance improvement compared with existing methods based on distributed lock. [[EENNDD]] concurrency control; consistency maintenance; xml"}, "Kawalan konsistensi bebas kunci untuk aplikasi web 2.0 Kolaborasi dan perkongsian dalam talian adalah tema utama banyak perkhidmatan berasaskan web yang mencipta apa yang disebut fenomena Web 2.0. Menggunakan Internet sebagai platform pengkomputeran, banyak aplikasi Web 2.0 menyiapkan laman cermin untuk menyediakan ketersediaan skala besar dan untuk mencapai keseimbangan beban. Namun, pada usia Web 2.0, di mana setiap pengguna juga merupakan penulis dan penerbit, penyebaran laman cermin menjadikan penyelenggaraan konsistensi sebagai masalah skala Web. Kaedah kawalan serentak tradisional (mis. Kunci dua fasa, siriisasi, dan lain-lain) tidak sesuai dengan tugas kerana beberapa sebab. Pertama, latensi rangkaian yang besar antara laman cermin akan menjadikan dua fasa mengunci kemacetan throughput. Kedua, penguncian akan menyekat sebahagian besar operasi serentak, yang menjadikannya mustahil untuk menyediakan ketersediaan skala besar. Sebaliknya, kebanyakan operasi Web 2.0 tidak memerlukan kemampuan bersiri yang ketat - bukan niat pengguna yang memperbetulkan kesalahan ketik dalam dokumen bersama untuk menyekat orang lain yang menambahkan komen, selagi konsistensi masih dapat dicapai. Oleh itu, untuk membolehkan kolaborasi dan perkongsian dalam talian secara maksimum, kami memerlukan mekanisme bebas kunci yang dapat mengekalkan konsistensi di antara laman web cermin di Web. Dalam makalah ini, kami mencadangkan kaedah yang fleksibel dan efisien untuk mencapai penyelenggaraan konsistensi di dunia Web 2.0. Eksperimen kami menunjukkan peningkatan prestasi yang baik berbanding dengan kaedah yang ada berdasarkan kunci diedarkan. [[EENNDD]] kawalan serentak; penyelenggaraan konsistensi; xml"], [{"string": "Ranking the web frontier No contact information provided yet.", "keywords": ["hypertext", "pagerank", "information search and retrieval", "ranking"], "combined": "Ranking the web frontier No contact information provided yet. [[EENNDD]] hypertext; pagerank; information search and retrieval; ranking"}, "Kedudukan sempadan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] hiperteks; pagerank; pencarian dan pengambilan maklumat; peringkat"], [{"string": "Recrawl scheduling based on information longevity It is crucial for a web crawler to distinguish between ephemeral and persistent content. Ephemeral content (e.g., quote of the day) is usually not worth crawling, because by the time it reaches the index it is no longer representative of the web page from which it was acquired. On the other hand, content that persists across multiple page updates (e.g., recent blog postings) may be worth acquiring, because it matches the page's true content for a sustained period of time.", "keywords": ["information longevity", "information search and retrieval", "recrawling"], "combined": "Recrawl scheduling based on information longevity It is crucial for a web crawler to distinguish between ephemeral and persistent content. Ephemeral content (e.g., quote of the day) is usually not worth crawling, because by the time it reaches the index it is no longer representative of the web page from which it was acquired. On the other hand, content that persists across multiple page updates (e.g., recent blog postings) may be worth acquiring, because it matches the page's true content for a sustained period of time. [[EENNDD]] information longevity; information search and retrieval; recrawling"}, "Penjadualan semula penjelajahan berdasarkan umur panjang maklumat Adalah penting bagi perayap web untuk membezakan antara kandungan sementara dan kandungan berterusan. Kandungan singkat (mis., Kutipan hari ini) biasanya tidak layak dirayapi, kerana pada saat ia mencapai indeks, ia tidak lagi mewakili laman web dari mana ia diperoleh. Sebaliknya, kandungan yang berterusan di banyak kemas kini halaman (mis., Posting blog baru-baru ini) mungkin bernilai diperoleh, kerana sesuai dengan kandungan sebenar halaman untuk jangka waktu yang lama. [[EENNDD]] jangka hayat maklumat; pencarian dan pengambilan maklumat; mengumpul semula"], [{"string": "A P2P based distributed services network for next generation mobile internet communications In this poster, we present a novel P2P (Peer to Peer) based distributed services network (DSN), which is a next generation operable and manageable distributed core network architecture and functional structure, proposed by China Mobile for telecommunication services and wireless Internet. Our preliminary implementations of P2P VoIP (Voice over Internet Protocol) system over DSN platform demonstrate its effectiveness and promising future.", "keywords": ["mobile communication", "dsn", "distributed computing", "p2p"], "combined": "A P2P based distributed services network for next generation mobile internet communications In this poster, we present a novel P2P (Peer to Peer) based distributed services network (DSN), which is a next generation operable and manageable distributed core network architecture and functional structure, proposed by China Mobile for telecommunication services and wireless Internet. Our preliminary implementations of P2P VoIP (Voice over Internet Protocol) system over DSN platform demonstrate its effectiveness and promising future. [[EENNDD]] mobile communication; dsn; distributed computing; p2p"}, "Rangkaian perkhidmatan terdistribusi berasaskan P2P untuk komunikasi internet mudah alih generasi akan datang. Dalam poster ini, kami menyajikan sebuah rangkaian perkhidmatan terdistribusi (DSN) berasaskan P2P (Peer to Peer), yang merupakan senibina dan struktur fungsi rangkaian teras diedarkan generasi seterusnya yang dapat dikendalikan dan dikendalikan, dicadangkan oleh China Mobile untuk perkhidmatan telekomunikasi dan Internet tanpa wayar. Pelaksanaan awal sistem P2P VoIP (Voice over Internet Protocol) melalui platform DSN menunjukkan keberkesanannya dan masa depan yang menjanjikan. [[EENNDD]] komunikasi mudah alih; dsn; pengkomputeran diedarkan; p2p"], [{"string": "Netprobe: a fast and scalable system for fraud detection in online auction networks Given a large online network of online auction users and their histories of transactions, how can we spot anomalies and auction fraud? This paper describes the design and implementation of NetProbe, a system that we propose for solving this problem. NetProbe models auction users and transactions as a Markov Random Field tuned to detect the suspicious patterns that fraudsters create, and employs a Belief Propagation mechanism to detect likely fraudsters. Our experiments show that NetProbe is both efficient and effective for fraud detection. We report experiments on synthetic graphs with as many as 7,000 nodes and 30,000 edges, where NetProbe was able to spot fraudulent nodes with over 90% precision and recall, within a matter of seconds. We also report experiments on a real dataset crawled from eBay, with nearly 700,000 transactions between more than 66,000users, where NetProbe was highly effective at unearthing hidden networks of fraudsters, within a realistic response time of about 6 minutes. For scenarios where the underlying data is dynamic in nature, we propose IncrementalNetProbe, which is an approximate, but fast, variant of NetProbe. Our experiments prove that Incremental NetProbe executes nearly doubly fast as compared to NetProbe, while retaining over 99% of its accuracy.", "keywords": ["bipartite cores", "fraud detection", "miscellaneous", "markov random fields", "belief propagation"], "combined": "Netprobe: a fast and scalable system for fraud detection in online auction networks Given a large online network of online auction users and their histories of transactions, how can we spot anomalies and auction fraud? This paper describes the design and implementation of NetProbe, a system that we propose for solving this problem. NetProbe models auction users and transactions as a Markov Random Field tuned to detect the suspicious patterns that fraudsters create, and employs a Belief Propagation mechanism to detect likely fraudsters. Our experiments show that NetProbe is both efficient and effective for fraud detection. We report experiments on synthetic graphs with as many as 7,000 nodes and 30,000 edges, where NetProbe was able to spot fraudulent nodes with over 90% precision and recall, within a matter of seconds. We also report experiments on a real dataset crawled from eBay, with nearly 700,000 transactions between more than 66,000users, where NetProbe was highly effective at unearthing hidden networks of fraudsters, within a realistic response time of about 6 minutes. For scenarios where the underlying data is dynamic in nature, we propose IncrementalNetProbe, which is an approximate, but fast, variant of NetProbe. Our experiments prove that Incremental NetProbe executes nearly doubly fast as compared to NetProbe, while retaining over 99% of its accuracy. [[EENNDD]] bipartite cores; fraud detection; miscellaneous; markov random fields; belief propagation"}, "Netprobe: sistem cepat dan berskala untuk mengesan penipuan dalam rangkaian lelongan dalam talian Memandangkan rangkaian pengguna lelong dalam talian yang besar dan sejarah urus niaga mereka, bagaimana kita dapat melihat anomali dan penipuan lelong? Makalah ini menerangkan reka bentuk dan pelaksanaan NetProbe, sistem yang kami cadangkan untuk menyelesaikan masalah ini. Model NetProbe melelong pengguna dan transaksi sebagai Markov Random Field yang diselaraskan untuk mengesan corak mencurigakan yang dibuat oleh penipu, dan menggunakan mekanisme Belief Propagation untuk mengesan kemungkinan penipu. Eksperimen kami menunjukkan bahawa NetProbe cekap dan berkesan untuk mengesan penipuan. Kami melaporkan eksperimen pada grafik sintetik dengan sebanyak 7,000 nod dan 30,000 tepi, di mana NetProbe dapat mengesan nod palsu dengan ketepatan dan penarikan lebih dari 90%, dalam masa beberapa saat. Kami juga melaporkan eksperimen pada set data sebenar yang dirangkak dari eBay, dengan hampir 700,000 transaksi antara lebih daripada 66,000 pengguna, di mana NetProbe sangat berkesan untuk menjumpai rangkaian penipu yang tersembunyi, dalam masa tindak balas yang realistik sekitar 6 minit. Untuk senario di mana data yang mendasari bersifat dinamik, kami mencadangkan IncrementalNetProbe, yang merupakan varian NetProbe yang hampir, tetapi cepat. Eksperimen kami membuktikan bahawa Incremental NetProbe melaksanakan hampir dua kali ganda cepat berbanding dengan NetProbe, sambil mengekalkan lebih dari 99% ketepatannya. [[EENNDD]] teras bipartit; pengesanan penipuan; pelbagai; medan rawak markov; penyebaran kepercayaan"], [{"string": "Who says what to whom on twitter We study several longstanding questions in media communications research, in the context of the microblogging service Twitter, regarding the production, flow, and consumption of information. To do so, we exploit a recently introduced feature of Twitter known as \"lists\" to distinguish between elite users - by which we mean celebrities, bloggers, and representatives of media outlets and other formal organizations - and ordinary users. Based on this classification, we find a striking concentration of attention on Twitter, in that roughly 50% of URLs consumed are generated by just 20K elite users, where the media produces the most information, but celebrities are the most followed. We also find significant homophily within categories: celebrities listen to celebrities, while bloggers listen to bloggers etc; however, bloggers in general rebroadcast more information than the other categories. Next we re-examine the classical \"two-step flow\" theory of communications, finding considerable support for it on Twitter. Third, we find that URLs broadcast by different categories of users or containing different types of content exhibit systematically different lifespans. And finally, we examine the attention paid by the different user categories to different news topics.", "keywords": ["communications", "classification", "communication networks", "information flow", "user/machine systems", "twitter", "two-step flow"], "combined": "Who says what to whom on twitter We study several longstanding questions in media communications research, in the context of the microblogging service Twitter, regarding the production, flow, and consumption of information. To do so, we exploit a recently introduced feature of Twitter known as \"lists\" to distinguish between elite users - by which we mean celebrities, bloggers, and representatives of media outlets and other formal organizations - and ordinary users. Based on this classification, we find a striking concentration of attention on Twitter, in that roughly 50% of URLs consumed are generated by just 20K elite users, where the media produces the most information, but celebrities are the most followed. We also find significant homophily within categories: celebrities listen to celebrities, while bloggers listen to bloggers etc; however, bloggers in general rebroadcast more information than the other categories. Next we re-examine the classical \"two-step flow\" theory of communications, finding considerable support for it on Twitter. Third, we find that URLs broadcast by different categories of users or containing different types of content exhibit systematically different lifespans. And finally, we examine the attention paid by the different user categories to different news topics. [[EENNDD]] communications; classification; communication networks; information flow; user/machine systems; twitter; two-step flow"}, "Siapa yang mengatakan apa kepada siapa di twitter Kami meneliti beberapa pertanyaan lama dalam penyelidikan komunikasi media, dalam konteks layanan microblogging Twitter, mengenai pengeluaran, aliran, dan penggunaan maklumat. Untuk melakukannya, kami memanfaatkan ciri Twitter yang baru diperkenalkan yang dikenali sebagai \"senarai\" untuk membezakan antara pengguna elit - yang bermaksud kami selebriti, blogger, dan perwakilan outlet media dan organisasi formal lain - dan pengguna biasa. Berdasarkan klasifikasi ini, kami mendapati tumpuan perhatian yang menarik di Twitter, kerana kira-kira 50% URL yang digunakan dihasilkan oleh hanya 20K pengguna elit, di mana media menghasilkan maklumat paling banyak, tetapi selebriti paling banyak diikuti. Kami juga menjumpai homofilia yang signifikan dalam kategori: selebriti mendengar selebriti, sementara blogger mendengar blogger dll; namun, blogger pada umumnya menyiarkan semula lebih banyak maklumat daripada kategori lain. Seterusnya kita mengkaji semula teori komunikasi \"aliran dua langkah\" klasik, dan mendapat sokongan yang besar di Twitter. Ketiga, kami mendapati bahawa URL yang disiarkan oleh kategori pengguna yang berlainan atau mengandungi pelbagai jenis kandungan menunjukkan jangka hayat yang berbeza secara sistematik. Dan akhirnya, kami meneliti perhatian yang diberikan oleh kategori pengguna yang berbeza terhadap topik berita yang berbeza. [[EENNDD]] komunikasi; pengelasan; rangkaian komunikasi; aliran maklumat; sistem pengguna / mesin; twitter; aliran dua langkah"], [{"string": "Strategic formation of credit networks Credit networks are an abstraction for modeling trust between agents in a network. Agents who do not directly trust each other can transact through exchange of IOUs (obligations) along a chain of trust in the network. Credit networks are robust to intrusion, can enable transactions between strangers in exchange economies, and have the liquidity to support a high rate of transactions. We study the formation of such networks when agents strategically decide how much credit to extend each other. When each agent trusts a fixed set of other agents, and transacts directly only with those it trusts, the formation game is a potential game and all Nash equilibria are social optima. Moreover, the Nash equilibria of this game are equivalent in a very strong sense: the sequences of transactions that can be supported from each equilibrium credit network are identical. When we allow transactions over longer paths, the game may not admit a Nash equilibrium, and even when it does, the price of anarchy may be unbounded. Hence, we study two special cases. First, when agents have a shared belief about the trustworthiness of each agent, the networks formed in equilibrium have a star-like structure. Though the price of anarchy is unbounded, myopic best response quickly converges to a social optimum. Similar star-like structures are found in equilibria of heuristic strategies found via simulation. In addition, we simulate a second case where agents may have varying information about each others' trustworthiness based on their distance in a social network. Empirical game analysis of these scenarios suggests that star structures arise only when defaults are relatively rare, and otherwise, credit tends to be issued over short social distances conforming to the locality of information.", "keywords": ["empirical game-theoretic simulations", "credit networks", "trust", "strategic network formation", "electronic commerce"], "combined": "Strategic formation of credit networks Credit networks are an abstraction for modeling trust between agents in a network. Agents who do not directly trust each other can transact through exchange of IOUs (obligations) along a chain of trust in the network. Credit networks are robust to intrusion, can enable transactions between strangers in exchange economies, and have the liquidity to support a high rate of transactions. We study the formation of such networks when agents strategically decide how much credit to extend each other. When each agent trusts a fixed set of other agents, and transacts directly only with those it trusts, the formation game is a potential game and all Nash equilibria are social optima. Moreover, the Nash equilibria of this game are equivalent in a very strong sense: the sequences of transactions that can be supported from each equilibrium credit network are identical. When we allow transactions over longer paths, the game may not admit a Nash equilibrium, and even when it does, the price of anarchy may be unbounded. Hence, we study two special cases. First, when agents have a shared belief about the trustworthiness of each agent, the networks formed in equilibrium have a star-like structure. Though the price of anarchy is unbounded, myopic best response quickly converges to a social optimum. Similar star-like structures are found in equilibria of heuristic strategies found via simulation. In addition, we simulate a second case where agents may have varying information about each others' trustworthiness based on their distance in a social network. Empirical game analysis of these scenarios suggests that star structures arise only when defaults are relatively rare, and otherwise, credit tends to be issued over short social distances conforming to the locality of information. [[EENNDD]] empirical game-theoretic simulations; credit networks; trust; strategic network formation; electronic commerce"}, "Pembentukan strategik rangkaian kredit Rangkaian kredit adalah penolakan untuk memodelkan kepercayaan antara ejen dalam rangkaian. Ejen yang tidak saling mempercayai secara langsung boleh bertransaksi melalui pertukaran IOU (kewajiban) di sepanjang rangkaian kepercayaan di rangkaian. Rangkaian kredit kuat untuk diceroboh, dapat memungkinkan transaksi antara orang asing dalam ekonomi pertukaran, dan mempunyai kecairan untuk menyokong kadar transaksi yang tinggi. Kami mengkaji pembentukan rangkaian tersebut apabila ejen secara strategik memutuskan berapa banyak kredit untuk saling memberi. Apabila setiap ejen mempercayai sekumpulan ejen lain yang tetap, dan berurusan secara langsung hanya dengan ejen yang dipercayainya, permainan formasi adalah permainan yang berpotensi dan semua keseimbangan Nash adalah optima sosial. Lebih-lebih lagi, keseimbangan Nash permainan ini setara dalam erti kata yang sangat kuat: urutan urus niaga yang dapat disokong dari setiap rangkaian kredit keseimbangan adalah serupa. Apabila kita membenarkan urus niaga melalui jalan yang lebih panjang, permainan mungkin tidak menerima keseimbangan Nash, dan walaupun berlaku, harga anarki mungkin tidak terbatas. Oleh itu, kami mengkaji dua kes khas. Pertama, apabila ejen mempunyai kepercayaan bersama mengenai kebolehpercayaan setiap ejen, rangkaian yang terbentuk dalam keseimbangan mempunyai struktur seperti bintang. Walaupun harga anarki tidak terbatas, tindak balas terbaik myopik cepat berubah menjadi optimum sosial. Struktur seperti bintang serupa dijumpai dalam keseimbangan strategi heuristik yang dijumpai melalui simulasi. Sebagai tambahan, kami mensimulasikan kes kedua di mana ejen mungkin mempunyai pelbagai maklumat mengenai kepercayaan masing-masing berdasarkan jarak mereka dalam rangkaian sosial. Analisis permainan empirik senario ini menunjukkan bahawa struktur bintang muncul hanya apabila lalai relatif jarang berlaku, dan sebaliknya, kredit cenderung dikeluarkan pada jarak sosial yang pendek sesuai dengan lokasi maklumat. [[EENNDD]] simulasi permainan-teori empirikal; rangkaian kredit; kepercayaan; pembentukan rangkaian strategik; perdagangan elektronik"], [{"string": "An audio/video analysis mechanism for web indexing No contact information provided yet.", "keywords": ["content analysis and indexing", "video indexing", "mpeg7-ddl", "automatic speech fecognition", "contents indexing", "shot boundary detection"], "combined": "An audio/video analysis mechanism for web indexing No contact information provided yet. [[EENNDD]] content analysis and indexing; video indexing; mpeg7-ddl; automatic speech fecognition; contents indexing; shot boundary detection"}, "Mekanisme analisis audio / video untuk pengindeksan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] analisis dan pengindeksan kandungan; pengindeksan video; mpeg7-ddl; pengecaman pertuturan automatik; pengindeksan kandungan; pengesanan sempadan tembakan"], [{"string": "Regular expressions considered harmful in client-side XSS filters Cross-site scripting flaws have now surpassed buffer overflows as the world's most common publicly-reported security vulnerability. In recent years, browser vendors and researchers have tried to develop client-side filters to mitigate these attacks. We analyze the best existing filters and find them to be either unacceptably slow or easily circumvented. Worse, some of these filters could introduce vulnerabilities into sites that were previously bug-free. We propose a new filter design that achieves both high performance and high precision by blocking scripts after HTML parsing but before execution. Compared to previous approaches, our approach is faster, protects against more vulnerabilities, and is harder for attackers to abuse. We have contributed an implementation of our filter design to the WebKit open source rendering engine, and the filter is now enabled by default in the Google Chrome browser.", "keywords": ["unauthorized access", "web", "xss", "cross-site scripting", "browser", "filter"], "combined": "Regular expressions considered harmful in client-side XSS filters Cross-site scripting flaws have now surpassed buffer overflows as the world's most common publicly-reported security vulnerability. In recent years, browser vendors and researchers have tried to develop client-side filters to mitigate these attacks. We analyze the best existing filters and find them to be either unacceptably slow or easily circumvented. Worse, some of these filters could introduce vulnerabilities into sites that were previously bug-free. We propose a new filter design that achieves both high performance and high precision by blocking scripts after HTML parsing but before execution. Compared to previous approaches, our approach is faster, protects against more vulnerabilities, and is harder for attackers to abuse. We have contributed an implementation of our filter design to the WebKit open source rendering engine, and the filter is now enabled by default in the Google Chrome browser. [[EENNDD]] unauthorized access; web; xss; cross-site scripting; browser; filter"}, "Ungkapan biasa yang dianggap berbahaya dalam penapis XSS sisi pelanggan, kelemahan skrip lintas tapak kini telah melampaui limpahan penyangga sebagai kerentanan keselamatan yang dilaporkan umum di dunia. Dalam beberapa tahun kebelakangan ini, vendor penyemak imbas dan penyelidik telah berusaha mengembangkan penapis sisi pelanggan untuk mengurangkan serangan ini. Kami menganalisis penapis terbaik yang ada dan mendapati ia perlahan atau tidak dapat dielakkan. Lebih buruk lagi, beberapa penapis ini dapat memperkenalkan kerentanan ke laman web yang sebelumnya bebas bug. Kami mencadangkan reka bentuk penapis baru yang mencapai prestasi tinggi dan ketepatan tinggi dengan menyekat skrip setelah penghuraian HTML tetapi sebelum pelaksanaan. Berbanding dengan pendekatan sebelumnya, pendekatan kami lebih cepat, melindungi dari lebih banyak kerentanan, dan lebih sukar bagi penyerang untuk menyalahgunakan. Kami telah memberikan kontribusi pelaksanaan desain penapis kami ke mesin rendering sumber terbuka WebKit, dan penapis kini diaktifkan secara lalai dalam penyemak imbas Google Chrome. [[EENNDD]] akses yang tidak dibenarkan; laman web; xss; skrip merentas laman web; penyemak imbas; tapis"], [{"string": "Mining multilingual topics from wikipedia In this paper, we try to leverage a large-scale and multilingual knowledge base, Wikipedia, to help effectively analyze and organize Web information written in different languages. Based on the observation that one Wikipedia concept may be described by articles in different languages, we adapt existing topic modeling algorithm for mining multilingual topics from this knowledge base. The extracted 'universal' topics have multiple types of representations, with each type corresponding to one language. Accordingly, new documents of different languages can be represented in a space using a group of universal topics, which makes various multilingual Web applications feasible.", "keywords": ["content analysis and indexing", "topic modeling", "wikipedia", "multilingual", "universal-topics"], "combined": "Mining multilingual topics from wikipedia In this paper, we try to leverage a large-scale and multilingual knowledge base, Wikipedia, to help effectively analyze and organize Web information written in different languages. Based on the observation that one Wikipedia concept may be described by articles in different languages, we adapt existing topic modeling algorithm for mining multilingual topics from this knowledge base. The extracted 'universal' topics have multiple types of representations, with each type corresponding to one language. Accordingly, new documents of different languages can be represented in a space using a group of universal topics, which makes various multilingual Web applications feasible. [[EENNDD]] content analysis and indexing; topic modeling; wikipedia; multilingual; universal-topics"}, "Melombong topik berbilang bahasa dari wikipedia Dalam makalah ini, kami berusaha memanfaatkan asas pengetahuan berskala besar dan multibahasa, Wikipedia, untuk membantu menganalisis dan mengatur maklumat Web yang ditulis dalam pelbagai bahasa dengan berkesan. Berdasarkan pemerhatian bahawa satu konsep Wikipedia dapat dijelaskan oleh artikel dalam pelbagai bahasa, kami menyesuaikan algoritma pemodelan topik yang ada untuk melombong topik berbilang bahasa dari pangkalan pengetahuan ini. Topik 'universal' yang diekstrak mempunyai pelbagai jenis representasi, dengan setiap jenis sesuai dengan satu bahasa. Dengan demikian, dokumen baru dari berbagai bahasa dapat ditunjukkan dalam ruang menggunakan sekelompok topik universal, yang menjadikan berbagai aplikasi Web berbilang bahasa dapat dilaksanakan. [[EENNDD]] analisis dan pengindeksan kandungan; pemodelan topik; wikipedia; berbilang bahasa; topik sejagat"], [{"string": "Supporting end-users in the creation of dependable web clips Web authoring environments enable end-users to create applications that integrate information from other web sources. Users can create web sites that include built-in components to dynamically incorporate, for example, weather information, stock-quotes, or the latest news from different web sources. Recent surveys conducted among end-users have indicated an increasing interest in creating such applications. Unfortunately, web authoring environments do not provide support beyond a limited set of built-in components. This work addresses this limitation by providing end-user support for \"clipping\" information from a target web site to incorporate it into the end-user site. The support consists of a mechanism to identify the target clipping with multiple markers to increase robustness, and a dynamic assessment of the retrieved information to quantify its reliability. The clipping approach has been integrated as a feature into a popular web authoring tool on which we present the results of two preliminary studies.", "keywords": ["end-users", "interaction styles", "dependability", "web authoring tools"], "combined": "Supporting end-users in the creation of dependable web clips Web authoring environments enable end-users to create applications that integrate information from other web sources. Users can create web sites that include built-in components to dynamically incorporate, for example, weather information, stock-quotes, or the latest news from different web sources. Recent surveys conducted among end-users have indicated an increasing interest in creating such applications. Unfortunately, web authoring environments do not provide support beyond a limited set of built-in components. This work addresses this limitation by providing end-user support for \"clipping\" information from a target web site to incorporate it into the end-user site. The support consists of a mechanism to identify the target clipping with multiple markers to increase robustness, and a dynamic assessment of the retrieved information to quantify its reliability. The clipping approach has been integrated as a feature into a popular web authoring tool on which we present the results of two preliminary studies. [[EENNDD]] end-users; interaction styles; dependability; web authoring tools"}, "Menyokong pengguna akhir dalam pembuatan klip web yang boleh dipercayai persekitaran pengarang Web membolehkan pengguna akhir membuat aplikasi yang mengintegrasikan maklumat dari sumber web lain. Pengguna dapat membuat laman web yang merangkumi komponen bawaan untuk menggabungkan secara dinamis, misalnya, maklumat cuaca, harga saham, atau berita terbaru dari sumber web yang berlainan. Tinjauan terkini yang dilakukan di kalangan pengguna akhir menunjukkan peningkatan minat untuk membuat aplikasi tersebut. Malangnya, persekitaran pengarang web tidak memberikan sokongan melangkaui sekumpulan komponen bawaan yang terhad. Karya ini mengatasi batasan ini dengan memberikan sokongan pengguna akhir untuk \"memotong\" maklumat dari laman web sasaran untuk memasukkannya ke dalam laman pengguna akhir. Sokongan ini terdiri daripada mekanisme untuk mengenal pasti pemotongan sasaran dengan beberapa penanda untuk meningkatkan kekuatan, dan penilaian dinamik terhadap maklumat yang diambil untuk mengukur kebolehpercayaannya. Pendekatan kliping telah disatukan sebagai fitur ke dalam alat pengarang web yang popular di mana kami memaparkan hasil dari dua kajian awal. [[EENNDD]] pengguna akhir; gaya interaksi; kebolehpercayaan; alat pengarang web"], [{"string": "Finding hierarchy in directed online social networks Social hierarchy and stratification among humans is a well studied concept in sociology. The popularity of online social networks presents an opportunity to study social hierarchy for different types of networks and at different scales. We adopt the premise that people form connections in a social network based on their perceived social hierarchy; as a result, the edge directions in directed social networks can be leveraged to infer hierarchy. In this paper, we define a measure of hierarchy in a directed online social network, and present an efficient algorithm to compute this measure. We validate our measure using ground truth including Wikipedia notability score. We use this measure to study hierarchy in several directed online social networks including Twitter, Delicious, YouTube, Flickr, LiveJournal, and curated lists of several categories of people based on different occupations, and different organizations. Our experiments on different online social networks show how hierarchy emerges as we increase the size of the network. This is in contrast to random graphs, where the hierarchy decreases as the network size increases. Further, we show that the degree of stratification in a network increases very slowly as we increase the size of the graph.", "keywords": ["measure", "general", "hierarchy", "social networks"], "combined": "Finding hierarchy in directed online social networks Social hierarchy and stratification among humans is a well studied concept in sociology. The popularity of online social networks presents an opportunity to study social hierarchy for different types of networks and at different scales. We adopt the premise that people form connections in a social network based on their perceived social hierarchy; as a result, the edge directions in directed social networks can be leveraged to infer hierarchy. In this paper, we define a measure of hierarchy in a directed online social network, and present an efficient algorithm to compute this measure. We validate our measure using ground truth including Wikipedia notability score. We use this measure to study hierarchy in several directed online social networks including Twitter, Delicious, YouTube, Flickr, LiveJournal, and curated lists of several categories of people based on different occupations, and different organizations. Our experiments on different online social networks show how hierarchy emerges as we increase the size of the network. This is in contrast to random graphs, where the hierarchy decreases as the network size increases. Further, we show that the degree of stratification in a network increases very slowly as we increase the size of the graph. [[EENNDD]] measure; general; hierarchy; social networks"}, "Mencari hierarki dalam rangkaian sosial dalam talian yang diarahkan Hierarki sosial dan stratifikasi antara manusia adalah konsep yang dikaji dengan baik dalam sosiologi. Populariti rangkaian sosial dalam talian memberi peluang untuk mengkaji hierarki sosial untuk pelbagai jenis rangkaian dan pada skala yang berbeza. Kami menggunakan premis bahawa orang membentuk hubungan dalam rangkaian sosial berdasarkan hierarki sosial mereka yang dirasakan; sebagai hasilnya, petunjuk arah dalam rangkaian sosial yang diarahkan dapat dimanfaatkan untuk menyimpulkan hierarki. Dalam makalah ini, kami menentukan ukuran hierarki dalam rangkaian sosial dalam talian yang diarahkan, dan mengemukakan algoritma yang cekap untuk menghitung ukuran ini. Kami mengesahkan ukuran kami dengan menggunakan kebenaran dasar termasuk skor ketenaran Wikipedia. Kami menggunakan ukuran ini untuk mengkaji hierarki di beberapa rangkaian sosial dalam talian yang diarahkan termasuk Twitter, Delicious, YouTube, Flickr, LiveJournal, dan senarai beberapa kategori orang berdasarkan pekerjaan yang berbeza, dan organisasi yang berbeza. Eksperimen kami di rangkaian sosial dalam talian yang berbeza menunjukkan bagaimana hierarki muncul ketika kami meningkatkan ukuran rangkaian. Ini berbeza dengan grafik rawak, di mana hierarki berkurang apabila saiz rangkaian meningkat. Selanjutnya, kami menunjukkan bahawa tahap stratifikasi dalam rangkaian meningkat dengan sangat perlahan ketika kami meningkatkan ukuran grafik. [[EENNDD]] ukuran; umum; hierarki; rangkaian sosial"], [{"string": "XAR-miner: efficient association rules mining for XML data No contact information provided yet.", "keywords": ["xml data", "meta-patterns", "association rule mining"], "combined": "XAR-miner: efficient association rules mining for XML data No contact information provided yet. [[EENNDD]] xml data; meta-patterns; association rule mining"}, "XAR-miner: perlombongan peraturan persatuan yang cekap untuk data XML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] data xml; corak meta; perlombongan peraturan persatuan"], [{"string": "Towards robust trust establishment in web-based social networks with socialtrust We propose the SocialTrust framework for tamper-resilient trust establishment in online social networks. Two of the salient features of SocialTrust are its dynamic revision of trust by (i) distinguishing relationship quality from trust; and (ii) incorporating a personalized feedback mechanism for adapting as the social network evolves.", "keywords": ["on-line information services", "trust", "social networks"], "combined": "Towards robust trust establishment in web-based social networks with socialtrust We propose the SocialTrust framework for tamper-resilient trust establishment in online social networks. Two of the salient features of SocialTrust are its dynamic revision of trust by (i) distinguishing relationship quality from trust; and (ii) incorporating a personalized feedback mechanism for adapting as the social network evolves. [[EENNDD]] on-line information services; trust; social networks"}, "Ke arah pembentukan kepercayaan yang kuat dalam rangkaian sosial berasaskan web dengan kepercayaan sosial. Kami mencadangkan rangka kerja SocialTrust untuk mewujudkan kepercayaan yang tahan terhadap gangguan dalam rangkaian sosial dalam talian. Dua ciri penting SocialTrust adalah semakan kepercayaan yang dinamik dengan (i) membezakan kualiti hubungan dengan kepercayaan; dan (ii) menggabungkan mekanisme maklum balas yang diperibadikan untuk menyesuaikan diri ketika rangkaian sosial berkembang. [[EENNDD]] perkhidmatan maklumat dalam talian; kepercayaan; rangkaian sosial"], [{"string": "Copyright protection on the web: a hybrid digital video watermarking scheme No contact information provided yet.", "keywords": ["scene change", "security and protection", "video", "digital watermarking", "hybrid"], "combined": "Copyright protection on the web: a hybrid digital video watermarking scheme No contact information provided yet. [[EENNDD]] scene change; security and protection; video; digital watermarking; hybrid"}, "Perlindungan hak cipta di web: skema tanda air video digital hibrid Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perubahan pemandangan; keselamatan dan perlindungan; video; penanda air digital; kacukan"], [{"string": "Updating pagerank with iterative aggregation No contact information provided yet.", "keywords": ["general", "pagerank", "markov chains", "disaggregation", "link analysis", "stationary vector", "aggregation", "updating", "power method"], "combined": "Updating pagerank with iterative aggregation No contact information provided yet. [[EENNDD]] general; pagerank; markov chains; disaggregation; link analysis; stationary vector; aggregation; updating; power method"}, "Mengemas kini pagerank dengan penggabungan berulang Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] umum; pagerank; rantai markov; pemisahan; analisis pautan; vektor pegun; penggabungan; mengemas kini; kaedah kuasa"], [{"string": "Semantic virtual environments No contact information provided yet.", "keywords": ["components", "knowledge representation formalisms and methods", "framework", "virtual environments", "semantic web", "integration"], "combined": "Semantic virtual environments No contact information provided yet. [[EENNDD]] components; knowledge representation formalisms and methods; framework; virtual environments; semantic web; integration"}, "Persekitaran maya semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] komponen; formalisme dan kaedah perwakilan pengetahuan; rangka kerja; persekitaran maya; web semantik; penyatuan"], [{"string": "A multimodal interaction manager for device independent mobile applications No contact information provided yet.", "keywords": ["interaction manager", "multimodal interface", "multi-user applications", "mobile network", "session management", "user interface management systems", "device independence"], "combined": "A multimodal interaction manager for device independent mobile applications No contact information provided yet. [[EENNDD]] interaction manager; multimodal interface; multi-user applications; mobile network; session management; user interface management systems; device independence"}, "Pengurus interaksi multimodal untuk aplikasi mudah alih bebas peranti Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengurus interaksi; antara muka multimodal; aplikasi berbilang pengguna; rangkaian mudah alih; pengurusan sesi; sistem pengurusan antara muka pengguna; kebebasan peranti"], [{"string": "An enhanced model for searching in semantic portals No contact information provided yet.", "keywords": ["fuzzy description logic", "semantic portal", "information retrieval", "information search and retrieval", "fuzzy reasoning", "semantic search"], "combined": "An enhanced model for searching in semantic portals No contact information provided yet. [[EENNDD]] fuzzy description logic; semantic portal; information retrieval; information search and retrieval; fuzzy reasoning; semantic search"}, "Model yang dipertingkatkan untuk mencari di portal semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] logik keterangan kabur; portal semantik; pengambilan maklumat; pencarian dan pengambilan maklumat; penaakulan kabur; carian semantik"], [{"string": "Jena: implementing the semantic web recommendations No contact information provided yet.", "keywords": ["jena", "software architectures", "semantic web", "rdf", "owl", "rdql"], "combined": "Jena: implementing the semantic web recommendations No contact information provided yet. [[EENNDD]] jena; software architectures; semantic web; rdf; owl; rdql"}, "Jena: melaksanakan cadangan semantik web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] jena; seni bina perisian; web semantik; rdf; burung hantu; rdql"], [{"string": "Multichannel publication of interactive media documents in a news environment No contact information provided yet.", "keywords": ["standards", "multimedia information systems", "framework", "xml", "device independence", "interactivity", "multichannel publication", "multimedia"], "combined": "Multichannel publication of interactive media documents in a news environment No contact information provided yet. [[EENNDD]] standards; multimedia information systems; framework; xml; device independence; interactivity; multichannel publication; multimedia"}, "Penerbitan pelbagai saluran dokumen media interaktif dalam persekitaran berita Belum ada maklumat hubungan yang diberikan. [[EENNDD]] standard; sistem maklumat multimedia; rangka kerja; xml; kebebasan peranti; interaktiviti; penerbitan pelbagai saluran; multimedia"], [{"string": "An experimental study of large-scale mobile social network Mobile social network is a typical social network where one or more individuals of similar interests or commonalities, conversing and connecting with one another using the mobile phone. Our works in this paper focus on the experimental study for this kind of social network with the support of large-scale real mobile call data. The main contributions can be summarized as three-fold: firstly, a large-scale real mobile phone call log of one city has been extracted from a mobile phone carrier in China to construct mobile social network; secondly, common features of traditional social networks, such as power law distribution and small diameter etc, have been experimented, with which we confirm that the mobile social network is a typical scale-free network and has small-world phenomenon; lastly, different from traditional analytical methods, important properties of the actors, such as gender and age, have been introduced into our experiments with some interesting findings about human behavior, for example, the middle-age people are more active than the young and old people, and the female is unusual more active than the male while in the old age.", "keywords": ["degree distribution", "clustering coefficient", "shortest path", "betweenness centrality", "diameter"], "combined": "An experimental study of large-scale mobile social network Mobile social network is a typical social network where one or more individuals of similar interests or commonalities, conversing and connecting with one another using the mobile phone. Our works in this paper focus on the experimental study for this kind of social network with the support of large-scale real mobile call data. The main contributions can be summarized as three-fold: firstly, a large-scale real mobile phone call log of one city has been extracted from a mobile phone carrier in China to construct mobile social network; secondly, common features of traditional social networks, such as power law distribution and small diameter etc, have been experimented, with which we confirm that the mobile social network is a typical scale-free network and has small-world phenomenon; lastly, different from traditional analytical methods, important properties of the actors, such as gender and age, have been introduced into our experiments with some interesting findings about human behavior, for example, the middle-age people are more active than the young and old people, and the female is unusual more active than the male while in the old age. [[EENNDD]] degree distribution; clustering coefficient; shortest path; betweenness centrality; diameter"}, "Kajian eksperimental rangkaian sosial mudah alih berskala besar Rangkaian sosial mudah alih adalah rangkaian sosial khas di mana satu atau lebih individu mempunyai minat atau kesamaan yang sama, bercakap dan berhubung antara satu sama lain menggunakan telefon bimbit. Karya kami dalam makalah ini menumpukan pada kajian eksperimen untuk rangkaian sosial semacam ini dengan sokongan data panggilan mudah alih berskala besar. Sumbangan utama dapat diringkaskan tiga kali lipat: pertama, log panggilan telefon bimbit berskala besar satu bandar telah diambil dari pembawa telefon bimbit di China untuk membina rangkaian sosial mudah alih; kedua, ciri umum rangkaian sosial tradisional, seperti pengedaran undang-undang kuasa dan diameter kecil dll, telah dieksperimen, dengan mana kami mengesahkan bahawa rangkaian sosial mudah alih adalah rangkaian bebas skala khas dan mempunyai fenomena dunia kecil; terakhir, berbeza dengan kaedah analitik tradisional, sifat penting pelakon, seperti jantina dan usia, telah diperkenalkan ke dalam eksperimen kami dengan beberapa penemuan menarik mengenai tingkah laku manusia, sebagai contoh, orang-orang usia pertengahan lebih aktif daripada yang muda dan tua orang, dan wanita tidak biasa lebih aktif daripada lelaki ketika di usia tua. [[EENNDD]] pengagihan darjah; pekali pengelompokan; jalan terpendek; sentraliti antara; diameter"], [{"string": "Designing personalized web applications An abstract is not available.", "keywords": ["hypertext/hypermedia"], "combined": "Designing personalized web applications An abstract is not available. [[EENNDD]] hypertext/hypermedia"}, "Merancang aplikasi web yang diperibadikan Abstrak tidak tersedia. [[EENNDD]] hiperteks / hipermedia"], [{"string": "Delivering web service coordination capability to users No contact information provided yet.", "keywords": ["bioinformatics", "scientific workflows", "web programming", "e-science", "web services", "web service coordination"], "combined": "Delivering web service coordination capability to users No contact information provided yet. [[EENNDD]] bioinformatics; scientific workflows; web programming; e-science; web services; web service coordination"}, "Menyampaikan keupayaan penyelarasan perkhidmatan web kepada pengguna Belum ada maklumat hubungan yang diberikan. [[EENNDD]] bioinformatik; aliran kerja saintifik; pengaturcaraan web; e-sains; perkhidmatan web; penyelarasan perkhidmatan web"], [{"string": "Enhancing diversity, coverage and balance for summarization through structure learning Document summarization plays an increasingly important role with the exponential growth of documents on the Web. Many supervised and unsupervised approaches have been proposed to generate summaries from documents. However, these approaches seldom simultaneously consider summary diversity, coverage, and balance issues which to a large extent determine the quality of summaries. In this paper, we consider extract-based summarization emphasizing the following three requirements: 1) diversity in summarization, which seeks to reduce redundancy among sentences in the summary; 2) sufficient coverage, which focuses on avoiding the loss of the document's main information when generating the summary; and 3) balance, which demands that different aspects of the document need to have about the same relative importance in the summary. We formulate the extract-based summarization problem as learning a mapping from a set of sentences of a given document to a subset of the sentences that satisfies the above three requirements. The mapping is learned by incorporating several constraints in a structure learning framework, and we explore the graph structure of the output variables and employ structural SVM for solving the resulted optimization problem. Experiments on the DUC2001 data sets demonstrate significant performance improvements in terms of F1 and ROUGE metrics.", "keywords": ["coverage", "diversity", "balance", "summarization", "structural svm"], "combined": "Enhancing diversity, coverage and balance for summarization through structure learning Document summarization plays an increasingly important role with the exponential growth of documents on the Web. Many supervised and unsupervised approaches have been proposed to generate summaries from documents. However, these approaches seldom simultaneously consider summary diversity, coverage, and balance issues which to a large extent determine the quality of summaries. In this paper, we consider extract-based summarization emphasizing the following three requirements: 1) diversity in summarization, which seeks to reduce redundancy among sentences in the summary; 2) sufficient coverage, which focuses on avoiding the loss of the document's main information when generating the summary; and 3) balance, which demands that different aspects of the document need to have about the same relative importance in the summary. We formulate the extract-based summarization problem as learning a mapping from a set of sentences of a given document to a subset of the sentences that satisfies the above three requirements. The mapping is learned by incorporating several constraints in a structure learning framework, and we explore the graph structure of the output variables and employ structural SVM for solving the resulted optimization problem. Experiments on the DUC2001 data sets demonstrate significant performance improvements in terms of F1 and ROUGE metrics. [[EENNDD]] coverage; diversity; balance; summarization; structural svm"}, "Meningkatkan kepelbagaian, liputan dan keseimbangan untuk ringkasan melalui pembelajaran struktur Ringkasan dokumen memainkan peranan yang semakin penting dengan pertumbuhan dokumen yang eksponensial di Web. Banyak pendekatan yang diawasi dan tidak diawasi telah diusulkan untuk menghasilkan ringkasan dari dokumen. Walau bagaimanapun, pendekatan ini jarang sekali mempertimbangkan isu kepelbagaian, liputan, dan keseimbangan ringkasan yang sebahagian besarnya menentukan kualiti ringkasan. Dalam makalah ini, kami mempertimbangkan ringkasan berdasarkan ekstrak menekankan tiga syarat berikut: 1) kepelbagaian dalam ringkasan, yang bertujuan untuk mengurangkan redundansi antara ayat dalam ringkasan; 2) liputan yang mencukupi, yang memberi tumpuan untuk mengelakkan kehilangan maklumat utama dokumen semasa membuat ringkasan; dan 3) keseimbangan, yang menuntut bahawa aspek yang berbeza dari dokumen perlu mempunyai kepentingan relatif yang sama dalam ringkasan. Kami merumuskan masalah ringkasan berdasarkan ekstrak sebagai pembelajaran pemetaan dari sekumpulan ayat dari dokumen tertentu hingga subset ayat yang memenuhi tiga syarat di atas. Pemetaan dipelajari dengan memasukkan beberapa kekangan dalam kerangka pembelajaran struktur, dan kami meneroka struktur grafik pemboleh ubah output dan menggunakan struktur SVM untuk menyelesaikan masalah pengoptimuman yang dihasilkan. Eksperimen pada set data DUC2001 menunjukkan peningkatan prestasi yang signifikan dari segi metrik F1 dan ROUGE. [[EENNDD]] liputan; kepelbagaian; seimbang; ringkasan; struktur svm"], [{"string": "Efficient training on biased minimax probability machine for imbalanced text classification The Biased Minimax Probability Machine (BMPM) constructs a classifier which deals with the imbalanced learning tasks. In this paper, we propose a Second Order Cone Programming (SOCP) based algorithm to train the model. We outline the theoretical derivatives of the biased classification model, and address the text classification tasks where negative training documents significantly outnumber the positive ones using the proposed strategy. We evaluated the learning scheme in comparison with traditional solutions on three different datasets. Empirical results have shown that our method is more effective and robust to handle imbalanced text classification problems.", "keywords": ["biased classification", "text classification", "second order cone programming", "biased minimax probability machine"], "combined": "Efficient training on biased minimax probability machine for imbalanced text classification The Biased Minimax Probability Machine (BMPM) constructs a classifier which deals with the imbalanced learning tasks. In this paper, we propose a Second Order Cone Programming (SOCP) based algorithm to train the model. We outline the theoretical derivatives of the biased classification model, and address the text classification tasks where negative training documents significantly outnumber the positive ones using the proposed strategy. We evaluated the learning scheme in comparison with traditional solutions on three different datasets. Empirical results have shown that our method is more effective and robust to handle imbalanced text classification problems. [[EENNDD]] biased classification; text classification; second order cone programming; biased minimax probability machine"}, "Latihan yang cekap mengenai mesin kebarangkalian minimax berat sebelah untuk klasifikasi teks yang tidak seimbang Mesin Biar Minimum Maksimum (BMPM) membina pengkelasan yang menangani tugas pembelajaran yang tidak seimbang. Dalam makalah ini, kami mengusulkan algoritma berdasarkan Pemrograman Kerucut Pesanan Kedua (SOCP) untuk melatih model. Kami menggariskan turunan teori dari model klasifikasi berat sebelah, dan menangani tugas-tugas klasifikasi teks di mana dokumen latihan negatif jauh lebih banyak daripada yang positif menggunakan strategi yang dicadangkan. Kami menilai skema pembelajaran dibandingkan dengan penyelesaian tradisional pada tiga set data yang berbeza. Hasil empirikal menunjukkan bahawa kaedah kami lebih berkesan dan mantap untuk menangani masalah klasifikasi teks yang tidak seimbang. [[EENNDD]] klasifikasi berat sebelah; pengelasan teks; pengaturcaraan kon pesanan kedua; mesin kebarangkalian minimax berat sebelah"], [{"string": "Authoring of learning styles in adaptive hypermedia: problems and solutions No contact information provided yet.", "keywords": ["authoring of adaptive hypermedia", "learning styles", "adaptive hypermedia", "user modeling"], "combined": "Authoring of learning styles in adaptive hypermedia: problems and solutions No contact information provided yet. [[EENNDD]] authoring of adaptive hypermedia; learning styles; adaptive hypermedia; user modeling"}, "Pengarang gaya pembelajaran dalam hipermedia adaptif: masalah dan penyelesaian Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengarang hipermedia adaptif; cara pembelajaran; hipermedia adaptif; pemodelan pengguna"], [{"string": "Personalization in distributed e-learning environments No contact information provided yet.", "keywords": ["personalization", "ontologies", "standards", "p2p", "adaptation", "web services", "learning repositories"], "combined": "Personalization in distributed e-learning environments No contact information provided yet. [[EENNDD]] personalization; ontologies; standards; p2p; adaptation; web services; learning repositories"}, "Pemperibadian dalam persekitaran e-pembelajaran yang diedarkan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pemperibadian; ontologi; standard; p2p; adaptasi; perkhidmatan web; repositori pembelajaran"], [{"string": "Open and decentralized access across location-based services Users now interact with multiple Location-Based Services (LBS) through a myriad set of location-aware devices and interfaces. However, current LBS tend to be centralized silos with ad-hoc APIs, which limits potential for information sharing and reuse. Further, LBS subscriptions and user experiences are not easily portable across devices. We propose a general architecture for providing open and decentralized access to LBS, based on Tiled Feeds - a RESTful protocol for access and interactions with LBS using feeds, and Feed Subscription Management (FSM) - a generalized feed-based service management protocol. We describe two client designs, and demonstrate how they enable standardized access to LBS services, promote information sharing and mashup creation, and offer service management across various types of location-enabled devices.", "keywords": ["on-line information services", "feeds", "location-based services", "atom", "rss"], "combined": "Open and decentralized access across location-based services Users now interact with multiple Location-Based Services (LBS) through a myriad set of location-aware devices and interfaces. However, current LBS tend to be centralized silos with ad-hoc APIs, which limits potential for information sharing and reuse. Further, LBS subscriptions and user experiences are not easily portable across devices. We propose a general architecture for providing open and decentralized access to LBS, based on Tiled Feeds - a RESTful protocol for access and interactions with LBS using feeds, and Feed Subscription Management (FSM) - a generalized feed-based service management protocol. We describe two client designs, and demonstrate how they enable standardized access to LBS services, promote information sharing and mashup creation, and offer service management across various types of location-enabled devices. [[EENNDD]] on-line information services; feeds; location-based services; atom; rss"}, "Akses terbuka dan terdesentralisasi di seluruh perkhidmatan berasaskan lokasi Pengguna kini berinteraksi dengan beberapa Perkhidmatan Berasaskan Lokasi (LBS) melalui sekumpulan pelbagai peranti dan antara muka yang mengetahui lokasi. Walau bagaimanapun, LBS semasa cenderung menjadi silo terpusat dengan API ad-hoc, yang membatasi potensi perkongsian dan penggunaan semula maklumat. Selanjutnya, langganan LBS dan pengalaman pengguna tidak mudah dibawa ke seluruh peranti. Kami mencadangkan seni bina umum untuk menyediakan akses terbuka dan terdesentralisasi ke LBS, berdasarkan Tiled Feeds - protokol RESTful untuk akses dan interaksi dengan LBS menggunakan feed, dan Feed Subscription Management (FSM) - protokol pengurusan perkhidmatan berasaskan feed yang umum. Kami menerangkan dua reka bentuk pelanggan, dan menunjukkan bagaimana mereka membolehkan akses standard ke perkhidmatan LBS, mempromosikan perkongsian maklumat dan pembuatan mashup, dan menawarkan pengurusan perkhidmatan di pelbagai jenis peranti yang membolehkan lokasi. [[EENNDD]] perkhidmatan maklumat dalam talian; makanan; perkhidmatan berasaskan lokasi; atom; rss"], [{"string": "On anonymizing query logs via token-based hashing In this paper we study the privacy preservation properties of aspecific technique for query log anonymization: token-based hashing. In this approach, each query is tokenized, and then a secure hash function is applied to each token. We show that statistical techniques may be applied to partially compromise the anonymization. We then analyze the specific risks that arise from these partial compromises, focused on revelation of identity from unambiguous names, addresses, and so forth, and the revelation of facts associated with an identity that are deemed to be highly sensitive. Our goal in this work is two fold: to show that token-based hashing is unsuitable for anonymization, and to present a concrete analysis of specific techniques that may be effective in breaching privacy, against which other anonymization schemes should be measured.", "keywords": ["miscellaneous", "privacy", "hash-based anonymization", "query logs"], "combined": "On anonymizing query logs via token-based hashing In this paper we study the privacy preservation properties of aspecific technique for query log anonymization: token-based hashing. In this approach, each query is tokenized, and then a secure hash function is applied to each token. We show that statistical techniques may be applied to partially compromise the anonymization. We then analyze the specific risks that arise from these partial compromises, focused on revelation of identity from unambiguous names, addresses, and so forth, and the revelation of facts associated with an identity that are deemed to be highly sensitive. Our goal in this work is two fold: to show that token-based hashing is unsuitable for anonymization, and to present a concrete analysis of specific techniques that may be effective in breaching privacy, against which other anonymization schemes should be measured. [[EENNDD]] miscellaneous; privacy; hash-based anonymization; query logs"}, "Pada log pertanyaan tanpa nama melalui hashing berasaskan token Dalam makalah ini kami mengkaji sifat pemeliharaan privasi teknik khusus untuk anonimisasi log pertanyaan: hashing berasaskan token. Dalam pendekatan ini, setiap pertanyaan diberi tanda, dan kemudian fungsi hash selamat diterapkan pada setiap token. Kami menunjukkan bahawa teknik statistik dapat diterapkan untuk mengorbankan sebilangan besar namanya. Kami kemudian menganalisis risiko khusus yang timbul dari kompromi separa ini, yang berfokus pada penyataan identiti dari nama, alamat, dan sebagainya yang tidak jelas, dan penyataan fakta yang berkaitan dengan identiti yang dianggap sangat sensitif. Matlamat kami dalam karya ini adalah dua kali lipat: untuk menunjukkan bahawa hashing berdasarkan token tidak sesuai untuk anonimisasi, dan mengemukakan analisis konkrit mengenai teknik khusus yang mungkin berkesan untuk melanggar privasi, yang mana skema anonimisasi lain harus diukur. [[EENNDD]] pelbagai; privasi; anonim berdasarkan hash; log pertanyaan"], [{"string": "A scheme of service discovery and control on ubiquitous devices No contact information provided yet.", "keywords": ["ad-hoc network", "ubiquitous computing", "user interfaces", "service discovery", "peer to peer"], "combined": "A scheme of service discovery and control on ubiquitous devices No contact information provided yet. [[EENNDD]] ad-hoc network; ubiquitous computing; user interfaces; service discovery; peer to peer"}, "Skema penemuan dan kawalan perkhidmatan pada peranti di mana-mana. Belum ada maklumat hubungan. [[EENNDD]] rangkaian ad-hoc; pengkomputeran di mana-mana; antara muka pengguna; penemuan perkhidmatan; rakan sebaya"], [{"string": "Exploring social dynamics in online media sharing It is now feasible to view media at home as easily as text-based pages were viewed when the World Wide Web (WWW) first emerged. This development has supported media sharing and search services providing hosting, indexing and access to large, online media repositories. Many of these sharing services also have a social aspect to them. This paper provides an initial analysis of the social interactions on a video sharing and search service (www.youtube.com). Results show that many users do not form social networks in the online community and a very small number do not appear to contribute to the wider community. However, it does seem those people who do use the available tools have much a greater tendency to form social connections.", "keywords": ["media", "social dynamics", "video", "user/machine systems", "search"], "combined": "Exploring social dynamics in online media sharing It is now feasible to view media at home as easily as text-based pages were viewed when the World Wide Web (WWW) first emerged. This development has supported media sharing and search services providing hosting, indexing and access to large, online media repositories. Many of these sharing services also have a social aspect to them. This paper provides an initial analysis of the social interactions on a video sharing and search service (www.youtube.com). Results show that many users do not form social networks in the online community and a very small number do not appear to contribute to the wider community. However, it does seem those people who do use the available tools have much a greater tendency to form social connections. [[EENNDD]] media; social dynamics; video; user/machine systems; search"}, "Meneroka dinamika sosial dalam perkongsian media dalam talian Kini dapat dilihat media di rumah semudah halaman berdasarkan teks dilihat ketika World Wide Web (WWW) pertama kali muncul. Perkembangan ini telah menyokong perkhidmatan perkongsian dan carian media yang menyediakan hosting, pengindeksan dan akses ke repositori media dalam talian yang besar. Sebilangan besar perkhidmatan perkongsian ini juga mempunyai aspek sosial bagi mereka. Makalah ini memberikan analisis awal mengenai interaksi sosial pada perkhidmatan perkongsian dan carian video (www.youtube.com). Hasil kajian menunjukkan bahawa banyak pengguna tidak membentuk rangkaian sosial dalam komuniti dalam talian dan sebilangan kecil tidak kelihatan menyumbang kepada komuniti yang lebih luas. Namun, nampaknya orang yang menggunakan alat yang ada mempunyai kecenderungan yang lebih besar untuk membentuk hubungan sosial. [[EENNDD]] media; dinamika sosial; video; sistem pengguna / mesin; cari"], [{"string": "A case study in web search using TREC algorithms An abstract is not available.", "keywords": ["keyword-based ranking", "search engines", "web", "trec ad-hoc", "link-based ranking"], "combined": "A case study in web search using TREC algorithms An abstract is not available. [[EENNDD]] keyword-based ranking; search engines; web; trec ad-hoc; link-based ranking"}, "Kajian kes dalam carian web menggunakan algoritma TREC Abstrak tidak tersedia. [[EENNDD]] peringkat berdasarkan kata kunci; enjin carian; laman web; trek ad-hoc; peringkat berdasarkan pautan"], [{"string": "Query-free news search No contact information provided yet.", "keywords": ["on-line information services", "information search and retrieval", "web information retrieval", "query-free search"], "combined": "Query-free news search No contact information provided yet. [[EENNDD]] on-line information services; information search and retrieval; web information retrieval; query-free search"}, "Pencarian berita tanpa pertanyaan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perkhidmatan maklumat dalam talian; pencarian dan pengambilan maklumat; pengambilan maklumat web; carian tanpa pertanyaan"], [{"string": "IRLbot: scaling to 6 billion pages and beyond This paper shares our experience in designing a web crawler that can download billions of pages using a single-server implementation and models its performance. We show that with the quadratically increasing complexity of verifying URL uniqueness, BFS crawl order, and fixed per-host rate-limiting, current crawling algorithms cannot effectively cope with the sheer volume of URLs generated in large crawls, highly-branching spam, legitimate multi-million-page blog sites, and infinite loops created by server-side scripts. We offer a set of techniques for dealing with these issues and test their performance in an implementation we call IRLbot. In our recent experiment that lasted 41 days, IRLbot running on a single server successfully crawled 6.3 billion valid HTML pages ($7.6$ billion connection requests) and sustained an average download rate of 319 mb/s (1,789 pages/s). Unlike our prior experiments with algorithms proposed in related work, this version of IRLbot did not experience any bottlenecks and successfully handled content from over 117 million hosts, parsed out 394 billion links, and discovered a subset of the web graph with 41 billion unique nodes.", "keywords": ["large-scale", "irlbot", "crawling"], "combined": "IRLbot: scaling to 6 billion pages and beyond This paper shares our experience in designing a web crawler that can download billions of pages using a single-server implementation and models its performance. We show that with the quadratically increasing complexity of verifying URL uniqueness, BFS crawl order, and fixed per-host rate-limiting, current crawling algorithms cannot effectively cope with the sheer volume of URLs generated in large crawls, highly-branching spam, legitimate multi-million-page blog sites, and infinite loops created by server-side scripts. We offer a set of techniques for dealing with these issues and test their performance in an implementation we call IRLbot. In our recent experiment that lasted 41 days, IRLbot running on a single server successfully crawled 6.3 billion valid HTML pages ($7.6$ billion connection requests) and sustained an average download rate of 319 mb/s (1,789 pages/s). Unlike our prior experiments with algorithms proposed in related work, this version of IRLbot did not experience any bottlenecks and successfully handled content from over 117 million hosts, parsed out 394 billion links, and discovered a subset of the web graph with 41 billion unique nodes. [[EENNDD]] large-scale; irlbot; crawling"}, "IRLbot: skala hingga 6 bilion halaman dan seterusnya Makalah ini berkongsi pengalaman kami dalam merancang perayap web yang boleh memuat turun berbilion halaman menggunakan pelaksanaan pelayan tunggal dan memodelkan prestasinya. Kami menunjukkan bahawa dengan kerumitan yang meningkat secara kuadratik untuk mengesahkan keunikan URL, pesanan perayapan BFS, dan pembatasan kadar per hos tetap, algoritma perayapan semasa tidak dapat mengatasi jumlah URL yang dihasilkan secara besar-besaran dalam perayapan besar, spam yang sangat bercabang, multi yang sah -laman laman blog berjuta-juta, dan gelung tanpa batas yang dibuat oleh skrip sisi pelayan. Kami menawarkan satu set teknik untuk menangani masalah ini dan menguji kinerjanya dalam pelaksanaan yang kami sebut IRLbot. Dalam eksperimen baru-baru ini yang berlangsung selama 41 hari, IRLbot yang berjalan di satu pelayan berjaya merangkak 6,3 miliar halaman HTML yang sah (permintaan sambungan $ 7,6 miliar $) dan mengekalkan kadar muat turun purata 319 mb / s (1,789 halaman / s). Tidak seperti eksperimen kami sebelumnya dengan algoritma yang dicadangkan dalam karya yang berkaitan, versi IRLbot ini tidak mengalami kesulitan dan berjaya menangani kandungan dari lebih dari 117 juta host, menguraikan 394 bilion pautan, dan menemui subset grafik web dengan 41 bilion nod unik. [[EENNDD]] berskala besar; irlbot; merangkak"], [{"string": "sTeam - Designing an integrative infrastructure for Web-based computer-supported cooperative learning An abstract is not available.", "keywords": ["web-based learning and teaching", "steam - structuring information in a team", "cooperation support", "learner-centered approaches", "cooperative learning"], "combined": "sTeam - Designing an integrative infrastructure for Web-based computer-supported cooperative learning An abstract is not available. [[EENNDD]] web-based learning and teaching; steam - structuring information in a team; cooperation support; learner-centered approaches; cooperative learning"}, "sTeam - Merancang infrastruktur integratif untuk pembelajaran koperatif yang disokong oleh komputer berasaskan Web Abstrak tidak tersedia. [[EENNDD]] pembelajaran dan pengajaran berasaskan web; wap - menyusun maklumat dalam satu pasukan; sokongan kerjasama; pendekatan berpusatkan pelajar; pembelajaran koperatif"], [{"string": "Choosing the best knowledge base system for large semantic web applications No contact information provided yet.", "keywords": ["evaluation", "knowledge base system", "daml+oil", "semantic web", "benchmark"], "combined": "Choosing the best knowledge base system for large semantic web applications No contact information provided yet. [[EENNDD]] evaluation; knowledge base system; daml+oil; semantic web; benchmark"}, "Memilih sistem asas pengetahuan terbaik untuk aplikasi web semantik besar Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penilaian; sistem asas pengetahuan; daml + minyak; web semantik; penanda aras"], [{"string": "Dynamic placement for clustered web applications No contact information provided yet.", "keywords": ["system management", "performance management", "dynamic application placement"], "combined": "Dynamic placement for clustered web applications No contact information provided yet. [[EENNDD]] system management; performance management; dynamic application placement"}, "Penempatan dinamik untuk aplikasi web berkelompok Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengurusan sistem; pengurusan Prestasi; penempatan aplikasi dinamik"], [{"string": "XSLT by example No contact information provided yet.", "keywords": ["xquery", "data manipulation languages", "semi-structured data", "design tools and techniques", "visual query languages", "xml"], "combined": "XSLT by example No contact information provided yet. [[EENNDD]] xquery; data manipulation languages; semi-structured data; design tools and techniques; visual query languages; xml"}, "XSLT dengan contoh Belum ada maklumat hubungan yang diberikan. [[EENNDD]] xquery; bahasa manipulasi data; data separa berstruktur; alat dan teknik reka bentuk; bahasa pertanyaan visual; xml"], [{"string": "Automatic web news extraction using tree edit distance No contact information provided yet.", "keywords": ["web", "schema inference", "edit distance", "data extraction"], "combined": "Automatic web news extraction using tree edit distance No contact information provided yet. [[EENNDD]] web; schema inference; edit distance; data extraction"}, "Pengekstrakan berita web automatik menggunakan jarak edit pokok Belum ada maklumat hubungan yang diberikan. [[EENNDD]] web; inferens skema; mengedit jarak; pengekstrakan data"], [{"string": "LINKREC: a unified framework for link recommendation with user attributes and graph structure With the phenomenal success of networking sites (e.g., Facebook, Twitter and LinkedIn), social networks have drawn substantial attention. On online social networking sites, link recommendation is a critical task that not only helps improve user experience but also plays an essential role in network growth. In this paper we propose several link recommendation criteria, based on both user attributes and graph structure. To discover the candidates that satisfy these criteria, link relevance is estimated using a random walk algorithm on an augmented social graph with both attribute and structure information. The global and local influence of the attributes is leveraged in the framework as well. Besides link recommendation, our framework can also rank attributes in a social network. Experiments on DBLP and IMDB data sets demonstrate that our method outperforms state-of-the-art methods based on network structure and node attribute information for link recommendation.", "keywords": ["random walk", "link recommendation"], "combined": "LINKREC: a unified framework for link recommendation with user attributes and graph structure With the phenomenal success of networking sites (e.g., Facebook, Twitter and LinkedIn), social networks have drawn substantial attention. On online social networking sites, link recommendation is a critical task that not only helps improve user experience but also plays an essential role in network growth. In this paper we propose several link recommendation criteria, based on both user attributes and graph structure. To discover the candidates that satisfy these criteria, link relevance is estimated using a random walk algorithm on an augmented social graph with both attribute and structure information. The global and local influence of the attributes is leveraged in the framework as well. Besides link recommendation, our framework can also rank attributes in a social network. Experiments on DBLP and IMDB data sets demonstrate that our method outperforms state-of-the-art methods based on network structure and node attribute information for link recommendation. [[EENNDD]] random walk; link recommendation"}, "LINKREC: kerangka terpadu untuk cadangan pautan dengan atribut pengguna dan struktur grafik Dengan kejayaan laman web rangkaian (seperti Facebook, Twitter dan LinkedIn), rangkaian sosial telah menarik perhatian. Di laman rangkaian sosial dalam talian, cadangan pautan adalah tugas penting yang bukan hanya membantu meningkatkan pengalaman pengguna tetapi juga memainkan peranan penting dalam pertumbuhan rangkaian. Dalam makalah ini kami mencadangkan beberapa kriteria cadangan pautan, berdasarkan atribut pengguna dan struktur grafik. Untuk menemui calon yang memenuhi kriteria ini, relevansi pautan dianggarkan menggunakan algoritma jalan rawak pada grafik sosial tambahan dengan maklumat atribut dan struktur. Pengaruh atribut global dan tempatan dimanfaatkan dalam kerangka juga. Selain cadangan pautan, kerangka kerja kami juga dapat menentukan atribut dalam jaringan sosial. Eksperimen pada set data DBLP dan IMDB menunjukkan bahawa kaedah kami mengungguli kaedah canggih berdasarkan struktur rangkaian dan maklumat atribut nod untuk cadangan pautan. [[EENNDD]] jalan rawak; cadangan pautan"], [{"string": "Position paper: ontology construction from online ontologies No contact information provided yet.", "keywords": ["automatic ontology construction", "ontology reuse"], "combined": "Position paper: ontology construction from online ontologies No contact information provided yet. [[EENNDD]] automatic ontology construction; ontology reuse"}, "Kertas kedudukan: pembinaan ontologi dari ontologi dalam talian Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pembinaan ontologi automatik; penggunaan semula ontologi"], [{"string": "Educanext: a framework for sharing live educational resources with isabel No contact information provided yet.", "keywords": ["learning resource", "educational mediators", "live collaboration over the internet", "ieee standard", "isabel application", "videoconferencing", "educational activity", "lom", "educanext"], "combined": "Educanext: a framework for sharing live educational resources with isabel No contact information provided yet. [[EENNDD]] learning resource; educational mediators; live collaboration over the internet; ieee standard; isabel application; videoconferencing; educational activity; lom; educanext"}, "Educanext: kerangka kerja untuk berkongsi sumber pendidikan langsung dengan isabel Belum ada maklumat hubungan yang diberikan. [[EENNDD]] sumber pembelajaran; pengantara pendidikan; kolaborasi langsung melalui internet; standard ieee; permohonan isabel; persidangan video; aktiviti pendidikan; lom; educanext"], [{"string": "Sitemaps: above and beyond the crawl of duty Comprehensive coverage of the public web is crucial to web search engines. Search engines use crawlers to retrieve pages and then discover new ones by extracting the pages' outgoing links. However, the set of pages reachable from the publicly linked web is estimated to be significantly smaller than the invisible web, the set of documents that have no incoming links and can only be retrieved through web applications and web forms. The Sitemaps protocol is a fast-growing web protocol supported jointly by major search engines to help content creators and search engines unlock this hidden data by making it available to search engines. In this paper, we perform a detailed study of how \"classic\" discovery crawling compares with Sitemaps, in key measures such as coverage and freshness over key representative websites as well as over billions of URLs seen at Google. We observe that Sitemaps and discovery crawling complement each other very well, and offer different tradeoffs.", "keywords": ["search engines", "quality", "metrics", "crawling", "sitemaps"], "combined": "Sitemaps: above and beyond the crawl of duty Comprehensive coverage of the public web is crucial to web search engines. Search engines use crawlers to retrieve pages and then discover new ones by extracting the pages' outgoing links. However, the set of pages reachable from the publicly linked web is estimated to be significantly smaller than the invisible web, the set of documents that have no incoming links and can only be retrieved through web applications and web forms. The Sitemaps protocol is a fast-growing web protocol supported jointly by major search engines to help content creators and search engines unlock this hidden data by making it available to search engines. In this paper, we perform a detailed study of how \"classic\" discovery crawling compares with Sitemaps, in key measures such as coverage and freshness over key representative websites as well as over billions of URLs seen at Google. We observe that Sitemaps and discovery crawling complement each other very well, and offer different tradeoffs. [[EENNDD]] search engines; quality; metrics; crawling; sitemaps"}, "Peta Laman: di atas dan di luar jangkauan tugas Liputan menyeluruh web awam sangat penting untuk mesin carian web. Enjin carian menggunakan crawler untuk mengambil halaman dan kemudian mencari yang baru dengan mengekstrak pautan keluar halaman. Walau bagaimanapun, kumpulan halaman yang dapat dicapai dari web yang dihubungkan secara umum dianggarkan jauh lebih kecil daripada web yang tidak kelihatan, kumpulan dokumen yang tidak mempunyai pautan masuk dan hanya dapat diambil melalui aplikasi web dan borang web. Protokol Peta Laman adalah protokol web yang berkembang pesat yang disokong bersama oleh mesin pencari utama untuk membantu pencipta kandungan dan mesin pencari membuka kunci data tersembunyi ini dengan menjadikannya tersedia untuk mesin pencari. Dalam makalah ini, kami melakukan kajian terperinci mengenai bagaimana perayapan penemuan \"klasik\" dibandingkan dengan Peta Laman, dalam langkah-langkah utama seperti liputan dan kesegaran di laman web perwakilan utama serta lebih daripada berbilion URL yang dilihat di Google. Kami melihat bahawa Peta Situs dan perayapan penemuan saling melengkapi dengan baik, dan menawarkan pertukaran yang berbeza. [[EENNDD]] enjin carian; kualiti; sukatan; merangkak; peta laman"], [{"string": "Statistical properties of community structure in large social and information networks A large body of work has been devoted to identifying community structure in networks. A community is often though of as a set of nodes that has more connections between its members than to the remainder of the network. In this paper, we characterize as a function of size the statistical and structural properties of such sets of nodes. We define the network community profile plot, which characterizes the \"best\" possible community - according to the conductance measure - over a wide range of size scales, and we study over 70 large sparse real-world networks taken from a wide range of application domains. Our results suggest a significantly more refined picture of community structure in large real-world networks than has been appreciated previously.", "keywords": ["community structure", "random walks", "social networks", "conductance", "graph partitioning"], "combined": "Statistical properties of community structure in large social and information networks A large body of work has been devoted to identifying community structure in networks. A community is often though of as a set of nodes that has more connections between its members than to the remainder of the network. In this paper, we characterize as a function of size the statistical and structural properties of such sets of nodes. We define the network community profile plot, which characterizes the \"best\" possible community - according to the conductance measure - over a wide range of size scales, and we study over 70 large sparse real-world networks taken from a wide range of application domains. Our results suggest a significantly more refined picture of community structure in large real-world networks than has been appreciated previously. [[EENNDD]] community structure; random walks; social networks; conductance; graph partitioning"}, "Sifat statistik struktur komuniti dalam rangkaian sosial dan maklumat yang besar Sebilangan besar pekerjaan telah dikhaskan untuk mengenal pasti struktur masyarakat dalam rangkaian. Komuniti sering dianggap sebagai sekumpulan nod yang mempunyai lebih banyak hubungan antara anggotanya daripada rangkaian yang lain. Dalam makalah ini, kita mencirikan sebagai fungsi ukuran sifat statistik dan struktur set nod tersebut. Kami menentukan plot profil komuniti rangkaian, yang mencirikan komuniti \"terbaik\" yang mungkin - mengikut ukuran tingkah laku - pada skala skala yang luas, dan kami mengkaji lebih dari 70 rangkaian dunia nyata yang jarang diambil dari pelbagai domain aplikasi . Hasil kajian kami menunjukkan gambaran struktur masyarakat yang jauh lebih halus dalam rangkaian dunia nyata yang besar daripada yang telah dihargai sebelumnya. [[EENNDD]] struktur komuniti; jalan rawak; rangkaian sosial; kekonduksian; pembahagian graf"], [{"string": "Retrieving multimedia web objects based on PageRank algorithm No contact information provided yet.", "keywords": ["multimedia retrieval", "pagerank", "content based retrieval", "hyperlink analysis", "hits", "web search engines"], "combined": "Retrieving multimedia web objects based on PageRank algorithm No contact information provided yet. [[EENNDD]] multimedia retrieval; pagerank; content based retrieval; hyperlink analysis; hits; web search engines"}, "Mengambil objek web multimedia berdasarkan algoritma PageRank Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengambilan multimedia; pagerank; pengambilan berdasarkan kandungan; analisis hiperpautan; hits; enjin carian web"], [{"string": "Groupme! This paper presents the GroupMe! system, a resource sharing system with advanced tagging functionality. GroupMe! provides a novel user interface, which enables users to organize and arrange arbitrary Web resources into groups. The content of such groups can be overlooked and inspected immediately as resources are visualized in a multimedia-based fashion. In this paper, we furthermore introduce new folksonomy-based ranking strategies that exploit the group structure shipped with GroupMe! folksonomies. Experiments show that those strategies significantly improve the performance of such ranking algorithms.", "keywords": ["ranking", "folksonomies", "miscellaneous", "groupme!", "social media", "search"], "combined": "Groupme! This paper presents the GroupMe! system, a resource sharing system with advanced tagging functionality. GroupMe! provides a novel user interface, which enables users to organize and arrange arbitrary Web resources into groups. The content of such groups can be overlooked and inspected immediately as resources are visualized in a multimedia-based fashion. In this paper, we furthermore introduce new folksonomy-based ranking strategies that exploit the group structure shipped with GroupMe! folksonomies. Experiments show that those strategies significantly improve the performance of such ranking algorithms. [[EENNDD]] ranking; folksonomies; miscellaneous; groupme!; social media; search"}, "Kumpulan! Makalah ini membentangkan GroupMe! sistem, sistem perkongsian sumber dengan fungsi penandaan lanjutan. KumpulanMe! menyediakan antara muka pengguna baru, yang membolehkan pengguna mengatur dan mengatur sumber Web sewenang-wenangnya ke dalam kumpulan. Kandungan kumpulan seperti itu dapat dilihat dan diperiksa dengan segera kerana sumber daya digambarkan dengan cara yang berasaskan multimedia. Dalam makalah ini, kami juga memperkenalkan strategi peringkat berdasarkan folksonomi baru yang memanfaatkan struktur kumpulan yang dikirimkan bersama GroupMe! folksonomi. Eksperimen menunjukkan bahawa strategi tersebut meningkatkan prestasi algoritma peringkat sedemikian. [[EENNDD]] kedudukan; folksonomi; pelbagai; kumpulan !; media sosial; cari"], [{"string": "Unified analysis of streaming news News clustering, categorization and analysis are key components of any news portal. They require algorithms capable of dealing with dynamic data to cluster, interpret and to temporally aggregate news articles. These three tasks are often solved separately. In this paper we present a unified framework to group incoming news articles into temporary but tightly-focused storylines, to identify prevalent topics and key entities within these stories, and to reveal the temporal structure of stories as they evolve. We achieve this by building a hybrid clustering and topic model. To deal with the available wealth of data we build an efficient parallel inference algorithm by sequential Monte Carlo estimation. Time and memory costs are nearly constant in the length of the history, and the approach scales to hundreds of thousands of documents. We demonstrate the efficiency and accuracy on the publicly available TDT dataset and data of a major internet news site.", "keywords": ["dirichlet processes", "topic models", "learning", "online inference"], "combined": "Unified analysis of streaming news News clustering, categorization and analysis are key components of any news portal. They require algorithms capable of dealing with dynamic data to cluster, interpret and to temporally aggregate news articles. These three tasks are often solved separately. In this paper we present a unified framework to group incoming news articles into temporary but tightly-focused storylines, to identify prevalent topics and key entities within these stories, and to reveal the temporal structure of stories as they evolve. We achieve this by building a hybrid clustering and topic model. To deal with the available wealth of data we build an efficient parallel inference algorithm by sequential Monte Carlo estimation. Time and memory costs are nearly constant in the length of the history, and the approach scales to hundreds of thousands of documents. We demonstrate the efficiency and accuracy on the publicly available TDT dataset and data of a major internet news site. [[EENNDD]] dirichlet processes; topic models; learning; online inference"}, "Analisis bersatu untuk streaming berita Pengelompokan, pengkategorian dan analisis berita adalah komponen penting dari mana-mana portal berita. Mereka memerlukan algoritma yang mampu menangani data dinamik untuk mengumpulkan, menafsirkan dan menggabungkan artikel berita secara sementara. Ketiga-tiga tugas ini sering diselesaikan secara berasingan. Dalam makalah ini kami menyajikan kerangka kerja terpadu untuk mengelompokkan artikel berita masuk ke jalan cerita sementara tetapi berfokus ketat, untuk mengenal pasti topik dan entitas utama yang ada dalam cerita-cerita ini, dan untuk mengungkapkan struktur temporal cerita ketika mereka berkembang. Kami mencapainya dengan membina model pengelompokan dan topik hibrid. Untuk menangani kekayaan data yang ada, kami membina algoritma inferens selari yang efisien dengan anggaran Monte Carlo yang berurutan. Kos masa dan ingatan hampir berterusan sepanjang sejarah, dan pendekatannya mencapai ratusan ribu dokumen. Kami menunjukkan kecekapan dan ketepatan pada kumpulan data dan data TDT yang tersedia untuk umum dari laman berita internet utama. [[EENNDD]] proses dirichlet; model topik; belajar; inferens dalam talian"], [{"string": "Budget constrained bidding in keyword auctions and online knapsack problems We consider the budget-constrained bidding optimization problem for sponsored search auctions, and model it as an online (multiple-choice) knapsack problem. We design both deterministic and randomized algorithms for the online (multiple-choice) knapsack problems achieving a provably optimal competitive ratio. This translates back to fully automatic bidding strategies maximizing either profit or revenue for the budget-constrained advertiser. Our bidding strategy for revenue maximization is oblivious (i.e., without knowledge) of other bidders' prices and/or click-through-rates for those positions. We evaluate our bidding algorithms using both synthetic data and real bidding data gathered manually, and also discuss a sniping heuristic that strictly improves bidding performance. With sniping and parameter tuning enabled, our bidding algorithms can achieve a performance ratio above 90% against the optimum by the omniscient bidder.", "keywords": ["online knapsack problem", "multiple-choice knapsack problem", "keyword bidding", "sponsored search auction"], "combined": "Budget constrained bidding in keyword auctions and online knapsack problems We consider the budget-constrained bidding optimization problem for sponsored search auctions, and model it as an online (multiple-choice) knapsack problem. We design both deterministic and randomized algorithms for the online (multiple-choice) knapsack problems achieving a provably optimal competitive ratio. This translates back to fully automatic bidding strategies maximizing either profit or revenue for the budget-constrained advertiser. Our bidding strategy for revenue maximization is oblivious (i.e., without knowledge) of other bidders' prices and/or click-through-rates for those positions. We evaluate our bidding algorithms using both synthetic data and real bidding data gathered manually, and also discuss a sniping heuristic that strictly improves bidding performance. With sniping and parameter tuning enabled, our bidding algorithms can achieve a performance ratio above 90% against the optimum by the omniscient bidder. [[EENNDD]] online knapsack problem; multiple-choice knapsack problem; keyword bidding; sponsored search auction"}, "Penawaran terhad belanjawan dalam lelongan kata kunci dan masalah ransel dalam talian Kami menganggap masalah pengoptimuman pembidaan terhad anggaran untuk lelongan carian yang ditaja, dan memodelkannya sebagai masalah ransel dalam talian (pelbagai pilihan). Kami merancang algoritma deterministik dan rawak untuk masalah ransel dalam talian (pelbagai pilihan) untuk mencapai nisbah persaingan yang terbukti optimum. Ini diterjemahkan kembali ke strategi penawaran automatik sepenuhnya yang memaksimumkan keuntungan atau pendapatan untuk pengiklan yang dibatasi anggaran. Strategi pembidaan kami untuk memaksimumkan pendapatan tidak menyedari (iaitu tanpa pengetahuan) mengenai harga pembida lain dan / atau kadar klik-tayang untuk kedudukan tersebut. Kami menilai algoritma pembidaan kami menggunakan kedua-dua data sintetik dan data penawaran sebenar yang dikumpulkan secara manual, dan juga membincangkan heuristik pemotongan yang benar-benar meningkatkan prestasi penawaran. Dengan pengaktifan sniping dan parameter, algoritma penawaran kami dapat mencapai nisbah prestasi melebihi 90% berbanding optimum oleh penawar yang maha mengetahui. [[EENNDD]] masalah ransel dalam talian; masalah ransel pelbagai pilihan; pembidaan kata kunci; lelongan carian tajaan"], [{"string": "Modeling redirection in geographically diverse server sets No contact information provided yet.", "keywords": ["performance of systems", "web traffic redirection", "server selection", "content distribution network"], "combined": "Modeling redirection in geographically diverse server sets No contact information provided yet. [[EENNDD]] performance of systems; web traffic redirection; server selection; content distribution network"}, "Pengalihan model dalam set pelayan yang berbeza dari segi geografi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] prestasi sistem; pengalihan trafik web; pemilihan pelayan; rangkaian pengedaran kandungan"], [{"string": "Designing an architecture for delivering mobile information services to the rural developing world No contact information provided yet.", "keywords": ["mobile phones", "rural development", "communications applications", "client-server distributed systems", "paper user interface", "ict", "mobile computing"], "combined": "Designing an architecture for delivering mobile information services to the rural developing world No contact information provided yet. [[EENNDD]] mobile phones; rural development; communications applications; client-server distributed systems; paper user interface; ict; mobile computing"}, "Merancang seni bina untuk memberikan perkhidmatan maklumat mudah alih ke dunia membangun luar bandar Belum ada maklumat hubungan yang diberikan. [[EENNDD]] telefon bimbit; pembangunan luar bandar; aplikasi komunikasi; sistem diedarkan pelayan pelanggan; antara muka pengguna kertas; ict; pengkomputeran mudah alih"], [{"string": "Crowdsourcing with endogenous entry We investigate the design of mechanisms to incentivize high quality outcomes in crowdsourcing environments with strategic agents, when entry is an endogenous, strategic choice. Modeling endogenous entry in crowdsourcing markets is important because there is a nonzero cost to making a contribution of any quality which can be avoided by not participating, and indeed many sites based on crowdsourced content do not have adequate participation. We use a mechanism with monotone, rank-based, rewards in a model where agents strategically make participation and quality choices to capture a wide variety of crowdsourcing environments, ranging from conventional crowdsourcing contests with monetary rewards such as TopCoder, to crowdsourced content as in online Q&amp;A forums.", "keywords": ["social computing", "crowdsourcing", "game theory", "user generated content", "mechanism design", "contest design"], "combined": "Crowdsourcing with endogenous entry We investigate the design of mechanisms to incentivize high quality outcomes in crowdsourcing environments with strategic agents, when entry is an endogenous, strategic choice. Modeling endogenous entry in crowdsourcing markets is important because there is a nonzero cost to making a contribution of any quality which can be avoided by not participating, and indeed many sites based on crowdsourced content do not have adequate participation. We use a mechanism with monotone, rank-based, rewards in a model where agents strategically make participation and quality choices to capture a wide variety of crowdsourcing environments, ranging from conventional crowdsourcing contests with monetary rewards such as TopCoder, to crowdsourced content as in online Q&amp;A forums. [[EENNDD]] social computing; crowdsourcing; game theory; user generated content; mechanism design; contest design"}, ""], [{"string": "Dynamic personalized pagerank in entity-relation graphs Extractors and taggers turn unstructured text into entity-relation(ER) graphs where nodes are entities (email, paper, person,conference, company) and edges are relations (wrote, cited,works-for). Typed proximity search of the form &lt;B&gt;type=personNEAR company~\"IBM\", paper~\"XML\"&lt;/B&gt; is an increasingly usefulsearch paradigm in ER graphs. Proximity search implementations either perform a Pagerank-like computation at query time, which is slow, or precompute, store and combine per-word Pageranks, which can be very expensive in terms of preprocessing time and space. We present HubRank, a new system for fast, dynamic, space-efficient proximity searches in ER graphs. During preprocessing, HubRank computesand indexes certain \"sketchy\" random walk fingerprints for a small fraction of nodes, carefully chosen using query log statistics. At query time, a small \"active\" subgraph is identified, bordered bynodes with indexed fingerprints. These fingerprints are adaptively loaded to various resolutions to form approximate personalized Pagerank vectors (PPVs). PPVs at remaining active nodes are now computed iteratively. We report on experiments with CiteSeer's ER graph and millions of real Cite Seer queries. Some representative numbers follow. On our testbed, HubRank preprocesses and indexes 52 times faster than whole-vocabulary PPV computation. A text index occupies 56 MB. Whole-vocabulary PPVs would consume 102GB. If PPVs are truncated to 56 MB, precision compared to true Pagerank drops to 0.55; incontrast, HubRank has precision 0.91 at 63MB. HubRank's average querytime is 200-300 milliseconds; query-time Pagerank computation takes 11 seconds on average.", "keywords": ["graph proximity search", "personalized pagerank"], "combined": "Dynamic personalized pagerank in entity-relation graphs Extractors and taggers turn unstructured text into entity-relation(ER) graphs where nodes are entities (email, paper, person,conference, company) and edges are relations (wrote, cited,works-for). Typed proximity search of the form &lt;B&gt;type=personNEAR company~\"IBM\", paper~\"XML\"&lt;/B&gt; is an increasingly usefulsearch paradigm in ER graphs. Proximity search implementations either perform a Pagerank-like computation at query time, which is slow, or precompute, store and combine per-word Pageranks, which can be very expensive in terms of preprocessing time and space. We present HubRank, a new system for fast, dynamic, space-efficient proximity searches in ER graphs. During preprocessing, HubRank computesand indexes certain \"sketchy\" random walk fingerprints for a small fraction of nodes, carefully chosen using query log statistics. At query time, a small \"active\" subgraph is identified, bordered bynodes with indexed fingerprints. These fingerprints are adaptively loaded to various resolutions to form approximate personalized Pagerank vectors (PPVs). PPVs at remaining active nodes are now computed iteratively. We report on experiments with CiteSeer's ER graph and millions of real Cite Seer queries. Some representative numbers follow. On our testbed, HubRank preprocesses and indexes 52 times faster than whole-vocabulary PPV computation. A text index occupies 56 MB. Whole-vocabulary PPVs would consume 102GB. If PPVs are truncated to 56 MB, precision compared to true Pagerank drops to 0.55; incontrast, HubRank has precision 0.91 at 63MB. HubRank's average querytime is 200-300 milliseconds; query-time Pagerank computation takes 11 seconds on average. [[EENNDD]] graph proximity search; personalized pagerank"}, ""], [{"string": "Visualising student tracking data to support instructors in web-based distance education No contact information provided yet.", "keywords": ["student tracking", "information visualization", "web-based distance education"], "combined": "Visualising student tracking data to support instructors in web-based distance education No contact information provided yet. [[EENNDD]] student tracking; information visualization; web-based distance education"}, "Memvisualisasikan data penjejakan pelajar untuk menyokong tenaga pengajar dalam pendidikan jarak jauh berasaskan web Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] penjejakan pelajar; visualisasi maklumat; pendidikan jarak jauh berasaskan web"], [{"string": "Intelligent crawling on the World Wide Web with arbitrary predicates An abstract is not available.", "keywords": ["interaction styles", "world wide web", "crawling", "querying"], "combined": "Intelligent crawling on the World Wide Web with arbitrary predicates An abstract is not available. [[EENNDD]] interaction styles; world wide web; crawling; querying"}, "Penjelajahan pintar di World Wide Web dengan predikat sewenang-wenangnya Abstrak tidak tersedia. [[EENNDD]] gaya interaksi; laman web seluruh dunia; merangkak; membuat pertanyaan"], [{"string": "Near real time information mining in multilingual news This paper presents a near real-time multilingual news monitoring and analysis system that forms the backbone of our research work. The system integrates technologies to address the problems related to information extraction and analysis of open source intelligence on the World Wide Web. By chaining together different techniques in text mining, automated machine learning and statistical analysis, we can automatically determine who, where and, to a certain extent, what is being reported in news articles.", "keywords": ["multi-linguality", "open source text", "automated media monitoring", "information mining and analysis"], "combined": "Near real time information mining in multilingual news This paper presents a near real-time multilingual news monitoring and analysis system that forms the backbone of our research work. The system integrates technologies to address the problems related to information extraction and analysis of open source intelligence on the World Wide Web. By chaining together different techniques in text mining, automated machine learning and statistical analysis, we can automatically determine who, where and, to a certain extent, what is being reported in news articles. [[EENNDD]] multi-linguality; open source text; automated media monitoring; information mining and analysis"}, "Hampir penambangan maklumat masa nyata dalam berita pelbagai bahasa Makalah ini membentangkan sistem pemantauan dan analisis berita pelbagai bahasa hampir masa nyata yang menjadi tulang belakang kerja penyelidikan kami. Sistem ini mengintegrasikan teknologi untuk mengatasi masalah yang berkaitan dengan pengekstrakan maklumat dan analisis kecerdasan sumber terbuka di World Wide Web. Dengan mengikat teknik yang berbeza dalam perlombongan teks, pembelajaran mesin automatik dan analisis statistik, kita dapat secara automatik menentukan siapa, di mana dan, sampai batas tertentu, apa yang dilaporkan dalam artikel berita. [[EENNDD]] pelbagai bahasa; teks sumber terbuka; pemantauan media automatik; perlombongan dan analisis maklumat"], [{"string": "XJ: facilitating XML processing in Java No contact information provided yet.", "keywords": ["document preparation", "language constructs and features", "xml", "java", "language design"], "combined": "XJ: facilitating XML processing in Java No contact information provided yet. [[EENNDD]] document preparation; language constructs and features; xml; java; language design"}, "XJ: memudahkan pemprosesan XML di Java Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penyediaan dokumen; konstruk dan ciri bahasa; xml; jawa; reka bentuk bahasa"], [{"string": "Determining user interests about museum collections No contact information provided yet.", "keywords": ["personalization", "user profiling", "knowledge representation formalisms and methods", "semantic browsing", "museum collections", "recommender systems"], "combined": "Determining user interests about museum collections No contact information provided yet. [[EENNDD]] personalization; user profiling; knowledge representation formalisms and methods; semantic browsing; museum collections; recommender systems"}, "Menentukan minat pengguna mengenai koleksi muzium Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pemperibadian; profil pengguna; formalisme dan kaedah perwakilan pengetahuan; melayari semantik; koleksi muzium; sistem cadangan"], [{"string": "Invisible participants: how cultural capital relates to lurking behavior No contact information provided yet.", "keywords": ["e-learning", "cultural capital", "web forums", "lurking"], "combined": "Invisible participants: how cultural capital relates to lurking behavior No contact information provided yet. [[EENNDD]] e-learning; cultural capital; web forums; lurking"}, "Peserta yang tidak dapat dilihat: bagaimana modal budaya berkaitan dengan tingkah laku bersembunyi Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] e-pembelajaran; modal budaya; forum web; mengintai"], [{"string": "Association search in semantic web: search + inference No contact information provided yet.", "keywords": ["ontology", "knowledge management", "inference", "bayesian network"], "combined": "Association search in semantic web: search + inference No contact information provided yet. [[EENNDD]] ontology; knowledge management; inference; bayesian network"}, "Pencarian persatuan dalam web semantik: carian + inferensi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ontologi; pengurusan pengetahuan; kesimpulan; rangkaian bayesian"], [{"string": "An automatic semantic relationships discovery approach No contact information provided yet.", "keywords": ["data mining", "semantic link network", "semantic web", "algorithm", "analogical reasoning"], "combined": "An automatic semantic relationships discovery approach No contact information provided yet. [[EENNDD]] data mining; semantic link network; semantic web; algorithm; analogical reasoning"}, "Pendekatan penemuan hubungan semantik automatik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlombongan data; rangkaian pautan semantik; web semantik; algoritma; penaakulan analog"], [{"string": "Mining cultural differences from a large number of geotagged photos We propose a novel method to detect cultural differences over the world automatically by using a large amount of geotagged images on the photo sharingWeb sites such as Flickr. We employ the state-of-the-art object recognition technique developed in the research community of computer vision to mine representative photos of the given concept for representative local regions from a large-scale unorganized collection of consumer-generated geotagged photos. The results help us understand how objects, scenes or events corresponding to the same given concept are visually different depending on local regions over the world.", "keywords": ["information search and retrieval", "geotag", "object recognition", "representative images", "flickr"], "combined": "Mining cultural differences from a large number of geotagged photos We propose a novel method to detect cultural differences over the world automatically by using a large amount of geotagged images on the photo sharingWeb sites such as Flickr. We employ the state-of-the-art object recognition technique developed in the research community of computer vision to mine representative photos of the given concept for representative local regions from a large-scale unorganized collection of consumer-generated geotagged photos. The results help us understand how objects, scenes or events corresponding to the same given concept are visually different depending on local regions over the world. [[EENNDD]] information search and retrieval; geotag; object recognition; representative images; flickr"}, "Menambang perbezaan budaya dari sebilangan besar foto geotag. Kami mencadangkan kaedah baru untuk mengesan perbezaan budaya di seluruh dunia secara automatik dengan menggunakan sejumlah besar gambar geotag pada laman web perkongsian foto seperti Flickr. Kami menggunakan teknik pengecaman objek canggih yang dikembangkan dalam komuniti penyelidikan visi komputer untuk menambang foto perwakilan konsep yang diberikan untuk wilayah tempatan yang mewakili dari koleksi foto geoteg yang dihasilkan oleh pengguna berskala besar. Hasilnya membantu kita memahami bagaimana objek, pemandangan atau peristiwa yang sesuai dengan konsep yang diberikan berbeza secara visual bergantung pada kawasan tempatan di seluruh dunia. [[EENNDD]] carian dan pengambilan maklumat; geoteg; pengecaman objek; gambar wakil; flickr"], [{"string": "Using semantic web approach in augmented audio reality system for museum visitors No contact information provided yet.", "keywords": ["inference rules", "user model", "ontologies", "augmented-audio reality"], "combined": "Using semantic web approach in augmented audio reality system for museum visitors No contact information provided yet. [[EENNDD]] inference rules; user model; ontologies; augmented-audio reality"}, "Menggunakan pendekatan web semantik dalam sistem realiti audio tambahan untuk pengunjung muzium Belum ada maklumat hubungan yang diberikan. [[EENNDD]] peraturan inferens; model pengguna; ontologi; augmented-audio reality"], [{"string": "Metadata co-development: a process resulting in metadata about technical assistance to educators No contact information provided yet.", "keywords": ["computer uses in education", "semantic web", "education", "technical assistance", "rdf", "metadata", "resource cataloging"], "combined": "Metadata co-development: a process resulting in metadata about technical assistance to educators No contact information provided yet. [[EENNDD]] computer uses in education; semantic web; education; technical assistance; rdf; metadata; resource cataloging"}, "Pembangunan bersama metadata: proses yang menghasilkan metadata mengenai bantuan teknikal kepada pendidik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penggunaan komputer dalam pendidikan; web semantik; pendidikan; pembantu teknikal; rdf; metadata; pengkatalog sumber"], [{"string": "GoGetIt!: a tool for generating structure-driven web crawlers No contact information provided yet.", "keywords": ["web data extraction", "tree edit distance", "web crawlers"], "combined": "GoGetIt!: a tool for generating structure-driven web crawlers No contact information provided yet. [[EENNDD]] web data extraction; tree edit distance; web crawlers"}, "GoGetIt !: alat untuk menghasilkan perayap web berdasarkan struktur Belum ada maklumat hubungan yang disediakan. [[EENNDD]] pengekstrakan data web; jarak edit pokok; perayap web"], [{"string": "Learning block importance models for web pages No contact information provided yet.", "keywords": ["classification", "page segmentation", "miscellaneous", "block importance model", "web mining"], "combined": "Learning block importance models for web pages No contact information provided yet. [[EENNDD]] classification; page segmentation; miscellaneous; block importance model; web mining"}, "Model kepentingan blok pembelajaran untuk laman web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] klasifikasi; pembahagian halaman; pelbagai; model kepentingan blok; perlombongan web"], [{"string": "Design, implementation, and evaluation of a client characterization driven web server No contact information provided yet.", "keywords": ["general", "httperf", "computer system implementation", "server adaptation", "client classification", "performance of systems", "apache server", "web performance", "content delivery"], "combined": "Design, implementation, and evaluation of a client characterization driven web server No contact information provided yet. [[EENNDD]] general; httperf; computer system implementation; server adaptation; client classification; performance of systems; apache server; web performance; content delivery"}, "Reka bentuk, pelaksanaan, dan penilaian pelayan web berdasarkan ciri pelanggan Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] umum; httperf; pelaksanaan sistem komputer; penyesuaian pelayan; pengelasan pelanggan; prestasi sistem; pelayan apache; prestasi web; penyampaian kandungan"], [{"string": "Wake-on-WLAN No contact information provided yet.", "keywords": ["802.11 mesh network", "network architecture and design", "wake-on-wlan", "rural networking", "802.15.4", "power management"], "combined": "Wake-on-WLAN No contact information provided yet. [[EENNDD]] 802.11 mesh network; network architecture and design; wake-on-wlan; rural networking; 802.15.4; power management"}, "Wake-on-WLAN Belum ada maklumat hubungan yang diberikan. [[EENNDD]] Rangkaian mesh 802.11; seni bina dan reka bentuk rangkaian; bangun-di-wlan; rangkaian luar bandar; 802.15.4; pengurusan tenaga"], [{"string": "Learning causality for news events prediction The problem we tackle in this work is, given a present news event, to generate a plausible future event that can be caused by the given event. We present a new methodology for modeling and predicting such future news events using machine learning and data mining techniques. Our Pundit algorithm generalizes examples of causality pairs to infer a causality predictor. To obtain precise labeled causality examples, we mine 150 years of news articles, and apply semantic natural language modeling techniques to titles containing certain predefined causality patterns. For generalization, the model uses a vast amount of world knowledge ontologies mined from LinkedData, containing ~200 datasets with approximately 20 billion relations. Empirical evaluation on real news articles shows that our Pundit algorithm reaches a human-level performance.", "keywords": ["future prediction", "applications and expert systems", "news prediction", "web knowledge for future prediction"], "combined": "Learning causality for news events prediction The problem we tackle in this work is, given a present news event, to generate a plausible future event that can be caused by the given event. We present a new methodology for modeling and predicting such future news events using machine learning and data mining techniques. Our Pundit algorithm generalizes examples of causality pairs to infer a causality predictor. To obtain precise labeled causality examples, we mine 150 years of news articles, and apply semantic natural language modeling techniques to titles containing certain predefined causality patterns. For generalization, the model uses a vast amount of world knowledge ontologies mined from LinkedData, containing ~200 datasets with approximately 20 billion relations. Empirical evaluation on real news articles shows that our Pundit algorithm reaches a human-level performance. [[EENNDD]] future prediction; applications and expert systems; news prediction; web knowledge for future prediction"}, "Pembelajaran kausalitas untuk ramalan peristiwa berita Masalah yang kita atasi dalam karya ini adalah, jika diberikan acara berita sekarang, untuk menghasilkan peristiwa masa depan yang masuk akal yang dapat disebabkan oleh peristiwa yang diberikan. Kami menyajikan metodologi baru untuk memodelkan dan meramalkan peristiwa berita masa depan seperti menggunakan pembelajaran mesin dan teknik perlombongan data. Algoritma Pundit kami menggeneralisasi contoh pasangan sebab untuk menyimpulkan peramal penyebab. Untuk mendapatkan contoh kausalitas berlabel yang tepat, kami meneliti artikel berita selama 150 tahun, dan menerapkan teknik pemodelan bahasa semula jadi semantik pada tajuk yang mengandungi corak sebab akibat tertentu. Untuk generalisasi, model ini menggunakan sejumlah besar ontologi pengetahuan dunia yang ditambang dari LinkedData, yang mengandungi ~ 200 set data dengan kira-kira 20 bilion hubungan. Penilaian empirikal pada artikel berita sebenar menunjukkan bahawa algoritma Pundit kami mencapai prestasi di peringkat manusia. [[EENNDD]] ramalan masa depan; aplikasi dan sistem pakar; ramalan berita; pengetahuan web untuk ramalan masa depan"], [{"string": "The powerrank web link analysis algorithm No contact information provided yet.", "keywords": ["page rank algorithm", "graph theory", "power distribution", "hierarchy structure"], "combined": "The powerrank web link analysis algorithm No contact information provided yet. [[EENNDD]] page rank algorithm; graph theory; power distribution; hierarchy structure"}, "Algoritma analisis pautan web powerrank Belum ada maklumat hubungan yang diberikan. [[EENNDD]] algoritma kedudukan halaman; teori grafik; pengagihan kuasa; struktur hierarki"], [{"string": "Building reactive web applications No contact information provided yet.", "keywords": ["user modeling", "eca rule", "adaptive web", "design method"], "combined": "Building reactive web applications No contact information provided yet. [[EENNDD]] user modeling; eca rule; adaptive web; design method"}, "Membangun aplikasi web reaktif Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pemodelan pengguna; peraturan eca; web adaptif; kaedah reka bentuk"], [{"string": "Test case prioritization for regression testing of service-oriented business applications Regression testing assures the quality of modified service-oriented business applications against unintended changes. However, a typical regression test suite is large in size. Earlier execution of those test cases that may detect failures is attractive. Many existing prioritization techniques order test cases according to their respective coverage of program statements in a previous version of the application. On the other hand, industrial service-oriented business applications are typically written in orchestration languages such as WS-BPEL and integrated with workflow steps and web services via XPath and WSDL. Faults in these artifacts may cause the application to extract wrong data from messages, leading to failures in service compositions. Surprisingly, current regression testing research hardly considers these artifacts. We propose a multilevel coverage model to capture the business process, XPath, and WSDL from the perspective of regression testing. We develop a family of test case prioritization techniques atop the model. Empirical results show that our techniques can achieve significantly higher rates of fault detection than existing techniques.", "keywords": ["testing tools", "xpath", "wsdl", "test case prioritization", "service orientation"], "combined": "Test case prioritization for regression testing of service-oriented business applications Regression testing assures the quality of modified service-oriented business applications against unintended changes. However, a typical regression test suite is large in size. Earlier execution of those test cases that may detect failures is attractive. Many existing prioritization techniques order test cases according to their respective coverage of program statements in a previous version of the application. On the other hand, industrial service-oriented business applications are typically written in orchestration languages such as WS-BPEL and integrated with workflow steps and web services via XPath and WSDL. Faults in these artifacts may cause the application to extract wrong data from messages, leading to failures in service compositions. Surprisingly, current regression testing research hardly considers these artifacts. We propose a multilevel coverage model to capture the business process, XPath, and WSDL from the perspective of regression testing. We develop a family of test case prioritization techniques atop the model. Empirical results show that our techniques can achieve significantly higher rates of fault detection than existing techniques. [[EENNDD]] testing tools; xpath; wsdl; test case prioritization; service orientation"}, "Keutamaan kes ujian untuk ujian regresi aplikasi perniagaan berorientasikan perkhidmatan Uji regresi menjamin kualiti aplikasi perniagaan berorientasikan perkhidmatan yang diubah terhadap perubahan yang tidak diinginkan. Walau bagaimanapun, ukuran ujian regresi khas adalah besar. Pelaksanaan kes ujian yang lebih awal yang dapat mengesan kegagalan adalah menarik. Banyak teknik keutamaan yang ada memerintahkan kes ujian mengikut liputan pernyataan program masing-masing dalam versi aplikasi sebelumnya. Sebaliknya, aplikasi perniagaan berorientasikan perkhidmatan industri biasanya ditulis dalam bahasa orkestrasi seperti WS-BPEL dan disatukan dengan langkah-langkah aliran kerja dan perkhidmatan web melalui XPath dan WSDL. Kesalahan dalam artifak ini boleh menyebabkan aplikasi mengekstrak data yang salah dari mesej, yang menyebabkan kegagalan dalam komposisi perkhidmatan. Anehnya, penyelidikan ujian regresi semasa hampir tidak menganggap artifak ini. Kami mencadangkan model liputan bertingkat untuk menangkap proses perniagaan, XPath, dan WSDL dari perspektif pengujian regresi. Kami mengembangkan keluarga teknik mengutamakan kes ujian di atas model. Hasil empirikal menunjukkan bahawa teknik kita dapat mencapai kadar pengesanan kesalahan yang jauh lebih tinggi daripada teknik yang ada. [[EENNDD]] alat ujian; xpath; wsdl; keutamaan kes ujian; orientasi perkhidmatan"], [{"string": "Social search and discovery using a unified approach We explore new ways of improving a search engine using data from Web 2.0 applications such as blogs and social bookmarks. This data contains entities such as documents, people and tags, and relationships between them. We propose a simple yet effective method, based on faceted search, that treats all entities in a unified manner: returning all of them (documents, people and tags) on every search, and allowing all of them to be used as search terms. We describe an implementation of such a social search engine on the intranet of a large enterprise, and present large-scale experiments which verify the validity of our approach.", "keywords": ["enterprise search", "information search and retrieval", "faceted search", "social search"], "combined": "Social search and discovery using a unified approach We explore new ways of improving a search engine using data from Web 2.0 applications such as blogs and social bookmarks. This data contains entities such as documents, people and tags, and relationships between them. We propose a simple yet effective method, based on faceted search, that treats all entities in a unified manner: returning all of them (documents, people and tags) on every search, and allowing all of them to be used as search terms. We describe an implementation of such a social search engine on the intranet of a large enterprise, and present large-scale experiments which verify the validity of our approach. [[EENNDD]] enterprise search; information search and retrieval; faceted search; social search"}, "Pencarian dan penemuan sosial menggunakan pendekatan terpadu Kami meneroka cara baru untuk meningkatkan enjin carian menggunakan data dari aplikasi Web 2.0 seperti blog dan penanda buku sosial. Data ini mengandungi entiti seperti dokumen, orang dan tanda nama, dan hubungan antara mereka. Kami mencadangkan kaedah yang mudah tetapi berkesan, berdasarkan carian yang berwawasan, yang memperlakukan semua entiti secara bersatu: mengembalikan semuanya (dokumen, orang dan tag) pada setiap carian, dan membenarkan semuanya digunakan sebagai istilah carian. Kami menerangkan pelaksanaan mesin pencari sosial seperti itu di intranet perusahaan besar, dan menyajikan eksperimen berskala besar yang mengesahkan kesahihan pendekatan kami. [[EENNDD]] carian perusahaan; pencarian dan pengambilan maklumat; carian pelbagai segi; carian sosial"], [{"string": "Cat and mouse: content delivery tradeoffs in web access No contact information provided yet.", "keywords": ["privacy", "web registration", "anonymity", "computer-communication networks", "content blocking"], "combined": "Cat and mouse: content delivery tradeoffs in web access No contact information provided yet. [[EENNDD]] privacy; web registration; anonymity; computer-communication networks; content blocking"}, "Kucing dan tetikus: pertukaran penghantaran kandungan dalam akses web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] privasi; pendaftaran web; tanpa nama; rangkaian komunikasi komputer; penyekat kandungan"], [{"string": "A large-scale study of robots.txt Search engines largely rely on Web robots to collect information from the Web. Due to the unregulated open-access nature of the Web, robot activities are extremely diverse. Such crawling activities can be regulated from the server side by deploying the Robots Exclusion Protocol in a file called robots.txt. Although it is not an enforcement standard, ethical robots (and many commercial) will follow the rules specified in robots.txt. With our focused crawler, we investigate 7,593 websites from education, government, news, and business domains. Five crawls have been conducted in succession to study the temporal changes. Through statistical analysis of the data, we present a survey of the usage of Web robots rules at the Web scale. The results also show that the usage of robots.txt has increased over time.", "keywords": ["search engine", "robots.txt", "robots exclusion protocol", "crawler"], "combined": "A large-scale study of robots.txt Search engines largely rely on Web robots to collect information from the Web. Due to the unregulated open-access nature of the Web, robot activities are extremely diverse. Such crawling activities can be regulated from the server side by deploying the Robots Exclusion Protocol in a file called robots.txt. Although it is not an enforcement standard, ethical robots (and many commercial) will follow the rules specified in robots.txt. With our focused crawler, we investigate 7,593 websites from education, government, news, and business domains. Five crawls have been conducted in succession to study the temporal changes. Through statistical analysis of the data, we present a survey of the usage of Web robots rules at the Web scale. The results also show that the usage of robots.txt has increased over time. [[EENNDD]] search engine; robots.txt; robots exclusion protocol; crawler"}, "Kajian besar-besaran mengenai mesin pencari robots.txt banyak bergantung pada robot Web untuk mengumpulkan maklumat dari Web. Oleh kerana sifat akses terbuka Web yang tidak diatur, aktiviti robot sangat pelbagai. Kegiatan perayapan seperti itu dapat diatur dari sisi pelayan dengan menggunakan Protokol Pengecualian Robot dalam file yang disebut robots.txt. Walaupun ia bukan standard penguatkuasaan, robot etika (dan banyak komersial) akan mengikuti peraturan yang ditentukan dalam robots.txt. Dengan perayap fokus kami, kami menyiasat 7,593 laman web dari domain pendidikan, pemerintahan, berita, dan perniagaan. Lima perayapan telah dilakukan secara berturut-turut untuk mengkaji perubahan temporal. Melalui analisis statistik data, kami menyajikan tinjauan penggunaan peraturan robot Web pada skala Web. Hasilnya juga menunjukkan bahawa penggunaan robots.txt meningkat dari masa ke masa. [[EENNDD]] enjin carian; robots.txt; protokol pengecualian robot; perangkak"], [{"string": "Clustering for opportunistic communication No contact information provided yet.", "keywords": ["context", "awareness", "agents", "opportunistic communication", "collaboration", "critical mass"], "combined": "Clustering for opportunistic communication No contact information provided yet. [[EENNDD]] context; awareness; agents; opportunistic communication; collaboration; critical mass"}, "Pengelompokan untuk komunikasi oportunis Belum ada maklumat hubungan yang diberikan. [[EENNDD]] konteks; kesedaran; ejen; komunikasi oportunistik; kerjasama; jisim kritikal"], [{"string": "An intelligent distributed environment for active learning An abstract is not available.", "keywords": ["active learning", "xmeta", "xml", "web-based education", "multi-agent system"], "combined": "An intelligent distributed environment for active learning An abstract is not available. [[EENNDD]] active learning; xmeta; xml; web-based education; multi-agent system"}, "Persekitaran yang diedarkan pintar untuk pembelajaran aktif Abstrak tidak tersedia. [[EENNDD]] pembelajaran aktif; xmeta; xml; pendidikan berasaskan web; sistem pelbagai ejen"], [{"string": "A proposal for an owl rules language No contact information provided yet.", "keywords": ["representation", "model-theoretic semantics", "semantic web"], "combined": "A proposal for an owl rules language No contact information provided yet. [[EENNDD]] representation; model-theoretic semantics; semantic web"}, "Cadangan untuk bahasa peraturan burung hantu Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perwakilan; semantik model-teori; web semantik"], [{"string": "Information flow using edge stress factor No contact information provided yet.", "keywords": ["graph clustering", "social network analysis"], "combined": "Information flow using edge stress factor No contact information provided yet. [[EENNDD]] graph clustering; social network analysis"}, "Aliran maklumat menggunakan faktor tekanan tepi Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] pengelompokan grafik; analisis rangkaian sosial"], [{"string": "Clarifying the fundamentals of HTTP No contact information provided yet.", "keywords": ["http", "protocol design", "applications"], "combined": "Clarifying the fundamentals of HTTP No contact information provided yet. [[EENNDD]] http; protocol design; applications"}, "Menjelaskan asas HTTP Belum ada maklumat hubungan yang diberikan. [[EENNDD]] http; reka bentuk protokol; aplikasi"], [{"string": "Extracting context to improve accuracy for HTML content extraction No contact information provided yet.", "keywords": ["dom trees", "context", "accessibility", "html", "content extraction", "reformatting", "speech rendering", "electronic publishing"], "combined": "Extracting context to improve accuracy for HTML content extraction No contact information provided yet. [[EENNDD]] dom trees; context; accessibility; html; content extraction; reformatting; speech rendering; electronic publishing"}, "Mengekstrak konteks untuk meningkatkan ketepatan pengekstrakan kandungan HTML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pokok dom; konteks; kebolehcapaian; html; pengekstrakan kandungan; memformat semula; penyampaian ucapan; penerbitan elektronik"], [{"string": "The web structure of e-government - developing a methodology for quantitative evaluation No contact information provided yet.", "keywords": ["ranking", "network", "e-government", "webmetric", "national audit offices"], "combined": "The web structure of e-government - developing a methodology for quantitative evaluation No contact information provided yet. [[EENNDD]] ranking; network; e-government; webmetric; national audit offices"}, "Struktur web e-kerajaan - mengembangkan metodologi untuk penilaian kuantitatif Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kedudukan; rangkaian; e-kerajaan; webmetrik; pejabat audit negara"], [{"string": "Better abstractions for secure server-side scripting It is notoriously difficult to program a solid web application. Besides addressing web interactions, state maintenance, and whimsical user navigation behaviors, programmers must also avoid a minefield of security vulnerabilities. The problem is twofold. First, we lack a clear understanding of the new computation model underlying web applications. Second, we lack proper abstractions for hiding common and subtle coding details that are orthogonal to the business functionalities of specific web applications.", "keywords": ["server-side scripting", "web application security", "formal definitions and theory"], "combined": "Better abstractions for secure server-side scripting It is notoriously difficult to program a solid web application. Besides addressing web interactions, state maintenance, and whimsical user navigation behaviors, programmers must also avoid a minefield of security vulnerabilities. The problem is twofold. First, we lack a clear understanding of the new computation model underlying web applications. Second, we lack proper abstractions for hiding common and subtle coding details that are orthogonal to the business functionalities of specific web applications. [[EENNDD]] server-side scripting; web application security; formal definitions and theory"}, "Abstraksi yang lebih baik untuk skrip sisi pelayan yang selamat Sangat sukar untuk memprogramkan aplikasi web yang kukuh. Selain menangani interaksi web, penyelenggaraan keadaan, dan tingkah laku navigasi pengguna yang pelik, pengaturcara juga harus menghindari kelemahan ranjau keselamatan. Masalahnya dua kali ganda. Pertama, kita tidak mempunyai pemahaman yang jelas mengenai model pengiraan baru yang mendasari aplikasi web. Kedua, kami tidak mempunyai abstraksi yang tepat untuk menyembunyikan perincian pengekodan umum dan halus yang bersifat ortogonal dengan fungsi perniagaan aplikasi web tertentu. [[EENNDD]] skrip sisi pelayan; keselamatan aplikasi web; definisi formal dan teori"], [{"string": "A search-based method for forecasting ad impression in contextual advertising Contextual advertising (also called content match) refers to the placement of small textual ads within the content of a generic web page. It has become a significant source of revenue for publishers ranging from individual bloggers to major newspapers. At the same time it is an important way for advertisers to reach their intended audience. This reach depends on the total number of exposures of the ad (impressions) and its click-through-rate (CTR) that can be viewed as the probability of an end-user clicking on the ad when shown. These two orthogonal, critical factors are both difficult to estimate and even individually can still be very informative and useful in planning and budgeting advertising campaigns.", "keywords": ["content match", "contextual advertising", "information search and retrieval", "impression forecasting", "wand", "online advertising"], "combined": "A search-based method for forecasting ad impression in contextual advertising Contextual advertising (also called content match) refers to the placement of small textual ads within the content of a generic web page. It has become a significant source of revenue for publishers ranging from individual bloggers to major newspapers. At the same time it is an important way for advertisers to reach their intended audience. This reach depends on the total number of exposures of the ad (impressions) and its click-through-rate (CTR) that can be viewed as the probability of an end-user clicking on the ad when shown. These two orthogonal, critical factors are both difficult to estimate and even individually can still be very informative and useful in planning and budgeting advertising campaigns. [[EENNDD]] content match; contextual advertising; information search and retrieval; impression forecasting; wand; online advertising"}, "Kaedah berdasarkan carian untuk meramalkan tayangan iklan dalam periklanan kontekstual Pengiklanan kontekstual (juga disebut pencocokan konten) merujuk pada penempatan iklan teks kecil dalam konten halaman web generik. Ini telah menjadi sumber pendapatan yang besar bagi penerbit mulai dari blogger individu hingga surat khabar utama. Pada masa yang sama, ini adalah cara penting bagi pengiklan untuk menjangkau khalayak yang mereka inginkan. Jangkauan ini bergantung pada jumlah pendedahan iklan (tera) dan kadar klik-tayang (RKT) yang dapat dilihat sebagai kebarangkalian pengguna akhir mengklik iklan ketika ditunjukkan. Kedua-dua faktor kritikal ortogonal ini sukar dinilai dan bahkan secara individu masih boleh sangat bermaklumat dan berguna dalam merancang dan menganggarkan kempen iklan. [[EENNDD]] padanan kandungan; pengiklanan kontekstual; pencarian dan pengambilan maklumat; ramalan kesan; tongkat; iklan dalam talian"], [{"string": "A multi-threaded PIPELINED Web server architecture for SMP/SoC machines No contact information provided yet.", "keywords": ["asynchronous multi-process event-driven", "symmetric multi-processor", "multi-process", "multi-thread", "single event-driven process", "system-on-chip"], "combined": "A multi-threaded PIPELINED Web server architecture for SMP/SoC machines No contact information provided yet. [[EENNDD]] asynchronous multi-process event-driven; symmetric multi-processor; multi-process; multi-thread; single event-driven process; system-on-chip"}, "Senibina pelayan Web PIPELINED berbilang utas untuk mesin SMP / SoC Belum ada maklumat hubungan yang diberikan. [[EENNDD]] berdasarkan proses pelbagai proses tak segerak; multi-pemproses simetri; pelbagai proses; berbilang benang; proses berdasarkan acara tunggal; sistem-pada-cip"], [{"string": "Statistical models of music-listening sessions in social media User experience in social media involves rich interactions with the media content and other participants in the community. In order to support such communities, it is important to understand the factors that drive the users' engagement. In this paper we show how to define statistical models of different complexity to describe patterns of song listening in an online music community. First, we adapt the LDA model to capture music taste from listening activities across users and identify both the groups of songs associated with the specific taste and the groups of listeners who share the same taste. Second, we define a graphical model that takes into account listening sessions and captures the listening moods of users in the community. Our session model leads to groups of songs and groups of listeners with similar behavior across listening sessions and enables faster inference when compared to the LDA model. Our experiments with the data from an online media site demonstrate that the session model is better in terms of the perplexity compared to two other models: the LDA-based taste model that does not incorporate cross-session information and a baseline model that does not use latent groupings of songs.", "keywords": ["mood", "taste", "collaborative filtering", "graphical models", "recommendations", "database applications", "music", "sessions", "social media"], "combined": "Statistical models of music-listening sessions in social media User experience in social media involves rich interactions with the media content and other participants in the community. In order to support such communities, it is important to understand the factors that drive the users' engagement. In this paper we show how to define statistical models of different complexity to describe patterns of song listening in an online music community. First, we adapt the LDA model to capture music taste from listening activities across users and identify both the groups of songs associated with the specific taste and the groups of listeners who share the same taste. Second, we define a graphical model that takes into account listening sessions and captures the listening moods of users in the community. Our session model leads to groups of songs and groups of listeners with similar behavior across listening sessions and enables faster inference when compared to the LDA model. Our experiments with the data from an online media site demonstrate that the session model is better in terms of the perplexity compared to two other models: the LDA-based taste model that does not incorporate cross-session information and a baseline model that does not use latent groupings of songs. [[EENNDD]] mood; taste; collaborative filtering; graphical models; recommendations; database applications; music; sessions; social media"}, "Model statistik sesi mendengar muzik di media sosial Pengalaman pengguna di media sosial melibatkan interaksi yang kaya dengan kandungan media dan peserta lain dalam komuniti. Untuk menyokong komuniti tersebut, penting untuk memahami faktor-faktor yang mendorong penglibatan pengguna. Dalam makalah ini kami menunjukkan bagaimana mendefinisikan model statistik dengan kerumitan yang berbeza untuk menggambarkan corak mendengar lagu dalam komuniti muzik dalam talian. Pertama, kami menyesuaikan model LDA untuk menangkap rasa muzik dari aktiviti mendengar di kalangan pengguna dan mengenal pasti kedua-dua kumpulan lagu yang berkaitan dengan rasa tertentu dan kumpulan pendengar yang mempunyai rasa yang sama. Kedua, kami mendefinisikan model grafik yang mengambil kira sesi mendengar dan menangkap mood mendengar pengguna di komuniti. Model sesi kami membawa kepada kumpulan lagu dan kumpulan pendengar dengan tingkah laku yang serupa sepanjang sesi mendengar dan membolehkan inferens lebih cepat jika dibandingkan dengan model LDA. Eksperimen kami dengan data dari laman media dalam talian menunjukkan bahawa model sesi lebih baik dari segi kebingungan dibandingkan dengan dua model lain: model rasa berdasarkan LDA yang tidak memasukkan maklumat lintas sesi dan model asas yang tidak menggunakan kumpulan lagu pendam. [[EENNDD]] mood; rasa; penapisan kolaboratif; model grafik; cadangan; aplikasi pangkalan data; muzik; sesi; media sosial"], [{"string": "Modeling user behavior in recommender systems based on maximum entropy We propose a model for user purchase behavior in online stores that provide recommendation services. We model the purchase probability given recommendations for each user based on the maximum entropy principle using features that deal with recommendations and user interests. The proposed model enable us to measure the effect of recommendations on user purchase behavior, and the effect can be used to evaluate recommender systems. We show the validity of our model using the log data of an online cartoon distribution service, and measure the recommendation effects for evaluating the recommender system.", "keywords": ["user model", "recommendation", "maximum entropy principle"], "combined": "Modeling user behavior in recommender systems based on maximum entropy We propose a model for user purchase behavior in online stores that provide recommendation services. We model the purchase probability given recommendations for each user based on the maximum entropy principle using features that deal with recommendations and user interests. The proposed model enable us to measure the effect of recommendations on user purchase behavior, and the effect can be used to evaluate recommender systems. We show the validity of our model using the log data of an online cartoon distribution service, and measure the recommendation effects for evaluating the recommender system. [[EENNDD]] user model; recommendation; maximum entropy principle"}, "Memodelkan tingkah laku pengguna dalam sistem pengesyorkan berdasarkan entropi maksimum Kami mencadangkan model untuk tingkah laku pembelian pengguna di kedai dalam talian yang menyediakan perkhidmatan cadangan. Kami memodelkan kemungkinan pembelian yang diberikan cadangan untuk setiap pengguna berdasarkan prinsip entropi maksimum menggunakan ciri yang berkaitan dengan cadangan dan minat pengguna. Model yang dicadangkan membolehkan kita mengukur kesan cadangan terhadap tingkah laku pembelian pengguna, dan kesannya dapat digunakan untuk menilai sistem pengesyoran. Kami menunjukkan kesahan model kami menggunakan data log dari perkhidmatan pengedaran kartun dalam talian, dan mengukur kesan cadangan untuk menilai sistem pengesyorkan. [[EENNDD]] model pengguna; cadangan; prinsip entropi maksimum"], [{"string": "Template-based question answering over RDF data As an increasing amount of RDF data is published as Linked Data, intuitive ways of accessing this data become more and more important. Question answering approaches have been proposed as a good compromise between intuitiveness and expressivity. Most question answering systems translate questions into triples which are matched against the RDF data to retrieve an answer, typically relying on some similarity metric. However, in many cases, triples do not represent a faithful representation of the semantic structure of the natural language question, with the result that more expressive queries can not be answered. To circumvent this problem, we present a novel approach that relies on a parse of the question to produce a SPARQL template that directly mirrors the internal structure of the question. This template is then instantiated using statistical entity identification and predicate detection. We show that this approach is competitive and discuss cases of questions that can be answered with our approach but not with competing approaches.", "keywords": ["natural language patterns", "question answering", "sparql", "semantic web"], "combined": "Template-based question answering over RDF data As an increasing amount of RDF data is published as Linked Data, intuitive ways of accessing this data become more and more important. Question answering approaches have been proposed as a good compromise between intuitiveness and expressivity. Most question answering systems translate questions into triples which are matched against the RDF data to retrieve an answer, typically relying on some similarity metric. However, in many cases, triples do not represent a faithful representation of the semantic structure of the natural language question, with the result that more expressive queries can not be answered. To circumvent this problem, we present a novel approach that relies on a parse of the question to produce a SPARQL template that directly mirrors the internal structure of the question. This template is then instantiated using statistical entity identification and predicate detection. We show that this approach is competitive and discuss cases of questions that can be answered with our approach but not with competing approaches. [[EENNDD]] natural language patterns; question answering; sparql; semantic web"}, "Pertanyaan berdasarkan templat menjawab data RDF Sebagai semakin banyak data RDF diterbitkan sebagai Data Terhubung, cara intuitif untuk mengakses data ini menjadi semakin penting. Pendekatan menjawab soalan telah diusulkan sebagai kompromi yang baik antara intuitif dan ekspresif. Sebilangan besar sistem menjawab soalan menerjemahkan soalan menjadi tiga kali ganda yang dipadankan dengan data RDF untuk mendapatkan jawapan, biasanya bergantung pada beberapa metrik persamaan. Namun, dalam banyak kes, rangkap tiga tidak mewakili perwakilan setia struktur semantik dari pertanyaan bahasa semula jadi, dengan hasilnya pertanyaan yang lebih ekspresif tidak dapat dijawab. Untuk mengatasi masalah ini, kami menyajikan pendekatan baru yang bergantung pada parsial soalan untuk menghasilkan templat SPARQL yang secara langsung mencerminkan struktur dalaman soalan. Templat ini kemudian dibuat menggunakan pengenalan entiti statistik dan pengesanan predikat. Kami menunjukkan bahawa pendekatan ini kompetitif dan membincangkan kes-kes soalan yang dapat dijawab dengan pendekatan kami tetapi tidak dengan pendekatan bersaing. [[EENNDD]] corak bahasa semula jadi; menjawab soalan; sparql; web semantik"], [{"string": "Embedding MindMap as a service for user-driven composition of web applications The World Wide Web is evolving towards a very large distributed platform allowing ubiquitous access to a wide range of Web applications with minimal delay and no installation required. Such Web applications range from having users undertake simple tasks, such as filling a form, to more complex tasks including collaborative work, project management, and more generally, creating, consulting, annotating, and sharing Web content. However, users are lacking a simple but yet powerful mechanism to compose Web applications, similarly to what desktop environments allowed for decades using the file explorer paradigm and the desktop metaphor. Attempts have been made to adapt the desktop metaphor to the Web environment giving birth to Webtops (Web desktops). It essentially consisted of embedding a desktop environment in a Web browser and provide access to various Web applications within the same User Interface. However, those attempts did not take into consideration to the radical differences between Web and desktop environments and applications. In this work, we introduce a new approach for Web application composition based on the mindmap metaphor. It allows browsing artifacts (Web resources) and enabling user-driven composition of their associated Web applications. Essentially, a mindmap is a graph of widgets representing artifacts created or used by Web applications and allow to list and launch all possible Web applications associated to each artifact. A tool has been developed to experiment the new metaphor and is provided as a service to be embedded in Web applications via a Web browser's plug-in. We demonstrate in this paper three case studies regarding the DBLP Web site, Wikipedia and Google Picasa Web applications.", "keywords": ["software engineering", "web application", "miscellaneous", "mindmap", "user-driven composition"], "combined": "Embedding MindMap as a service for user-driven composition of web applications The World Wide Web is evolving towards a very large distributed platform allowing ubiquitous access to a wide range of Web applications with minimal delay and no installation required. Such Web applications range from having users undertake simple tasks, such as filling a form, to more complex tasks including collaborative work, project management, and more generally, creating, consulting, annotating, and sharing Web content. However, users are lacking a simple but yet powerful mechanism to compose Web applications, similarly to what desktop environments allowed for decades using the file explorer paradigm and the desktop metaphor. Attempts have been made to adapt the desktop metaphor to the Web environment giving birth to Webtops (Web desktops). It essentially consisted of embedding a desktop environment in a Web browser and provide access to various Web applications within the same User Interface. However, those attempts did not take into consideration to the radical differences between Web and desktop environments and applications. In this work, we introduce a new approach for Web application composition based on the mindmap metaphor. It allows browsing artifacts (Web resources) and enabling user-driven composition of their associated Web applications. Essentially, a mindmap is a graph of widgets representing artifacts created or used by Web applications and allow to list and launch all possible Web applications associated to each artifact. A tool has been developed to experiment the new metaphor and is provided as a service to be embedded in Web applications via a Web browser's plug-in. We demonstrate in this paper three case studies regarding the DBLP Web site, Wikipedia and Google Picasa Web applications. [[EENNDD]] software engineering; web application; miscellaneous; mindmap; user-driven composition"}, "Menanamkan MindMap sebagai perkhidmatan untuk komposisi aplikasi web yang didorong oleh pengguna World Wide Web berkembang menuju platform diedarkan yang sangat besar yang membolehkan akses di mana-mana ke pelbagai aplikasi Web dengan kelewatan minimum dan tidak diperlukan pemasangan. Aplikasi Web semacam itu berkisar dari meminta pengguna melakukan tugas-tugas sederhana, seperti mengisi formulir, hingga tugas yang lebih kompleks termasuk pekerjaan kolaboratif, manajemen proyek, dan lebih umum, membuat, berunding, memberi penjelasan, dan berkongsi kandungan Web. Namun, pengguna kekurangan mekanisme yang mudah tetapi kuat untuk menyusun aplikasi Web, sama seperti persekitaran desktop yang dibenarkan selama beberapa dekad menggunakan paradigma penjelajah fail dan metafora desktop. Percubaan telah dilakukan untuk menyesuaikan metafora desktop dengan persekitaran Web yang melahirkan Webtops (desktop Web). Ini pada dasarnya terdiri dari penyisipan lingkungan desktop dalam penyemak imbas Web dan menyediakan akses ke berbagai aplikasi Web dalam Antaramuka Pengguna yang sama. Namun, usaha tersebut tidak mempertimbangkan perbedaan radikal antara lingkungan dan aplikasi Web dan desktop. Dalam karya ini, kami memperkenalkan pendekatan baru untuk komposisi aplikasi Web berdasarkan metafora peta minda. Ia membolehkan melayari artifak (sumber Web) dan membolehkan komposisi aplikasi Web yang berkaitan oleh pengguna. Pada dasarnya, peta minda adalah grafik widget yang mewakili artifak yang dibuat atau digunakan oleh aplikasi Web dan memungkinkan untuk menyenaraikan dan melancarkan semua kemungkinan aplikasi Web yang berkaitan dengan setiap artifak. Alat telah dikembangkan untuk mengeksperimen metafora baru dan disediakan sebagai perkhidmatan yang akan disematkan dalam aplikasi Web melalui pemalam penyemak imbas Web. Kami menunjukkan dalam makalah ini tiga kajian kes mengenai laman web DBLP, Wikipedia dan aplikasi Google Picasa Web. [[EENNDD]] kejuruteraan perisian; aplikasi sesawang; pelbagai; peta minda; komposisi berdasarkan pengguna"], [{"string": "Social recommender systems The goal of this tutorial is to expose participants to the current research on social recommender systems (i.e., recommender systems for the social web). Participants will become familiar with state-of-the-art recommendation methods, their classifications according to various criteria, common evaluation methodologies, and potential applications that can utilize social recommender systems. Additionally, open issues and challenges in the field will be discussed.", "keywords": ["social media", "recommender systems", "social recommendation"], "combined": "Social recommender systems The goal of this tutorial is to expose participants to the current research on social recommender systems (i.e., recommender systems for the social web). Participants will become familiar with state-of-the-art recommendation methods, their classifications according to various criteria, common evaluation methodologies, and potential applications that can utilize social recommender systems. Additionally, open issues and challenges in the field will be discussed. [[EENNDD]] social media; recommender systems; social recommendation"}, "Sistem pengesyorkan sosial Matlamat tutorial ini adalah untuk mendedahkan peserta kepada penyelidikan terkini mengenai sistem pengesyorkan sosial (iaitu, sistem pengesyorkan untuk laman sosial). Peserta akan terbiasa dengan kaedah cadangan canggih, klasifikasi mereka mengikut pelbagai kriteria, metodologi penilaian umum, dan aplikasi berpotensi yang dapat menggunakan sistem pengesyorkan sosial. Selain itu, isu dan cabaran terbuka di lapangan akan dibincangkan. [[EENNDD]] media sosial; sistem pengesyorkan; cadangan sosial"], [{"string": "Predicting popular messages in Twitter Social network services have become a viable source of information for users. In Twitter, information deemed important by the community propagates through retweets. Studying the characteristics of such popular messages is important for a number of tasks, such as breaking news detection, personalized message recommendation, viral marketing and others. This paper investigates the problem of predicting the popularity of messages as measured by the number of future retweets and sheds some light on what kinds of factors influence information propagation in Twitter. We formulate the task into a classification problem and study two of its variants by investigating a wide spectrum of features based on the content of the messages, temporal information, metadata of messages and users, as well as structural properties of the users' social graph on a large scale dataset. We show that our method can successfully predict messages which will attract thousands of retweets with good performance.", "keywords": ["classification", "information search and retrieval", "information diffusion", "microblogs", "social media"], "combined": "Predicting popular messages in Twitter Social network services have become a viable source of information for users. In Twitter, information deemed important by the community propagates through retweets. Studying the characteristics of such popular messages is important for a number of tasks, such as breaking news detection, personalized message recommendation, viral marketing and others. This paper investigates the problem of predicting the popularity of messages as measured by the number of future retweets and sheds some light on what kinds of factors influence information propagation in Twitter. We formulate the task into a classification problem and study two of its variants by investigating a wide spectrum of features based on the content of the messages, temporal information, metadata of messages and users, as well as structural properties of the users' social graph on a large scale dataset. We show that our method can successfully predict messages which will attract thousands of retweets with good performance. [[EENNDD]] classification; information search and retrieval; information diffusion; microblogs; social media"}, "Meramalkan mesej popular di perkhidmatan rangkaian Sosial Twitter telah menjadi sumber maklumat yang berguna bagi pengguna. Di Twitter, maklumat yang dianggap penting oleh masyarakat disebarkan melalui retweet. Mempelajari ciri-ciri mesej popular seperti itu penting untuk sejumlah tugas, seperti pengesanan berita terkini, cadangan pesanan peribadi, pemasaran viral dan lain-lain. Makalah ini menyelidiki masalah memprediksi populariti mesej yang diukur dengan jumlah retweet masa depan dan menjelaskan beberapa jenis faktor yang mempengaruhi penyebaran maklumat di Twitter. Kami merumuskan tugas itu menjadi masalah klasifikasi dan mempelajari dua variannya dengan menyelidiki pelbagai ciri berdasarkan kandungan pesan, maklumat temporal, metadata mesej dan pengguna, serta sifat struktur grafik sosial pengguna di set data berskala besar. Kami menunjukkan bahawa kaedah kami dapat meramalkan mesej yang berjaya menarik ribuan retweet dengan prestasi yang baik. [[EENNDD]] klasifikasi; carian dan pengambilan maklumat; penyebaran maklumat; blog mikro; media sosial"], [{"string": "On the temporal dimension of search No contact information provided yet.", "keywords": ["publication search", "web search", "temporal dimension of search"], "combined": "On the temporal dimension of search No contact information provided yet. [[EENNDD]] publication search; web search; temporal dimension of search"}, "Mengenai dimensi temporal carian Belum ada maklumat hubungan yang diberikan. [[EENNDD]] carian penerbitan; carian sesawang; dimensi carian sementara"], [{"string": "Model characterization curves for federated search using click-logs: predicting user engagement metrics for the span of feasible operating points Modern day federated search engines aggregate heterogeneous types of results from multiple vertical search engines and compose a single search engine result page (SERP). The search engine aggregates the results and produces one ranked list, constraining the vertical results to specific slots on the SERP.", "keywords": ["randomization", "information search and retrieval", "offline prediction", "user engagement", "metrics", "online evaluation"], "combined": "Model characterization curves for federated search using click-logs: predicting user engagement metrics for the span of feasible operating points Modern day federated search engines aggregate heterogeneous types of results from multiple vertical search engines and compose a single search engine result page (SERP). The search engine aggregates the results and produces one ranked list, constraining the vertical results to specific slots on the SERP. [[EENNDD]] randomization; information search and retrieval; offline prediction; user engagement; metrics; online evaluation"}, "Keluk pencirian model untuk carian gabungan menggunakan log klik: meramalkan metrik penglibatan pengguna untuk jangka masa operasi yang boleh dilaksanakan Mesin pencari gabungan moden Hari ini mengumpulkan jenis hasil yang beragam dari pelbagai enjin carian menegak dan menyusun satu halaman hasil enjin carian (SERP). Mesin pencari menggabungkan hasil dan menghasilkan satu senarai peringkat, mengekang hasil menegak ke slot tertentu di SERP. [[EENNDD]] rawak; pencarian dan pengambilan maklumat; ramalan luar talian; penglibatan pengguna; sukatan; penilaian dalam talian"], [{"string": "A self-training approach for resolving object coreference on the semantic web An object on the Semantic Web is likely to be denoted with multiple URIs by different parties. Object coreference resolution is to identify \"equivalent\" URIs that denote the same object. Driven by the Linking Open Data (LOD) initiative, millions of URIs have been explicitly linked with owl:sameAs statements, but potentially coreferent ones are still considerable. Existing approaches address the problem mainly from two directions: one is based upon equivalence inference mandated by OWL semantics, which finds semantically coreferent URIs but probably omits many potential ones; the other is via similarity computation between property-value pairs, which is not always accurate enough. In this paper, we propose a self-training approach for object coreference resolution on the Semantic Web, which leverages the two classes of approaches to bridge the gap between semantically coreferent URIs and potential candidates. For an object URI, we firstly establish a kernel that consists of semantically coreferent URIs based on owl:sameAs, (inverse) functional properties and (max-)cardinalities, and then extend such kernel iteratively in terms of discriminative property-value pairs in the descriptions of URIs. In particular, the discriminability is learnt with a statistical measurement, which not only exploits key characteristics for representing an object, but also takes into account the matchability between properties from pragmatics. In addition, frequent property combinations are mined to improve the accuracy of the resolution. We implement a scalable system and demonstrate that our approach achieves good precision and recall for resolving object coreference, on both benchmark and large-scale datasets.", "keywords": ["interoperability", "data fusion", "self-training", "object coreference", "object consolidation", "property combination"], "combined": "A self-training approach for resolving object coreference on the semantic web An object on the Semantic Web is likely to be denoted with multiple URIs by different parties. Object coreference resolution is to identify \"equivalent\" URIs that denote the same object. Driven by the Linking Open Data (LOD) initiative, millions of URIs have been explicitly linked with owl:sameAs statements, but potentially coreferent ones are still considerable. Existing approaches address the problem mainly from two directions: one is based upon equivalence inference mandated by OWL semantics, which finds semantically coreferent URIs but probably omits many potential ones; the other is via similarity computation between property-value pairs, which is not always accurate enough. In this paper, we propose a self-training approach for object coreference resolution on the Semantic Web, which leverages the two classes of approaches to bridge the gap between semantically coreferent URIs and potential candidates. For an object URI, we firstly establish a kernel that consists of semantically coreferent URIs based on owl:sameAs, (inverse) functional properties and (max-)cardinalities, and then extend such kernel iteratively in terms of discriminative property-value pairs in the descriptions of URIs. In particular, the discriminability is learnt with a statistical measurement, which not only exploits key characteristics for representing an object, but also takes into account the matchability between properties from pragmatics. In addition, frequent property combinations are mined to improve the accuracy of the resolution. We implement a scalable system and demonstrate that our approach achieves good precision and recall for resolving object coreference, on both benchmark and large-scale datasets. [[EENNDD]] interoperability; data fusion; self-training; object coreference; object consolidation; property combination"}, "Pendekatan latihan kendiri untuk menyelesaikan inti objek pada web semantik Objek di Web Semantik cenderung dilambangkan dengan pelbagai URI oleh pihak yang berlainan. Resolusi inti objek adalah untuk mengenal pasti \"setara\" URI yang menunjukkan objek yang sama. Didorong oleh inisiatif Menghubungkan Data Terbuka (LOD), berjuta-juta URI telah dihubungkan secara jelas dengan burung hantu: pernyataan yang sama, tetapi yang berpotensi berpusat masih dapat dipertimbangkan. Pendekatan yang ada mengatasi masalah terutamanya dari dua arah: satu berdasarkan kesamaan kesamaan yang diamanatkan oleh semantik OWL, yang menemui URI inti semantik tetapi mungkin menghilangkan banyak yang berpotensi; yang lain adalah melalui pengiraan kesamaan antara pasangan nilai-harta, yang tidak selalu tepat. Dalam makalah ini, kami mencadangkan pendekatan latihan diri untuk penyelesaian inti objek di Web Semantik, yang memanfaatkan dua kelas pendekatan untuk merapatkan jurang antara URI inti semantik dan calon yang berpotensi. Untuk URI objek, pertama-tama kami mewujudkan kernel yang terdiri daripada URI inti semantik berdasarkan burung hantu: sameAs, (terbalik) sifat fungsional dan (maksimum-) kardinaliti, dan kemudian memperluas kernel tersebut secara berulang dari segi pasangan nilai-nilai diskriminasi di penerangan mengenai URI. Khususnya, diskriminasi dipelajari dengan pengukuran statistik, yang tidak hanya memanfaatkan ciri-ciri utama untuk mewakili objek, tetapi juga mempertimbangkan kesesuaian antara sifat dari pragmatik. Di samping itu, kombinasi harta tanah yang kerap ditambang untuk meningkatkan ketepatan resolusi. Kami menerapkan sistem yang dapat diskalakan dan menunjukkan bahawa pendekatan kami mencapai ketepatan dan penarikan yang baik untuk menyelesaikan inti objek, baik pada set data penanda aras dan skala besar. [[EENNDD]] kebolehoperasian; gabungan data; latihan kendiri; inti objek; penyatuan objek; gabungan harta tanah"], [{"string": "Efficient application placement in a dynamic hosting platform Web hosting providers are increasingly looking into dynamic hosting to reduce costs and improve the performance of their platforms. Instead of provisioning fixed resources to each customer, dynamic hosting maintains a variable number of application instances to satisfy current demand. While existing research in this area has mostly focused on the algorithms that decide on the number and location of application instances, we address the problem of efficient enactment of these decisions once they are made. We propose a new approach to application placement and experimentally show that it dramatically reduces the cost of application placement, which in turn improves the end-to-end agility of the hosting platform in reacting to demand changes.", "keywords": ["dynamic placement", "application servers", "web hosting", "startup performance"], "combined": "Efficient application placement in a dynamic hosting platform Web hosting providers are increasingly looking into dynamic hosting to reduce costs and improve the performance of their platforms. Instead of provisioning fixed resources to each customer, dynamic hosting maintains a variable number of application instances to satisfy current demand. While existing research in this area has mostly focused on the algorithms that decide on the number and location of application instances, we address the problem of efficient enactment of these decisions once they are made. We propose a new approach to application placement and experimentally show that it dramatically reduces the cost of application placement, which in turn improves the end-to-end agility of the hosting platform in reacting to demand changes. [[EENNDD]] dynamic placement; application servers; web hosting; startup performance"}, "Penempatan aplikasi yang cekap dalam platform hosting dinamik Penyedia hosting web semakin mencari hosting dinamik untuk mengurangkan kos dan meningkatkan prestasi platform mereka. Daripada menyediakan sumber tetap kepada setiap pelanggan, hosting dinamik mengekalkan sejumlah contoh aplikasi untuk memenuhi permintaan semasa. Walaupun penyelidikan yang ada di bidang ini sebagian besar memfokuskan pada algoritma yang memutuskan jumlah dan lokasi kejadian aplikasi, kami menangani masalah pengesahan keputusan ini dengan efisien setelah mereka dibuat. Kami mengusulkan pendekatan baru untuk penempatan aplikasi dan menunjukkan secara eksperimental bahawa ia secara dramatik mengurangi biaya penempatan aplikasi, yang pada gilirannya meningkatkan kelincahan ujung ke ujung platform hosting dalam bereaksi terhadap perubahan permintaan. [[EENNDD]] penempatan dinamik; pelayan aplikasi; hosting web; prestasi permulaan"], [{"string": "Dynamic assembly of learning objects No contact information provided yet.", "keywords": ["organization", "metadata", "information retrieval", "instruction", "content management", "lom", "semantic web", "computer-managed instruction", "rdf", "learning object", "assembly", "linking", "data retrieval"], "combined": "Dynamic assembly of learning objects No contact information provided yet. [[EENNDD]] organization; metadata; information retrieval; instruction; content management; lom; semantic web; computer-managed instruction; rdf; learning object; assembly; linking; data retrieval"}, "Pemasangan objek pembelajaran yang dinamik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] organisasi; metadata; pengambilan maklumat; arahan; pengurusan kandungan; lom; web semantik; arahan yang dikendalikan oleh komputer; rdf; objek pembelajaran; perhimpunan; memaut; pengambilan data"], [{"string": "Using symbolic objects to cluster web documents No contact information provided yet.", "keywords": ["miscellaneous", "symbolic data analysis", "web clustering", "metrics"], "combined": "Using symbolic objects to cluster web documents No contact information provided yet. [[EENNDD]] miscellaneous; symbolic data analysis; web clustering; metrics"}, "Menggunakan objek simbolik untuk mengumpulkan dokumen web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pelbagai; analisis data simbolik; pengelompokan web; sukatan"], [{"string": "A method for transparent admission control and request scheduling in e-commerce web sites No contact information provided yet.", "keywords": ["web servers", "admission control", "load control", "dynamic web content", "request scheduling"], "combined": "A method for transparent admission control and request scheduling in e-commerce web sites No contact information provided yet. [[EENNDD]] web servers; admission control; load control; dynamic web content; request scheduling"}, "Kaedah untuk kawalan kemasukan yang telus dan penjadualan permintaan di laman web e-dagang Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pelayan web; kawalan kemasukan; kawalan beban; kandungan web dinamik; permintaan penjadualan"], [{"string": "Price modeling in standards for electronic product catalogs based on XML No contact information provided yet.", "keywords": ["e-business", "pricing", "b2b", "e-procurement", "xml", "e-catalog"], "combined": "Price modeling in standards for electronic product catalogs based on XML No contact information provided yet. [[EENNDD]] e-business; pricing; b2b; e-procurement; xml; e-catalog"}, "Pemodelan harga dalam standard untuk katalog produk elektronik berdasarkan XML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] e-perniagaan; harga; b2b; e-perolehan; xml; e-katalog"], [{"string": "A clustering method for news articles retrieval system No contact information provided yet.", "keywords": ["document clustering", "named entity", "search result organization"], "combined": "A clustering method for news articles retrieval system No contact information provided yet. [[EENNDD]] document clustering; named entity; search result organization"}, "Kaedah kluster untuk sistem pengambilan artikel berita Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengelompokan dokumen; entiti bernama; organisasi hasil carian"], [{"string": "Extraction and classification of dense communities in the web The World Wide Web (WWW) is rapidly becoming important for society as a medium for sharing data, information and services, and there is a growing interest in tools for understanding collective behaviors and emerging phenomena in the WWW. In this paper we focus on the problem of searching and classifying communities in the web. Loosely speaking a community is a group of pages related to a common interest. More formally communities have been associated in the computer science literature with the existence of a locally dense sub-graph of the web-graph (where web pages are nodes and hyper-links are arcs of the web-graph). The core of our contribution is a new scalable algorithm for finding relatively dense subgraphs in massive graphs. We apply our algorithm on web-graphs built on three publicly available large crawls of the web (with raw sizes up to 120M nodes and 1G arcs). The effectiveness of our algorithm in finding dense subgraphs is demonstrated experimentally by embedding artificial communities in the web-graph and counting how many of these are blindly found. Effectiveness increases with the size and density of the communities: it is close to 100% for communities of a thirty nodes or more (even at low density). It is still about 80% even for communities of twenty nodes with density over 50% of the arcs present. At the lower extremes the algorithm catches 35% of dense communities made of ten nodes. We complete our Community Watch system by clustering the communities found in the web-graph into homogeneous groups by topic and labelling each group by representative keywords.", "keywords": ["communities", "web graph", "dense subgraphs"], "combined": "Extraction and classification of dense communities in the web The World Wide Web (WWW) is rapidly becoming important for society as a medium for sharing data, information and services, and there is a growing interest in tools for understanding collective behaviors and emerging phenomena in the WWW. In this paper we focus on the problem of searching and classifying communities in the web. Loosely speaking a community is a group of pages related to a common interest. More formally communities have been associated in the computer science literature with the existence of a locally dense sub-graph of the web-graph (where web pages are nodes and hyper-links are arcs of the web-graph). The core of our contribution is a new scalable algorithm for finding relatively dense subgraphs in massive graphs. We apply our algorithm on web-graphs built on three publicly available large crawls of the web (with raw sizes up to 120M nodes and 1G arcs). The effectiveness of our algorithm in finding dense subgraphs is demonstrated experimentally by embedding artificial communities in the web-graph and counting how many of these are blindly found. Effectiveness increases with the size and density of the communities: it is close to 100% for communities of a thirty nodes or more (even at low density). It is still about 80% even for communities of twenty nodes with density over 50% of the arcs present. At the lower extremes the algorithm catches 35% of dense communities made of ten nodes. We complete our Community Watch system by clustering the communities found in the web-graph into homogeneous groups by topic and labelling each group by representative keywords. [[EENNDD]] communities; web graph; dense subgraphs"}, "Pengekstrakan dan klasifikasi komuniti padat dalam web World Wide Web (WWW) dengan cepat menjadi penting bagi masyarakat sebagai media untuk berkongsi data, maklumat dan perkhidmatan, dan terdapat minat yang semakin meningkat terhadap alat untuk memahami tingkah laku kolektif dan fenomena yang muncul di WWW. Dalam makalah ini kami menumpukan pada masalah mencari dan mengklasifikasikan komuniti di web. Secara longgar komuniti adalah sekumpulan halaman yang berkaitan dengan kepentingan bersama. Komuniti secara lebih formal telah dikaitkan dalam sastera sains komputer dengan adanya sub-grafik web-grafik yang padat secara tempatan (di mana laman web adalah simpul dan pautan-pautan adalah lengkungan grafik-web). Inti sumbangan kami adalah algoritma berskala baru untuk mencari subgraf yang agak padat dalam graf besar. Kami menggunakan algoritma kami pada grafik web yang dibina di atas tiga rangkak besar web yang tersedia untuk umum (dengan saiz mentah hingga 120M nod dan busur 1G). Keberkesanan algoritma kami dalam mencari subgraf yang padat ditunjukkan secara eksperimen dengan memasukkan komuniti tiruan dalam web-grafik dan mengira berapa banyak yang ditemui secara membabi buta. Keberkesanan meningkat dengan ukuran dan kepadatan komuniti: hampir 100% bagi komuniti yang mempunyai tiga puluh nod atau lebih (walaupun pada kepadatan rendah). Masih kira-kira 80% walaupun bagi komuniti yang terdiri daripada dua puluh nod dengan ketumpatan lebih daripada 50% busur yang ada. Di hujung bawah algoritma menangkap 35% komuniti padat yang terdiri daripada sepuluh nod. Kami melengkapkan sistem Community Watch kami dengan mengelompokkan komuniti yang terdapat dalam grafik web ke dalam kumpulan homogen mengikut topik dan melabelkan setiap kumpulan dengan kata kunci yang mewakili. [[EENNDD]] komuniti; grafik web; subgraf padat"], [{"string": "Topic modeling with network regularization In this paper, we formally define the problem of topic modeling with network structure (TMN). We propose a novel solution to this problem, which regularizes a statistical topic model with a harmonic regularizer based on a graph structure in the data. The proposed method bridges topic modeling and social network analysis, which leverages the power of both statistical topic models and discrete regularization. The output of this model well summarizes topics in text, maps a topic on the network, and discovers topical communities. With concrete selection of a topic model and a graph-based regularizer, our model can be applied to text mining problems such as author-topic analysis, community discovery, and spatial text mining. Empirical experiments on two different genres of data show that our approach is effective, which improves text-oriented methods as well as network-oriented methods. The proposed model is general; it can be applied to any text collections with a mixture of topics and an associated network structure.", "keywords": ["social networks", "information search and retrieval", "graph-based regularization", "statistical topic models"], "combined": "Topic modeling with network regularization In this paper, we formally define the problem of topic modeling with network structure (TMN). We propose a novel solution to this problem, which regularizes a statistical topic model with a harmonic regularizer based on a graph structure in the data. The proposed method bridges topic modeling and social network analysis, which leverages the power of both statistical topic models and discrete regularization. The output of this model well summarizes topics in text, maps a topic on the network, and discovers topical communities. With concrete selection of a topic model and a graph-based regularizer, our model can be applied to text mining problems such as author-topic analysis, community discovery, and spatial text mining. Empirical experiments on two different genres of data show that our approach is effective, which improves text-oriented methods as well as network-oriented methods. The proposed model is general; it can be applied to any text collections with a mixture of topics and an associated network structure. [[EENNDD]] social networks; information search and retrieval; graph-based regularization; statistical topic models"}, "Pemodelan topik dengan regulasiisasi jaringan Dalam makalah ini, kami secara formal menentukan masalah pemodelan topik dengan struktur jaringan (TMN). Kami mencadangkan penyelesaian baru untuk masalah ini, yang mengatur model topik statistik dengan pengatur harmonik berdasarkan struktur grafik dalam data. Kaedah yang dicadangkan merapatkan pemodelan topik dan analisis rangkaian sosial, yang memanfaatkan kekuatan kedua-dua model topik statistik dan regularisasi diskrit. Keluaran model ini merangkum topik dalam teks, memetakan topik di rangkaian, dan menemui komuniti topikal. Dengan pemilihan model topik yang konkrit dan pengatur berdasarkan grafik, model kami dapat digunakan untuk masalah perlombongan teks seperti analisis topik pengarang, penemuan masyarakat, dan perlombongan teks spasial. Eksperimen empirikal pada dua genre data yang berbeza menunjukkan bahawa pendekatan kami berkesan, yang meningkatkan kaedah berorientasikan teks dan juga kaedah berorientasikan rangkaian. Model yang dicadangkan adalah umum; ia dapat diterapkan pada semua koleksi teks dengan campuran topik dan struktur rangkaian yang berkaitan. [[EENNDD]] rangkaian sosial; carian dan pengambilan maklumat; pengaturcaraan berasaskan grafik; model topik statistik"], [{"string": "Protocol-aware matching of web service interfaces for adapter development With the rapid growth in the number of online Web services, the problem of service adaptation has received significant attention. In matching and adaptation, the functional description of services including interface and data as well as behavioral descriptions are important. Existing work on matching and adaptation focuses only on one aspect.", "keywords": ["business process", "adaptation", "web services", "interoperability"], "combined": "Protocol-aware matching of web service interfaces for adapter development With the rapid growth in the number of online Web services, the problem of service adaptation has received significant attention. In matching and adaptation, the functional description of services including interface and data as well as behavioral descriptions are important. Existing work on matching and adaptation focuses only on one aspect. [[EENNDD]] business process; adaptation; web services; interoperability"}, "Pemadanan protokol antara muka perkhidmatan web untuk pengembangan penyesuai dengan pertumbuhan pesat dalam jumlah perkhidmatan Web dalam talian, masalah penyesuaian perkhidmatan telah mendapat perhatian yang besar. Dalam pemadanan dan penyesuaian, deskripsi fungsional perkhidmatan termasuk antara muka dan data serta deskripsi tingkah laku adalah penting. Kerja yang ada pada pemadanan dan penyesuaian hanya tertumpu pada satu aspek. [[EENNDD]] proses perniagaan; adaptasi; perkhidmatan web; saling kendali"], [{"string": "Model-based version and configuration management for a web engineering lifecycle No contact information provided yet.", "keywords": ["hypertext/hypermedia", "web engineering", "versioned hypermedia", "model-based configuration management"], "combined": "Model-based version and configuration management for a web engineering lifecycle No contact information provided yet. [[EENNDD]] hypertext/hypermedia; web engineering; versioned hypermedia; model-based configuration management"}, "Versi berasaskan model dan pengurusan konfigurasi untuk kitaran hayat kejuruteraan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] hiperteks / hipermedia; kejuruteraan web; hipermedia versi; pengurusan konfigurasi berasaskan model"], [{"string": "Analysis of topological characteristics of huge online social networking services Social networking services are a fast-growing business in the Internet. However, it is unknown if online relationships and their growth patterns are the same as in real-life social networks. In this paper, we compare the structures of three online social networking services: Cyworld, MySpace, and orkut, each with more than 10 million users, respectively. We have access to complete data of Cyworld's ilchon (friend) relationships and analyze its degree distribution, clustering property, degree correlation, and evolution over time. We also use Cyworld data to evaluate the validity of snowball sampling method, which we use to crawl and obtain partial network topologies of MySpace and orkut. Cyworld, the oldest of the three, demonstrates a changing scaling behavior over time in degree distribution. The latest Cyworld data's degree distribution exhibits a multi-scaling behavior, while those of MySpace and orkut have simple scaling behaviors with different exponents. Very interestingly, each of the two e ponents corresponds to the different segments in Cyworld's degree distribution. Certain online social networking services encourage online activities that cannot be easily copied in real life; we show that they deviate from close-knit online social networks which show a similar degree correlation pattern to real-life social networks.", "keywords": ["social network", "sampling"], "combined": "Analysis of topological characteristics of huge online social networking services Social networking services are a fast-growing business in the Internet. However, it is unknown if online relationships and their growth patterns are the same as in real-life social networks. In this paper, we compare the structures of three online social networking services: Cyworld, MySpace, and orkut, each with more than 10 million users, respectively. We have access to complete data of Cyworld's ilchon (friend) relationships and analyze its degree distribution, clustering property, degree correlation, and evolution over time. We also use Cyworld data to evaluate the validity of snowball sampling method, which we use to crawl and obtain partial network topologies of MySpace and orkut. Cyworld, the oldest of the three, demonstrates a changing scaling behavior over time in degree distribution. The latest Cyworld data's degree distribution exhibits a multi-scaling behavior, while those of MySpace and orkut have simple scaling behaviors with different exponents. Very interestingly, each of the two e ponents corresponds to the different segments in Cyworld's degree distribution. Certain online social networking services encourage online activities that cannot be easily copied in real life; we show that they deviate from close-knit online social networks which show a similar degree correlation pattern to real-life social networks. [[EENNDD]] social network; sampling"}, "Analisis ciri topologi perkhidmatan rangkaian sosial dalam talian yang besar Perkhidmatan rangkaian sosial adalah perniagaan yang berkembang pesat di Internet. Namun, tidak diketahui apakah hubungan dalam talian dan corak pertumbuhannya sama seperti dalam rangkaian sosial kehidupan sebenar. Dalam makalah ini, kami membandingkan struktur tiga perkhidmatan rangkaian sosial dalam talian: Cyworld, MySpace, dan orkut, masing-masing dengan lebih dari 10 juta pengguna. Kami mempunyai akses untuk melengkapkan data hubungan ilchon (rakan) Cyworld dan menganalisis taburan darjahnya, pengelompokan harta benda, korelasi darjah, dan evolusi dari masa ke masa. Kami juga menggunakan data Cyworld untuk menilai kesahihan kaedah pensampelan bola salji, yang kami gunakan untuk merangkak dan mendapatkan topologi rangkaian separa MySpace dan orkut. Cyworld, yang tertua dari tiga, menunjukkan tingkah laku penskalaan yang berubah dari masa ke masa dalam pengedaran darjah. Pengedaran darjah data Cyworld terkini memperlihatkan tingkah laku berbilang skala, sementara MySpace dan orkut mempunyai tingkah laku penskalaan sederhana dengan eksponen yang berbeza. Sangat menarik, masing-masing kedua-dua perkara itu sesuai dengan segmen yang berbeza dalam pengedaran ijazah Cyworld. Perkhidmatan rangkaian sosial dalam talian tertentu mendorong aktiviti dalam talian yang tidak dapat disalin dengan mudah dalam kehidupan nyata; kami menunjukkan bahawa mereka menyimpang dari rangkaian sosial dalam talian yang rapat yang menunjukkan corak korelasi darjah yang serupa dengan rangkaian sosial kehidupan sebenar. [[EENNDD]] rangkaian sosial; persampelan"], [{"string": "Extracting events and event descriptions from Twitter This paper describes methods for automatically detecting events involving known entities from Twitter and understanding both the events as well as the audience reaction to them. We show that NLP techniques can be used to extract events, their main actors and the audience reactions with encouraging results.", "keywords": ["opinion mining", "information extraction", "social media", "twitter"], "combined": "Extracting events and event descriptions from Twitter This paper describes methods for automatically detecting events involving known entities from Twitter and understanding both the events as well as the audience reaction to them. We show that NLP techniques can be used to extract events, their main actors and the audience reactions with encouraging results. [[EENNDD]] opinion mining; information extraction; social media; twitter"}, "Mengekstrak peristiwa dan perihalan peristiwa dari Twitter Makalah ini menerangkan kaedah untuk mengesan peristiwa secara automatik yang melibatkan entiti yang diketahui dari Twitter dan memahami kedua-dua peristiwa tersebut serta reaksi penonton terhadapnya. Kami menunjukkan bahawa teknik NLP dapat digunakan untuk mengekstrak acara, pelakon utama mereka dan reaksi penonton dengan hasil yang menggembirakan. [[EENNDD]] perlombongan pendapat; pengekstrakan maklumat; media sosial; twitter"], [{"string": "Defeating script injection attacks with browser-enforced embedded policies Web sites that accept and display content such as wiki articles or comments typically filter the content to prevent injected script code from running in browsers that view the site. The diversity of browser rendering algorithms and the desire to allow rich content make filtering quite difficult, however, and attacks such as the Samy and Yamanner worms have exploited filtering weaknesses. This paper proposes a simple alternative mechanism for preventing script injection called Browser-Enforced Embedded Policies (BEEP). The idea is that a web site can embed a policy in its pages that specifies which scripts are allowed to run. The browser, which knows exactly when it will run a script, can enforce this policy perfectly. We have added BEEP support to several browsers, and built tools to simplify adding policies to web applications. We found that supporting BEEP in browsers requires only small and localized modifications, modifying web applications requires minimal effort, and enforcing policies is generally lightweight.", "keywords": ["unauthorized access", "web application security", "invasive software", "script injection", "cross-site scripting"], "combined": "Defeating script injection attacks with browser-enforced embedded policies Web sites that accept and display content such as wiki articles or comments typically filter the content to prevent injected script code from running in browsers that view the site. The diversity of browser rendering algorithms and the desire to allow rich content make filtering quite difficult, however, and attacks such as the Samy and Yamanner worms have exploited filtering weaknesses. This paper proposes a simple alternative mechanism for preventing script injection called Browser-Enforced Embedded Policies (BEEP). The idea is that a web site can embed a policy in its pages that specifies which scripts are allowed to run. The browser, which knows exactly when it will run a script, can enforce this policy perfectly. We have added BEEP support to several browsers, and built tools to simplify adding policies to web applications. We found that supporting BEEP in browsers requires only small and localized modifications, modifying web applications requires minimal effort, and enforcing policies is generally lightweight. [[EENNDD]] unauthorized access; web application security; invasive software; script injection; cross-site scripting"}, "Mengalahkan serangan suntikan skrip dengan dasar tertanam yang disahkan oleh penyemak imbas Laman web yang menerima dan memaparkan kandungan seperti artikel atau komen wiki biasanya menyaring kandungan untuk mengelakkan kod skrip yang disuntik berjalan di penyemak imbas yang melihat laman web tersebut. Kepelbagaian algoritma rendering penyemak imbas dan keinginan untuk membenarkan kandungan yang kaya menjadikan penyaringan agak sukar, bagaimanapun, dan serangan seperti cacing Samy dan Yamanner telah memanfaatkan kelemahan penyaringan. Makalah ini mencadangkan mekanisme alternatif yang mudah untuk mencegah suntikan skrip yang disebut Polisi Penyisipan yang Diperakui oleh Penyemak Imbas (BEEP). Idenya adalah bahawa laman web dapat memasukkan kebijakan di halamannya yang menentukan skrip mana yang dibenarkan untuk dijalankan. Penyemak imbas, yang mengetahui dengan tepat kapan ia akan menjalankan skrip, dapat melaksanakan dasar ini dengan sempurna. Kami telah menambahkan sokongan BEEP ke beberapa penyemak imbas, dan membina alat untuk mempermudah penambahan dasar ke aplikasi web. Kami mendapati bahawa menyokong BEEP dalam penyemak imbas hanya memerlukan pengubahsuaian kecil dan dilokalisasi, mengubah aplikasi web memerlukan sedikit usaha, dan menegakkan kebijakan umumnya ringan. [[EENNDD]] akses yang tidak dibenarkan; keselamatan aplikasi web; perisian invasif; suntikan skrip; skrip merentas laman web"], [{"string": "Relationship between web links and trade Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.", "keywords": ["national web domains", "world trade graph", "miscellaneous"], "combined": "Relationship between web links and trade Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references. [[EENNDD]] national web domains; world trade graph; miscellaneous"}, "Hubungan antara pautan web dan perdagangan Catatan: Kesalahan OCR mungkin terdapat dalam Senarai Rujukan ini yang diambil dari artikel teks lengkap. ACM memilih untuk mendedahkan Senarai lengkap dan bukan hanya rujukan yang betul dan berkaitan. [[EENNDD]] domain web kebangsaan; grafik perdagangan dunia; pelbagai"], [{"string": "Large scale integration of senses for the semantic web Nowadays, the increasing amount of semantic data available on the Web leads to a new stage in the potential of Semantic Web applications. However, it also introduces new issues due to the heterogeneity of the available semantic resources. One of the most remarkable is redundancy, that is, the excess of different semantic descriptions, coming from different sources, to describe the same intended meaning.", "keywords": ["scalable sense integration", "ontologies", "semantic web", "miscellaneous"], "combined": "Large scale integration of senses for the semantic web Nowadays, the increasing amount of semantic data available on the Web leads to a new stage in the potential of Semantic Web applications. However, it also introduces new issues due to the heterogeneity of the available semantic resources. One of the most remarkable is redundancy, that is, the excess of different semantic descriptions, coming from different sources, to describe the same intended meaning. [[EENNDD]] scalable sense integration; ontologies; semantic web; miscellaneous"}, "Penyatuan indera skala besar untuk web semantik Pada masa ini, semakin banyak data semantik yang terdapat di Web membawa ke tahap baru dalam potensi aplikasi Web Semantik. Namun, ia juga memperkenalkan isu-isu baru kerana heterogenitas sumber semantik yang ada. Salah satu yang paling luar biasa adalah kelebihan, iaitu, keterangan semantik yang berlainan, yang berasal dari sumber yang berlainan, untuk menggambarkan maksud yang sama. [[EENNDD]] penyatuan rasa berskala; ontologi; web semantik; pelbagai"], [{"string": "Online team formation in social networks We study the problem of online team formation. We consider a setting in which people possess different skills and compatibility among potential team members is modeled by a social network. A sequence of tasks arrives in an online fashion, and each task requires a specific set of skills. The goal is to form a new team upon arrival of each task, so that (i) each team possesses all skills required by the task, (ii) each team has small communication overhead, and (iii) the workload of performing the tasks is balanced among people in the fairest possible way.", "keywords": ["scheduling", "team formation", "task assignment"], "combined": "Online team formation in social networks We study the problem of online team formation. We consider a setting in which people possess different skills and compatibility among potential team members is modeled by a social network. A sequence of tasks arrives in an online fashion, and each task requires a specific set of skills. The goal is to form a new team upon arrival of each task, so that (i) each team possesses all skills required by the task, (ii) each team has small communication overhead, and (iii) the workload of performing the tasks is balanced among people in the fairest possible way. [[EENNDD]] scheduling; team formation; task assignment"}, "Pembentukan pasukan dalam talian di rangkaian sosial Kami mengkaji masalah pembentukan pasukan dalam talian. Kami mempertimbangkan keadaan di mana orang mempunyai kemahiran dan keserasian yang berbeza di antara calon anggota pasukan yang dimodelkan oleh rangkaian sosial. Urutan tugas tiba secara dalam talian, dan setiap tugas memerlukan satu set kemahiran tertentu. Tujuannya adalah untuk membentuk pasukan baru setelah tiba setiap tugas, sehingga (i) setiap pasukan memiliki semua kemahiran yang diperlukan oleh tugas itu, (ii) setiap pasukan memiliki overhead komunikasi kecil, dan (iii) beban kerja melaksanakan tugas adalah seimbang antara orang dengan cara yang paling adil. [[EENNDD]] penjadualan; pembentukan pasukan; penugasan tugas"], [{"string": "Collective privacy management in social networks Social Networking is one of the major technological phenomena of the Web 2.0, with hundreds of millions of people participating. Social networks enable a form of self expression for users, and help them to socialize and share content with other users. In spite of the fact that content sharing represents one of the prominent features of existing Social Network sites, Social Networks yet do not support any mechanism for collaborative management of privacy settings for shared content. In this paper, we model the problem of collaborative enforcement of privacy policies on shared data by using game theory. In particular, we propose a solution that offers automated ways to share images based on an extended notion of content ownership. Building upon the Clarke-Tax mechanism, we describe a simple mechanism that promotes truthfulness, and that rewards users who promote co-ownership. We integrate our design with inference techniques that free the users from the burden of manually selecting privacy preferences for each picture. To the best of our knowledge this is the first time such a protection mechanism for Social Networking has been proposed. In the paper, we also show a proof-of-concept application, which we implemented in the context of Facebook, one of today's most popular social networks. We show that supporting these type of solutions is not also feasible, but can be implemented through a minimal increase in overhead to end-users.", "keywords": ["social networks", "privacy", "game theory"], "combined": "Collective privacy management in social networks Social Networking is one of the major technological phenomena of the Web 2.0, with hundreds of millions of people participating. Social networks enable a form of self expression for users, and help them to socialize and share content with other users. In spite of the fact that content sharing represents one of the prominent features of existing Social Network sites, Social Networks yet do not support any mechanism for collaborative management of privacy settings for shared content. In this paper, we model the problem of collaborative enforcement of privacy policies on shared data by using game theory. In particular, we propose a solution that offers automated ways to share images based on an extended notion of content ownership. Building upon the Clarke-Tax mechanism, we describe a simple mechanism that promotes truthfulness, and that rewards users who promote co-ownership. We integrate our design with inference techniques that free the users from the burden of manually selecting privacy preferences for each picture. To the best of our knowledge this is the first time such a protection mechanism for Social Networking has been proposed. In the paper, we also show a proof-of-concept application, which we implemented in the context of Facebook, one of today's most popular social networks. We show that supporting these type of solutions is not also feasible, but can be implemented through a minimal increase in overhead to end-users. [[EENNDD]] social networks; privacy; game theory"}, "Pengurusan privasi kolektif dalam rangkaian sosial Jaringan Sosial adalah salah satu fenomena teknologi utama Web 2.0, dengan ratusan juta orang mengambil bahagian. Rangkaian sosial membolehkan bentuk ekspresi diri untuk pengguna, dan membantu mereka untuk bersosial dan berkongsi kandungan dengan pengguna lain. Walaupun perkongsian kandungan mewakili salah satu ciri utama laman Jaringan Sosial yang ada, Rangkaian Sosial belum menyokong sebarang mekanisme untuk pengurusan kolaborasi tetapan privasi untuk kandungan bersama. Dalam makalah ini, kami memodelkan masalah penguatkuasaan kolaborasi dasar privasi pada data bersama dengan menggunakan teori permainan. Khususnya, kami mencadangkan penyelesaian yang menawarkan cara automatik untuk berkongsi gambar berdasarkan konsep kepemilikan kandungan yang diperluas. Berdasarkan mekanisme Clarke-Tax, kami menerangkan mekanisme sederhana yang mempromosikan kebenaran, dan memberi ganjaran kepada pengguna yang mempromosikan pemilikan bersama. Kami menggabungkan reka bentuk kami dengan teknik inferensi yang membebaskan pengguna dari beban memilih pilihan privasi secara manual untuk setiap gambar. Sepengetahuan kami ini adalah pertama kalinya mekanisme perlindungan untuk Jaringan Sosial dicadangkan. Di dalam makalah ini, kami juga menunjukkan aplikasi bukti-konsep, yang kami laksanakan dalam konteks Facebook, salah satu rangkaian sosial paling popular saat ini. Kami menunjukkan bahawa menyokong jenis penyelesaian ini juga tidak dapat dilaksanakan, tetapi dapat dilaksanakan melalui peningkatan minimum overhead kepada pengguna akhir. [[EENNDD]] rangkaian sosial; privasi; teori permainan"], [{"string": "Exploiting web search engines to search structured databases Web search engines often federate many user queries to relevant structured databases. For example, a product related query might be federated to a product database containing their descriptions and specifications. The relevant structured data items are then returned to the user along with web search results. However, each structured database is searched in isolation. Hence, the search often produces empty or incomplete results as the database may not contain the required information to answer the query. In this paper, we propose a novel integrated search architecture. We establish and exploit the relationships between web search results and the items in structured databases to identify the relevant structured data items for a much wider range of queries.Our architecture leverages existing search engine components to implement this functionality at very low overhead. We demonstrate the quality and efficiency of our techniques through an extensive experimental study.", "keywords": ["content analysis and indexing", "information search and retrieval", "entity extraction", "entity ranking", "entity search", "structured database search"], "combined": "Exploiting web search engines to search structured databases Web search engines often federate many user queries to relevant structured databases. For example, a product related query might be federated to a product database containing their descriptions and specifications. The relevant structured data items are then returned to the user along with web search results. However, each structured database is searched in isolation. Hence, the search often produces empty or incomplete results as the database may not contain the required information to answer the query. In this paper, we propose a novel integrated search architecture. We establish and exploit the relationships between web search results and the items in structured databases to identify the relevant structured data items for a much wider range of queries.Our architecture leverages existing search engine components to implement this functionality at very low overhead. We demonstrate the quality and efficiency of our techniques through an extensive experimental study. [[EENNDD]] content analysis and indexing; information search and retrieval; entity extraction; entity ranking; entity search; structured database search"}, "Mengeksploitasi enjin carian web untuk mencari pangkalan data berstruktur Mesin carian web sering menggabungkan banyak pertanyaan pengguna ke pangkalan data berstruktur yang relevan. Sebagai contoh, pertanyaan berkaitan produk mungkin digabungkan ke pangkalan data produk yang mengandungi keterangan dan spesifikasi mereka. Item data berstruktur yang relevan kemudian dikembalikan kepada pengguna bersama dengan hasil carian web. Walau bagaimanapun, setiap pangkalan data berstruktur dicari secara terpisah. Oleh itu, carian sering menghasilkan hasil kosong atau tidak lengkap kerana pangkalan data mungkin tidak mengandungi maklumat yang diperlukan untuk menjawab pertanyaan. Dalam makalah ini, kami mencadangkan seni bina carian bersepadu yang baru. Kami menjalin dan memanfaatkan hubungan antara hasil carian web dan item dalam pangkalan data berstruktur untuk mengenal pasti item data berstruktur yang relevan untuk pelbagai pertanyaan yang lebih luas. Senibina kami memanfaatkan komponen mesin pencari yang ada untuk melaksanakan fungsi ini dengan overhead yang sangat rendah. Kami menunjukkan kualiti dan kecekapan teknik kami melalui kajian eksperimen yang luas. [[EENNDD]] analisis kandungan dan pengindeksan; carian dan pengambilan maklumat; pengekstrakan entiti; kedudukan entiti; carian entiti; carian pangkalan data berstruktur"], [{"string": "Model checking cobweb protocols for verification of HTML frames behavior No contact information provided yet.", "keywords": ["browsing semantics", "html", "frames", "literary hypertext", "formal semantics", "temporal logic", "model checking"], "combined": "Model checking cobweb protocols for verification of HTML frames behavior No contact information provided yet. [[EENNDD]] browsing semantics; html; frames; literary hypertext; formal semantics; temporal logic; model checking"}, "Model memeriksa protokol web jaring untuk pengesahan tingkah laku bingkai HTML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] semantik melayari; html; bingkai; hiperteks sastera; semantik formal; logik temporal; pemeriksaan model"], [{"string": "Analysis and tracking of emotions in english and bengali texts: a computational approach The present discussion highlights the aspects of an ongoing doctoral thesis grounded on the analysis and tracking of emotions from English and Bengali texts. Development of lexical resources and corpora meets the preliminary urgencies. The research spectrum aims to identify the evaluative emotional expressions at word, phrase, sentence, and document level granularities along with their associated holders and topics. Tracking of emotions based on topic or event was carried out by employing sense based affect scoring techniques. The labeled emotion corpora are being prepared from unlabeled examples to cope with the scarcity of emotional resources, especially for the resource constraint language like Bengali. Different unsupervised, supervised and semi-supervised strategies, adopted for coloring each outline of the research spectrum produce satisfactory outcomes", "keywords": ["crf", "expression", "emotions", "svm", "syntactic argument structure", "blogs", "tracking", "topic", "holder", "bengwal"], "combined": "Analysis and tracking of emotions in english and bengali texts: a computational approach The present discussion highlights the aspects of an ongoing doctoral thesis grounded on the analysis and tracking of emotions from English and Bengali texts. Development of lexical resources and corpora meets the preliminary urgencies. The research spectrum aims to identify the evaluative emotional expressions at word, phrase, sentence, and document level granularities along with their associated holders and topics. Tracking of emotions based on topic or event was carried out by employing sense based affect scoring techniques. The labeled emotion corpora are being prepared from unlabeled examples to cope with the scarcity of emotional resources, especially for the resource constraint language like Bengali. Different unsupervised, supervised and semi-supervised strategies, adopted for coloring each outline of the research spectrum produce satisfactory outcomes [[EENNDD]] crf; expression; emotions; svm; syntactic argument structure; blogs; tracking; topic; holder; bengwal"}, "Analisis dan penjejakan emosi dalam teks bahasa inggeris dan bengali: pendekatan komputasi Perbincangan ini menyoroti aspek tesis kedoktoran yang berterusan berdasarkan analisis dan penjejakan emosi dari teks Inggeris dan Bengali. Pembangunan sumber leksikal dan korpora memenuhi keperluan awal. Spektrum kajian bertujuan untuk mengenal pasti ekspresi emosi evaluatif pada butiran tahap kata, frasa, ayat, dan dokumen bersama dengan pemegang dan topik yang berkaitan. Penjejakan emosi berdasarkan topik atau peristiwa dilakukan dengan menggunakan teknik penilaian mempengaruhi berdasarkan akal. Syarikat emosi yang dilabel sedang dipersiapkan dari contoh yang tidak berlabel untuk mengatasi kekurangan sumber emosi, terutama untuk bahasa kekangan sumber seperti bahasa Bengali. Strategi yang tidak diawasi, diawasi dan separa penyeliaan yang berbeza, yang digunakan untuk mewarnai setiap garis besar spektrum penyelidikan menghasilkan hasil yang memuaskan [[EENNDD]] crf; ungkapan; emosi; svm; struktur hujah sintaksis; blog; Penjejakan; topik; pemegang; bengwal"], [{"string": "Effective Web data extraction with standard XML technologies An abstract is not available.", "keywords": ["web", "data extraction", "methodologies", "xml", "semistructured data", "crawling", "deep web", "wrappers"], "combined": "Effective Web data extraction with standard XML technologies An abstract is not available. [[EENNDD]] web; data extraction; methodologies; xml; semistructured data; crawling; deep web; wrappers"}, "Pengekstrakan data Web yang berkesan dengan teknologi XML standard Abstrak tidak tersedia. [[EENNDD]] web; pengekstrakan data; metodologi; xml; data separa struktur; merangkak; web dalam; pembungkus"], [{"string": "Mobile web publishing and surfing based on environmental sensing data An abstract is not available.", "keywords": ["gps", "systems and software", "personalization", "sensor", "web publishing", "location", "rfid", "web browsing"], "combined": "Mobile web publishing and surfing based on environmental sensing data An abstract is not available. [[EENNDD]] gps; systems and software; personalization; sensor; web publishing; location; rfid; web browsing"}, "Penerbitan dan melayari web mudah alih berdasarkan data penginderaan persekitaran Abstrak tidak tersedia. [[EENNDD]] gps; sistem dan perisian; pemperibadian; sensor; penerbitan web; lokasi; rfid; melayari laman web"], [{"string": "Protecting web servers from distributed denial of service attacks An abstract is not available.", "keywords": ["distributed denial of service attacks", "web server security", "ddos", "security and protection", "linux", "class based routing", "unix"], "combined": "Protecting web servers from distributed denial of service attacks An abstract is not available. [[EENNDD]] distributed denial of service attacks; web server security; ddos; security and protection; linux; class based routing; unix"}, "Melindungi pelayan web dari serangan penolakan perkhidmatan yang diedarkan Abstrak tidak tersedia. [[EENNDD]] mengedarkan serangan penolakan perkhidmatan; keselamatan pelayan web; ddos; keselamatan dan perlindungan; linux; penghalaan berdasarkan kelas; unix"], [{"string": "A large-scale study of the evolution of web pages No contact information provided yet.", "keywords": ["degree of change", "hypertext/hypermedia", "miscellaneous", "rate of change", "web characterization", "web evolution", "web pages"], "combined": "A large-scale study of the evolution of web pages No contact information provided yet. [[EENNDD]] degree of change; hypertext/hypermedia; miscellaneous; rate of change; web characterization; web evolution; web pages"}, "Kajian berskala besar mengenai evolusi laman web Belum ada maklumat hubungan. [[EENNDD]] darjah perubahan; hiperteks / hipermedia; pelbagai; kadar perubahan; pencirian web; evolusi web; laman sesawang"], [{"string": "Adaptive page ranking with neural networks No contact information provided yet.", "keywords": ["neural networks", "adaptive page rank", "graph processing"], "combined": "Adaptive page ranking with neural networks No contact information provided yet. [[EENNDD]] neural networks; adaptive page rank; graph processing"}, "Kedudukan halaman adaptif dengan rangkaian neural Belum ada maklumat hubungan yang diberikan. [[EENNDD]] rangkaian saraf; kedudukan halaman adaptif; pemprosesan grafik"], [{"string": "Identifying link farm spam pages No contact information provided yet.", "keywords": ["pagerank", "information search and retrieval", "spam", "link analysis", "hits", "web search engine"], "combined": "Identifying link farm spam pages No contact information provided yet. [[EENNDD]] pagerank; information search and retrieval; spam; link analysis; hits; web search engine"}, "Mengenal pautan halaman spam ladang Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pagerank; pencarian dan pengambilan maklumat; spam; analisis pautan; hits; enjin carian web"], [{"string": "WAP5: black-box performance debugging for wide-area systems No contact information provided yet.", "keywords": ["performance analysis", "performance debugging", "testing tools", "black box systems", "distributed systems"], "combined": "WAP5: black-box performance debugging for wide-area systems No contact information provided yet. [[EENNDD]] performance analysis; performance debugging; testing tools; black box systems; distributed systems"}, "WAP5: penyahpepijatan prestasi kotak hitam untuk sistem kawasan luas Belum ada maklumat hubungan yang diberikan. [[EENNDD]] analisis prestasi; penyahpepijatan prestasi; alat ujian; sistem kotak hitam; sistem yang diedarkan"], [{"string": "Influence and passivity in social media The ever-increasing amount of information flowing through Social Media forces the members of these networks to compete for attention and influence by relying on other people to spread their message. A large study of information propagation within Twitter reveals that the majority of users act as passive information consumers and do not forward the content to the network. Therefore, in order for individuals to become influential they must not only obtain attention and thus be popular, but also overcome user passivity. We propose an algorithm that determines the influence and passivity of users based on their information forwarding activity. An evaluation performed with a 2.5 million user dataset shows that our influence measure is a good predictor of URL clicks, outperforming several other measures that do not explicitly take user passivity into account. We demonstrate that high popularity does not necessarily imply high influence and vice-versa.", "keywords": ["general", "influence", "passivity"], "combined": "Influence and passivity in social media The ever-increasing amount of information flowing through Social Media forces the members of these networks to compete for attention and influence by relying on other people to spread their message. A large study of information propagation within Twitter reveals that the majority of users act as passive information consumers and do not forward the content to the network. Therefore, in order for individuals to become influential they must not only obtain attention and thus be popular, but also overcome user passivity. We propose an algorithm that determines the influence and passivity of users based on their information forwarding activity. An evaluation performed with a 2.5 million user dataset shows that our influence measure is a good predictor of URL clicks, outperforming several other measures that do not explicitly take user passivity into account. We demonstrate that high popularity does not necessarily imply high influence and vice-versa. [[EENNDD]] general; influence; passivity"}, "Pengaruh dan sikap pasif di media sosial Jumlah maklumat yang terus meningkat melalui Media Sosial memaksa anggota rangkaian ini untuk bersaing mendapatkan perhatian dan pengaruh dengan bergantung pada orang lain untuk menyebarkan mesej mereka. Kajian besar penyebaran maklumat di Twitter menunjukkan bahawa majoriti pengguna bertindak sebagai pengguna maklumat pasif dan tidak meneruskan kandungan ke rangkaian. Oleh itu, agar individu menjadi berpengaruh, mereka bukan sahaja mesti mendapat perhatian dan dengan itu menjadi popular, tetapi juga mengatasi sifat pasif pengguna. Kami mencadangkan algoritma yang menentukan pengaruh dan pasif pengguna berdasarkan aktiviti penerusan maklumat mereka. Penilaian yang dilakukan dengan 2.5 juta set data pengguna menunjukkan bahawa ukuran pengaruh kami adalah peramal klik URL yang baik, mengatasi beberapa langkah lain yang tidak secara jelas memperhitungkan pasif pengguna. Kami menunjukkan bahawa populariti yang tinggi tidak semestinya menunjukkan pengaruh yang tinggi dan sebaliknya. [[EENNDD]] umum; pengaruh; pasif"], [{"string": "Answering search queries with CrowdSearcher Web users are increasingly relying on social interaction to complete and validate the results of their search activities. While search systems are superior machines to get world-wide information, the opinions collected within friends and expert/local communities can ultimately determine our decisions: human curiosity and creativity is often capable of going much beyond the capabilities of search systems in scouting \"interesting\" results, or suggesting new, unexpected search directions. Such personalized interaction occurs in most times aside of the search systems and processes, possibly instrumented and mediated by a social network; when such interaction is completed and users resort to the use of search systems, they do it through new queries, loosely related to the previous search or to the social interaction. In this paper we propose CrowdSearcher, a novel search paradigm that embodies crowds as first-class sources for the information seeking process. CrowdSearcher aims at filling the gap between generalized search systems, which operate upon world-wide information - including facts and recommendations as crawled and indexed by computerized systems - with social systems, capable of interacting with real people, in real time, to capture their opinions, suggestions, emotions. The technical contribution of this paper is the discussion of a model and architecture for integrating computerized search with human interaction, by showing how search systems can drive and encapsulate social systems. In particular we show how social platforms, such as Facebook, LinkedIn and Twitter, can be used for crowdsourcing search-related tasks; we demonstrate our approach with several prototypes and we report on our experiment upon real user communities.", "keywords": ["search engine", "exploratory search", "crowd sourcing", "search service", "multi-domain search", "social network", "information seeking"], "combined": "Answering search queries with CrowdSearcher Web users are increasingly relying on social interaction to complete and validate the results of their search activities. While search systems are superior machines to get world-wide information, the opinions collected within friends and expert/local communities can ultimately determine our decisions: human curiosity and creativity is often capable of going much beyond the capabilities of search systems in scouting \"interesting\" results, or suggesting new, unexpected search directions. Such personalized interaction occurs in most times aside of the search systems and processes, possibly instrumented and mediated by a social network; when such interaction is completed and users resort to the use of search systems, they do it through new queries, loosely related to the previous search or to the social interaction. In this paper we propose CrowdSearcher, a novel search paradigm that embodies crowds as first-class sources for the information seeking process. CrowdSearcher aims at filling the gap between generalized search systems, which operate upon world-wide information - including facts and recommendations as crawled and indexed by computerized systems - with social systems, capable of interacting with real people, in real time, to capture their opinions, suggestions, emotions. The technical contribution of this paper is the discussion of a model and architecture for integrating computerized search with human interaction, by showing how search systems can drive and encapsulate social systems. In particular we show how social platforms, such as Facebook, LinkedIn and Twitter, can be used for crowdsourcing search-related tasks; we demonstrate our approach with several prototypes and we report on our experiment upon real user communities. [[EENNDD]] search engine; exploratory search; crowd sourcing; search service; multi-domain search; social network; information seeking"}, "Menjawab pertanyaan carian dengan pengguna Web CrowdSearcher semakin bergantung pada interaksi sosial untuk menyelesaikan dan mengesahkan hasil aktiviti carian mereka. Walaupun sistem carian adalah mesin yang unggul untuk mendapatkan maklumat di seluruh dunia, pendapat yang dikumpulkan dalam kalangan rakan dan ahli / komuniti setempat akhirnya dapat menentukan keputusan kami: rasa ingin tahu dan kreativiti manusia sering mampu melampaui kemampuan sistem carian dalam mencari \"menarik\" hasil, atau mencadangkan arah carian baru yang tidak dijangka. Interaksi yang diperibadikan seperti itu berlaku pada kebanyakan masa selain sistem dan proses carian, mungkin diinstruksikan dan dimediasi oleh rangkaian sosial; apabila interaksi tersebut selesai dan pengguna menggunakan penggunaan sistem carian, mereka melakukannya melalui pertanyaan baru, yang berkaitan dengan carian sebelumnya atau dengan interaksi sosial. Dalam makalah ini kami mencadangkan CrowdSearcher, sebuah paradigma pencarian novel yang merangkumi orang ramai sebagai sumber kelas pertama untuk proses pencarian maklumat. CrowdSearcher bertujuan untuk mengisi jurang antara sistem carian umum, yang beroperasi berdasarkan maklumat di seluruh dunia - termasuk fakta dan cadangan yang dirangkak dan diindeks oleh sistem berkomputer - dengan sistem sosial, yang mampu berinteraksi dengan orang sebenar, dalam masa nyata, untuk mendapatkan pendapat mereka , cadangan, emosi. Sumbangan teknikal makalah ini adalah perbincangan model dan seni bina untuk mengintegrasikan carian berkomputer dengan interaksi manusia, dengan menunjukkan bagaimana sistem carian dapat mendorong dan merangkum sistem sosial. Secara khusus kami menunjukkan bagaimana platform sosial, seperti Facebook, LinkedIn dan Twitter, dapat digunakan untuk tugas-tugas yang berkaitan dengan carian ramai; kami menunjukkan pendekatan kami dengan beberapa prototaip dan kami melaporkan eksperimen kami kepada komuniti pengguna sebenar. [[EENNDD]] enjin carian; carian penerokaan; sumber orang ramai; perkhidmatan carian; carian berbilang domain; rangkaian sosial; mencari maklumat"], [{"string": "A combinatorial allocation mechanism with penalties for banner advertising Most current banner advertising is sold through negotiation thereby incurring large transaction costs and possibly suboptimal allocations. We propose a new automated system for selling banner advertising. In this system, each advertiser specifies a collection of host webpages which are relevant to his product, a desired total quantity of impressions on these pages, and a maximum per-impression price. The system selects a subset of advertisers as 'winners' and maps each winner to a set of impressions on pages within his desired collection. The distinguishing feature of our system as opposed to current combinatorial allocation mechanisms is that, mimicking the current negotiation system, we guarantee that winners receive at least as many advertising opportunities as they requested or else receive ample compensation in the form of a monetary payment by the host. Such guarantees are essential in markets like banner advertising where a major goal of the advertising campaign is developing brand recognition.", "keywords": ["internet advertising", "general", "structural approximation", "combinatorial auctions", "supply guarantee"], "combined": "A combinatorial allocation mechanism with penalties for banner advertising Most current banner advertising is sold through negotiation thereby incurring large transaction costs and possibly suboptimal allocations. We propose a new automated system for selling banner advertising. In this system, each advertiser specifies a collection of host webpages which are relevant to his product, a desired total quantity of impressions on these pages, and a maximum per-impression price. The system selects a subset of advertisers as 'winners' and maps each winner to a set of impressions on pages within his desired collection. The distinguishing feature of our system as opposed to current combinatorial allocation mechanisms is that, mimicking the current negotiation system, we guarantee that winners receive at least as many advertising opportunities as they requested or else receive ample compensation in the form of a monetary payment by the host. Such guarantees are essential in markets like banner advertising where a major goal of the advertising campaign is developing brand recognition. [[EENNDD]] internet advertising; general; structural approximation; combinatorial auctions; supply guarantee"}, "Mekanisme peruntukan gabungan dengan denda untuk iklan sepanduk Sebilangan besar iklan sepanduk terkini dijual melalui rundingan sehingga menanggung kos transaksi yang besar dan kemungkinan peruntukan yang kurang optimum. Kami mencadangkan sistem automatik baru untuk menjual iklan sepanduk. Dalam sistem ini, setiap pengiklan menentukan koleksi halaman web host yang relevan dengan produknya, jumlah tayangan yang diinginkan di halaman ini, dan harga per tayangan maksimum. Sistem ini memilih subkumpulan pengiklan sebagai 'pemenang' dan memetakan setiap pemenang ke satu set tayangan di halaman dalam koleksi yang diinginkannya. Ciri yang membezakan sistem kami berbanding dengan mekanisme peruntukan gabungan sekarang adalah bahawa, meniru sistem perundingan semasa, kami menjamin bahawa pemenang menerima sekurang-kurangnya sebanyak peluang iklan seperti yang mereka minta atau menerima pampasan yang cukup dalam bentuk pembayaran wang oleh tuan rumah. Jaminan seperti itu sangat mustahak di pasaran seperti iklan sepanduk di mana tujuan utama kempen pengiklanan adalah mengembangkan pengiktirafan jenama. [[EENNDD]] pengiklanan internet; umum; pendekatan struktur; lelong gabungan; jaminan bekalan"], [{"string": "ARROW: GenerAting SignatuRes to Detect DRive-By DOWnloads A drive-by download attack occurs when a user visits a webpage which attempts to automatically download malware without the user's consent. Attackers sometimes use a malware distribution network (MDN) to manage a large number of malicious webpages, exploits, and malware executables. In this paper, we provide a new method to determine these MDNs from the secondary URLs and redirect chains recorded by a high-interaction client honeypot. In addition, we propose a novel drive-by download detection method. Instead of depending on the malicious content used by previous methods, our algorithm first identifies and then leverages the URLs of the MDN's central servers, where a central server is a common server shared by a large percentage of the drive-by download attacks in the same MDN. A set of regular expression-based signatures are then generated based on the URLs of each central server. This method allows additional malicious webpages to be identified which launched but failed to execute a successful drive-by download attack. The new drive-by detection system named ARROW has been implemented, and we provide a large-scale evaluation on the output of a production drive-by detection system. The experimental results demonstrate the effectiveness of our method, where the detection coverage has been boosted by 96% with an extremely low false positive rate.", "keywords": ["malware distribution network", "detection", "invasive software", "signature generation", "drive-by download"], "combined": "ARROW: GenerAting SignatuRes to Detect DRive-By DOWnloads A drive-by download attack occurs when a user visits a webpage which attempts to automatically download malware without the user's consent. Attackers sometimes use a malware distribution network (MDN) to manage a large number of malicious webpages, exploits, and malware executables. In this paper, we provide a new method to determine these MDNs from the secondary URLs and redirect chains recorded by a high-interaction client honeypot. In addition, we propose a novel drive-by download detection method. Instead of depending on the malicious content used by previous methods, our algorithm first identifies and then leverages the URLs of the MDN's central servers, where a central server is a common server shared by a large percentage of the drive-by download attacks in the same MDN. A set of regular expression-based signatures are then generated based on the URLs of each central server. This method allows additional malicious webpages to be identified which launched but failed to execute a successful drive-by download attack. The new drive-by detection system named ARROW has been implemented, and we provide a large-scale evaluation on the output of a production drive-by detection system. The experimental results demonstrate the effectiveness of our method, where the detection coverage has been boosted by 96% with an extremely low false positive rate. [[EENNDD]] malware distribution network; detection; invasive software; signature generation; drive-by download"}, "ARROW: GenerAting SignatuRes untuk Mengesan DRive-By DOWnloads Serangan muat turun drive-by berlaku ketika pengguna mengunjungi halaman web yang cuba memuat turun malware secara automatik tanpa persetujuan pengguna. Penyerang kadang-kadang menggunakan rangkaian pengedaran malware (MDN) untuk menguruskan sebilangan besar laman web, eksploitasi, dan perisian hasad yang boleh dilaksanakan. Dalam makalah ini, kami menyediakan kaedah baru untuk menentukan MDN ini dari URL sekunder dan redirect rantai yang direkodkan oleh honeypot klien berinteraksi tinggi. Sebagai tambahan, kami mencadangkan kaedah pengesanan muat turun drive-by. Daripada bergantung pada kandungan berniat jahat yang digunakan oleh kaedah sebelumnya, algoritma kami terlebih dahulu mengenal pasti dan kemudian memanfaatkan URL pelayan pusat MDN, di mana pelayan pusat adalah pelayan biasa yang dikongsi oleh sebahagian besar serangan muat turun pemacu dengan cara yang sama MDN. Satu set tandatangan berdasarkan ekspresi biasa kemudian dihasilkan berdasarkan URL setiap pelayan pusat. Kaedah ini membolehkan laman web tambahan berbahaya dikenal pasti yang dilancarkan tetapi gagal melaksanakan serangan muat turun drive-by yang berjaya. Sistem pengesanan drive-by baru bernama ARROW telah dilaksanakan, dan kami memberikan penilaian skala besar terhadap output sistem pengesanan drive-by produksi. Hasil eksperimen menunjukkan keberkesanan kaedah kami, di mana liputan pengesanan telah meningkat sebanyak 96% dengan kadar positif palsu yang sangat rendah. [[EENNDD]] rangkaian pengedaran perisian hasad; pengesanan; perisian invasif; penjanaan tandatangan; muat turun pemacu"], [{"string": "CaTTS: calendar types and constraints for Web applications No contact information provided yet.", "keywords": ["calendars", "types", "time", "web reasoning"], "combined": "CaTTS: calendar types and constraints for Web applications No contact information provided yet. [[EENNDD]] calendars; types; time; web reasoning"}, "CaTTS: jenis kalendar dan kekangan untuk aplikasi Web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kalendar; jenis; masa; penaakulan web"], [{"string": "Navigating the intranet with high precision Despite the success of web search engines, search over large enterprise intranets still suffers from poor result quality. Earlier work [6] that compared intranets and the Internet from the view point of keyword search has pointed to several reasons why the search problem is quite different in these two domains. In this paper, we address the problem of providing high quality answers to navigational queries in the intranet (e.g., queries intended to find product or personal home pages, service pages, etc.). Our approach is based on offline identification of navigational pages, intelligent generation of term-variants to associate with each page, and the construction of separate indices exclusively devoted to answering navigational queries. Using a testbed of 5.5M pages from the IBM intranet, we present evaluation results that demonstrate that for navigational queries, our approach of using custom indices produces results of significantly higher precision than those produced by a general purpose search algorithm.", "keywords": ["enterprise search", "intranet search", "information search and retrieval", "high precision information retrieval"], "combined": "Navigating the intranet with high precision Despite the success of web search engines, search over large enterprise intranets still suffers from poor result quality. Earlier work [6] that compared intranets and the Internet from the view point of keyword search has pointed to several reasons why the search problem is quite different in these two domains. In this paper, we address the problem of providing high quality answers to navigational queries in the intranet (e.g., queries intended to find product or personal home pages, service pages, etc.). Our approach is based on offline identification of navigational pages, intelligent generation of term-variants to associate with each page, and the construction of separate indices exclusively devoted to answering navigational queries. Using a testbed of 5.5M pages from the IBM intranet, we present evaluation results that demonstrate that for navigational queries, our approach of using custom indices produces results of significantly higher precision than those produced by a general purpose search algorithm. [[EENNDD]] enterprise search; intranet search; information search and retrieval; high precision information retrieval"}, "Menavigasi intranet dengan ketepatan tinggi Walaupun kejayaan mesin carian web, carian di intranet perusahaan besar masih mengalami kualiti hasil yang buruk. Karya sebelumnya [6] yang membandingkan intranet dan Internet dari sudut carian kata kunci telah menunjukkan beberapa sebab mengapa masalah carian agak berbeza di kedua domain ini. Dalam makalah ini, kami menangani masalah memberikan jawapan berkualiti tinggi untuk pertanyaan navigasi di intranet (mis., Pertanyaan yang dimaksudkan untuk mencari halaman rumah produk atau peribadi, halaman perkhidmatan, dll.). Pendekatan kami didasarkan pada pengenalan luar talian halaman navigasi, generasi terma-varian pintar untuk dihubungkan dengan setiap halaman, dan pembinaan indeks berasingan yang dikhaskan untuk menjawab pertanyaan navigasi. Dengan menggunakan halaman ujian 5.5 juta halaman dari intranet IBM, kami menyajikan hasil penilaian yang menunjukkan bahawa untuk pertanyaan navigasi, pendekatan kami menggunakan indeks khusus menghasilkan hasil ketepatan yang jauh lebih tinggi daripada yang dihasilkan oleh algoritma carian tujuan umum. [[EENNDD]] carian perusahaan; carian intranet; carian dan pengambilan maklumat; pengambilan maklumat berketepatan tinggi"], [{"string": "An expressive mechanism for auctions on the web Auctions are widely used on the Web. Applications range from internet advertising to platforms such as eBay. In most of these applications the auctions in use are single/multi-item auctions with unit demand. The main drawback of standard mechanisms for this type of auctions, such as VCG and GSP, is the limited expressiveness that they offer to the bidders. The General Auction Mechanism (GAM) of [1] is taking a first step towards addressing the problem of limited expressiveness by computing a bidder optimal, envy free outcome for linear utility functions with identical slopes and a single discontinuity per bidder-item pair. We show that in many practical situations this does not suffice to adequately model the preferences of the bidders, and we overcome this problem by presenting the first mechanism for piece-wise linear utility functions with non-identical slopes and multiple discontinuities. Our mechanism runs in polynomial time. Like GAM it is incentive compatible for inputs that fulfill a certain non-degeneracy requirement, but our requirement is more general than the requirement of GAM. For discontinuous utility functions that are non-degenerate as well as for continuous utility functions the outcome of our mechanism is a competitive equilibrium. We also show how our mechanism can be used to compute approximately bidder optimal, envy free outcomes for a general class of continuous utility functions via piece-wise linear approximation. Finally, we prove hardness results for even more expressive settings.", "keywords": ["competitive equilibrium", "gsp", "vcg", "expressiveness", "envy freeness", "nonnumerical algorithms and problems", "general auction mechanism", "bidder optimality"], "combined": "An expressive mechanism for auctions on the web Auctions are widely used on the Web. Applications range from internet advertising to platforms such as eBay. In most of these applications the auctions in use are single/multi-item auctions with unit demand. The main drawback of standard mechanisms for this type of auctions, such as VCG and GSP, is the limited expressiveness that they offer to the bidders. The General Auction Mechanism (GAM) of [1] is taking a first step towards addressing the problem of limited expressiveness by computing a bidder optimal, envy free outcome for linear utility functions with identical slopes and a single discontinuity per bidder-item pair. We show that in many practical situations this does not suffice to adequately model the preferences of the bidders, and we overcome this problem by presenting the first mechanism for piece-wise linear utility functions with non-identical slopes and multiple discontinuities. Our mechanism runs in polynomial time. Like GAM it is incentive compatible for inputs that fulfill a certain non-degeneracy requirement, but our requirement is more general than the requirement of GAM. For discontinuous utility functions that are non-degenerate as well as for continuous utility functions the outcome of our mechanism is a competitive equilibrium. We also show how our mechanism can be used to compute approximately bidder optimal, envy free outcomes for a general class of continuous utility functions via piece-wise linear approximation. Finally, we prove hardness results for even more expressive settings. [[EENNDD]] competitive equilibrium; gsp; vcg; expressiveness; envy freeness; nonnumerical algorithms and problems; general auction mechanism; bidder optimality"}, "Mekanisme ekspresif untuk lelongan di web Lelong banyak digunakan di Web. Aplikasi berkisar dari pengiklanan internet hingga platform seperti eBay. Dalam kebanyakan aplikasi ini, lelongan yang digunakan adalah lelong tunggal / pelbagai item dengan permintaan unit. Kelemahan utama mekanisme standard untuk lelongan jenis ini, seperti VCG dan GSP, adalah ekspresi terhad yang mereka tawarkan kepada pembida. Mekanisme Lelongan Umum (GAM) [1] mengambil langkah pertama untuk mengatasi masalah ekspresi terhad dengan mengira hasil penawar yang optimum, bebas iri hati untuk fungsi utiliti linier dengan cerun yang sama dan satu putusan tunggal bagi pasangan penawar-item. Kami menunjukkan bahawa dalam banyak situasi praktikal, ini tidak mencukupi untuk memodelkan pilihan para penawar dengan secukupnya, dan kami mengatasi masalah ini dengan membentangkan mekanisme pertama untuk fungsi utiliti linier dengan cerun yang tidak serupa dan banyak putaran. Mekanisme kita berjalan dalam masa polinomial. Seperti GAM, insentif sesuai untuk input yang memenuhi syarat non-degenerasi tertentu, tetapi keperluan kita lebih umum daripada keperluan GAM. Untuk fungsi utiliti terputus yang tidak merosot serta untuk fungsi utiliti berterusan hasil mekanisme kami adalah keseimbangan yang kompetitif. Kami juga menunjukkan bagaimana mekanisme kami dapat digunakan untuk menghitung kira-kira hasil penawar yang optimum, hasil hasad dengki untuk kelas umum fungsi utiliti berterusan melalui pendekatan linear sepotong bijak. Akhirnya, kami membuktikan hasil kekerasan untuk tetapan yang lebih ekspresif. [[EENNDD]] keseimbangan kompetitif; gsp; vcg; ekspresi; iri hati; algoritma dan masalah bukan berangka; mekanisme lelongan am; optimum penawar"], [{"string": "Fine-grained privilege separation for web applications We present a programming model for building web applications with security properties that can be confidently verified during a security review. In our model, applications are divided into isolated, privilege-separated components, enabling rich security policies to be enforced in a way that can be checked by reviewers. In our model, the web framework enforces privilege separation and isolation of web applications by requiring the use of an object-capability language and providing interfaces that expose limited, explicitly-specified privileges to application components. This approach restricts what each component of the application can do and quarantines buggy or compromised code. It also provides a way to more safely integrate third-party, less-trusted code into a web application. We have implemented a prototype of this model based upon the Java Servlet framework and used it to build a webmail application. Our experience with this example suggests that the approach is viable and helpful at establishing reviewable application-specific security properties.", "keywords": ["privilege separation", "web applications", "software architectures", "object-capabilities"], "combined": "Fine-grained privilege separation for web applications We present a programming model for building web applications with security properties that can be confidently verified during a security review. In our model, applications are divided into isolated, privilege-separated components, enabling rich security policies to be enforced in a way that can be checked by reviewers. In our model, the web framework enforces privilege separation and isolation of web applications by requiring the use of an object-capability language and providing interfaces that expose limited, explicitly-specified privileges to application components. This approach restricts what each component of the application can do and quarantines buggy or compromised code. It also provides a way to more safely integrate third-party, less-trusted code into a web application. We have implemented a prototype of this model based upon the Java Servlet framework and used it to build a webmail application. Our experience with this example suggests that the approach is viable and helpful at establishing reviewable application-specific security properties. [[EENNDD]] privilege separation; web applications; software architectures; object-capabilities"}, "Pemisahan hak istimewa untuk aplikasi web Kami menyajikan model pengaturcaraan untuk membina aplikasi web dengan sifat keselamatan yang dapat disahkan dengan yakin semasa tinjauan keselamatan. Dalam model kami, aplikasi dibahagikan kepada komponen terpencil yang dipisahkan hak istimewa, yang membolehkan dasar keselamatan yang kaya dapat ditegakkan dengan cara yang dapat diperiksa oleh pengulas. Dalam model kami, kerangka web menegakkan pemisahan hak istimewa dan pengasingan aplikasi web dengan mengharuskan penggunaan bahasa kemampuan objek dan menyediakan antarmuka yang memperlihatkan hak istimewa yang ditentukan secara eksplisit untuk komponen aplikasi. Pendekatan ini mengehadkan apa yang dapat dilakukan oleh setiap komponen aplikasi dan karantina kod kereta atau dikompromikan. Ini juga menyediakan cara untuk menggabungkan kod pihak ketiga yang kurang dipercayai ke dalam aplikasi web dengan lebih selamat. Kami telah menerapkan prototipe model ini berdasarkan kerangka Java Servlet dan menggunakannya untuk membangun aplikasi email web. Pengalaman kami dengan contoh ini menunjukkan bahawa pendekatan ini dapat dilaksanakan dan bermanfaat dalam mewujudkan sifat keselamatan khusus aplikasi yang dapat ditinjau. [[EENNDD]] pemisahan hak istimewa; aplikasi web; seni bina perisian; keupayaan-objek"], [{"string": "Ontology-based learning content repurposing No contact information provided yet.", "keywords": ["content models", "ontologies", "miscellaneous", "repurposing", "learning objects", "metadata"], "combined": "Ontology-based learning content repurposing No contact information provided yet. [[EENNDD]] content models; ontologies; miscellaneous; repurposing; learning objects; metadata"}, "Kandungan pembelajaran berasaskan ontologi muncul semula Belum ada maklumat hubungan yang diberikan. [[EENNDD]] model kandungan; ontologi; pelbagai; mengadaptasi semula; objek pembelajaran; metadata"], [{"string": "A hybrid phish detection approach by identity discovery and keywords retrieval Phishing is a significant security threat to the Internet, which causes tremendous economic loss every year. In this paper, we proposed a novel hybrid phish detection method based on information extraction (IE) and information retrieval (IR) techniques. The identity-based component of our method detects phishing webpages by directly discovering the inconsistency between their identity and the identity they are imitating. The keywords-retrieval component utilizes IR algorithms exploiting the power of search engines to identify phish. Our method requires no training data, no prior knowledge of phishing signatures and specific implementations, and thus is able to adapt quickly to constantly appearing new phishing patterns. Comprehensive experiments over a diverse spectrum of data sources with 11449 pages show that both components have a low false positive rate and the stacked approach achieves a true positive rate of 90.06% with a false positive rate of 1.95%.", "keywords": ["anti-phishing", "named entity recognition", "information retrieval", "natural language processing", "information search and retrieval", "security and protection"], "combined": "A hybrid phish detection approach by identity discovery and keywords retrieval Phishing is a significant security threat to the Internet, which causes tremendous economic loss every year. In this paper, we proposed a novel hybrid phish detection method based on information extraction (IE) and information retrieval (IR) techniques. The identity-based component of our method detects phishing webpages by directly discovering the inconsistency between their identity and the identity they are imitating. The keywords-retrieval component utilizes IR algorithms exploiting the power of search engines to identify phish. Our method requires no training data, no prior knowledge of phishing signatures and specific implementations, and thus is able to adapt quickly to constantly appearing new phishing patterns. Comprehensive experiments over a diverse spectrum of data sources with 11449 pages show that both components have a low false positive rate and the stacked approach achieves a true positive rate of 90.06% with a false positive rate of 1.95%. [[EENNDD]] anti-phishing; named entity recognition; information retrieval; natural language processing; information search and retrieval; security and protection"}, "Pendekatan pengesanan pancingan data hibrid dengan penemuan identiti dan pengambilan kata kunci Phishing adalah ancaman keselamatan yang signifikan terhadap Internet, yang menyebabkan kerugian ekonomi yang luar biasa setiap tahun. Dalam makalah ini, kami mencadangkan kaedah pengesanan pancingan data hibrid berdasarkan teknik pengekstrakan maklumat (IE) dan pengambilan maklumat (IR). Komponen berasaskan identiti kaedah kami mengesan laman web pancingan data dengan mengetahui secara langsung ketidakkonsistenan antara identiti mereka dan identiti yang mereka tiru. Komponen pengambilan kata kunci menggunakan algoritma IR yang memanfaatkan kekuatan enjin carian untuk mengenal pasti pancingan data. Kaedah kami tidak memerlukan data latihan, tidak ada pengetahuan sebelumnya mengenai tandatangan pancingan data dan pelaksanaan tertentu, dan dengan demikian dapat menyesuaikan diri dengan cepat terhadap corak pancingan data baru yang selalu muncul. Eksperimen yang komprehensif terhadap spektrum sumber data yang beragam dengan 11449 halaman menunjukkan bahawa kedua-dua komponen mempunyai kadar positif palsu yang rendah dan pendekatan bertumpuk mencapai kadar positif benar 90.06% dengan kadar positif palsu 1.95%. [[EENNDD]] anti-pancingan data; pengiktirafan entiti bernama; pengambilan maklumat; pemprosesan bahasa semula jadi; carian dan pengambilan maklumat; keselamatan dan perlindungan"], [{"string": "Information search and re-access strategies of experienced web users No contact information provided yet.", "keywords": ["web search", "information re-access", "experienced web users", "questionnaire study", "user interface management systems"], "combined": "Information search and re-access strategies of experienced web users No contact information provided yet. [[EENNDD]] web search; information re-access; experienced web users; questionnaire study; user interface management systems"}, "Strategi mencari maklumat dan mengakses semula pengguna web berpengalaman Belum ada maklumat hubungan yang diberikan. [[EENNDD]] carian web; capaian semula maklumat; pengguna web yang berpengalaman; kajian soal selidik; sistem pengurusan antara muka pengguna"], [{"string": "Using proximity to predict activity in social networks The structure of a social network contains information useful for predicting its evolution. We show that structural information also helps predict activity. People who are \"close\" in some sense in a social network are more likely to perform similar actions than more distant people. We use network proximity to capture the degree to which people are \"close\" to each other. In addition to standard proximity metrics used in the link prediction task, such as neighborhood overlap, we introduce new metrics that model different types of interactions that take place between people. We study this claim empirically using data about URL forwarding activity on the social media sites Digg and Twitter. We show that structural proximity of two users in the follower graph is related to similarity of their activity, i.e., how many URLs they both forward. We also show that given friends' activity, knowing their proximity to the user can help better predict which URLs the user will forward. We compare the performance of different proximity metrics on the activity prediction task and find that metrics that take into account the attention-limited nature of interactions in social media lead to substantially better predictions.", "keywords": ["proximity", "miscellaneous", "social networks", "activity prediction", "social media"], "combined": "Using proximity to predict activity in social networks The structure of a social network contains information useful for predicting its evolution. We show that structural information also helps predict activity. People who are \"close\" in some sense in a social network are more likely to perform similar actions than more distant people. We use network proximity to capture the degree to which people are \"close\" to each other. In addition to standard proximity metrics used in the link prediction task, such as neighborhood overlap, we introduce new metrics that model different types of interactions that take place between people. We study this claim empirically using data about URL forwarding activity on the social media sites Digg and Twitter. We show that structural proximity of two users in the follower graph is related to similarity of their activity, i.e., how many URLs they both forward. We also show that given friends' activity, knowing their proximity to the user can help better predict which URLs the user will forward. We compare the performance of different proximity metrics on the activity prediction task and find that metrics that take into account the attention-limited nature of interactions in social media lead to substantially better predictions. [[EENNDD]] proximity; miscellaneous; social networks; activity prediction; social media"}, "Menggunakan jarak untuk meramalkan aktiviti di rangkaian sosial Struktur rangkaian sosial mengandungi maklumat yang berguna untuk meramalkan evolusi. Kami menunjukkan bahawa maklumat struktur juga membantu meramalkan aktiviti. Orang yang \"dekat\" dalam arti tertentu dalam rangkaian sosial lebih cenderung melakukan tindakan serupa daripada orang yang lebih jauh. Kami menggunakan jarak rangkaian untuk mengetahui sejauh mana orang \"dekat\" antara satu sama lain. Sebagai tambahan kepada metrik jarak standard yang digunakan dalam tugas ramalan pautan, seperti pertindihan kawasan sekitar, kami memperkenalkan metrik baru yang memodelkan pelbagai jenis interaksi yang berlaku antara orang. Kami mengkaji tuntutan ini secara empirik menggunakan data mengenai aktiviti pemajuan URL di laman media sosial Digg dan Twitter. Kami menunjukkan bahawa jarak struktur dua pengguna dalam grafik pengikut berkaitan dengan persamaan aktiviti mereka, iaitu berapa banyak URL yang mereka berdua maju. Kami juga menunjukkan bahawa berdasarkan aktiviti rakan, mengetahui jarak mereka dengan pengguna dapat membantu meramalkan URL yang akan diteruskan oleh pengguna dengan lebih baik. Kami membandingkan prestasi metrik jarak yang berbeza pada tugas ramalan aktiviti dan mendapati bahawa metrik yang mengambil kira sifat interaksi yang terhad perhatian di media sosial membawa kepada ramalan yang jauh lebih baik. [[EENNDD]] berdekatan; pelbagai; rangkaian sosial; ramalan aktiviti; media sosial"], [{"string": "Extraction and mining of an academic social network This paper addresses several key issues in extraction and mining of an academic social network: 1) extraction of a researcher social network from the existing Web; 2) integration of the publications from existing digital libraries; 3) expertise search on a given topic; and 4) association search between researchers. We developed a social network system, called ArnetMiner, based on proposed methods to the above problems. In total, 448,470 researcher profiles and 981,599 publications were extracted/integrated after the system having been in operation for two years. The paper describes the architecture and main features of the system. It also briefly presents the experimental results of the proposed methods.", "keywords": ["expertise search", "information extraction", "information search and retrieval", "learning", "database applications", "social network"], "combined": "Extraction and mining of an academic social network This paper addresses several key issues in extraction and mining of an academic social network: 1) extraction of a researcher social network from the existing Web; 2) integration of the publications from existing digital libraries; 3) expertise search on a given topic; and 4) association search between researchers. We developed a social network system, called ArnetMiner, based on proposed methods to the above problems. In total, 448,470 researcher profiles and 981,599 publications were extracted/integrated after the system having been in operation for two years. The paper describes the architecture and main features of the system. It also briefly presents the experimental results of the proposed methods. [[EENNDD]] expertise search; information extraction; information search and retrieval; learning; database applications; social network"}, "Pengekstrakan dan perlombongan rangkaian sosial akademik Makalah ini membahas beberapa isu utama dalam pengekstrakan dan perlombongan rangkaian sosial akademik: 1) pengekstrakan rangkaian sosial penyelidik dari Web yang ada; 2) penyatuan penerbitan dari perpustakaan digital yang ada; 3) pencarian kepakaran pada topik tertentu; dan 4) pencarian perkaitan antara penyelidik. Kami mengembangkan sistem rangkaian sosial, yang disebut ArnetMiner, berdasarkan kaedah yang dicadangkan untuk masalah di atas. Secara keseluruhan, 448.470 profil penyelidik dan 981.599 penerbitan diekstraksi / disatukan setelah sistem ini beroperasi selama dua tahun. Makalah ini menerangkan seni bina dan ciri utama sistem. Ia juga secara ringkas menunjukkan hasil eksperimen kaedah yang dicadangkan. [[EENNDD]] carian kepakaran; pengekstrakan maklumat; pencarian dan pengambilan maklumat; belajar; aplikasi pangkalan data; rangkaian sosial"], [{"string": "Using semantic rules to determine access control for web services No contact information provided yet.", "keywords": ["web service security", "authorisation", "security and protection", "swrl", "owl"], "combined": "Using semantic rules to determine access control for web services No contact information provided yet. [[EENNDD]] web service security; authorisation; security and protection; swrl; owl"}, "Menggunakan peraturan semantik untuk menentukan kawalan akses untuk perkhidmatan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] keselamatan perkhidmatan web; kebenaran; keselamatan dan perlindungan; swrl; burung hantu"], [{"string": "Composite events for xml No contact information provided yet.", "keywords": ["active behavior", "event algebra", "event-condition-action rule", "xml", "composite event"], "combined": "Composite events for xml No contact information provided yet. [[EENNDD]] active behavior; event algebra; event-condition-action rule; xml; composite event"}, "Acara komposit untuk xml Belum ada maklumat hubungan yang diberikan. [[EENNDD]] tingkah laku aktif; aljabar acara; peraturan peristiwa-keadaan-tindakan; xml; acara komposit"], [{"string": "Tag-oriented document summarization Social annotations on a Web document are highly generalized description of topics contained in that page. Their tagged frequency indicates the user attentions with various degrees. This makes annotations a good resource for summarizing multiple topics in a Web page. In this paper, we present a tag-oriented Web document summarization approach by using both document content and the tags annotated on that document. To improve summarization performance, a new tag ranking algorithm named EigenTag is proposed in this paper to reduce noise in tags. Meanwhile, association mining technique is employed to expand tag set to tackle the sparsity problem. Experimental results show our tag-oriented summarization has a significant improvement over those not using tags.", "keywords": ["document summarization", "ranking", "tag"], "combined": "Tag-oriented document summarization Social annotations on a Web document are highly generalized description of topics contained in that page. Their tagged frequency indicates the user attentions with various degrees. This makes annotations a good resource for summarizing multiple topics in a Web page. In this paper, we present a tag-oriented Web document summarization approach by using both document content and the tags annotated on that document. To improve summarization performance, a new tag ranking algorithm named EigenTag is proposed in this paper to reduce noise in tags. Meanwhile, association mining technique is employed to expand tag set to tackle the sparsity problem. Experimental results show our tag-oriented summarization has a significant improvement over those not using tags. [[EENNDD]] document summarization; ranking; tag"}, "Ringkasan dokumen berorientasikan tag Anotasi sosial pada dokumen Web adalah gambaran umum mengenai topik yang terdapat dalam halaman itu. Kekerapan tag mereka menunjukkan perhatian pengguna dengan pelbagai darjah. Ini menjadikan anotasi sumber yang baik untuk meringkaskan banyak topik dalam halaman Web. Dalam makalah ini, kami menyajikan pendekatan ringkasan dokumen Web berorientasikan tag dengan menggunakan kedua kandungan dokumen dan tag yang dijelaskan pada dokumen tersebut. Untuk meningkatkan prestasi ringkasan, algoritma pemeringkatan tag baru bernama EigenTag dicadangkan dalam makalah ini untuk mengurangkan kebisingan dalam tag. Sementara itu, teknik perlombongan persatuan digunakan untuk memperluaskan set tag untuk mengatasi masalah jarak. Hasil eksperimen menunjukkan ringkasan berorientasikan teg kami mempunyai peningkatan yang ketara berbanding mereka yang tidak menggunakan teg. [[EENNDD]] ringkasan dokumen; peringkat; teg"], [{"string": "A machine learning based approach for table detection on the web No contact information provided yet.", "keywords": ["support vector machine", "information retrieval", "decision tree", "table detection", "layout analysis", "machine learning"], "combined": "A machine learning based approach for table detection on the web No contact information provided yet. [[EENNDD]] support vector machine; information retrieval; decision tree; table detection; layout analysis; machine learning"}, "Pendekatan berasaskan pembelajaran mesin untuk pengesanan jadual di web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] mesin vektor sokongan; pengambilan maklumat; pokok keputusan; pengesanan jadual; analisis susun atur; pembelajaran mesin"], [{"string": "Automatic extraction of web search interfaces for interface schema integration No contact information provided yet.", "keywords": ["search engine", "search interface extraction", "search interface representation", "user interface management systems", "metasearch engine"], "combined": "Automatic extraction of web search interfaces for interface schema integration No contact information provided yet. [[EENNDD]] search engine; search interface extraction; search interface representation; user interface management systems; metasearch engine"}, "Pengambilan automatik antara muka carian web untuk penyatuan skema antara muka Belum ada maklumat hubungan yang diberikan. [[EENNDD]] enjin carian; pengekstrakan antara muka carian; perwakilan antara muka carian; sistem pengurusan antara muka pengguna; enjin carian metas"], [{"string": "Map adaptation for users of mobile systems An abstract is not available.", "keywords": ["web", "quality of service", "adaptive application", "portable devices"], "combined": "Map adaptation for users of mobile systems An abstract is not available. [[EENNDD]] web; quality of service; adaptive application; portable devices"}, "Penyesuaian peta untuk pengguna sistem mudah alih Abstrak tidak tersedia. [[EENNDD]] web; kualiti sesuatu servis; aplikasi adaptif; peranti mudah alih"], [{"string": "Discovering the best web service Major research challenges in discovering Web services include, provisioning of services across multiple or heterogeneous registries, differentiating between services that share similar functionalities, improving end-to-end Quality of Service (QoS), and enabling clients to customize the discovery process. Proliferation and interoperability of this multitude of Web services have lead to the emergence of new standards on how services can be published, discovered, or used (i.e. UDDI, WSDL, SOAP). Such standards can potentially provide many of these features and much more, however, there are technical challenges associated with existing standards. One of these challenges is the client.s ability to control the discovery process across accessible service registries for finding services of interest. This work proposes a solution to this problem and introduces the Web Service Relevancy Function (WsRF) used for measuring the relevancy ranking of a particular Web service based on QoS metrics and client preferences. We present experimental validation, results, and analysis of the presented ideas.", "keywords": ["tmodel", "uddi", "quality of services", "web services", "uddi business registries", "uddi extension", "qos"], "combined": "Discovering the best web service Major research challenges in discovering Web services include, provisioning of services across multiple or heterogeneous registries, differentiating between services that share similar functionalities, improving end-to-end Quality of Service (QoS), and enabling clients to customize the discovery process. Proliferation and interoperability of this multitude of Web services have lead to the emergence of new standards on how services can be published, discovered, or used (i.e. UDDI, WSDL, SOAP). Such standards can potentially provide many of these features and much more, however, there are technical challenges associated with existing standards. One of these challenges is the client.s ability to control the discovery process across accessible service registries for finding services of interest. This work proposes a solution to this problem and introduces the Web Service Relevancy Function (WsRF) used for measuring the relevancy ranking of a particular Web service based on QoS metrics and client preferences. We present experimental validation, results, and analysis of the presented ideas. [[EENNDD]] tmodel; uddi; quality of services; web services; uddi business registries; uddi extension; qos"}, "Menemui perkhidmatan web terbaik Cabaran penyelidikan utama dalam menemui perkhidmatan Web termasuk, penyediaan perkhidmatan di pelbagai pendaftar atau heterogen, membezakan antara perkhidmatan yang mempunyai fungsi yang serupa, meningkatkan Kualiti Perkhidmatan (QoS) dari hujung ke hujung, dan membolehkan pelanggan menyesuaikan proses penemuan. Penyebaran dan pengoperasian banyak perkhidmatan Web ini telah membawa kepada munculnya standard baru mengenai bagaimana perkhidmatan dapat diterbitkan, dijumpai, atau digunakan (iaitu UDDI, WSDL, SOAP). Piawaian sedemikian berpotensi memberikan banyak ciri ini dan banyak lagi, namun, terdapat cabaran teknikal yang berkaitan dengan standard yang ada. Salah satu cabaran ini adalah kemampuan pelanggan untuk mengawal proses penemuan di seluruh daftar perkhidmatan yang dapat diakses untuk mencari perkhidmatan yang menarik. Karya ini mencadangkan penyelesaian untuk masalah ini dan memperkenalkan Fungsi Perkaitan Perkhidmatan Web (WsRF) yang digunakan untuk mengukur peringkat relevansi perkhidmatan Web tertentu berdasarkan metrik QoS dan pilihan pelanggan. Kami memaparkan pengesahan eksperimen, hasil, dan analisis idea yang dikemukakan. [[EENNDD]] tmodel; uddi; kualiti perkhidmatan; perkhidmatan web; daftar perniagaan uddi; sambungan uddi; qos"], [{"string": "Beyond PageRank: machine learning for static ranking No contact information provided yet.", "keywords": ["pagerank", "relevance", "search engines", "static ranking", "information search and retrieval", "learning", "ranknet"], "combined": "Beyond PageRank: machine learning for static ranking No contact information provided yet. [[EENNDD]] pagerank; relevance; search engines; static ranking; information search and retrieval; learning; ranknet"}, "Beyond PageRank: pembelajaran mesin untuk kedudukan statik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pagerank; perkaitan; enjin carian; kedudukan statik; pencarian dan pengambilan maklumat; belajar; ranknet"], [{"string": "XML query forms (XQForms): declarative specification of XML query interfaces An abstract is not available.", "keywords": ["xsl", "web", "xml", "xml query language", "query forms &amp; reports"], "combined": "XML query forms (XQForms): declarative specification of XML query interfaces An abstract is not available. [[EENNDD]] xsl; web; xml; xml query language; query forms &amp; reports"}, ""], [{"string": "Defection detection: predicting search engine switching Searchers have a choice about which Web search engine they use when looking for information online. If they are unsuccessful on one engine, users may switch to a different engine to continue their search. By predicting when switches are likely to occur, the search experience can be modified to retain searchers or ensure a quality experience for incoming searchers. In this poster, we present research on a technique for predicting search engine switches. Our findings show that prediction is possible at a reasonable level of accuracy, particularly when personalization or user grouping is employed. These findings have implications for the design of applications to support more effective online searching.", "keywords": ["search engine switching"], "combined": "Defection detection: predicting search engine switching Searchers have a choice about which Web search engine they use when looking for information online. If they are unsuccessful on one engine, users may switch to a different engine to continue their search. By predicting when switches are likely to occur, the search experience can be modified to retain searchers or ensure a quality experience for incoming searchers. In this poster, we present research on a technique for predicting search engine switches. Our findings show that prediction is possible at a reasonable level of accuracy, particularly when personalization or user grouping is employed. These findings have implications for the design of applications to support more effective online searching. [[EENNDD]] search engine switching"}, "Pengesanan kerosakan: meramalkan pertukaran mesin pencari Pencari mempunyai pilihan mengenai enjin carian Web mana yang mereka gunakan ketika mencari maklumat dalam talian. Sekiranya mereka tidak berjaya pada satu enjin, pengguna boleh beralih ke mesin yang lain untuk meneruskan pencarian. Dengan meramalkan kapan kemungkinan pertukaran berlaku, pengalaman carian dapat diubah untuk mengekalkan pencari atau memastikan pengalaman berkualiti bagi pencari masuk. Dalam poster ini, kami menyajikan penyelidikan mengenai teknik untuk meramalkan pertukaran mesin pencari. Hasil kajian kami menunjukkan bahawa ramalan dapat dilakukan pada tahap ketepatan yang wajar, terutamanya ketika pemperibadian atau pengelompokan pengguna digunakan. Penemuan ini mempunyai implikasi terhadap reka bentuk aplikasi untuk menyokong pencarian dalam talian yang lebih berkesan. [[EENNDD]] pertukaran mesin carian"], [{"string": "Relational duality: unsupervised extraction of semantic relations between entities on the web Extracting semantic relations among entities is an important first step in various tasks in Web mining and natural language processing such as information extraction, relation detection, and social network mining. A relation can be expressed extensionally by stating all the instances of that relation or intensionally by defining all the paraphrases of that relation. For example, consider the ACQUISITION relation between two companies. An extensional definition of ACQUISITION contains all pairs of companies in which one company is acquired by another (e.g. (YouTube, Google) or (Powerset, Microsoft)). On the other hand we can intensionally define ACQUISITION as the relation described by lexical patterns such as X is acquired by Y, or Y purchased X, where X and Y denote two companies. We use this dual representation of semantic relations to propose a novel sequential co-clustering algorithm that can extract numerous relations efficiently from unlabeled data. We provide an efficient heuristic to find the parameters of the proposed coclustering algorithm. Using the clusters produced by the algorithm, we train an L1 regularized logistic regression model to identify the representative patterns that describe the relation expressed by each cluster. We evaluate the proposed method in three different tasks: measuring relational similarity between entity pairs, open information extraction (Open IE), and classifying relations in a social network system. Experiments conducted using a benchmark dataset show that the proposed method improves existing relational similarity measures. Moreover, the proposed method significantly outperforms the current state-of-the-art Open IE systems in terms of both precision and recall. The proposed method correctly classifies 53 relation types in an online social network containing 470; 671 nodes and 35; 652; 475 edges, thereby demonstrating its efficacy in real-world relation detection tasks.", "keywords": ["relational duality", "relational similarity", "relation extraction", "web mining"], "combined": "Relational duality: unsupervised extraction of semantic relations between entities on the web Extracting semantic relations among entities is an important first step in various tasks in Web mining and natural language processing such as information extraction, relation detection, and social network mining. A relation can be expressed extensionally by stating all the instances of that relation or intensionally by defining all the paraphrases of that relation. For example, consider the ACQUISITION relation between two companies. An extensional definition of ACQUISITION contains all pairs of companies in which one company is acquired by another (e.g. (YouTube, Google) or (Powerset, Microsoft)). On the other hand we can intensionally define ACQUISITION as the relation described by lexical patterns such as X is acquired by Y, or Y purchased X, where X and Y denote two companies. We use this dual representation of semantic relations to propose a novel sequential co-clustering algorithm that can extract numerous relations efficiently from unlabeled data. We provide an efficient heuristic to find the parameters of the proposed coclustering algorithm. Using the clusters produced by the algorithm, we train an L1 regularized logistic regression model to identify the representative patterns that describe the relation expressed by each cluster. We evaluate the proposed method in three different tasks: measuring relational similarity between entity pairs, open information extraction (Open IE), and classifying relations in a social network system. Experiments conducted using a benchmark dataset show that the proposed method improves existing relational similarity measures. Moreover, the proposed method significantly outperforms the current state-of-the-art Open IE systems in terms of both precision and recall. The proposed method correctly classifies 53 relation types in an online social network containing 470; 671 nodes and 35; 652; 475 edges, thereby demonstrating its efficacy in real-world relation detection tasks. [[EENNDD]] relational duality; relational similarity; relation extraction; web mining"}, "Dualitas hubungan: pengekstrakan hubungan semantik antara entiti di web tanpa pengawasan Pengambilan hubungan semantik antara entiti adalah langkah pertama yang penting dalam pelbagai tugas dalam perlombongan Web dan pemprosesan bahasa semula jadi seperti pengekstrakan maklumat, pengesanan hubungan, dan perlombongan rangkaian sosial. Suatu hubungan dapat dinyatakan secara meluas dengan menyatakan semua kejadian hubungan tersebut atau secara tidak sengaja dengan menentukan semua parafrasa hubungan itu. Contohnya, pertimbangkan hubungan AKUISISI antara dua syarikat. Definisi ekstensif mengenai AKUISISI mengandungi semua pasangan syarikat di mana satu syarikat diambil alih oleh syarikat lain (mis. (YouTube, Google) atau (Powerset, Microsoft)). Sebaliknya kita dapat secara tidak sengaja mendefinisikan AKUISISI sebagai hubungan yang dijelaskan oleh corak leksikal seperti X diperoleh oleh Y, atau Y dibeli X, di mana X dan Y menunjukkan dua syarikat. Kami menggunakan perwakilan dua hubungan semantik ini untuk mengusulkan algoritma kluster sekuensial baru yang dapat mengekstrak banyak hubungan dengan cekap dari data yang tidak berlabel. Kami memberikan heuristik yang cekap untuk mencari parameter algoritma coclustering yang dicadangkan. Dengan menggunakan kluster yang dihasilkan oleh algoritma, kami melatih model regresi logistik teratur L1 untuk mengenal pasti corak perwakilan yang menggambarkan hubungan yang dinyatakan oleh setiap kluster. Kami menilai kaedah yang dicadangkan dalam tiga tugas yang berbeza: mengukur kesamaan hubungan antara pasangan entiti, pengekstrakan maklumat terbuka (Open IE), dan mengklasifikasikan hubungan dalam sistem rangkaian sosial. Eksperimen yang dilakukan menggunakan set data penanda aras menunjukkan bahawa kaedah yang dicadangkan meningkatkan langkah-langkah kesamaan hubungan yang ada. Lebih-lebih lagi, kaedah yang dicadangkan secara signifikan mengatasi sistem Open IE terkini yang canggih dari segi ketepatan dan penarikan semula. Kaedah yang dicadangkan mengklasifikasikan 53 jenis hubungan dengan betul dalam rangkaian sosial dalam talian yang mengandungi 470; 671 nod dan 35; 652; 475 sisi, dengan demikian menunjukkan keberkesanannya dalam tugas pengesanan hubungan dunia nyata. [[EENNDD]] dualitas hubungan; persamaan hubungan; pengekstrakan hubungan; perlombongan web"], [{"string": "Dynamic cost-per-action mechanisms and applications to online advertising We study the Cost-Per-Action or Cost-Per-Acquisition (CPA) charging scheme in online advertising. In this scheme, instead of paying per click, the advertisers pay only when a user takes a specific action (e.g. fills out a form) or completes a transaction on their websites.", "keywords": ["internet advertising", "general", "learning", "mechanism design", "cost-per-action"], "combined": "Dynamic cost-per-action mechanisms and applications to online advertising We study the Cost-Per-Action or Cost-Per-Acquisition (CPA) charging scheme in online advertising. In this scheme, instead of paying per click, the advertisers pay only when a user takes a specific action (e.g. fills out a form) or completes a transaction on their websites. [[EENNDD]] internet advertising; general; learning; mechanism design; cost-per-action"}, "Mekanisme kos per tindakan dinamik dan aplikasi untuk pengiklanan dalam talian Kami mengkaji skema pengisian Kos-Per-Tindakan atau Kos-Per-Perolehan (BPA) dalam iklan dalam talian. Dalam skema ini, bukannya membayar per klik, pengiklan membayar hanya ketika pengguna melakukan tindakan tertentu (mis. Mengisi formulir) atau menyelesaikan transaksi di laman web mereka. [[EENNDD]] pengiklanan internet; umum; belajar; reka bentuk mekanisme; kos setiap tindakan"], [{"string": "Template detection via data mining and its applications No contact information provided yet.", "keywords": ["data mining", "web searching", "information retrieval", "information search and retrieval", "hypertext"], "combined": "Template detection via data mining and its applications No contact information provided yet. [[EENNDD]] data mining; web searching; information retrieval; information search and retrieval; hypertext"}, "Pengesanan templat melalui perlombongan data dan aplikasinya Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlombongan data; carian web; pengambilan maklumat; pencarian dan pengambilan maklumat; hiperteks"], [{"string": "Delay tolerant applications for low bandwidth and intermittently connected users: the aAQUA experience With the explosive growth and spread of Internet, web access from mobile and rural users has become significant. But these users face problems of low bandwidth and intermittent Internet connectivity. To make the benefits of the Internet reach the common man in developing countries, accessibility and availability of the information has to be improved. aAQUA is an online multilingual, multimedia agricultural portal for disseminating information from and to rural communities. Considering resource constrained rural environments, we have designed and implemented an offline solution which provides an online experience to users in disconnected mode. Our solution is based on heterogeneous database synchronization which involves only a small synchronization payload ensuring an efficient use of available bandwidth. Offline aAQUA has been deployed in the field and systematic studies of our solution show that user experience has improved tremendously not only in disconnected mode but also in connected mode.", "keywords": ["offline access", "resource constrained low end pcs", "heterogeneous database synchronization", "caching", "information and communication technologies for development", "low-bandwidth application"], "combined": "Delay tolerant applications for low bandwidth and intermittently connected users: the aAQUA experience With the explosive growth and spread of Internet, web access from mobile and rural users has become significant. But these users face problems of low bandwidth and intermittent Internet connectivity. To make the benefits of the Internet reach the common man in developing countries, accessibility and availability of the information has to be improved. aAQUA is an online multilingual, multimedia agricultural portal for disseminating information from and to rural communities. Considering resource constrained rural environments, we have designed and implemented an offline solution which provides an online experience to users in disconnected mode. Our solution is based on heterogeneous database synchronization which involves only a small synchronization payload ensuring an efficient use of available bandwidth. Offline aAQUA has been deployed in the field and systematic studies of our solution show that user experience has improved tremendously not only in disconnected mode but also in connected mode. [[EENNDD]] offline access; resource constrained low end pcs; heterogeneous database synchronization; caching; information and communication technologies for development; low-bandwidth application"}, "Aplikasi toleransi kelewatan untuk lebar jalur rendah dan pengguna yang bersambung sekejap: pengalaman aAQUA Dengan pertumbuhan dan penyebaran Internet yang meletup, akses web dari pengguna mudah alih dan luar bandar menjadi penting. Tetapi pengguna ini menghadapi masalah lebar jalur rendah dan kesambungan Internet sekejap-sekejap. Untuk menjadikan manfaat Internet dapat dicapai oleh orang biasa di negara-negara membangun, aksesibilitas dan ketersediaan maklumat harus ditingkatkan. aAQUA adalah portal pertanian multimedia pelbagai bahasa dalam talian untuk menyebarkan maklumat dari dan ke masyarakat luar bandar. Memandangkan persekitaran luar bandar yang terhad sumber, kami telah merancang dan melaksanakan penyelesaian luar talian yang memberikan pengalaman dalam talian kepada pengguna dalam mod terputus. Penyelesaian kami didasarkan pada penyegerakan pangkalan data yang heterogen yang hanya melibatkan muatan penyegerakan kecil yang memastikan penggunaan lebar jalur yang cekap. Offline aAQUA telah dikerahkan di lapangan dan kajian sistematik penyelesaian kami menunjukkan bahawa pengalaman pengguna telah bertambah baik bukan hanya dalam mod terputus tetapi juga dalam mod bersambung. [[EENNDD]] akses luar talian; sumber terhad komputer rendah; penyegerakan pangkalan data yang heterogen; caching; teknologi maklumat dan komunikasi untuk pembangunan; aplikasi lebar jalur rendah"], [{"string": "Information credibility on twitter We analyze the information credibility of news propagated through Twitter, a popular microblogging service. Previous research has shown that most of the messages posted on Twitter are truthful, but the service is also used to spread misinformation and false rumors, often unintentionally.", "keywords": ["twitter", "social media analytics", "social media credibility"], "combined": "Information credibility on twitter We analyze the information credibility of news propagated through Twitter, a popular microblogging service. Previous research has shown that most of the messages posted on Twitter are truthful, but the service is also used to spread misinformation and false rumors, often unintentionally. [[EENNDD]] twitter; social media analytics; social media credibility"}, "Kredibiliti maklumat di twitter Kami menganalisis kredibiliti maklumat berita yang disebarkan melalui Twitter, perkhidmatan microblogging yang popular. Penyelidikan sebelumnya menunjukkan bahawa kebanyakan mesej yang disiarkan di Twitter adalah benar, tetapi perkhidmatan ini juga digunakan untuk menyebarkan maklumat yang salah dan khabar angin palsu, sering kali tidak disengajakan. [[EENNDD]] twitter; analisis media sosial; kredibiliti media sosial"], [{"string": "Web engineering with the visual software circuit board No contact information provided yet.", "keywords": ["rapid application development", "web engineering", "visual programming", "computer-aided software engineering", "component based development", "web application development", "circuit based software development", "patterns"], "combined": "Web engineering with the visual software circuit board No contact information provided yet. [[EENNDD]] rapid application development; web engineering; visual programming; computer-aided software engineering; component based development; web application development; circuit based software development; patterns"}, "Kejuruteraan web dengan papan litar perisian visual Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengembangan aplikasi yang pesat; kejuruteraan web; pengaturcaraan visual; kejuruteraan perisian berbantukan komputer; pembangunan berasaskan komponen; pembangunan aplikasi web; pembangunan perisian berasaskan litar; corak"], [{"string": "Limiting the spread of misinformation in social networks In this work, we study the notion of competing campaigns in a social network and address the problem of influence limitation where a \"bad\" campaign starts propagating from a certain node in the network and use the notion of limiting campaigns to counteract the effect of misinformation. The problem can be summarized as identifying a subset of individuals that need to be convinced to adopt the competing (or \"good\") campaign so as to minimize the number of people that adopt the \"bad\" campaign at the end of both propagation processes. We show that this optimization problem is NP-hard and provide approximation guarantees for a greedy solution for various definitions of this problem by proving that they are submodular. We experimentally compare the performance of the greedy method to various heuristics. The experiments reveal that in most cases inexpensive heuristics such as degree centrality compare well with the greedy approach. We also study the influence limitation problem in the presence of missing data where the current states of nodes in the network are only known with a certain probability and show that prediction in this setting is a supermodular problem. We propose a prediction algorithm that is based on generating random spanning trees and evaluate the performance of this approach. The experiments reveal that using the prediction algorithm, we are able to tolerate about 90% missing data before the performance of the algorithm starts degrading and even with large amounts of missing data the performance degrades only to 75% of the performance that would be achieved with complete data.", "keywords": ["nonnumerical algorithms and problems", "information cascades", "social networks", "misinformation", "competing campaigns", "submodular functions", "supermodular functions"], "combined": "Limiting the spread of misinformation in social networks In this work, we study the notion of competing campaigns in a social network and address the problem of influence limitation where a \"bad\" campaign starts propagating from a certain node in the network and use the notion of limiting campaigns to counteract the effect of misinformation. The problem can be summarized as identifying a subset of individuals that need to be convinced to adopt the competing (or \"good\") campaign so as to minimize the number of people that adopt the \"bad\" campaign at the end of both propagation processes. We show that this optimization problem is NP-hard and provide approximation guarantees for a greedy solution for various definitions of this problem by proving that they are submodular. We experimentally compare the performance of the greedy method to various heuristics. The experiments reveal that in most cases inexpensive heuristics such as degree centrality compare well with the greedy approach. We also study the influence limitation problem in the presence of missing data where the current states of nodes in the network are only known with a certain probability and show that prediction in this setting is a supermodular problem. We propose a prediction algorithm that is based on generating random spanning trees and evaluate the performance of this approach. The experiments reveal that using the prediction algorithm, we are able to tolerate about 90% missing data before the performance of the algorithm starts degrading and even with large amounts of missing data the performance degrades only to 75% of the performance that would be achieved with complete data. [[EENNDD]] nonnumerical algorithms and problems; information cascades; social networks; misinformation; competing campaigns; submodular functions; supermodular functions"}, "Mengehadkan penyebaran maklumat yang salah dalam rangkaian sosial Dalam karya ini, kami mempelajari gagasan mengenai kempen yang bersaing dalam rangkaian sosial dan mengatasi masalah batasan pengaruh di mana kempen \"buruk\" mula menyebarkan dari simpul tertentu dalam rangkaian dan menggunakan gagasan mengehadkan kempen untuk mengatasi kesan salah maklumat. Masalahnya dapat diringkaskan sebagai mengenal pasti subset individu yang perlu diyakinkan untuk mengadopsi kempen yang bersaing (atau \"baik\") sehingga dapat meminimumkan jumlah orang yang mengadopsi kempen \"buruk\" pada akhir kedua proses penyebaran. Kami menunjukkan bahawa masalah pengoptimuman ini sukar dilakukan oleh NP dan memberikan jaminan penghampiran untuk penyelesaian tamak untuk pelbagai definisi masalah ini dengan membuktikan bahawa mereka adalah submodular. Kami secara eksperimen membandingkan prestasi kaedah tamak dengan pelbagai heuristik. Eksperimen menunjukkan bahawa dalam kebanyakan kes heuristik yang murah seperti sentraliti darjah dibandingkan dengan pendekatan tamak. Kami juga mengkaji masalah pembatasan pengaruh dengan adanya data yang hilang di mana keadaan node semasa dalam rangkaian hanya diketahui dengan kebarangkalian tertentu dan menunjukkan bahawa ramalan dalam tetapan ini adalah masalah supermodular. Kami mencadangkan algoritma ramalan yang berdasarkan pada menghasilkan pokok rentang rawak dan menilai prestasi pendekatan ini. Eksperimen menunjukkan bahawa dengan menggunakan algoritma ramalan, kita dapat mentolerir kira-kira 90% data yang hilang sebelum prestasi algoritma mulai merosot dan walaupun dengan sejumlah besar data yang hilang, prestasi hanya merosot kepada 75% prestasi yang akan dicapai dengan data lengkap. [[EENNDD]] algoritma dan masalah bukan angka; lata maklumat; rangkaian sosial; maklumat yang salah; kempen bersaing; fungsi submodular; fungsi supermodular"], [{"string": "Parallel crawling for online social networks Given a huge online social network, how do we retrieve information from it through crawling? Even better, how do we improve the crawling performance by using parallel crawlers that work independently? In this paper, we present the framework of parallel crawlers for online social networks, utilizing a centralized queue. To show how this works in practice, we describe our implementation of the crawlers for an online auction website. The crawlers work independently, therefore the failing of one crawler does not affect the others at all. The framework ensures that no redundant crawling would occur. Using the crawlers that we built, we visited a total of approximately 11 million auction users, about 66,000 of which were completely crawled.", "keywords": ["web crawler", "parallelization", "web spider", "software architectures", "online social networks"], "combined": "Parallel crawling for online social networks Given a huge online social network, how do we retrieve information from it through crawling? Even better, how do we improve the crawling performance by using parallel crawlers that work independently? In this paper, we present the framework of parallel crawlers for online social networks, utilizing a centralized queue. To show how this works in practice, we describe our implementation of the crawlers for an online auction website. The crawlers work independently, therefore the failing of one crawler does not affect the others at all. The framework ensures that no redundant crawling would occur. Using the crawlers that we built, we visited a total of approximately 11 million auction users, about 66,000 of which were completely crawled. [[EENNDD]] web crawler; parallelization; web spider; software architectures; online social networks"}, "Perayapan selari untuk rangkaian sosial dalam talian Memandangkan rangkaian sosial dalam talian yang besar, bagaimana kita mendapatkan maklumat daripadanya melalui perayapan? Lebih baik lagi, bagaimana kita meningkatkan prestasi perayapan dengan menggunakan perayap selari yang berfungsi secara bebas? Dalam makalah ini, kami memaparkan kerangka perayap selari untuk rangkaian sosial dalam talian, menggunakan barisan terpusat. Untuk menunjukkan bagaimana ini berfungsi dalam praktik, kami menerangkan pelaksanaan perayap kami untuk laman web lelongan dalam talian. Crawler berfungsi secara bebas, oleh itu kegagalan satu crawler sama sekali tidak mempengaruhi yang lain. Kerangka kerja memastikan bahawa tidak ada perayapan berlebihan yang akan berlaku. Dengan menggunakan perayap yang kami buat, kami telah mengunjungi sekitar 11 juta pengguna lelong, kira-kira 66,000 di antaranya dirayapi sepenuhnya. [[EENNDD]] perayap web; selari; labah-labah jaring; seni bina perisian; rangkaian sosial dalam talian"], [{"string": "First-order focused crawling This paper reports a new general framework of focused web crawling based on \"relational subgroup discovery\". Predicates are used explicitly to represent the relevance clues of those unvisited pages in the crawl frontier, and then first-order classification rules are induced using subgroup discovery technique. The learned relational rules with sufficient support and confidence will guide the crawling process afterwards. We present the many interesting features of our proposed first-order focused crawler, together with preliminary promising experimental results.", "keywords": ["relational subgroup discovery", "focused crawling"], "combined": "First-order focused crawling This paper reports a new general framework of focused web crawling based on \"relational subgroup discovery\". Predicates are used explicitly to represent the relevance clues of those unvisited pages in the crawl frontier, and then first-order classification rules are induced using subgroup discovery technique. The learned relational rules with sufficient support and confidence will guide the crawling process afterwards. We present the many interesting features of our proposed first-order focused crawler, together with preliminary promising experimental results. [[EENNDD]] relational subgroup discovery; focused crawling"}, "Perayapan terfokus pesanan pertama Makalah ini melaporkan kerangka umum perayapan web terfokus baru berdasarkan \"penemuan subkumpulan relasional\". Predikat digunakan secara eksplisit untuk mewakili petunjuk relevan dari halaman yang tidak dilawati di perbatasan perayapan, dan kemudian peraturan klasifikasi orde pertama diinduksi menggunakan teknik penemuan subkumpulan. Peraturan hubungan yang dipelajari dengan sokongan dan keyakinan yang mencukupi akan memandu proses merangkak selepas itu. Kami menyajikan banyak ciri menarik dari crawler fokus pertama kami yang dicadangkan, bersama dengan hasil eksperimen yang menjanjikan awal. [[EENNDD]] penemuan subkumpulan hubungan; fokus merangkak"], [{"string": "Investigating behavioral variability in web search Understanding the extent to which people's search behaviors differ in terms of the interaction flow and information targeted is important in designing interfaces to help World Wide Web users search more effectively. In this paper we describe a longitudinal log-based study that investigated variability in people.s interaction behavior when engaged in search-related activities on the Web.allWe analyze the search interactions of more than two thousand volunteer users over a five-month period, with the aim of characterizing differences in their interaction styles.allThe findings of our study suggest that there are dramatic differences in variability in key aspects of the interaction within and between users, and within and between the search queries they submit.allOur findings also suggest two classes of extreme user. navigators and explorers. whose search interaction is highly consistent or highly variable. Lessons learned from these users can inform the design of tools to support effective Web-search interactions for everyone.", "keywords": ["web search", "behavioral variability"], "combined": "Investigating behavioral variability in web search Understanding the extent to which people's search behaviors differ in terms of the interaction flow and information targeted is important in designing interfaces to help World Wide Web users search more effectively. In this paper we describe a longitudinal log-based study that investigated variability in people.s interaction behavior when engaged in search-related activities on the Web.allWe analyze the search interactions of more than two thousand volunteer users over a five-month period, with the aim of characterizing differences in their interaction styles.allThe findings of our study suggest that there are dramatic differences in variability in key aspects of the interaction within and between users, and within and between the search queries they submit.allOur findings also suggest two classes of extreme user. navigators and explorers. whose search interaction is highly consistent or highly variable. Lessons learned from these users can inform the design of tools to support effective Web-search interactions for everyone. [[EENNDD]] web search; behavioral variability"}, "Menyiasat perubahan tingkah laku dalam carian web Memahami sejauh mana tingkah laku carian orang berbeza dari segi aliran interaksi dan maklumat yang disasarkan adalah penting dalam merancang antara muka untuk membantu pengguna World Wide Web mencari dengan lebih berkesan. Dalam makalah ini kami menerangkan kajian berdasarkan log membujur yang menyelidiki kebolehubahan tingkah laku interaksi orang ketika terlibat dalam aktiviti yang berkaitan dengan carian di Web. Semua Kami menganalisis interaksi carian lebih dari dua ribu pengguna sukarelawan dalam tempoh lima bulan, dengan tujuan untuk mencirikan perbezaan dalam gaya interaksi mereka. semua Penemuan kajian kami menunjukkan bahawa terdapat perbezaan dramatik dalam kebolehubahan dalam aspek utama interaksi dalam dan antara pengguna, dan dalam dan antara pertanyaan carian yang mereka kirimkan. semua penemuan kami juga mencadangkan dua kelas pengguna yang melampau. pelayar dan penjelajah. interaksi cariannya sangat konsisten atau sangat berubah-ubah. Pelajaran yang dipelajari dari pengguna ini dapat memberitahu reka bentuk alat untuk menyokong interaksi carian Web yang berkesan untuk semua orang. [[EENNDD]] carian web; kebolehubahan tingkah laku"], [{"string": "Rare item detection in e-commerce site As the largest online marketplace in the world, eBay has a huge inventory where there are plenty of great rare items with potentially large, even rapturous buyers. These items are obscured in long tail of eBay item listing and hard to find through existing searching or browsing methods. It is observed that there are great rarity demands from users according to eBay query log. To keep up with the demands, the paper proposes a method to automatically detect rare items in eBay online listing. A large set of features relevant to the task are investigated to filter items and further measure item rareness. The experiments on the most rarity-demand-intensitive domains show that the method may effectively detect rare items (&gt;90% precision).", "keywords": ["rare item detection", "information search and retrieval", "learning", "long tail theory", "rareness measure"], "combined": "Rare item detection in e-commerce site As the largest online marketplace in the world, eBay has a huge inventory where there are plenty of great rare items with potentially large, even rapturous buyers. These items are obscured in long tail of eBay item listing and hard to find through existing searching or browsing methods. It is observed that there are great rarity demands from users according to eBay query log. To keep up with the demands, the paper proposes a method to automatically detect rare items in eBay online listing. A large set of features relevant to the task are investigated to filter items and further measure item rareness. The experiments on the most rarity-demand-intensitive domains show that the method may effectively detect rare items (&gt;90% precision). [[EENNDD]] rare item detection; information search and retrieval; learning; long tail theory; rareness measure"}, ""], [{"string": "Automatically generating metadata for digital photographs with geographic coordinates No contact information provided yet.", "keywords": ["multimedia information systems"], "combined": "Automatically generating metadata for digital photographs with geographic coordinates No contact information provided yet. [[EENNDD]] multimedia information systems"}, "Menjana metadata secara automatik untuk gambar digital dengan koordinat geografi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] sistem maklumat multimedia"], [{"string": "Web resource geographic location classification and detection No contact information provided yet.", "keywords": ["serving location", "provider location", "location-based web application", "content location", "web location", "geographic location"], "combined": "Web resource geographic location classification and detection No contact information provided yet. [[EENNDD]] serving location; provider location; location-based web application; content location; web location; geographic location"}, "Pengelasan dan pengesanan lokasi geografi sumber web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] lokasi perkhidmatan; lokasi pembekal; aplikasi web berasaskan lokasi; lokasi kandungan; lokasi web; lokasi geografi"], [{"string": "Preserving XML queries during schema evolution In XML databases, new schema versions may be released as frequently as once every two weeks. This poster describes a taxonomy of changes for XML schema evolution. It examines the impact of those changes on schema validation and query evaluation. Based on that study, it proposes guidelines for XML schema evolution and for writing queries in such a way that they continue to operate as expected across evolving schemas.", "keywords": ["xml schema evolution", "xml query"], "combined": "Preserving XML queries during schema evolution In XML databases, new schema versions may be released as frequently as once every two weeks. This poster describes a taxonomy of changes for XML schema evolution. It examines the impact of those changes on schema validation and query evaluation. Based on that study, it proposes guidelines for XML schema evolution and for writing queries in such a way that they continue to operate as expected across evolving schemas. [[EENNDD]] xml schema evolution; xml query"}, "Memelihara pertanyaan XML semasa evolusi skema Dalam pangkalan data XML, versi skema baru dapat dilepaskan sekerap dua minggu sekali. Poster ini menerangkan taksonomi perubahan untuk evolusi skema XML. Ini mengkaji kesan perubahan tersebut pada pengesahan skema dan penilaian pertanyaan. Berdasarkan kajian itu, ia mencadangkan panduan untuk evolusi skema XML dan untuk menulis pertanyaan sedemikian rupa sehingga mereka terus beroperasi seperti yang diharapkan di seluruh skema yang sedang berkembang. [[EENNDD]] evolusi skema xml; pertanyaan xml"], [{"string": "Flash crowds and denial of service attacks: characterization and implications for CDNs and web sites No contact information provided yet.", "keywords": ["content distribution network performance", "electronic data interchange", "flash crowd", "web workload characterization", "denial of service attack", "world wide web"], "combined": "Flash crowds and denial of service attacks: characterization and implications for CDNs and web sites No contact information provided yet. [[EENNDD]] content distribution network performance; electronic data interchange; flash crowd; web workload characterization; denial of service attack; world wide web"}, "Kilat ramai dan penolakan serangan perkhidmatan: pencirian dan implikasi terhadap CDN dan laman web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] prestasi rangkaian pengedaran kandungan; pertukaran data elektronik; orang ramai kilat; pencirian beban kerja web; serangan penolakan perkhidmatan; web seluruh dunia"], [{"string": "Fine-grained, structured configuration management for web projects No contact information provided yet.", "keywords": ["version control", "web engineering"], "combined": "Fine-grained, structured configuration management for web projects No contact information provided yet. [[EENNDD]] version control; web engineering"}, "Pengurusan konfigurasi berstruktur halus untuk projek web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kawalan versi; kejuruteraan web"], [{"string": "Search shortcuts: driving users towards their goals Giving suggestions to users of Web-based services is a common practice aimed at enhancing their navigation experience. Major Web Search Engines usually provide \"Suggestions\" under the form of queries that are, to some extent, related to the current query typed by the user, and the knowledge learned from the past usage of the system. In this work we introduce \"Search Shortcuts\" as \"Successful\" queries allowed, in the past, users to satisfy their information needs. Differently from conventional suggestion techniques, our search shortcuts allows to evaluate effectiveness by exploiting a simple train-and-test approach. We have applied several Collaborative Filtering algorithms to this problem, evaluating them on a real query log data. We generate the shortcuts from all user sessions belonging to the testing set, and measure the quality of the shortcuts suggested by considering the similarity between them and the navigational user behavior.", "keywords": ["model", "search shortcut", "evaluation"], "combined": "Search shortcuts: driving users towards their goals Giving suggestions to users of Web-based services is a common practice aimed at enhancing their navigation experience. Major Web Search Engines usually provide \"Suggestions\" under the form of queries that are, to some extent, related to the current query typed by the user, and the knowledge learned from the past usage of the system. In this work we introduce \"Search Shortcuts\" as \"Successful\" queries allowed, in the past, users to satisfy their information needs. Differently from conventional suggestion techniques, our search shortcuts allows to evaluate effectiveness by exploiting a simple train-and-test approach. We have applied several Collaborative Filtering algorithms to this problem, evaluating them on a real query log data. We generate the shortcuts from all user sessions belonging to the testing set, and measure the quality of the shortcuts suggested by considering the similarity between them and the navigational user behavior. [[EENNDD]] model; search shortcut; evaluation"}, "Jalan pintas carian: mendorong pengguna ke arah tujuannya Memberi cadangan kepada pengguna perkhidmatan berasaskan Web adalah amalan biasa yang bertujuan untuk meningkatkan pengalaman navigasi mereka. Enjin Pencari Web Utama biasanya memberikan \"Cadangan\" dalam bentuk pertanyaan yang, sampai tahap tertentu, berkaitan dengan pertanyaan semasa yang diketik oleh pengguna, dan pengetahuan yang dipelajari dari penggunaan sistem sebelumnya. Dalam karya ini kami memperkenalkan \"Pintasan Carian\" sebagai pertanyaan \"Berjaya\" yang diizinkan, pada masa lalu, pengguna untuk memenuhi keperluan maklumat mereka. Berbeza dengan teknik cadangan konvensional, jalan pintas carian kami memungkinkan untuk menilai keberkesanan dengan memanfaatkan pendekatan latihan dan ujian yang mudah. Kami telah menggunakan beberapa algoritma Penapisan Kolaboratif untuk masalah ini, menilai mereka pada data log pertanyaan sebenar. Kami menghasilkan jalan pintas dari semua sesi pengguna yang termasuk dalam set ujian, dan mengukur kualiti jalan pintas yang disarankan dengan mempertimbangkan persamaan antara mereka dan tingkah laku pengguna navigasi. [[EENNDD]] model; jalan pintas carian; penilaian"], [{"string": "Extensible schema documentation with XSLT 2.0 XML Schema documents are defined using an XML syntax, which means that the idea of generating schema documentation through standard XML technologies is intriguing. We present X2Doc, a framework for generating schema-documentation solely through XSLT. The framework uses SCX, an XML syntax for XML Schema components, as intermediate format and produces XML-based output formats. Using a modular set of XSLT stylesheets, X2Doc is highly configurable and carefully crafted towards extensibility. This proves especially useful for composite schemas, where additional schema information like Schematron rules are embedded into XML Schemas.", "keywords": ["xml schema", "scx", "x2doc"], "combined": "Extensible schema documentation with XSLT 2.0 XML Schema documents are defined using an XML syntax, which means that the idea of generating schema documentation through standard XML technologies is intriguing. We present X2Doc, a framework for generating schema-documentation solely through XSLT. The framework uses SCX, an XML syntax for XML Schema components, as intermediate format and produces XML-based output formats. Using a modular set of XSLT stylesheets, X2Doc is highly configurable and carefully crafted towards extensibility. This proves especially useful for composite schemas, where additional schema information like Schematron rules are embedded into XML Schemas. [[EENNDD]] xml schema; scx; x2doc"}, "Dokumentasi skema yang dapat diperluas dengan XSLT 2.0 XML Schema didefinisikan menggunakan sintaks XML, yang bermaksud bahawa idea untuk menghasilkan dokumentasi skema melalui teknologi XML standard sangat menarik. Kami membentangkan X2Doc, kerangka kerja untuk menghasilkan dokumentasi skema hanya melalui XSLT. Rangka kerja menggunakan SCX, sintaks XML untuk komponen Skema XML, sebagai format perantaraan dan menghasilkan format output berasaskan XML. Menggunakan satu set lembaran gaya XSLT modular, X2Doc sangat dikonfigurasi dan dibuat dengan teliti ke arah kepanjangan. Ini terbukti sangat berguna untuk skema komposit, di mana maklumat skema tambahan seperti peraturan Schematron dimasukkan ke dalam Skema XML. [[EENNDD]] skema xml; scx; x2doc"], [{"string": "Live web search experiments for the rest of us There are significant barriers to academic research into user Web search preferences. Academic researchers are unable to manipulate the results shown by a major search engine to users and would have no access to the interaction data collected by the engine. Our initial approach to overcoming this was to ask participants to submit queries to an experimental search engine rather than their usual search tool. Over several different experiments we found that initial user buy-in was high but that people quickly drifted back to their old habits and stopped contributing data. Here, we report our investigation of possible reasons why this occurs. An alternative approach is exemplified by the Lemur browser toolbar, which allows local collection of user interaction data from search engine sessions, but does not allow result pages to be modified. We will demonstrate a new Firefox toolbar that we have developed to support experiments in which search results may be arbitrarily manipulated. Using our toolbar, academics can set up the experiments they want to conduct, while collecting (subject to human experimentation guidelines) queries, clicks and dwell times as well as optional explicit judgments.", "keywords": ["web search", "implicit measures", "browser extensions", "performance evaluation"], "combined": "Live web search experiments for the rest of us There are significant barriers to academic research into user Web search preferences. Academic researchers are unable to manipulate the results shown by a major search engine to users and would have no access to the interaction data collected by the engine. Our initial approach to overcoming this was to ask participants to submit queries to an experimental search engine rather than their usual search tool. Over several different experiments we found that initial user buy-in was high but that people quickly drifted back to their old habits and stopped contributing data. Here, we report our investigation of possible reasons why this occurs. An alternative approach is exemplified by the Lemur browser toolbar, which allows local collection of user interaction data from search engine sessions, but does not allow result pages to be modified. We will demonstrate a new Firefox toolbar that we have developed to support experiments in which search results may be arbitrarily manipulated. Using our toolbar, academics can set up the experiments they want to conduct, while collecting (subject to human experimentation guidelines) queries, clicks and dwell times as well as optional explicit judgments. [[EENNDD]] web search; implicit measures; browser extensions; performance evaluation"}, "Eksperimen carian web langsung untuk kita semua Terdapat halangan yang signifikan untuk penyelidikan akademik terhadap pilihan carian Web pengguna. Penyelidik akademik tidak dapat memanipulasi hasil yang ditunjukkan oleh enjin carian utama kepada pengguna dan tidak akan mempunyai akses ke data interaksi yang dikumpulkan oleh mesin. Pendekatan awal kami untuk mengatasi masalah ini adalah dengan meminta peserta menyerahkan pertanyaan ke mesin carian eksperimental dan bukannya alat carian biasa mereka. Melalui beberapa eksperimen yang berbeza, kami mendapati bahawa pembelian pengguna awal adalah tinggi tetapi orang dengan cepat beralih ke tabiat lama mereka dan berhenti menyumbang data. Di sini, kami melaporkan siasatan kami mengenai kemungkinan sebab mengapa ini berlaku. Pendekatan alternatif dicontohkan oleh bar alat penyemak imbas Lemur, yang memungkinkan pengumpulan data interaksi pengguna tempatan dari sesi mesin pencari, tetapi tidak membenarkan halaman hasil diubah. Kami akan menunjukkan bar alat Firefox baru yang kami kembangkan untuk menyokong eksperimen di mana hasil carian boleh dimanipulasi sewenang-wenangnya. Dengan menggunakan bar alat kami, para akademik dapat mengatur eksperimen yang ingin mereka jalankan, sambil mengumpulkan (tertakluk kepada garis panduan eksperimen manusia) pertanyaan, klik dan masa tinggal serta penilaian eksplisit pilihan. [[EENNDD]] carian web; langkah-langkah tersirat; pelanjutan penyemak imbas; penilaian prestasi"], [{"string": "P2Cast: peer-to-peer patching scheme for VoD service No contact information provided yet.", "keywords": ["performance evaluation", "patching", "video on-demand service", "peer-to-peer networks"], "combined": "P2Cast: peer-to-peer patching scheme for VoD service No contact information provided yet. [[EENNDD]] performance evaluation; patching; video on-demand service; peer-to-peer networks"}, "P2Cast: skema tampalan peer-to-peer untuk perkhidmatan VoD Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penilaian prestasi; menampal; perkhidmatan video atas permintaan; rangkaian peer-to-peer"], [{"string": "Matching web site structure and content No contact information provided yet.", "keywords": ["web structure", "applications", "information search and retrieval", "semantic description", "web content mining"], "combined": "Matching web site structure and content No contact information provided yet. [[EENNDD]] web structure; applications; information search and retrieval; semantic description; web content mining"}, "Struktur dan kandungan laman web yang sepadan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] struktur web; permohonan; pencarian dan pengambilan maklumat; penerangan semantik; perlombongan kandungan web"], [{"string": "Combining anchor text categorization and graph analysis for paid link detection In order to artificially boost the rank of commercial pages in search engine results, search engine optimizers pay for links to these pages on other websites. Identifying paid links is important for a web search engine to produce highly relevant results. In this paper we introduce a novel method of identifying such links. We start with training a classifier of anchor text topics and analyzing web pages for diversity of their outgoing commercial links. Then we use this information and analyze link graph of the Russian Web to find pages that sell links and sites that buy links and to identify the paid links. Testing on manually marked samples showed high efficiency of the algorithm.", "keywords": ["language model", "search engines", "link analysis", "categorization", "web mining", "machine learning"], "combined": "Combining anchor text categorization and graph analysis for paid link detection In order to artificially boost the rank of commercial pages in search engine results, search engine optimizers pay for links to these pages on other websites. Identifying paid links is important for a web search engine to produce highly relevant results. In this paper we introduce a novel method of identifying such links. We start with training a classifier of anchor text topics and analyzing web pages for diversity of their outgoing commercial links. Then we use this information and analyze link graph of the Russian Web to find pages that sell links and sites that buy links and to identify the paid links. Testing on manually marked samples showed high efficiency of the algorithm. [[EENNDD]] language model; search engines; link analysis; categorization; web mining; machine learning"}, "Menggabungkan pengkategorian teks utama dan analisis grafik untuk pengesanan pautan berbayar Untuk meningkatkan peringkat halaman komersial secara buatan dalam hasil mesin pencari, pengoptimum mesin pencari membayar pautan ke halaman ini di laman web lain. Mengenal pasti pautan berbayar adalah penting bagi mesin carian web untuk menghasilkan hasil yang sangat relevan. Dalam makalah ini kami memperkenalkan kaedah baru untuk mengenal pasti pautan tersebut. Kami bermula dengan melatih pengkelasan topik teks utama dan menganalisis laman web untuk kepelbagaian pautan komersial keluar mereka. Kemudian kami menggunakan maklumat ini dan menganalisis grafik pautan Web Rusia untuk mencari halaman yang menjual pautan dan laman web yang membeli pautan dan untuk mengenal pasti pautan berbayar. Ujian pada sampel yang ditandakan secara manual menunjukkan kecekapan tinggi algoritma. [[EENNDD]] model bahasa; enjin carian; analisis pautan; pengkategorian; perlombongan web; pembelajaran mesin"], [{"string": "Automatically generating labels based on unified click model Ground truth labels are one of the most important parts in many test collections for information retrieval. Each label, depicting the relevance between a query-document pair, is usually judged by a human, and this process is time-consuming and labor-intensive. Automatically Generating labels from click-through data has attracted increasing attention. In this paper, we propose a Unified Click Model to predict the multi-level labels, which aims at comprehensively considering the advantages of the Position Models and Cascade Models. Experiments show that the proposed click model outperforms the existing click models in predicting the multi-level labels, and could replace the labels judged by humans for test collections.", "keywords": ["learning to rank", "click model", "ranking svm"], "combined": "Automatically generating labels based on unified click model Ground truth labels are one of the most important parts in many test collections for information retrieval. Each label, depicting the relevance between a query-document pair, is usually judged by a human, and this process is time-consuming and labor-intensive. Automatically Generating labels from click-through data has attracted increasing attention. In this paper, we propose a Unified Click Model to predict the multi-level labels, which aims at comprehensively considering the advantages of the Position Models and Cascade Models. Experiments show that the proposed click model outperforms the existing click models in predicting the multi-level labels, and could replace the labels judged by humans for test collections. [[EENNDD]] learning to rank; click model; ranking svm"}, "Menjana label secara automatik berdasarkan model klik bersatu Label kebenaran tanah adalah salah satu bahagian yang paling penting dalam banyak koleksi ujian untuk mendapatkan maklumat. Setiap label, yang menggambarkan kesesuaian antara pasangan pertanyaan-dokumen, biasanya dinilai oleh manusia, dan proses ini memakan waktu dan memerlukan tenaga kerja. Menjana label secara automatik dari data klik-tayang telah menarik perhatian yang semakin meningkat. Dalam makalah ini, kami mencadangkan Model Klik Bersatu untuk meramalkan label pelbagai peringkat, yang bertujuan mempertimbangkan secara komprehensif kelebihan Model Posisi dan Model Cascade. Eksperimen menunjukkan bahawa model klik yang dicadangkan mengungguli model klik yang ada dalam meramalkan label pelbagai peringkat, dan dapat menggantikan label yang dinilai oleh manusia untuk koleksi ujian. [[EENNDD]] belajar berpangkat; model klik; kedudukan svm"], [{"string": "Enhancing the SCORM metadata model No contact information provided yet.", "keywords": ["metadata", "scorm", "standards", "modelling", "e-learning"], "combined": "Enhancing the SCORM metadata model No contact information provided yet. [[EENNDD]] metadata; scorm; standards; modelling; e-learning"}, "Meningkatkan model metadata SCORM Belum ada maklumat hubungan yang diberikan. [[EENNDD]] metadata; ribut; standard; pemodelan; e-pembelajaran"], [{"string": "Understanding task-driven information flow in collaborative networks Collaborative networks are a special type of social network formed by members who collectively achieve specific goals, such as fixing software bugs and resolving customers' problems. In such networks, information flow among members is driven by the tasks assigned to the network, and by the expertise of its members to complete those tasks. In this work, we analyze real-life collaborative networks to understand their common characteristics and how information is routed in these networks. Our study shows that collaborative networks exhibit significantly different properties compared with other complex networks. Collaborative networks have truncated power-law node degree distributions and other organizational constraints. Furthermore, the number of steps along which information is routed follows a truncated power-law distribution. Based on these observations, we developed a network model that can generate synthetic collaborative networks subject to certain structure constraints. Moreover, we developed a routing model that emulates task-driven information routing conducted by human beings in a collaborative network. Together, these two models can be used to study the efficiency of information routing for different types of collaborative networks -- a problem that is important in practice yet difficult to solve without the method proposed in this paper.", "keywords": ["social routing", "information flow", "collaborative network"], "combined": "Understanding task-driven information flow in collaborative networks Collaborative networks are a special type of social network formed by members who collectively achieve specific goals, such as fixing software bugs and resolving customers' problems. In such networks, information flow among members is driven by the tasks assigned to the network, and by the expertise of its members to complete those tasks. In this work, we analyze real-life collaborative networks to understand their common characteristics and how information is routed in these networks. Our study shows that collaborative networks exhibit significantly different properties compared with other complex networks. Collaborative networks have truncated power-law node degree distributions and other organizational constraints. Furthermore, the number of steps along which information is routed follows a truncated power-law distribution. Based on these observations, we developed a network model that can generate synthetic collaborative networks subject to certain structure constraints. Moreover, we developed a routing model that emulates task-driven information routing conducted by human beings in a collaborative network. Together, these two models can be used to study the efficiency of information routing for different types of collaborative networks -- a problem that is important in practice yet difficult to solve without the method proposed in this paper. [[EENNDD]] social routing; information flow; collaborative network"}, "Memahami aliran maklumat berdasarkan tugas dalam rangkaian kolaboratif Jaringan kolaboratif adalah jenis rangkaian sosial khas yang dibentuk oleh ahli yang secara kolektif mencapai tujuan tertentu, seperti memperbaiki bug perisian dan menyelesaikan masalah pelanggan. Dalam rangkaian seperti itu, aliran maklumat di antara anggota didorong oleh tugas-tugas yang diberikan kepada rangkaian, dan oleh kepakaran anggotanya untuk menyelesaikan tugas-tugas tersebut. Dalam karya ini, kami menganalisis rangkaian kolaboratif kehidupan nyata untuk memahami ciri umum mereka dan bagaimana maklumat disalurkan dalam rangkaian ini. Kajian kami menunjukkan bahawa rangkaian kolaboratif menunjukkan sifat yang berbeza secara signifikan berbanding dengan rangkaian kompleks lain. Rangkaian kolaboratif telah memotong pengedaran darjah simpul kuasa-undang-undang dan kekangan organisasi lain. Selanjutnya, jumlah langkah di mana maklumat disalurkan mengikuti pembahagian undang-undang kuasa yang dipotong. Berdasarkan pemerhatian ini, kami mengembangkan model rangkaian yang dapat menghasilkan rangkaian kolaboratif sintetik tertakluk kepada kekangan struktur tertentu. Lebih-lebih lagi, kami mengembangkan model penghalaan yang meniru peralihan maklumat berdasarkan tugas yang dilakukan oleh manusia dalam rangkaian kolaboratif. Bersama-sama, kedua model ini dapat digunakan untuk mengkaji kecekapan penghalaan maklumat untuk pelbagai jenis rangkaian kolaborasi - masalah yang penting dalam praktiknya tetapi sukar untuk diselesaikan tanpa kaedah yang dicadangkan dalam makalah ini. [[EENNDD]] penghalaan sosial; aliran maklumat; rangkaian kerjasama"], [{"string": "On the lack of typical behavior in the global Web traffic network No contact information provided yet.", "keywords": ["power laws", "degree", "applications", "web usage", "strength", "performance evaluation", "network flows", "scale-free networks", "traffic statistics"], "combined": "On the lack of typical behavior in the global Web traffic network No contact information provided yet. [[EENNDD]] power laws; degree; applications; web usage; strength; performance evaluation; network flows; scale-free networks; traffic statistics"}, "Mengenai kekurangan tingkah laku khas dalam rangkaian lalu lintas Web global Belum ada maklumat hubungan yang diberikan. [[EENNDD]] undang-undang kuasa; ijazah; permohonan; penggunaan laman web; kekuatan; penilaian prestasi; aliran rangkaian; rangkaian bebas skala; statistik lalu lintas"], [{"string": "A diagrammatic inference system for the web No contact information provided yet.", "keywords": ["inference", "searchable diagrams", "xml", "inference system", "search"], "combined": "A diagrammatic inference system for the web No contact information provided yet. [[EENNDD]] inference; searchable diagrams; xml; inference system; search"}, "Sistem inferens diagram untuk web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] inferens; gambar rajah yang boleh dicari; xml; sistem inferens; cari"], [{"string": "Network arts: exposing cultural reality No contact information provided yet.", "keywords": ["information retrieval", "software agents", "culture", "network arts", "world wide web", "media arts"], "combined": "Network arts: exposing cultural reality No contact information provided yet. [[EENNDD]] information retrieval; software agents; culture; network arts; world wide web; media arts"}, "Seni rangkaian: mendedahkan realiti budaya Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengambilan maklumat; ejen perisian; budaya; seni rangkaian; laman web seluruh dunia; seni media"], [{"string": "A uniform approach to accelerated PageRank computation No contact information provided yet.", "keywords": ["pagerank", "random walks", "web graph", "information search and retrieval", "link analysis"], "combined": "A uniform approach to accelerated PageRank computation No contact information provided yet. [[EENNDD]] pagerank; random walks; web graph; information search and retrieval; link analysis"}, "Pendekatan yang seragam untuk pengiraan PageRank yang dipercepat Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] pagerank; jalan rawak; grafik web; pencarian dan pengambilan maklumat; analisis pautan"], [{"string": "A graph-theoretic approach to webpage segmentation We consider the problem of segmenting a webpage into visually and semantically cohesive pieces. Our approach is based on formulating an appropriate optimization problem on weighted graphs, where the weights capture if two nodes in the DOM tree should be placed together or apart in the segmentation; we present a learning framework to learn these weights from manually labeled data in a principled manner. Our work is a significant departure from previous heuristic and rule-based solutions to the segmentation problem. The results of our empirical analysis bring out interesting aspects of our framework, including variants of the optimization problem and the role of learning.", "keywords": ["webpage sectioning", "energy minimization", "webpage segmentation", "miscellaneous", "correlation clustering", "graph cuts"], "combined": "A graph-theoretic approach to webpage segmentation We consider the problem of segmenting a webpage into visually and semantically cohesive pieces. Our approach is based on formulating an appropriate optimization problem on weighted graphs, where the weights capture if two nodes in the DOM tree should be placed together or apart in the segmentation; we present a learning framework to learn these weights from manually labeled data in a principled manner. Our work is a significant departure from previous heuristic and rule-based solutions to the segmentation problem. The results of our empirical analysis bring out interesting aspects of our framework, including variants of the optimization problem and the role of learning. [[EENNDD]] webpage sectioning; energy minimization; webpage segmentation; miscellaneous; correlation clustering; graph cuts"}, "Pendekatan grafik-teori untuk segmentasi halaman web Kami menganggap masalah pemisahan halaman web menjadi kepingan yang kohesif secara visual dan semantik. Pendekatan kami didasarkan pada merumuskan masalah pengoptimuman yang sesuai pada grafik berwajaran, di mana bobot menangkap jika dua nod di pohon DOM harus diletakkan bersama atau terpisah dalam segmentasi; kami membentangkan kerangka pembelajaran untuk mempelajari bobot ini dari data berlabel secara manual dengan prinsip. Kerja kami adalah penyimpangan yang signifikan dari penyelesaian heuristik dan berdasarkan peraturan sebelumnya untuk masalah segmentasi. Hasil analisis empirikal kami memaparkan aspek kerangka kerja kami yang menarik, termasuk varian masalah pengoptimuman dan peranan pembelajaran. [[EENNDD]] pembahagian halaman web; pengurangan tenaga; segmentasi laman web; pelbagai; pengelompokan korelasi; pemotongan graf"], [{"string": "Knowledge encapsulation for focused search from pervasive devices An abstract is not available.", "keywords": ["pervasive devices", "web", "knowledge agents", "focused search", "disconnected search", "portable devices"], "combined": "Knowledge encapsulation for focused search from pervasive devices An abstract is not available. [[EENNDD]] pervasive devices; web; knowledge agents; focused search; disconnected search; portable devices"}, "Enkapsulasi pengetahuan untuk carian terfokus dari peranti yang meluas Abstrak tidak tersedia. [[EENNDD]] peranti yang meluas; laman web; ejen pengetahuan; carian tertumpu; carian terputus; peranti mudah alih"], [{"string": "A web services architecture for learning object discovery and assembly No contact information provided yet.", "keywords": ["teaching and learning environments", "interface descriptions", "discovery", "web services", "metadata", "architecture", "assembly"], "combined": "A web services architecture for learning object discovery and assembly No contact information provided yet. [[EENNDD]] teaching and learning environments; interface descriptions; discovery; web services; metadata; architecture; assembly"}, "Senibina perkhidmatan web untuk mempelajari penemuan dan pemasangan objek Belum ada maklumat hubungan yang diberikan. [[EENNDD]] persekitaran pengajaran dan pembelajaran; penerangan antara muka; penemuan; perkhidmatan web; metadata; seni bina; perhimpunan"], [{"string": "Is question answering an acquired skill? No contact information provided yet.", "keywords": ["question answering", "machine learning"], "combined": "Is question answering an acquired skill? No contact information provided yet. [[EENNDD]] question answering; machine learning"}, "Adakah soalan menjawab kemahiran yang diperoleh? Belum ada maklumat hubungan yang diberikan. [[EENNDD]] menjawab soalan; pembelajaran mesin"], [{"string": "An agent system for ontology sharing on WWW No contact information provided yet.", "keywords": ["ontology", "web services", "semantic web", "agent technologies"], "combined": "An agent system for ontology sharing on WWW No contact information provided yet. [[EENNDD]] ontology; web services; semantic web; agent technologies"}, "Sistem ejen untuk perkongsian ontologi di WWW Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ontologi; perkhidmatan web; web semantik; teknologi ejen"], [{"string": "Exploiting the web for point-in-time file sharing No contact information provided yet.", "keywords": ["file sharing", "instant messaging", "personal web server"], "combined": "Exploiting the web for point-in-time file sharing No contact information provided yet. [[EENNDD]] file sharing; instant messaging; personal web server"}, "Mengeksploitasi web untuk perkongsian fail point-in-time Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perkongsian fail; mesej segera; pelayan web peribadi"], [{"string": "Adapting databases and WebDAV protocol No contact information provided yet.", "keywords": ["performance evaluation"], "combined": "Adapting databases and WebDAV protocol No contact information provided yet. [[EENNDD]] performance evaluation"}, "Menyesuaikan pangkalan data dan protokol WebDAV Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penilaian prestasi"], [{"string": "Predictive caching and prefetching of query results in search engines No contact information provided yet.", "keywords": ["query processing and optimization", "caching", "information storage and retrieval"], "combined": "Predictive caching and prefetching of query results in search engines No contact information provided yet. [[EENNDD]] query processing and optimization; caching; information storage and retrieval"}, "Predictive caching dan prefetching hasil carian di enjin carian Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] pemprosesan dan pengoptimuman pertanyaan; caching; penyimpanan dan pengambilan maklumat"], [{"string": "Entity relation discovery from web tables and links The World-Wide Web consists not only of a huge number of unstructured texts, but also a vast amount of valuable structured data. Web tables [2] are a typical type of structured information that are pervasive on the web, and Web-scale methods that automatically extract web tables have been studied extensively [1]. Many powerful systems (e.g.OCTOPUS [4], Mesa [3]) use extracted web tables as a fundamental component.", "keywords": ["web table", "entity relation discovery", "link"], "combined": "Entity relation discovery from web tables and links The World-Wide Web consists not only of a huge number of unstructured texts, but also a vast amount of valuable structured data. Web tables [2] are a typical type of structured information that are pervasive on the web, and Web-scale methods that automatically extract web tables have been studied extensively [1]. Many powerful systems (e.g.OCTOPUS [4], Mesa [3]) use extracted web tables as a fundamental component. [[EENNDD]] web table; entity relation discovery; link"}, "Penemuan hubungan entiti dari jadual dan pautan web World-Wide Web bukan sahaja terdiri daripada sebilangan besar teks tidak berstruktur, tetapi juga sejumlah besar data berstruktur yang berharga. Jadual web [2] adalah jenis informasi terstruktur khas yang meluas di web, dan kaedah skala Web yang secara automatik mengekstrak jadual web telah dipelajari secara meluas [1]. Banyak sistem yang kuat (mis .OCTOPUS [4], Mesa [3]) menggunakan jadual web yang diekstrak sebagai komponen asas. [[EENNDD]] jadual web; penemuan hubungan entiti; pautan"], [{"string": "Choreography conformance via synchronizability Choreography analysis has been a crucial problem in service oriented computing. Interactions among services involve message exchanges across organizational boundaries in a distributed computing environment, and in order to build such systems in a reliable manner, it is necessary to develop techniques for analyzing such interactions. Choreography conformance involves verifying that a set of services behave according to a given choreography specification that characterizes their interactions. Unfortunately this is an undecidable problem when services interact with asynchronous communication. In this paper we present techniques that identify if the interaction behavior for a set of services remain the same when asynchronous communication is replaced with synchronous communication. This is called the synchronizability problem and determining the synchronizability of a set of services has been an open problem for several years. We solve this problem in this paper. Our results can be used to identify synchronizable services for which choreography conformance can be checked efficiently. Our results on synchronizability are applicable to any software infrastructure that supports message-based interactions.", "keywords": ["synchronizability", "message-based interactions"], "combined": "Choreography conformance via synchronizability Choreography analysis has been a crucial problem in service oriented computing. Interactions among services involve message exchanges across organizational boundaries in a distributed computing environment, and in order to build such systems in a reliable manner, it is necessary to develop techniques for analyzing such interactions. Choreography conformance involves verifying that a set of services behave according to a given choreography specification that characterizes their interactions. Unfortunately this is an undecidable problem when services interact with asynchronous communication. In this paper we present techniques that identify if the interaction behavior for a set of services remain the same when asynchronous communication is replaced with synchronous communication. This is called the synchronizability problem and determining the synchronizability of a set of services has been an open problem for several years. We solve this problem in this paper. Our results can be used to identify synchronizable services for which choreography conformance can be checked efficiently. Our results on synchronizability are applicable to any software infrastructure that supports message-based interactions. [[EENNDD]] synchronizability; message-based interactions"}, "Kesesuaian koreografi melalui penyegerakan Analisis koreografi telah menjadi masalah penting dalam pengkomputeran berorientasikan perkhidmatan. Interaksi antara perkhidmatan melibatkan pertukaran mesej melintasi batas organisasi dalam persekitaran pengkomputeran yang diedarkan, dan untuk membangun sistem sedemikian dengan cara yang boleh dipercayai, perlu mengembangkan teknik untuk menganalisis interaksi tersebut. Kesesuaian koreografi melibatkan pengesahan bahawa sekumpulan perkhidmatan berkelakuan mengikut spesifikasi koreografi tertentu yang menjadi ciri interaksi mereka. Sayangnya ini adalah masalah yang tidak dapat diatasi ketika perkhidmatan berinteraksi dengan komunikasi tak segerak. Dalam makalah ini kami menyajikan teknik yang mengenal pasti apakah tingkah laku interaksi untuk satu set perkhidmatan tetap sama ketika komunikasi asinkron digantikan dengan komunikasi segerak. Ini dipanggil masalah penyegerakan dan menentukan keboleh sinkronan serangkaian perkhidmatan telah menjadi masalah terbuka selama beberapa tahun. Kami menyelesaikan masalah ini dalam makalah ini. Hasil kami dapat digunakan untuk mengenal pasti perkhidmatan yang dapat diselaraskan yang kesesuaian koreografi dapat diperiksa dengan cekap. Hasil kami mengenai penyegerakan berlaku untuk infrastruktur perisian yang menyokong interaksi berdasarkan mesej. [[EENNDD]] penyegerakan; interaksi berdasarkan mesej"], [{"string": "Mining collective local knowledge from Google MyMaps The emerging popularity of location-aware devices and location-based services has generated a growing archive of digital traces of people's activities and opinions in physical space. In this study, we leverage geo-referenced user-generated content from Google MyMaps to discover collective local knowledge and understand the differing perceptions of urban space. Working with the large collection of publicly available, annotation-rich MyMaps data, we propose a highly parallelizable approach in order to merge identical places, discover landmarks, and recommend places. Additionally, we conduct interviews with New York City residents/visitors to validate the quantitative findings.", "keywords": ["geo-tagged data", "user-generated content", "place recommendation", "miscellaneous"], "combined": "Mining collective local knowledge from Google MyMaps The emerging popularity of location-aware devices and location-based services has generated a growing archive of digital traces of people's activities and opinions in physical space. In this study, we leverage geo-referenced user-generated content from Google MyMaps to discover collective local knowledge and understand the differing perceptions of urban space. Working with the large collection of publicly available, annotation-rich MyMaps data, we propose a highly parallelizable approach in order to merge identical places, discover landmarks, and recommend places. Additionally, we conduct interviews with New York City residents/visitors to validate the quantitative findings. [[EENNDD]] geo-tagged data; user-generated content; place recommendation; miscellaneous"}, "Menggali pengetahuan tempatan kolektif dari Google MyMaps Populariti munculnya peranti yang menyedari lokasi dan perkhidmatan berdasarkan lokasi telah menghasilkan arkib jejak digital mengenai aktiviti dan pendapat orang di ruang fizikal. Dalam kajian ini, kami memanfaatkan kandungan buatan pengguna yang dirujuk secara geografis dari Google MyMaps untuk mengetahui pengetahuan tempatan kolektif dan memahami persepsi yang berbeza mengenai ruang bandar. Bekerja dengan koleksi data MyMaps yang kaya dengan anotasi yang banyak tersedia untuk umum, kami mencadangkan pendekatan yang sangat paralel untuk menggabungkan tempat yang sama, menemui mercu tanda, dan mengesyorkan tempat. Selain itu, kami melakukan temu ramah dengan penduduk / pelawat New York City untuk mengesahkan penemuan kuantitatif. [[EENNDD]] data bertanda geo; kandungan yang dihasilkan pengguna; cadangan tempat; pelbagai"], [{"string": "Analyzing web access control policies XACML has emerged as a popular access control language on the Web, but because of its rich expressiveness, it has proved difficult to analyze in an automated fashion. In this paper, we present a formalization of XACML using description logics (DL), which are a decidable fragment of First-Order logic. This formalization allows us to cover a more expressive subset of XACML than propositional logic-based analysis tools, and in addition we provide a new analysis service (policy redundancy). Also, mapping XACML to description logics allows us to use off-the-shelf DL reasoners for analysis tasks such as policy comparison, verification and querying. We provide empirical evaluation of a policy analysis tool that was implemented on top of open source DL reasoner Pellet.", "keywords": ["description logics", "policy analysis", "access control", "xacml"], "combined": "Analyzing web access control policies XACML has emerged as a popular access control language on the Web, but because of its rich expressiveness, it has proved difficult to analyze in an automated fashion. In this paper, we present a formalization of XACML using description logics (DL), which are a decidable fragment of First-Order logic. This formalization allows us to cover a more expressive subset of XACML than propositional logic-based analysis tools, and in addition we provide a new analysis service (policy redundancy). Also, mapping XACML to description logics allows us to use off-the-shelf DL reasoners for analysis tasks such as policy comparison, verification and querying. We provide empirical evaluation of a policy analysis tool that was implemented on top of open source DL reasoner Pellet. [[EENNDD]] description logics; policy analysis; access control; xacml"}, "Menganalisis dasar kawalan akses web XACML telah muncul sebagai bahasa kawalan akses yang popular di Web, tetapi kerana ekspresi yang kaya, terbukti sukar untuk dianalisis secara automatik. Dalam makalah ini, kami menyajikan formalisasi XACML menggunakan deskripsi logik (DL), yang merupakan pecahan logik Pesanan Pertama. Formalisasi ini memungkinkan kita untuk merangkumi subset XACML yang lebih ekspresif daripada alat analisis berasaskan logik cadangan, dan di samping itu kami menyediakan perkhidmatan analisis baru (redundancy polisi). Juga, pemetaan XACML ke logik deskripsi membolehkan kita menggunakan penaakulan DL yang tidak sesuai untuk tugas analisis seperti perbandingan dasar, pengesahan dan pertanyaan. Kami memberikan penilaian empirik alat analisis dasar yang dilaksanakan di atas Pellet penaakulan DL sumber terbuka. [[EENNDD]] logik keterangan; analisis dasar; kawalan akses; xacml"], [{"string": "Towards efficient dominant relationship exploration of the product items on the web In recent years, there has been a prevalence of search engines being employed to find useful information in the Web as they efficiently explore hyperlinks between web pages which define a natural graph structure that yields a good ranking. Unfortunately, current search engines cannot effectively rank those relational data, which exists on dynamic websites supported by online databases. In this study, to rank such structured data (i.e., find the \"best\" items), we propose an integrated online system consisting of compressed data structure to encode the dominant relationship of the relational data. Efficient querying strategies and updating scheme are devised to facilitate the ranking process. Extensive experiments illustrate the effectiveness and efficiency of our methods. As such, we believe the work in this poster can be complementary to traditional search engines.", "keywords": ["information extraction", "information search and retrieval", "search process"], "combined": "Towards efficient dominant relationship exploration of the product items on the web In recent years, there has been a prevalence of search engines being employed to find useful information in the Web as they efficiently explore hyperlinks between web pages which define a natural graph structure that yields a good ranking. Unfortunately, current search engines cannot effectively rank those relational data, which exists on dynamic websites supported by online databases. In this study, to rank such structured data (i.e., find the \"best\" items), we propose an integrated online system consisting of compressed data structure to encode the dominant relationship of the relational data. Efficient querying strategies and updating scheme are devised to facilitate the ranking process. Extensive experiments illustrate the effectiveness and efficiency of our methods. As such, we believe the work in this poster can be complementary to traditional search engines. [[EENNDD]] information extraction; information search and retrieval; search process"}, "Ke arah penerokaan hubungan dominan yang cekap terhadap item produk di web Beberapa tahun kebelakangan ini, terdapat kelaziman mesin pencari digunakan untuk mencari maklumat yang berguna di Web kerana mereka menjelajah hyperlink antara laman web dengan cekap yang menentukan struktur grafik semula jadi yang menghasilkan kedudukan yang baik. Malangnya, mesin carian semasa tidak dapat menilai data hubungan tersebut dengan berkesan, yang terdapat di laman web dinamik yang disokong oleh pangkalan data dalam talian. Dalam kajian ini, untuk menentukan data terstruktur seperti itu (iaitu mencari item \"terbaik\"), kami mencadangkan sistem dalam talian bersepadu yang terdiri daripada struktur data termampat untuk menyandikan hubungan dominan data hubungan. Strategi pertanyaan yang cekap dan skema pengemaskinian dirancang untuk memudahkan proses pemeringkatan. Eksperimen yang meluas menggambarkan keberkesanan dan kecekapan kaedah kami. Oleh itu, kami yakin karya dalam poster ini dapat melengkapi mesin carian tradisional. [[EENNDD]] pengekstrakan maklumat; carian dan pengambilan maklumat; proses pencarian"], [{"string": "Popular web hot spots identification and visualization No contact information provided yet.", "keywords": ["usage mining", "hypertext/hypermedia", "maximal forward path", "access visualization", "metrics"], "combined": "Popular web hot spots identification and visualization No contact information provided yet. [[EENNDD]] usage mining; hypertext/hypermedia; maximal forward path; access visualization; metrics"}, "Pengenalan dan visualisasi hot spot web yang popular Belum ada maklumat hubungan. [[EENNDD]] perlombongan penggunaan; hiperteks / hipermedia; jalan maju maksimum; capaian visualisasi; sukatan"], [{"string": "DSNotify: handling broken links in the web of data The Web of Data has emerged as a way of exposing structured linked data on the Web. It builds on the central building blocks of the Web (URIs, HTTP) and benefits from its simplicity and wide-spread adoption. It does, however, also inherit the unresolved issues such as the broken link problem. Broken links constitute a major challenge for actors consuming Linked Data as they require them to deal with reduced accessibility of data. We believe that the broken link problem is a major threat to the whole Web of Data idea and that both Linked Data consumers and providers will require solutions that deal with this problem. Since no general solutions for fixing such links in the Web of Data have emerged, we make three contributions into this direction: first, we provide a concise definition of the broken link problem and a comprehensive analysis of existing approaches. Second, we present DSNotify, a generic framework able to assist human and machine actors in fixing broken links. It uses heuristic feature comparison and employs a time-interval-based blocking technique for the underlying instance matching problem. Third, we derived benchmark datasets from knowledge bases such as DBpedia and evaluated the effectiveness of our approach with respect to the broken link problem. Our results show the feasibility of a time-interval-based blocking approach for systems that aim at detecting and fixing broken links in the Web of Data.", "keywords": ["link integrity", "linked data", "information search and retrieval", "blocking", "broken links", "instance matching"], "combined": "DSNotify: handling broken links in the web of data The Web of Data has emerged as a way of exposing structured linked data on the Web. It builds on the central building blocks of the Web (URIs, HTTP) and benefits from its simplicity and wide-spread adoption. It does, however, also inherit the unresolved issues such as the broken link problem. Broken links constitute a major challenge for actors consuming Linked Data as they require them to deal with reduced accessibility of data. We believe that the broken link problem is a major threat to the whole Web of Data idea and that both Linked Data consumers and providers will require solutions that deal with this problem. Since no general solutions for fixing such links in the Web of Data have emerged, we make three contributions into this direction: first, we provide a concise definition of the broken link problem and a comprehensive analysis of existing approaches. Second, we present DSNotify, a generic framework able to assist human and machine actors in fixing broken links. It uses heuristic feature comparison and employs a time-interval-based blocking technique for the underlying instance matching problem. Third, we derived benchmark datasets from knowledge bases such as DBpedia and evaluated the effectiveness of our approach with respect to the broken link problem. Our results show the feasibility of a time-interval-based blocking approach for systems that aim at detecting and fixing broken links in the Web of Data. [[EENNDD]] link integrity; linked data; information search and retrieval; blocking; broken links; instance matching"}, "DSNotify: menangani pautan terputus di web data Web of Data telah muncul sebagai cara untuk mendedahkan data terpaut berstruktur di Web. Ia dibina di blok bangunan pusat Web (URI, HTTP) dan mendapat keuntungan dari kesederhanaan dan penggunaannya yang luas. Namun, ia juga mewarisi masalah yang tidak dapat diselesaikan seperti masalah pautan putus. Pautan yang rosak merupakan cabaran besar bagi pelaku yang menggunakan Data Berkaitan kerana mereka memerlukan mereka untuk menangani aksesibilitas data yang berkurang. Kami percaya bahawa masalah pautan yang terputus adalah ancaman besar bagi keseluruhan idea Web Data dan bahawa pengguna dan penyedia Data Berkaitan akan memerlukan penyelesaian yang menangani masalah ini. Oleh kerana tidak ada penyelesaian umum untuk memperbaiki pautan tersebut di Web Data yang muncul, kami memberikan tiga sumbangan ke arah ini: pertama, kami memberikan definisi ringkas mengenai masalah pautan yang terputus dan analisis komprehensif mengenai pendekatan yang ada. Kedua, kami menghadirkan DSNotify, kerangka umum yang dapat membantu pelaku manusia dan mesin dalam memperbaiki pautan yang terputus. Ia menggunakan perbandingan ciri heuristik dan menggunakan teknik penyekat berdasarkan selang waktu untuk masalah pencocokan contoh yang mendasari. Ketiga, kami memperoleh set data penanda aras dari pangkalan pengetahuan seperti DBpedia dan menilai keberkesanan pendekatan kami berkenaan dengan masalah pautan yang rosak. Hasil kami menunjukkan kemungkinan pendekatan penyekat berasaskan selang waktu untuk sistem yang bertujuan untuk mengesan dan memperbaiki pautan yang terputus di Web Data. [[EENNDD]] integriti pautan; data yang dipautkan; carian dan pengambilan maklumat; menyekat; pautan yang rosak; pemadanan contoh"], [{"string": "Parsing owl dl: trees or triples? No contact information provided yet.", "keywords": ["rdf", "owl", "semantic web"], "combined": "Parsing owl dl: trees or triples? No contact information provided yet. [[EENNDD]] rdf; owl; semantic web"}, "Menghuraikan burung hantu dl: pokok atau tiga kali ganda? Belum ada maklumat hubungan yang diberikan. [[EENNDD]] rdf; burung hantu; web semantik"], [{"string": "A management and performance framework for semantic web servers The unification of Semantic Web query languages under the SPARQL standard and the development of commercial-quality implementations are encouraging industries to use semantic technologies for managing information. Current implementations, however, lack the performance monitoring and management services that the industry expects. In this paper, we present a performance and management framework interface to a generic SPARQL web server. We leverage existing standards for instrumentation to make the system ready-to-manage through existing monitoring applications, and we provide a performance framework which has the distinct feature of providing measurement results through the same SPARQL interface used to query data, eliminating the need for special interfaces.", "keywords": ["semantic web"], "combined": "A management and performance framework for semantic web servers The unification of Semantic Web query languages under the SPARQL standard and the development of commercial-quality implementations are encouraging industries to use semantic technologies for managing information. Current implementations, however, lack the performance monitoring and management services that the industry expects. In this paper, we present a performance and management framework interface to a generic SPARQL web server. We leverage existing standards for instrumentation to make the system ready-to-manage through existing monitoring applications, and we provide a performance framework which has the distinct feature of providing measurement results through the same SPARQL interface used to query data, eliminating the need for special interfaces. [[EENNDD]] semantic web"}, "Kerangka kerja pengurusan dan prestasi untuk pelayan web semantik Penyatuan bahasa pertanyaan Web Semantik di bawah standard SPARQL dan pengembangan pelaksanaan kualiti komersial mendorong industri menggunakan teknologi semantik untuk menguruskan maklumat. Namun, implementasi yang ada saat ini tidak mempunyai perkhidmatan pemantauan dan pengurusan prestasi yang diharapkan oleh industri. Dalam makalah ini, kami menyajikan antara muka kerangka kerja prestasi dan pengurusan ke pelayan web SPARQL generik. Kami memanfaatkan standard yang ada untuk instrumentasi agar sistem siap dikelola melalui aplikasi pemantauan yang ada, dan kami menyediakan kerangka kerja kinerja yang memiliki ciri khas untuk memberikan hasil pengukuran melalui antarmuka SPARQL yang sama yang digunakan untuk meminta data, menghilangkan kebutuhan khusus antara muka. [[EENNDD]] web semantik"], [{"string": "Graphical representation of RDF queries No contact information provided yet.", "keywords": ["rdf", "query", "metadata", "semistructured data"], "combined": "Graphical representation of RDF queries No contact information provided yet. [[EENNDD]] rdf; query; metadata; semistructured data"}, "Perwakilan grafik pertanyaan RDF Belum ada maklumat hubungan yang diberikan. [[EENNDD]] rdf; pertanyaan; metadata; data separa struktur"], [{"string": "Exploiting content redundancy for web information extraction We propose a novel extraction approach that exploits content redundancy on the web to extract structured data from template-based web sites. We start by populating a seed database with records extracted from a few initial sites. We then identify values within the pages of each new site that match attribute values contained in the seed set of records. To filter out noisy attribute value matches, we exploit the fact that attribute values occur at fixed positions within template-based sites. We develop an efficient Apriori-style algorithm to systematically enumerate attribute position configurations with sufficient matching values across pages. Finally, we conduct an extensive experimental study with real-life web data to demonstrate the effectiveness of our extraction approach.", "keywords": ["miscellaneous", "information extraction", "content redundancy"], "combined": "Exploiting content redundancy for web information extraction We propose a novel extraction approach that exploits content redundancy on the web to extract structured data from template-based web sites. We start by populating a seed database with records extracted from a few initial sites. We then identify values within the pages of each new site that match attribute values contained in the seed set of records. To filter out noisy attribute value matches, we exploit the fact that attribute values occur at fixed positions within template-based sites. We develop an efficient Apriori-style algorithm to systematically enumerate attribute position configurations with sufficient matching values across pages. Finally, we conduct an extensive experimental study with real-life web data to demonstrate the effectiveness of our extraction approach. [[EENNDD]] miscellaneous; information extraction; content redundancy"}, "Mengeksploitasi kelebihan kandungan untuk pengekstrakan maklumat web Kami mencadangkan pendekatan pengekstrakan novel yang memanfaatkan kelebihan kandungan di web untuk mengekstrak data berstruktur dari laman web berasaskan templat. Kami mulakan dengan mengisi pangkalan data benih dengan catatan yang diekstrak dari beberapa laman web awal. Kami kemudian mengenal pasti nilai dalam halaman setiap laman web baru yang sepadan dengan nilai atribut yang terdapat dalam kumpulan rekod. Untuk menyaring pencocokan nilai atribut yang bising, kami memanfaatkan fakta bahawa nilai atribut terjadi pada kedudukan tetap dalam laman web berdasarkan templat. Kami mengembangkan algoritma gaya Apriori yang cekap untuk menghitung konfigurasi kedudukan atribut secara sistematik dengan nilai padanan yang mencukupi di seluruh halaman. Akhirnya, kami melakukan kajian eksperimen yang luas dengan data web kehidupan sebenar untuk menunjukkan keberkesanan pendekatan pengekstrakan kami. [[EENNDD]] pelbagai; pengekstrakan maklumat; kelebihan kandungan"], [{"string": "A larger scale study of robots.txt A website can regulate search engine crawler access to its content using the robots exclusion protocol, specified in its robots.txt file. The rules in the protocol enable the site to allow or disallow part or all of its content to certain crawlers, resulting in a favorable or unfavorable bias towards some of them. A 2007 survey on the robots.txt usage of about 7,593 sites found some evidence of such biases, the news of which led to widespread discussions on the web. In this paper, we report on our survey of about 6 million sites. Our survey tries to correct the shortcomings of the previous survey and shows the lack of any significant preferences towards any particular search engine.", "keywords": ["search engine", "robots.txt", "crawler", "robots exclusion"], "combined": "A larger scale study of robots.txt A website can regulate search engine crawler access to its content using the robots exclusion protocol, specified in its robots.txt file. The rules in the protocol enable the site to allow or disallow part or all of its content to certain crawlers, resulting in a favorable or unfavorable bias towards some of them. A 2007 survey on the robots.txt usage of about 7,593 sites found some evidence of such biases, the news of which led to widespread discussions on the web. In this paper, we report on our survey of about 6 million sites. Our survey tries to correct the shortcomings of the previous survey and shows the lack of any significant preferences towards any particular search engine. [[EENNDD]] search engine; robots.txt; crawler; robots exclusion"}, "Kajian skala besar robots.txt Sebuah laman web dapat mengatur akses perayap mesin carian ke kandungannya menggunakan protokol pengecualian robot, yang ditentukan dalam fail robots.txt. Peraturan dalam protokol membolehkan laman web untuk membenarkan atau melarang sebahagian atau seluruh kandungannya kepada perayap tertentu, sehingga menimbulkan bias yang baik atau tidak baik terhadap beberapa dari mereka. Satu tinjauan tahun 2007 mengenai penggunaan robots.txt di sekitar 7,593 laman web menemui beberapa bukti bias seperti itu, yang mana berita tersebut menyebabkan perbincangan meluas di web. Dalam makalah ini, kami melaporkan tinjauan kami mengenai sekitar 6 juta laman web. Tinjauan kami berusaha untuk memperbaiki kekurangan tinjauan sebelumnya dan menunjukkan kekurangan pilihan yang signifikan terhadap mesin carian tertentu. [[EENNDD]] enjin carian; robots.txt; perangkak; pengecualian robot"], [{"string": "A content and structure website mining model No contact information provided yet.", "keywords": ["web mining", "website improvement"], "combined": "A content and structure website mining model No contact information provided yet. [[EENNDD]] web mining; website improvement"}, "Model perlombongan laman web kandungan dan struktur Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlombongan web; penambahbaikan laman web"], [{"string": "Design and implementation of a feedback controller for slowdown differentiation on internet servers No contact information provided yet.", "keywords": ["slowdown", "performance of systems", "quality of service", "feedback control"], "combined": "Design and implementation of a feedback controller for slowdown differentiation on internet servers No contact information provided yet. [[EENNDD]] slowdown; performance of systems; quality of service; feedback control"}, "Reka bentuk dan pelaksanaan pengawal maklum balas untuk perbezaan perlahan pada pelayan internet Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kelembapan; prestasi sistem; kualiti sesuatu servis; kawalan maklum balas"], [{"string": "Capturing the essentials of federated systems No contact information provided yet.", "keywords": ["modeling", "federation", "web services", "architecture"], "combined": "Capturing the essentials of federated systems No contact information provided yet. [[EENNDD]] modeling; federation; web services; architecture"}, "Memahami keperluan sistem gabungan Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] pemodelan; persekutuan; perkhidmatan web; seni bina"], [{"string": "Automatic generation of web portals using artificial ants No contact information provided yet.", "keywords": ["artificial ants", "web", "portals sites", "hierarchical clustering"], "combined": "Automatic generation of web portals using artificial ants No contact information provided yet. [[EENNDD]] artificial ants; web; portals sites; hierarchical clustering"}, "Penjanaan portal web secara automatik menggunakan semut tiruan Belum ada maklumat hubungan. [[EENNDD]] semut tiruan; laman web; laman web portal; pengelompokan hierarki"], [{"string": "A possible simplification of the semantic web architecture No contact information provided yet.", "keywords": ["resource description framework schema", "ontology web language", "description logics", "resource description framework", "semantic web"], "combined": "A possible simplification of the semantic web architecture No contact information provided yet. [[EENNDD]] resource description framework schema; ontology web language; description logics; resource description framework; semantic web"}, "Kemungkinan penyederhanaan seni bina web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] skema rangka keterangan sumber; bahasa web ontologi; logik keterangan; kerangka penerangan sumber; web semantik"], [{"string": "High-performance spatial indexing for location-based services No contact information provided yet.", "keywords": ["spatial indexing", "mobile computing", "location-based services", "database applications"], "combined": "High-performance spatial indexing for location-based services No contact information provided yet. [[EENNDD]] spatial indexing; mobile computing; location-based services; database applications"}, "Pengindeksan spasial berprestasi tinggi untuk perkhidmatan berdasarkan lokasi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengindeksan ruang; pengkomputeran mudah alih; perkhidmatan berasaskan lokasi; aplikasi pangkalan data"], [{"string": "Analysis of communication models in web service compositions No contact information provided yet.", "keywords": ["bpel", "formal verification", "asynchronous communications", "web service composition", "systems and information theory"], "combined": "Analysis of communication models in web service compositions No contact information provided yet. [[EENNDD]] bpel; formal verification; asynchronous communications; web service composition; systems and information theory"}, "Analisis model komunikasi dalam komposisi perkhidmatan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] bpel; pengesahan rasmi; komunikasi tak segerak; komposisi perkhidmatan web; sistem dan teori maklumat"], [{"string": "Automatic generation of link collections and their visualization No contact information provided yet.", "keywords": ["hyperlink analysis", "hypertext/hypermedia", "link collection", "visualization"], "combined": "Automatic generation of link collections and their visualization No contact information provided yet. [[EENNDD]] hyperlink analysis; hypertext/hypermedia; link collection; visualization"}, "Penjanaan pautan automatik dan visualisasi mereka Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] analisis hiperpautan; hiperteks / hipermedia; koleksi pautan; visualisasi"], [{"string": "Supporting management reporting: a writable web case study No contact information provided yet.", "keywords": ["open hypermedia", "document preparation", "hypertext/hypermedia", "hypertext writing", "structural computing", "electronic publishing", "management reporting"], "combined": "Supporting management reporting: a writable web case study No contact information provided yet. [[EENNDD]] open hypermedia; document preparation; hypertext/hypermedia; hypertext writing; structural computing; electronic publishing; management reporting"}, "Menyokong pelaporan pengurusan: kajian kes web yang boleh ditulis Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] hipermedia terbuka; penyediaan dokumen; hiperteks / hipermedia; penulisan hiperteks; pengkomputeran struktur; penerbitan elektronik; pelaporan pengurusan"], [{"string": "Exploring social annotations for information retrieval Social annotation has gained increasing popularity in many Web-based applications, leading to an emerging research area in text analysis and information retrieval. This paper is concerned with developing probabilistic models and computational algorithms for social annotations. We propose a unified framework to combine the modeling of social annotations with the language modeling-based methods for information retrieval. The proposed approach consists of two steps: (1) discovering topics in the contents and annotations of documents while categorizing the users by domains; and (2) enhancing document and query language models by incorporating user domain interests as well as topical background models. In particular, we propose a new general generative model for social annotations, which is then simplified to a computationally tractable hierarchical Bayesian network. Then we apply smoothing techniques in a risk minimization framework to incorporate the topical information to language models. Experiments are carried out on a real-world annotation data set sampled from del.icio.us. Our results demonstrate significant improvements over the traditional approaches.", "keywords": ["language modeling", "folksonomy", "social annotations", "information retrieval"], "combined": "Exploring social annotations for information retrieval Social annotation has gained increasing popularity in many Web-based applications, leading to an emerging research area in text analysis and information retrieval. This paper is concerned with developing probabilistic models and computational algorithms for social annotations. We propose a unified framework to combine the modeling of social annotations with the language modeling-based methods for information retrieval. The proposed approach consists of two steps: (1) discovering topics in the contents and annotations of documents while categorizing the users by domains; and (2) enhancing document and query language models by incorporating user domain interests as well as topical background models. In particular, we propose a new general generative model for social annotations, which is then simplified to a computationally tractable hierarchical Bayesian network. Then we apply smoothing techniques in a risk minimization framework to incorporate the topical information to language models. Experiments are carried out on a real-world annotation data set sampled from del.icio.us. Our results demonstrate significant improvements over the traditional approaches. [[EENNDD]] language modeling; folksonomy; social annotations; information retrieval"}, "Meneroka anotasi sosial untuk mendapatkan maklumat Anotasi sosial telah mendapat populariti yang semakin meningkat di banyak aplikasi berasaskan Web, yang membawa kepada bidang penyelidikan yang muncul dalam analisis teks dan pengambilan maklumat. Makalah ini berkaitan dengan pengembangan model probabilistik dan algoritma komputasi untuk anotasi sosial. Kami mencadangkan kerangka kerja terpadu untuk menggabungkan pemodelan anotasi sosial dengan kaedah berasaskan pemodelan bahasa untuk pencarian maklumat. Pendekatan yang dicadangkan terdiri daripada dua langkah: (1) mencari topik dalam kandungan dan anotasi dokumen sambil mengkategorikan pengguna mengikut domain; dan (2) meningkatkan model bahasa dokumen dan pertanyaan dengan memasukkan minat domain pengguna dan juga model latar belakang topikal. Secara khusus, kami mencadangkan model generatif umum baru untuk anotasi sosial, yang kemudian disederhanakan menjadi rangkaian Bayesian hierarki yang dapat dikomputasi. Kemudian kami menerapkan teknik pelicinan dalam kerangka pengurangan risiko untuk memasukkan maklumat topikal pada model bahasa. Eksperimen dijalankan pada set data anotasi dunia nyata yang diambil sampel dari del.icio.us. Hasil kami menunjukkan peningkatan yang signifikan terhadap pendekatan tradisional. [[EENNDD]] pemodelan bahasa; folksonomi; anotasi sosial; pengambilan maklumat"], [{"string": "Diversifying web search results Result diversity is a topic of great importance as more facets of queries are discovered and users expect to find their desired facets in the first page of the results. However, the underlying questions of how 'diversity' interplays with 'quality' and when preference should be given to one or both are not well-understood. In this work, we model the problem as expectation maximization and study the challenges of estimating the model parameters and reaching an equilibrium. One model parameter, for example, is correlations between pages which we estimate using textual contents of pages and click data (when available). We conduct experiments on diversifying randomly selected queries from a query log and the queries chosen from the disambiguation topics of Wikipedia. Our algorithm improves upon Google in terms of the diversity of random queries, retrieving 14% to 38% more aspects of queries in top 5, while maintaining a precision very close to Google. On a more selective set of queries that are expected to benefit from diversification, our algorithm improves upon Google in terms of precision and diversity of the results, and significantly outperforms another baseline system for result diversification.", "keywords": ["information search and retrieval", "search diversity", "query ambiguity", "result diversity"], "combined": "Diversifying web search results Result diversity is a topic of great importance as more facets of queries are discovered and users expect to find their desired facets in the first page of the results. However, the underlying questions of how 'diversity' interplays with 'quality' and when preference should be given to one or both are not well-understood. In this work, we model the problem as expectation maximization and study the challenges of estimating the model parameters and reaching an equilibrium. One model parameter, for example, is correlations between pages which we estimate using textual contents of pages and click data (when available). We conduct experiments on diversifying randomly selected queries from a query log and the queries chosen from the disambiguation topics of Wikipedia. Our algorithm improves upon Google in terms of the diversity of random queries, retrieving 14% to 38% more aspects of queries in top 5, while maintaining a precision very close to Google. On a more selective set of queries that are expected to benefit from diversification, our algorithm improves upon Google in terms of precision and diversity of the results, and significantly outperforms another baseline system for result diversification. [[EENNDD]] information search and retrieval; search diversity; query ambiguity; result diversity"}, "Mempelbagaikan hasil carian web Kepelbagaian hasil adalah topik yang sangat penting kerana lebih banyak aspek pertanyaan dijumpai dan pengguna berharap dapat menemui aspek yang diinginkan di halaman pertama hasil. Walau bagaimanapun, persoalan yang mendasari bagaimana 'kepelbagaian' berinteraksi dengan 'kualiti' dan kapan pilihan harus diberikan kepada satu atau kedua-duanya tidak difahami dengan baik. Dalam karya ini, kami memodelkan masalah sebagai pemaksimalan harapan dan mengkaji cabaran menganggarkan parameter model dan mencapai keseimbangan. Salah satu parameter model, misalnya, adalah korelasi antara halaman yang kami perkirakan menggunakan kandungan teks halaman dan data klik (bila ada). Kami menjalankan eksperimen untuk mempelbagaikan pertanyaan yang dipilih secara rawak dari log pertanyaan dan pertanyaan yang dipilih dari topik penyingkiran Wikipedia. Algoritma kami bertambah baik dari segi Google dalam hal kepelbagaian pertanyaan rawak, mendapatkan 14% hingga 38% lebih banyak aspek pertanyaan di 5 teratas, sambil mengekalkan ketepatan yang sangat dekat dengan Google. Pada sekumpulan pertanyaan yang lebih selektif yang diharapkan dapat memperoleh keuntungan dari kepelbagaian, algoritma kami bertambah baik dari segi ketepatan dan kepelbagaian hasil Google, dan secara signifikan mengungguli sistem asas lain untuk kepelbagaian hasil. [[EENNDD]] carian dan pengambilan maklumat; kepelbagaian carian; kekaburan pertanyaan; kepelbagaian hasil"], [{"string": "Automatic extraction of clickable structured web contents for name entity queries Today the major web search engines answer queries by showing ten result snippets, which need to be inspected by users for identifying relevant results. In this paper we investigate how to extract structured information from the web, in order to directly answer queries by showing the contents being searched for. We treat users' search trails (i.e., post-search browsing behaviors) as implicit labels on the relevance between web contents and user queries. Based on such labels we use information extraction approach to build wrappers and extract structured information. An important observation is that many web sites contain pages for name entities of certain categories (e.g., AOL Music contains a page for each musician), and these pages have the same format. This makes it possible to build wrappers from a small amount of implicit labels, and use them to extract structured information from many web pages for different name entities. We propose STRUCLICK, a fully automated system for extracting structured information for queries containing name entities of certain categories. It can identify important web sites from web search logs, build wrappers from users' search trails, filter out bad wrappers built from random user clicks, and combine structured information from different web sites for each query. Comparing with existing approaches on information extraction, STRUCLICK can assign semantics to extracted data without any human labeling or supervision. We perform comprehensive experiments, which show STRUCLICK achieves high accuracy and good scalability.", "keywords": ["web search", "information extraction"], "combined": "Automatic extraction of clickable structured web contents for name entity queries Today the major web search engines answer queries by showing ten result snippets, which need to be inspected by users for identifying relevant results. In this paper we investigate how to extract structured information from the web, in order to directly answer queries by showing the contents being searched for. We treat users' search trails (i.e., post-search browsing behaviors) as implicit labels on the relevance between web contents and user queries. Based on such labels we use information extraction approach to build wrappers and extract structured information. An important observation is that many web sites contain pages for name entities of certain categories (e.g., AOL Music contains a page for each musician), and these pages have the same format. This makes it possible to build wrappers from a small amount of implicit labels, and use them to extract structured information from many web pages for different name entities. We propose STRUCLICK, a fully automated system for extracting structured information for queries containing name entities of certain categories. It can identify important web sites from web search logs, build wrappers from users' search trails, filter out bad wrappers built from random user clicks, and combine structured information from different web sites for each query. Comparing with existing approaches on information extraction, STRUCLICK can assign semantics to extracted data without any human labeling or supervision. We perform comprehensive experiments, which show STRUCLICK achieves high accuracy and good scalability. [[EENNDD]] web search; information extraction"}, "Pengekstrakan automatik kandungan web berstruktur yang dapat diklik untuk pertanyaan entiti nama Hari ini enjin carian web utama menjawab pertanyaan dengan menunjukkan sepuluh potongan hasil, yang perlu diperiksa oleh pengguna untuk mengenal pasti hasil yang relevan. Dalam makalah ini kami menyelidiki cara mengekstrak maklumat terstruktur dari web, untuk menjawab pertanyaan secara langsung dengan menunjukkan isi yang dicari. Kami memperlakukan jejak carian pengguna (iaitu, perilaku penyemakan imbas selepas carian) sebagai label tersirat mengenai perkaitan antara kandungan web dan pertanyaan pengguna. Berdasarkan label tersebut, kami menggunakan pendekatan pengekstrakan maklumat untuk membina pembungkus dan mengekstrak maklumat berstruktur. Pemerhatian penting ialah banyak laman web mengandungi halaman untuk entiti nama kategori tertentu (mis., Muzik AOL mengandungi halaman untuk setiap pemuzik), dan halaman ini mempunyai format yang sama. Ini memungkinkan untuk membina pembungkus dari sebilangan kecil label tersirat, dan menggunakannya untuk mengekstrak maklumat berstruktur dari banyak laman web untuk entiti nama yang berbeza. Kami mencadangkan STRUCLICK, sistem automatik sepenuhnya untuk mengekstrak maklumat berstruktur untuk pertanyaan yang mengandungi entiti nama dari kategori tertentu. Ia dapat mengenal pasti laman web penting dari log carian web, membina pembungkus dari jejak carian pengguna, menyaring pembungkus buruk yang dibina dari klik pengguna rawak, dan menggabungkan maklumat berstruktur dari laman web yang berbeza untuk setiap pertanyaan. Berbanding dengan pendekatan yang ada pada pengekstrakan maklumat, STRUCLICK dapat menetapkan semantik untuk mengekstrak data tanpa pelabelan atau pengawasan manusia. Kami melakukan eksperimen menyeluruh, yang menunjukkan STRUCLICK mencapai ketepatan tinggi dan skalabiliti yang baik. [[EENNDD]] carian web; pengekstrakan maklumat"], [{"string": "SRing: a structured non dht p2p overlay supporting string range queries This paper presents SRing, a structured non DHT P2P overlay that efficiently supports exact and range queries on multiple attribute values. In SRing, all attribute values are interpreted as strings formed by a base alphabet and are published in the lexicographical order. Two virtual rings are built: N-ring is built in a skip-list way for range partition and queries; D-ring is built in a small-world way for the construction of N-ring. A leave-and-join based load balancing method is used to balance range overload in the network with heterogeneous nodes.", "keywords": ["multi-attribute", "load balancing", "range queries", "p2p"], "combined": "SRing: a structured non dht p2p overlay supporting string range queries This paper presents SRing, a structured non DHT P2P overlay that efficiently supports exact and range queries on multiple attribute values. In SRing, all attribute values are interpreted as strings formed by a base alphabet and are published in the lexicographical order. Two virtual rings are built: N-ring is built in a skip-list way for range partition and queries; D-ring is built in a small-world way for the construction of N-ring. A leave-and-join based load balancing method is used to balance range overload in the network with heterogeneous nodes. [[EENNDD]] multi-attribute; load balancing; range queries; p2p"}, "SRing: hamparan bukan dht p2p berstruktur yang menyokong pertanyaan rentetan rentetan Kertas ini menyajikan SRing, hamparan bukan DHT P2P berstruktur yang secara berkesan menyokong pertanyaan tepat dan julat pada beberapa nilai atribut. Dalam SRing, semua nilai atribut ditafsirkan sebagai rentetan yang dibentuk oleh abjad dasar dan diterbitkan dalam susunan leksikografi. Dua cincin maya dibina: N-ring dibina dengan cara skip-list untuk partisi pelbagai dan pertanyaan; D-ring dibina dengan cara kecil untuk pembinaan N-ring. Kaedah pengimbangan beban berdasarkan cuti dan bergabung digunakan untuk menyeimbangkan julat beban berlebihan dalam rangkaian dengan nod heterogen. [[EENNDD]] pelbagai atribut; pengimbangan beban; pelbagai pertanyaan; p2p"], [{"string": "Learning information intent via observation Users in an organization frequently request help by sending request messages to assistants that express information intent: an intention to update data in an information system. Human assistants spend a significant amount of time and effort processing these requests. For example, human resource assistants process requests to update personnel records, and executive assistants process requests to schedule conference rooms or to make travel reservations. To process the intent of a request, assistants read the request and then locate, complete, and submit a form that corresponds to the expressed intent. Automatically or semi-automatically processing the intent expressed in a request on behalf of an assistant would ease the mundane and repetitive nature of this kind of work.For a well-understood domain, a straightforward application of natural language processing techniques can be used to build an intelligent form interface to semi-automatically process information intent request messages. However, high performance parsers are based on machine learning algorithms that require a large corpus of examples that have been labeled by an expert. The generation of a labeled corpus of requests is a major barrier to the construction of a parser. In this paper, we investigate the construction of a natural language processing system and an intelligent form system that observes an assistant processing requests. The intelligent form system then generates a labeled training corpus by interpreting the observations. This paper reports on the measurement of the performance of the machine learning algorithms based on real data. The combination of observations, machine learning and interaction design produces an effective intelligent form interface based on natural language processing.", "keywords": ["information intent", "interaction styles", "domestication", "weak labeling"], "combined": "Learning information intent via observation Users in an organization frequently request help by sending request messages to assistants that express information intent: an intention to update data in an information system. Human assistants spend a significant amount of time and effort processing these requests. For example, human resource assistants process requests to update personnel records, and executive assistants process requests to schedule conference rooms or to make travel reservations. To process the intent of a request, assistants read the request and then locate, complete, and submit a form that corresponds to the expressed intent. Automatically or semi-automatically processing the intent expressed in a request on behalf of an assistant would ease the mundane and repetitive nature of this kind of work.For a well-understood domain, a straightforward application of natural language processing techniques can be used to build an intelligent form interface to semi-automatically process information intent request messages. However, high performance parsers are based on machine learning algorithms that require a large corpus of examples that have been labeled by an expert. The generation of a labeled corpus of requests is a major barrier to the construction of a parser. In this paper, we investigate the construction of a natural language processing system and an intelligent form system that observes an assistant processing requests. The intelligent form system then generates a labeled training corpus by interpreting the observations. This paper reports on the measurement of the performance of the machine learning algorithms based on real data. The combination of observations, machine learning and interaction design produces an effective intelligent form interface based on natural language processing. [[EENNDD]] information intent; interaction styles; domestication; weak labeling"}, "Mempelajari maksud maklumat melalui pemerhatian Pengguna dalam organisasi sering meminta pertolongan dengan mengirim pesanan permintaan kepada pembantu yang menyatakan maksud maklumat: niat untuk mengemas kini data dalam sistem maklumat. Pembantu manusia menghabiskan banyak masa dan usaha memproses permintaan ini. Sebagai contoh, pembantu sumber manusia memproses permintaan untuk mengemas kini rekod personel, dan pembantu eksekutif memproses permintaan untuk menjadualkan bilik persidangan atau untuk membuat tempahan perjalanan. Untuk memproses maksud permintaan, pembantu membaca permintaan dan kemudian mencari, melengkapkan, dan menyerahkan formulir yang sesuai dengan maksud yang dinyatakan. Memproses maksud secara automatik atau separa automatik yang dinyatakan dalam permintaan bagi pihak pembantu akan memudahkan kerja biasa dan berulang dari jenis pekerjaan ini. Untuk domain yang difahami dengan baik, aplikasi teknik pemprosesan bahasa semula jadi boleh digunakan untuk membina antara muka bentuk pintar untuk memproses pesanan permintaan maklumat maklumat secara automatik. Walau bagaimanapun, penghurai berprestasi tinggi berdasarkan algoritma pembelajaran mesin yang memerlukan banyak contoh yang telah dilabel oleh seorang pakar. Penjanaan korpus permintaan berlabel merupakan penghalang utama untuk pembinaan penghurai. Dalam makalah ini, kami menyiasat pembinaan sistem pemprosesan bahasa semula jadi dan sistem bentuk cerdas yang memerhatikan permintaan pemprosesan pembantu. Sistem bentuk pintar kemudian menghasilkan corpus latihan berlabel dengan menafsirkan pemerhatian. Makalah ini melaporkan pengukuran prestasi algoritma pembelajaran mesin berdasarkan data sebenar. Gabungan pemerhatian, pembelajaran mesin dan reka bentuk interaksi menghasilkan antara muka bentuk pintar yang berkesan berdasarkan pemprosesan bahasa semula jadi. [[EENNDD]] maksud maklumat; gaya interaksi; peliharaan; pelabelan yang lemah"], [{"string": "A client-server architecture for state-dependent dynamic visualizations on the web As sophisticated enterprise applications move to the Web, some advanced user experiences become difficult to migrate due to prohibitively high computation, memory, and bandwidth requirements. State-dependent visualizations of large-scale data sets are particularly difficult since a change in the client's context necessitates a change in the displayed results. This paper describes a Web architecture where clients are served a session-specific image of the data, with this image divided into tiles dynamically generated by the server. This set of tiles is supplemented with a corpus of metadata describing the immediate vicinity of interest; additional metadata is delivered as needed in a progressive fashion in support and anticipation of the user's actions. We discuss how the design of this architecture was motivated by the goal of delivering a highly responsive user experience. As an example of a complete application built upon this architecture, we present OrgMaps, an interactive system for navigating hierarchical data, enabling fluid, low-latency navigation of trees of hundreds of thousands of nodes on standard Web browsers using only HTML and JavaScript.", "keywords": ["patterns", "rich internet applications"], "combined": "A client-server architecture for state-dependent dynamic visualizations on the web As sophisticated enterprise applications move to the Web, some advanced user experiences become difficult to migrate due to prohibitively high computation, memory, and bandwidth requirements. State-dependent visualizations of large-scale data sets are particularly difficult since a change in the client's context necessitates a change in the displayed results. This paper describes a Web architecture where clients are served a session-specific image of the data, with this image divided into tiles dynamically generated by the server. This set of tiles is supplemented with a corpus of metadata describing the immediate vicinity of interest; additional metadata is delivered as needed in a progressive fashion in support and anticipation of the user's actions. We discuss how the design of this architecture was motivated by the goal of delivering a highly responsive user experience. As an example of a complete application built upon this architecture, we present OrgMaps, an interactive system for navigating hierarchical data, enabling fluid, low-latency navigation of trees of hundreds of thousands of nodes on standard Web browsers using only HTML and JavaScript. [[EENNDD]] patterns; rich internet applications"}, "Senibina pelayan pelanggan untuk visualisasi dinamik yang bergantung pada keadaan di web Oleh kerana aplikasi perusahaan yang canggih berpindah ke Web, beberapa pengalaman pengguna yang canggih menjadi sukar untuk dimigrasikan kerana keperluan pengiraan, memori, dan lebar jalur yang tinggi. Visualisasi bergantung pada set data berskala besar sangat sukar kerana perubahan dalam konteks pelanggan memerlukan perubahan pada hasil yang ditunjukkan. Makalah ini menerangkan seni bina Web di mana pelanggan dilayan gambar khusus sesi data, dengan gambar ini dibahagikan kepada jubin yang dihasilkan secara dinamik oleh pelayan. Set jubin ini dilengkapi dengan kumpulan metadata yang menggambarkan kawasan yang paling dekat dengan minat; metadata tambahan disampaikan mengikut keperluan secara progresif sebagai sokongan dan jangkaan tindakan pengguna. Kami membincangkan bagaimana reka bentuk seni bina ini dimotivasi oleh matlamat untuk memberikan pengalaman pengguna yang sangat responsif. Sebagai contoh aplikasi lengkap yang dibina berdasarkan seni bina ini, kami menyajikan OrgMaps, sistem interaktif untuk menavigasi data hierarki, membolehkan navigasi pepohonan yang lancar, latensi rendah beratus-ratus ribu nod pada penyemak imbas Web standard yang hanya menggunakan HTML dan JavaScript. [[EENNDD]] corak; aplikasi internet yang kaya"], [{"string": "Towards context-aware search by learning a very large variable length hidden markov model from search logs Capturing the context of a user's query from the previous queries and clicks in the same session may help understand the user's information need. A context-aware approach to document re-ranking, query suggestion, and URL recommendation may improve users' search experience substantially. In this paper, we propose a general approach to context-aware search. To capture contexts of queries, we learn a variable length Hidden Markov Model (vlHMM) from search sessions extracted from log data. Although the mathematical model is intuitive, how to learn a large vlHMM with millions of states from hundreds of millions of search sessions poses a grand challenge. We develop a strategy for parameter initialization in vlHMM learning which can greatly reduce the number of parameters to be estimated in practice. We also devise a method for distributed vlHMM learning under the map-reduce model. We test our approach on a real data set consisting of 1.8 billion queries, 2.6 billion clicks, and 840 million search sessions, and evaluate the effectiveness of the vlHMM learned from the real data on three search applications: document re-ranking, query suggestion, and URL recommendation. The experimental results show that our approach is both effective and efficient.", "keywords": ["variable length hidden markov model", "context-aware search"], "combined": "Towards context-aware search by learning a very large variable length hidden markov model from search logs Capturing the context of a user's query from the previous queries and clicks in the same session may help understand the user's information need. A context-aware approach to document re-ranking, query suggestion, and URL recommendation may improve users' search experience substantially. In this paper, we propose a general approach to context-aware search. To capture contexts of queries, we learn a variable length Hidden Markov Model (vlHMM) from search sessions extracted from log data. Although the mathematical model is intuitive, how to learn a large vlHMM with millions of states from hundreds of millions of search sessions poses a grand challenge. We develop a strategy for parameter initialization in vlHMM learning which can greatly reduce the number of parameters to be estimated in practice. We also devise a method for distributed vlHMM learning under the map-reduce model. We test our approach on a real data set consisting of 1.8 billion queries, 2.6 billion clicks, and 840 million search sessions, and evaluate the effectiveness of the vlHMM learned from the real data on three search applications: document re-ranking, query suggestion, and URL recommendation. The experimental results show that our approach is both effective and efficient. [[EENNDD]] variable length hidden markov model; context-aware search"}, "Ke arah carian yang peka konteks dengan mempelajari model markov tersembunyi dengan panjang berubah yang sangat besar dari log carian Menangkap konteks pertanyaan pengguna dari pertanyaan dan klik sebelumnya dalam sesi yang sama dapat membantu memahami keperluan maklumat pengguna. Pendekatan yang peka konteks untuk pemeringkatan ulang dokumen, cadangan pertanyaan, dan cadangan URL dapat meningkatkan pengalaman pencarian pengguna secara substansial. Dalam makalah ini, kami mencadangkan pendekatan umum untuk pencarian berdasarkan konteks. Untuk menangkap konteks pertanyaan, kami mempelajari Model Markov Tersembunyi (vlHMM) panjang berubah dari sesi carian yang diekstrak dari data log. Walaupun model matematik intuitif, bagaimana mempelajari vlHMM yang besar dengan berjuta-juta negeri dari ratusan juta sesi carian menimbulkan cabaran besar. Kami mengembangkan strategi untuk inisialisasi parameter dalam pembelajaran vlHMM yang dapat mengurangkan jumlah parameter yang akan diperkirakan dalam praktik. Kami juga merancang kaedah untuk pembelajaran vlHMM yang diedarkan di bawah model pengurangan peta. Kami menguji pendekatan kami pada set data nyata yang terdiri daripada 1.8 bilion pertanyaan, 2.6 bilion klik, dan 840 juta sesi carian, dan menilai keberkesanan vlHMM yang dipelajari dari data sebenar pada tiga aplikasi carian: penyusunan semula dokumen, cadangan pertanyaan, dan cadangan URL. Hasil eksperimen menunjukkan bahawa pendekatan kami berkesan dan cekap. [[EENNDD]] model markov tersembunyi panjang berubah; carian mengikut konteks"], [{"string": "Preferential walk: towards efficient and scalable search in unstructured peer-to-peer networks No contact information provided yet.", "keywords": ["probabilistic algorithms", "p2p", "trust", "power-law", "probability", "search"], "combined": "Preferential walk: towards efficient and scalable search in unstructured peer-to-peer networks No contact information provided yet. [[EENNDD]] probabilistic algorithms; p2p; trust; power-law; probability; search"}, "Jalan pilihan: ke arah carian yang cekap dan berskala dalam rangkaian peer-to-peer yang tidak berstruktur Belum ada maklumat hubungan yang diberikan. [[EENNDD]] algoritma probabilistik; p2p; kepercayaan; kuasa undang-undang; kebarangkalian; cari"], [{"string": "A component model for stardardized web-based education An abstract is not available.", "keywords": ["learning technology", "authoring tools", "learing technology standardization", "educational web applications", "practice and experience", "collaborative systems", "web-based course delivery systems"], "combined": "A component model for stardardized web-based education An abstract is not available. [[EENNDD]] learning technology; authoring tools; learing technology standardization; educational web applications; practice and experience; collaborative systems; web-based course delivery systems"}, "Model komponen untuk pendidikan berasaskan web bertaraf abstrak Abstrak tidak tersedia. [[EENNDD]] teknologi pembelajaran; alat pengarang; memanfaatkan standardisasi teknologi; aplikasi web pendidikan; amalan dan pengalaman; sistem kolaboratif; sistem penyampaian kursus berasaskan web"], [{"string": "A messaging API for inter-widgets communication Widget containers are used everywhere on the Web, for instance as customizable start pages to Web desktops. In this poster, we describe the extension of a widget container with an inter-widgets communication layer, as well as the subsequent application programming interfaces (APIs) added to the Widget object to support this feature. We present the benefits of a drag and drop facility within widgets and conclude by a call for standardization of inter-widgets communication on the Web.", "keywords": ["drag and drop", "widget", "portal", "start page", "communication"], "combined": "A messaging API for inter-widgets communication Widget containers are used everywhere on the Web, for instance as customizable start pages to Web desktops. In this poster, we describe the extension of a widget container with an inter-widgets communication layer, as well as the subsequent application programming interfaces (APIs) added to the Widget object to support this feature. We present the benefits of a drag and drop facility within widgets and conclude by a call for standardization of inter-widgets communication on the Web. [[EENNDD]] drag and drop; widget; portal; start page; communication"}, "API pemesejan untuk komunikasi antara widget Wadah widget digunakan di mana-mana di Web, misalnya sebagai halaman permulaan yang dapat disesuaikan ke desktop Web. Dalam poster ini, kami menerangkan peluasan wadah widget dengan lapisan komunikasi antar-widget, serta antaramuka pengaturcaraan aplikasi (API) berikutnya yang ditambahkan ke objek Widget untuk menyokong fitur ini. Kami menyajikan kelebihan kemudahan seret dan lepas dalam widget dan diakhiri dengan panggilan untuk standardisasi komunikasi antara widget di Web. [[EENNDD]] seret dan lepas; widget; portal; muka surat permulaan; komunikasi"], [{"string": "The two cultures: mashing up web 2.0 and the semantic web A common perception is that there are two competing visions for the future evolution of the Web: the Semantic Web and Web 2.0. A closer look, though, reveals that the core technologies and concerns of these two approaches are complementary and that each field can and must draw from the other's strengths. We believe that future web applications will retain the Web 2.0 focus on community and usability, while drawing on Semantic Web infrastructure to facilitate mashup-like information sharing. However, there are several open issues that must be addressed before such applications can become commonplace. In this paper, we outline a semantic weblogs scenario that illustrates the potential for combining Web 2.0 and Semantic Web technologies, while highlighting the unresolved issues that impede its realization. Nevertheless, we believe that the scenario can be realized in the short-term. We point to recent progress made in resolving each of the issues as well as future research directions for each of the communities.", "keywords": ["vision", "on-line information services", "web 2.0", "semantic web", "rdf", "blog"], "combined": "The two cultures: mashing up web 2.0 and the semantic web A common perception is that there are two competing visions for the future evolution of the Web: the Semantic Web and Web 2.0. A closer look, though, reveals that the core technologies and concerns of these two approaches are complementary and that each field can and must draw from the other's strengths. We believe that future web applications will retain the Web 2.0 focus on community and usability, while drawing on Semantic Web infrastructure to facilitate mashup-like information sharing. However, there are several open issues that must be addressed before such applications can become commonplace. In this paper, we outline a semantic weblogs scenario that illustrates the potential for combining Web 2.0 and Semantic Web technologies, while highlighting the unresolved issues that impede its realization. Nevertheless, we believe that the scenario can be realized in the short-term. We point to recent progress made in resolving each of the issues as well as future research directions for each of the communities. [[EENNDD]] vision; on-line information services; web 2.0; semantic web; rdf; blog"}, "Dua budaya: membentuk web 2.0 dan web semantik Persepsi umum adalah bahawa terdapat dua visi bersaing untuk evolusi Web di masa depan: Web Semantik dan Web 2.0. Namun, dengan melihat lebih dekat, teknologi inti dan keprihatinan kedua pendekatan ini saling melengkapi dan setiap bidang dapat dan mesti diambil dari kekuatan yang lain. Kami percaya bahawa aplikasi web masa depan akan mengekalkan fokus Web 2.0 pada komuniti dan kegunaan, sambil menggunakan infrastruktur Web Semantik untuk memudahkan perkongsian maklumat seperti mashup. Namun, ada beberapa masalah terbuka yang harus ditangani sebelum aplikasi seperti itu dapat menjadi perkara biasa. Dalam makalah ini, kami menggariskan senario weblog semantik yang menggambarkan potensi untuk menggabungkan teknologi Web 2.0 dan Semantik Web, sambil menyoroti masalah yang tidak dapat diselesaikan yang menghalangnya. Walaupun begitu, kami percaya bahawa senario tersebut dapat direalisasikan dalam jangka pendek. Kami menunjukkan kemajuan terkini yang dicapai dalam menyelesaikan setiap masalah serta petunjuk penyelidikan masa depan untuk setiap komuniti. [[EENNDD]] penglihatan; perkhidmatan maklumat dalam talian; laman web 2.0; web semantik; rdf; blog"], [{"string": "Combining link and content analysis to estimate semantic similarity Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.", "keywords": ["content and link similarity", "precision", "content analysis and indexing", "web search", "information search and retrieval", "recall", "semantic maps"], "combined": "Combining link and content analysis to estimate semantic similarity Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references. [[EENNDD]] content and link similarity; precision; content analysis and indexing; web search; information search and retrieval; recall; semantic maps"}, "Menggabungkan pautan dan analisis kandungan untuk mengira kesamaan semantik Catatan: Kesalahan OCR mungkin terdapat dalam Senarai Rujukan ini yang diekstrak dari artikel teks lengkap. ACM memilih untuk mendedahkan Senarai lengkap dan bukan hanya rujukan yang betul dan berkaitan. [[EENNDD]] kandungan dan persamaan pautan; ketepatan; analisis kandungan dan pengindeksan; carian sesawang; pencarian dan pengambilan maklumat; ingat semula; peta semantik"], [{"string": "D2RQ/update: updating relational data via virtual RDF D2RQ is a popular RDB-to-RDF mapping platform that supports mapping relational databases to RDF and posing SPARQL queries to these relational databases. However, D2RQ merely provides a read-only RDF view on relational databases. Thus, we introduce D2RQ/Update---an extension of D2RQ to enable executing SPARQL/Update statements on the mapped data, and to facilitate the creation of a read-write Semantic Web.", "keywords": ["sparql/update", "relational database", "linked data", "semantic web", "sparql", "rdb-to-rdf mapping", "rdf", "d2rq"], "combined": "D2RQ/update: updating relational data via virtual RDF D2RQ is a popular RDB-to-RDF mapping platform that supports mapping relational databases to RDF and posing SPARQL queries to these relational databases. However, D2RQ merely provides a read-only RDF view on relational databases. Thus, we introduce D2RQ/Update---an extension of D2RQ to enable executing SPARQL/Update statements on the mapped data, and to facilitate the creation of a read-write Semantic Web. [[EENNDD]] sparql/update; relational database; linked data; semantic web; sparql; rdb-to-rdf mapping; rdf; d2rq"}, "D2RQ / kemas kini: mengemas kini data hubungan melalui RDF maya D2RQ adalah platform pemetaan RDB-to-RDF yang popular yang menyokong pemetaan pangkalan data hubungan ke RDF dan mengemukakan pertanyaan SPARQL ke pangkalan data hubungan ini. Walau bagaimanapun, D2RQ hanya memberikan pandangan RDF yang hanya boleh dibaca pada pangkalan data hubungan. Oleh itu, kami memperkenalkan D2RQ / Update --- lanjutan dari D2RQ untuk membolehkan pelaksanaan penyataan SPARQL / Update pada data yang dipetakan, dan untuk memudahkan penciptaan Web Semantik baca-tulis. [[EENNDD]] sparql / kemas kini; pangkalan data hubungan; data yang dipautkan; web semantik; sparql; pemetaan rdb-ke-rdf; rdf; d2rq"], [{"string": "What is disputed on the web? We present a method for automatically acquiring of a corpus of disputed claims from the web. We consider a factual claim to be disputed if a page on the web suggests both that the claim is false and also that other people say it is true.", "keywords": ["web", "credibility", "dispute"], "combined": "What is disputed on the web? We present a method for automatically acquiring of a corpus of disputed claims from the web. We consider a factual claim to be disputed if a page on the web suggests both that the claim is false and also that other people say it is true. [[EENNDD]] web; credibility; dispute"}, "Apa yang dipertikaikan di web? Kami menyajikan kaedah untuk memperoleh secara automatik korpus tuntutan yang dipertikaikan dari web. Kami menganggap tuntutan fakta boleh dipertikaikan sekiranya halaman di web menunjukkan bahawa tuntutan itu palsu dan juga orang lain mengatakan bahawa ia benar. [[EENNDD]] web; kredibiliti; pertikaian"], [{"string": "A metro map metaphor for guided tours on the Web: the Webvise guided tour system An abstract is not available.", "keywords": ["paths", "metro map metaphor", "trails", "guided tours", "xml", "hypermedia"], "combined": "A metro map metaphor for guided tours on the Web: the Webvise guided tour system An abstract is not available. [[EENNDD]] paths; metro map metaphor; trails; guided tours; xml; hypermedia"}, "Metafora peta metro untuk lawatan berpandu di Web: sistem lawatan berpandu Webvise Abstrak tidak tersedia. [[EENNDD]] jalan; metafora peta metro; jejak; lawatan berpandu; xml; hipermedia"], [{"string": "Online expansion of rare queries for sponsored search Sponsored search systems are tasked with matching queries", "keywords": ["tail queries", "information search and retrieval", "sponsored search", "query expansion"], "combined": "Online expansion of rare queries for sponsored search Sponsored search systems are tasked with matching queries [[EENNDD]] tail queries; information search and retrieval; sponsored search; query expansion"}, "Perluasan dalam talian untuk pertanyaan yang jarang berlaku untuk carian yang ditaja Sistem carian yang ditaja ditugaskan dengan pertanyaan yang sesuai [[EENNDD]] pertanyaan ekor; pencarian dan pengambilan maklumat; carian tajaan; pengembangan pertanyaan"], [{"string": "Exploring social annotations for the semantic web No contact information provided yet.", "keywords": ["emergent semantics", "miscellaneous", "semantic web", "social bookmarks", "social annotation"], "combined": "Exploring social annotations for the semantic web No contact information provided yet. [[EENNDD]] emergent semantics; miscellaneous; semantic web; social bookmarks; social annotation"}, "Meneroka anotasi sosial untuk web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] semantik baru muncul; pelbagai; web semantik; penanda buku sosial; anotasi sosial"], [{"string": "Efficient and robust streaming provisioning in VPNs No contact information provided yet.", "keywords": ["streaming server placement", "vpns"], "combined": "Efficient and robust streaming provisioning in VPNs No contact information provided yet. [[EENNDD]] streaming server placement; vpns"}, "Penyediaan aliran yang cekap dan mantap dalam VPN Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penempatan pelayan streaming; vpns"], [{"string": "WebQuilt: a framework for capturing and visualizing the web experience An abstract is not available.", "keywords": ["usability evaluation", "web proxy", "web", "webquilt", "web visualization", "log file analysis"], "combined": "WebQuilt: a framework for capturing and visualizing the web experience An abstract is not available. [[EENNDD]] usability evaluation; web proxy; web; webquilt; web visualization; log file analysis"}, "WebQuilt: kerangka untuk menangkap dan memvisualisasikan pengalaman web Abstrak tidak tersedia. [[EENNDD]] penilaian kebolehgunaan; proksi web; laman web; webquilt; visualisasi web; analisis fail log"], [{"string": "HyperANF: approximating the neighbourhood function of very large graphs on a budget The neighbourhood function NG(t) of a graph G gives, for each t \u2208 N, the number of pairs of nodes x, y such that y is reachable from x in less that t hops. The neighbourhood function provides a wealth of information about the graph [10] (e.g., it easily allows one to compute its diameter), but it is very expensive to compute it exactly. Recently, the ANF algorithm [10] (approximate neighbourhood function) has been proposed with the purpose of approximating NG(t) on large graphs. We describe a breakthrough improvement over ANF in terms of speed and scalability. Our algorithm, called HyperANF, uses the new HyperLogLog counters [5] and combines them efficiently through broadword programming [8]; our implementation uses talk decomposition to exploit multi-core parallelism. With HyperANF, for the first time we can compute in a few hours the neighbourhood function of graphs with billions of nodes with a small error and good confidence using a standard workstation.", "keywords": ["neighbourhood function", "probabilistic algorithms", "shortest paths", "effective diameter", "probabilistic counters"], "combined": "HyperANF: approximating the neighbourhood function of very large graphs on a budget The neighbourhood function NG(t) of a graph G gives, for each t \u2208 N, the number of pairs of nodes x, y such that y is reachable from x in less that t hops. The neighbourhood function provides a wealth of information about the graph [10] (e.g., it easily allows one to compute its diameter), but it is very expensive to compute it exactly. Recently, the ANF algorithm [10] (approximate neighbourhood function) has been proposed with the purpose of approximating NG(t) on large graphs. We describe a breakthrough improvement over ANF in terms of speed and scalability. Our algorithm, called HyperANF, uses the new HyperLogLog counters [5] and combines them efficiently through broadword programming [8]; our implementation uses talk decomposition to exploit multi-core parallelism. With HyperANF, for the first time we can compute in a few hours the neighbourhood function of graphs with billions of nodes with a small error and good confidence using a standard workstation. [[EENNDD]] neighbourhood function; probabilistic algorithms; shortest paths; effective diameter; probabilistic counters"}, "HyperANF: menghampiri fungsi kejiranan grafik yang sangat besar dengan anggaran Fungsi kejiranan NG (t) dari graf G memberikan, untuk setiap t \u2208 N, bilangan pasang nod x, y sehingga y dapat dicapai dari x dalam masa kurang semestinya. Fungsi kejiranan memberikan banyak maklumat mengenai grafik [10] (mis., Ia membolehkan seseorang menghitung diameternya), tetapi sangat mahal untuk menghitungnya dengan tepat. Baru-baru ini, algoritma ANF [10] (fungsi lingkungan sekitar) telah dicadangkan dengan tujuan menghampiri NG (t) pada graf besar. Kami menerangkan peningkatan terunggul berbanding ANF dari segi kelajuan dan skalabiliti. Algoritma kami, yang dipanggil HyperANF, menggunakan pembilang HyperLogLog baru [5] dan menggabungkannya dengan cekap melalui pengaturcaraan kata kunci [8]; pelaksanaan kami menggunakan penguraian ceramah untuk mengeksploitasi paralelisme pelbagai teras. Dengan HyperANF, untuk pertama kalinya kita dapat mengira dalam beberapa jam fungsi persekitaran grafik dengan berbilion-bilion nod dengan ralat kecil dan keyakinan yang baik menggunakan stesen kerja standard. [[EENNDD]] fungsi kejiranan; algoritma probabilistik; jalan terpendek; diameter berkesan; pembilang kebarangkalian"], [{"string": "An incremental XSLT transformation processor for XML document manipulation No contact information provided yet.", "keywords": ["incremental transformations", "xml", "xslt", "authoring tools"], "combined": "An incremental XSLT transformation processor for XML document manipulation No contact information provided yet. [[EENNDD]] incremental transformations; xml; xslt; authoring tools"}, "Pemproses transformasi XSLT tambahan untuk manipulasi dokumen XML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] transformasi tambahan; xml; xslt; alat pengarang"], [{"string": "Buzz-based recommender system In this paper, we describe a buzz-based recommender system based on a large source of queries in an eCommerce application. The system detects bursts in query trends. These bursts are linked to external entities like news and inventory information to find the queries currently in-demand which we refer to as buzz queries. The system follows the paradigm of limited quantity merchandising, in the sense that on a per-day basis the system shows recommendations around a single buzz query with the intent of increasing user curiosity, and improving activity and stickiness on the site. A semantic neighborhood of the chosen buzz query is selected and appropriate recommendations are made on products that relate to this neighborhood.", "keywords": ["novelty", "buzz", "recommenders", "serendipity"], "combined": "Buzz-based recommender system In this paper, we describe a buzz-based recommender system based on a large source of queries in an eCommerce application. The system detects bursts in query trends. These bursts are linked to external entities like news and inventory information to find the queries currently in-demand which we refer to as buzz queries. The system follows the paradigm of limited quantity merchandising, in the sense that on a per-day basis the system shows recommendations around a single buzz query with the intent of increasing user curiosity, and improving activity and stickiness on the site. A semantic neighborhood of the chosen buzz query is selected and appropriate recommendations are made on products that relate to this neighborhood. [[EENNDD]] novelty; buzz; recommenders; serendipity"}, "Sistem pengesyorkan berasaskan buzz Dalam makalah ini, kami menerangkan sistem pengesyorkan berdasarkan buzz berdasarkan sumber pertanyaan besar dalam aplikasi eCommerce. Sistem ini mengesan ledakan tren pertanyaan. Letupan ini dihubungkan dengan entiti luaran seperti berita dan maklumat inventori untuk mencari pertanyaan yang sedang dalam permintaan yang kami sebut sebagai pertanyaan buzz. Sistem ini mengikuti paradigma penjualan barang dalam kuantiti terhad, dalam arti bahawa setiap hari sistem menunjukkan cadangan di sekitar satu pertanyaan buzz dengan tujuan untuk meningkatkan rasa ingin tahu pengguna, dan meningkatkan aktiviti dan kekakuan di laman web ini. Kawasan semantik dari pertanyaan buzz yang dipilih dipilih dan cadangan yang sesuai dibuat pada produk yang berkaitan dengan kawasan ini. [[EENNDD]] kebaruan; dengung; pengesyorkan; kebiasaan"], [{"string": "Improved techniques for result caching in web search engines Query processing is a major cost factor in operating large web search engines. In this paper, we study query result caching, one of the main techniques used to optimize query processing performance. Our first contribution is a study of result caching as a weighted caching problem. Most previous work has focused on optimizing cache hit ratios, but given that processing costs of queries can vary very significantly we argue that total cost savings also need to be considered. We describe and evaluate several algorithms for weighted result caching, and study the impact of Zipf-based query distributions on result caching. Our second and main contribution is a new set of feature-based cache eviction policies that achieve significant improvements over all previous methods, substantially narrowing the existing performance gap to the theoretically optimal (clairvoyant) method. Finally, using the same approach, we also obtain performance gains for the related problem of inverted list caching.", "keywords": ["search engines", "information search and retrieval", "weighted caching", "index caching", "result caching"], "combined": "Improved techniques for result caching in web search engines Query processing is a major cost factor in operating large web search engines. In this paper, we study query result caching, one of the main techniques used to optimize query processing performance. Our first contribution is a study of result caching as a weighted caching problem. Most previous work has focused on optimizing cache hit ratios, but given that processing costs of queries can vary very significantly we argue that total cost savings also need to be considered. We describe and evaluate several algorithms for weighted result caching, and study the impact of Zipf-based query distributions on result caching. Our second and main contribution is a new set of feature-based cache eviction policies that achieve significant improvements over all previous methods, substantially narrowing the existing performance gap to the theoretically optimal (clairvoyant) method. Finally, using the same approach, we also obtain performance gains for the related problem of inverted list caching. [[EENNDD]] search engines; information search and retrieval; weighted caching; index caching; result caching"}, "Teknik peningkatan hasil caching di mesin carian web Pemprosesan pertanyaan adalah faktor kos utama dalam operasi mesin carian web yang besar. Dalam makalah ini, kami mempelajari cache hasil pertanyaan, salah satu teknik utama yang digunakan untuk mengoptimumkan prestasi pemprosesan pertanyaan. Sumbangan pertama kami adalah kajian hasil caching sebagai masalah cached berwajaran. Sebilangan besar karya sebelumnya telah menumpukan pada mengoptimumkan nisbah hit cache, tetapi memandangkan kos pemprosesan pertanyaan boleh berbeza sangat ketara, kami berpendapat bahawa penjimatan kos total juga perlu dipertimbangkan. Kami menerangkan dan menilai beberapa algoritma untuk hasil cache yang berwajaran, dan mengkaji kesan pengedaran pertanyaan berdasarkan Zipf pada hasil cache. Sumbangan kedua dan utama kami adalah satu set dasar pengusiran cache berasaskan ciri yang mencapai peningkatan yang ketara berbanding semua kaedah sebelumnya, dengan ketara merapatkan jurang prestasi yang ada kepada kaedah teori (optimum) yang optimum. Akhirnya, dengan menggunakan pendekatan yang sama, kami juga memperoleh keuntungan prestasi untuk masalah berkaitan cache senarai terbalik. [[EENNDD]] enjin carian; carian dan pengambilan maklumat; cached berwajaran; caching indeks; hasil caching"], [{"string": "Web log mining with adaptive support thresholds No contact information provided yet.", "keywords": ["path traversal pattern", "web mining", "markov model"], "combined": "Web log mining with adaptive support thresholds No contact information provided yet. [[EENNDD]] path traversal pattern; web mining; markov model"}, "Perlombongan log web dengan ambang sokongan adaptif Belum ada maklumat hubungan yang diberikan. [[EENNDD]] corak melintasi jalan; perlombongan web; model markov"], [{"string": "Hierarchical substring caching for efficient content distribution to low-bandwidth clients No contact information provided yet.", "keywords": ["applications", "web caching", "compression", "www", "web proxies", "http"], "combined": "Hierarchical substring caching for efficient content distribution to low-bandwidth clients No contact information provided yet. [[EENNDD]] applications; web caching; compression; www; web proxies; http"}, "Pencetakan substring hierarki untuk pengedaran kandungan yang cekap kepada pelanggan lebar jalur rendah Belum ada maklumat hubungan yang diberikan. [[EENNDD]] aplikasi; caching web; pemampatan; www; proksi web; http"], [{"string": "Clustering query refinements by user intent We address the problem of clustering the refinements of a user search query. The clusters computed by our proposed algorithm can be used to improve the selection and placement of the query suggestions proposed by a search engine, and can also serve to summarize the different aspects of information relevant to the original user query. Our algorithm clusters refinements based on their likely underlying user intents by combining document click and session co-occurrence information. At its core, our algorithm operates by performing multiple random walks on a Markov graph that approximates user search behavior. A user study performed on top search engine queries shows that our clusters are rated better than corresponding clusters computed using approaches that use only document click or only sessions co-occurrence information.", "keywords": ["query refinements", "random walks"], "combined": "Clustering query refinements by user intent We address the problem of clustering the refinements of a user search query. The clusters computed by our proposed algorithm can be used to improve the selection and placement of the query suggestions proposed by a search engine, and can also serve to summarize the different aspects of information relevant to the original user query. Our algorithm clusters refinements based on their likely underlying user intents by combining document click and session co-occurrence information. At its core, our algorithm operates by performing multiple random walks on a Markov graph that approximates user search behavior. A user study performed on top search engine queries shows that our clusters are rated better than corresponding clusters computed using approaches that use only document click or only sessions co-occurrence information. [[EENNDD]] query refinements; random walks"}, "Penggabungan permintaan klaster dengan maksud pengguna Kami mengatasi masalah pengelompokan penyempurnaan pertanyaan carian pengguna. Kluster yang dikira oleh algoritma yang dicadangkan kami dapat digunakan untuk meningkatkan pemilihan dan penempatan cadangan pertanyaan yang diusulkan oleh mesin pencari, dan juga dapat berfungsi untuk merangkum berbagai aspek maklumat yang relevan dengan pertanyaan pengguna asal. Penyempurnaan kluster algoritma kami berdasarkan kemungkinan pengguna yang mendasari dengan menggabungkan maklumat klik dan sesi sesi dokumen. Pada intinya, algoritma kami beroperasi dengan melakukan beberapa jalan rawak pada grafik Markov yang menghampiri tingkah laku carian pengguna. Kajian pengguna yang dilakukan pada pertanyaan mesin pencari teratas menunjukkan bahawa kluster kami dinilai lebih baik daripada kluster yang sesuai yang dihitung menggunakan pendekatan yang hanya menggunakan klik dokumen atau hanya maklumat sesi pertemuan. [[EENNDD]] penyempurnaan pertanyaan; jalan rawak"], [{"string": "Improving portlet interoperability through deep annotation No contact information provided yet.", "keywords": ["event", "interoperability", "reusable software", "portal ontology", "deep-annotation", "software architectures", "data-flow", "portlet interoperability"], "combined": "Improving portlet interoperability through deep annotation No contact information provided yet. [[EENNDD]] event; interoperability; reusable software; portal ontology; deep-annotation; software architectures; data-flow; portlet interoperability"}, "Meningkatkan interoperabiliti portlet melalui penjelasan mendalam Belum ada maklumat hubungan yang diberikan. [[EENNDD]] acara; saling kendali; perisian yang boleh digunakan semula; ontologi portal; anotasi mendalam; seni bina perisian; aliran data; kebolehoperasian portlet"], [{"string": "A user-tunable approach to marketplace search The notion of relevance is key to the performance of search engines as they interpret the user queries and respond with matching results. Online search engines have used other features beyond pure IR features to return relevant matching documents. However, over-emphasis on relevance could lead to redundancy in search results. In document search, diversity is simply the variety of documents that span the result set. In an online marketplace the diversity in the result set is represented by items for sale by different sellers at different prices with different sales options. For such a marketplace, in order to minimize query abandonment and the risk of dissatisfaction to the average user, several factors like diversity, trust and value need to be taken into account. Previous work in this field [4] has shown an impossibility result that there exists no such function that can optimize for all these factors. Since these factors and the measures associated with the factors could be subjective we take an approach of giving the control back to the user.", "keywords": ["ecommerce", "diversity", "trust", "relevance", "value", "search"], "combined": "A user-tunable approach to marketplace search The notion of relevance is key to the performance of search engines as they interpret the user queries and respond with matching results. Online search engines have used other features beyond pure IR features to return relevant matching documents. However, over-emphasis on relevance could lead to redundancy in search results. In document search, diversity is simply the variety of documents that span the result set. In an online marketplace the diversity in the result set is represented by items for sale by different sellers at different prices with different sales options. For such a marketplace, in order to minimize query abandonment and the risk of dissatisfaction to the average user, several factors like diversity, trust and value need to be taken into account. Previous work in this field [4] has shown an impossibility result that there exists no such function that can optimize for all these factors. Since these factors and the measures associated with the factors could be subjective we take an approach of giving the control back to the user. [[EENNDD]] ecommerce; diversity; trust; relevance; value; search"}, "Pendekatan pengguna yang dapat disesuaikan untuk pencarian pasar Idea yang relevan adalah kunci kepada prestasi mesin pencari kerana mereka menafsirkan pertanyaan pengguna dan bertindak balas dengan hasil yang sesuai. Enjin carian dalam talian telah menggunakan ciri lain di luar ciri IR tulen untuk mengembalikan dokumen yang sesuai. Namun, penekanan yang berlebihan terhadap relevansi boleh menyebabkan redundansi dalam hasil carian. Dalam pencarian dokumen, kepelbagaian hanyalah pelbagai dokumen yang merangkumi hasilnya. Di pasaran dalam talian, kepelbagaian dalam set hasil ditunjukkan oleh item yang dijual oleh penjual yang berbeza dengan harga yang berbeza dengan pilihan penjualan yang berbeza. Untuk pasaran seperti itu, untuk meminimumkan pengabaian pertanyaan dan risiko ketidakpuasan kepada pengguna rata-rata, beberapa faktor seperti kepelbagaian, kepercayaan dan nilai perlu dipertimbangkan. Kerja sebelumnya dalam bidang ini [4] telah menunjukkan hasil kemustahilan bahawa tidak ada fungsi yang dapat mengoptimumkan semua faktor ini. Oleh kerana faktor-faktor ini dan langkah-langkah yang berkaitan dengan faktor-faktor tersebut boleh menjadi subjektif, kami mengambil pendekatan untuk memberikan kawalan kembali kepada pengguna. [[EENNDD]] e-dagang; kepelbagaian; kepercayaan; perkaitan; nilai; cari"], [{"string": "Dealing with different distributions in learning from No contact information provided yet.", "keywords": ["classification", "positive and unlabeled learning"], "combined": "Dealing with different distributions in learning from No contact information provided yet. [[EENNDD]] classification; positive and unlabeled learning"}, "Berurusan dengan sebaran yang berbeza dalam pembelajaran dari Belum ada maklumat hubungan yang diberikan. [[EENNDD]] klasifikasi; pembelajaran yang positif dan tidak berlabel"], [{"string": "Small world peer networks in distributed web search No contact information provided yet.", "keywords": ["small world", "information search and retrieval", "peer collaborative search", "distributed systems", "topical crawlers"], "combined": "Small world peer networks in distributed web search No contact information provided yet. [[EENNDD]] small world; information search and retrieval; peer collaborative search; distributed systems; topical crawlers"}, "Rangkaian rakan sebaya kecil dalam carian web yang diedarkan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] dunia kecil; pencarian dan pengambilan maklumat; carian rakan sebaya; sistem yang diedarkan; crawler topikal"], [{"string": "Exploiting social context for review quality prediction Online reviews in which users publish detailed commentary about their experiences and opinions with products, services, or events are extremely valuable to users who rely on them to make informed decisions. However, reviews vary greatly in quality and are constantly increasing in number, therefore, automatic assessment of review helpfulness is of growing importance. Previous work has addressed the problem by treating a review as a stand-alone document, extracting features from the review text, and learning a function based on these features for predicting the review quality. In this work, we exploit contextual information about authors' identities and social networks for improving review quality prediction. We propose a generic framework for incorporating social context information by adding regularization constraints to the text-based predictor. Our approach can effectively use the social context information available for large quantities of unlabeled reviews. It also has the advantage that the resulting predictor is usable even when social context is unavailable. We validate our framework within a real commerce portal and experimentally demonstrate that using social context information can help improve the accuracy of review quality prediction especially when the available training data is sparse.", "keywords": ["review helpfulness", "learning", "miscellaneous", "graph regularization", "social network", "review quality"], "combined": "Exploiting social context for review quality prediction Online reviews in which users publish detailed commentary about their experiences and opinions with products, services, or events are extremely valuable to users who rely on them to make informed decisions. However, reviews vary greatly in quality and are constantly increasing in number, therefore, automatic assessment of review helpfulness is of growing importance. Previous work has addressed the problem by treating a review as a stand-alone document, extracting features from the review text, and learning a function based on these features for predicting the review quality. In this work, we exploit contextual information about authors' identities and social networks for improving review quality prediction. We propose a generic framework for incorporating social context information by adding regularization constraints to the text-based predictor. Our approach can effectively use the social context information available for large quantities of unlabeled reviews. It also has the advantage that the resulting predictor is usable even when social context is unavailable. We validate our framework within a real commerce portal and experimentally demonstrate that using social context information can help improve the accuracy of review quality prediction especially when the available training data is sparse. [[EENNDD]] review helpfulness; learning; miscellaneous; graph regularization; social network; review quality"}, "Mengeksploitasi konteks sosial untuk meramalkan ramalan kualiti Ulasan dalam talian di mana pengguna menerbitkan komen terperinci mengenai pengalaman dan pendapat mereka dengan produk, perkhidmatan, atau acara sangat berharga bagi pengguna yang bergantung pada mereka untuk membuat keputusan yang tepat. Walau bagaimanapun, tinjauan sangat berbeza dari segi kualiti dan jumlahnya terus meningkat, oleh itu, penilaian automatik untuk membantu semakan semakin penting. Karya sebelumnya telah mengatasi masalah tersebut dengan menganggap tinjauan sebagai dokumen yang berdiri sendiri, mengekstrak ciri dari teks ulasan, dan mempelajari fungsi berdasarkan ciri-ciri ini untuk meramalkan kualiti tinjauan. Dalam karya ini, kami memanfaatkan maklumat kontekstual mengenai identiti pengarang dan rangkaian sosial untuk meningkatkan ramalan kualiti tinjauan. Kami mencadangkan kerangka kerja generik untuk memasukkan maklumat konteks sosial dengan menambahkan batasan regularisasi pada peramal berasaskan teks. Pendekatan kami dengan berkesan dapat menggunakan maklumat konteks sosial yang tersedia untuk sejumlah besar ulasan tanpa label. Ia juga mempunyai kelebihan bahawa ramalan yang dihasilkan dapat digunakan walaupun konteks sosial tidak tersedia. Kami mengesahkan kerangka kerja kami dalam portal perdagangan sebenar dan menunjukkan secara eksperimen bahawa menggunakan maklumat konteks sosial dapat membantu meningkatkan ketepatan ramalan kualiti tinjauan terutama ketika data latihan yang tersedia jarang. [[EENNDD]] meninjau kebaikan; belajar; pelbagai; pengaturcaraan grafik; rangkaian sosial; mengkaji semula kualiti"], [{"string": "Citizen sensor data mining, social media analytics and development centric web applications With the rapid rise in the popularity of social media (500M+ Facebook users, 100M+ twitter users), and near ubiquitous mobile access (4+ billion actively-used mobile phones), the sharing of observations and opinions has become common-place (nearly 100M tweets a day, 1.8 trillion SMSs in US last year). This has given us an unprecedented access to the pulse of a populace and the ability to perform analytics on social data to support a variety of socially intelligent applications -- be it towards targeted online content delivery, crisis management, organizing revolutions or promoting social development in underdeveloped and developing countries.", "keywords": ["social signals", "social media analysis", "people-content-network view of social media", "mobile development application", "on-line information services", "citizen sensing", "user generated content", "semantic social mashup", "social development application", "semantic social web"], "combined": "Citizen sensor data mining, social media analytics and development centric web applications With the rapid rise in the popularity of social media (500M+ Facebook users, 100M+ twitter users), and near ubiquitous mobile access (4+ billion actively-used mobile phones), the sharing of observations and opinions has become common-place (nearly 100M tweets a day, 1.8 trillion SMSs in US last year). This has given us an unprecedented access to the pulse of a populace and the ability to perform analytics on social data to support a variety of socially intelligent applications -- be it towards targeted online content delivery, crisis management, organizing revolutions or promoting social development in underdeveloped and developing countries. [[EENNDD]] social signals; social media analysis; people-content-network view of social media; mobile development application; on-line information services; citizen sensing; user generated content; semantic social mashup; social development application; semantic social web"}, "Perlombongan data sensor Citizen, analitik media sosial dan aplikasi web yang berpusat pada pengembangan Dengan peningkatan populariti media sosial yang pesat (500 juta + pengguna Facebook, 100 juta + pengguna twitter), dan hampir di mana-mana akses mudah alih (4+ bilion telefon bimbit yang digunakan secara aktif), perkongsian pemerhatian dan pendapat telah menjadi hal biasa (hampir 100 juta tweet sehari, 1.8 trilion SMS di AS tahun lalu). Ini telah memberi kita akses yang belum pernah terjadi sebelumnya ke nadi penduduk dan kemampuan untuk melakukan analisis data sosial untuk menyokong pelbagai aplikasi pintar sosial - baik untuk penyampaian kandungan dalam talian yang disasarkan, pengurusan krisis, mengatur revolusi atau mempromosikan pembangunan sosial di negara maju dan membangun. [[EENNDD]] isyarat sosial; analisis media sosial; pandangan orang-kandungan-rangkaian media sosial; aplikasi pembangunan mudah alih; perkhidmatan maklumat dalam talian; sensasi warganegara; kandungan yang dihasilkan pengguna; mashup sosial semantik; aplikasi pembangunan sosial; laman sosial semantik"], [{"string": "Rapid prototyping of web applications combining domain specific languages and model driven design No contact information provided yet.", "keywords": ["hypermedia authoring", "model-based designs"], "combined": "Rapid prototyping of web applications combining domain specific languages and model driven design No contact information provided yet. [[EENNDD]] hypermedia authoring; model-based designs"}, "Prototaip cepat aplikasi web yang menggabungkan bahasa khusus domain dan reka bentuk berdasarkan model Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] pengarang hipermedia; reka bentuk berasaskan model"], [{"string": "Detecting spam web pages through content analysis No contact information provided yet.", "keywords": ["data mining", "web spam", "hypertext/hypermedia", "miscellaneous", "web characterization", "web pages"], "combined": "Detecting spam web pages through content analysis No contact information provided yet. [[EENNDD]] data mining; web spam; hypertext/hypermedia; miscellaneous; web characterization; web pages"}, "Mengesan laman web spam melalui analisis kandungan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlombongan data; spam web; hiperteks / hipermedia; pelbagai; pencirian web; laman sesawang"], [{"string": "WAND: a meta-data maintenance system over the internet No contact information provided yet.", "keywords": ["maintenance", "meta-data", "peer-to-peer", "wide-area distributed file system"], "combined": "WAND: a meta-data maintenance system over the internet No contact information provided yet. [[EENNDD]] maintenance; meta-data; peer-to-peer; wide-area distributed file system"}, "WAND: sistem penyelenggaraan meta-data melalui internet Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penyelenggaraan; meta-data; rakan sebaya; sistem fail diedarkan kawasan luas"], [{"string": "The semantic webscape: a view of the semantic web No contact information provided yet.", "keywords": ["markup languages", "content analysis and indexing", "ontology", "information search and retrieval", "semantic web", "rss"], "combined": "The semantic webscape: a view of the semantic web No contact information provided yet. [[EENNDD]] markup languages; content analysis and indexing; ontology; information search and retrieval; semantic web; rss"}, "Lanskap semantik: paparan web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] bahasa markup; analisis kandungan dan pengindeksan; ontologi; pencarian dan pengambilan maklumat; web semantik; rss"], [{"string": "Static approximation of dynamically generated Web pages No contact information provided yet.", "keywords": ["server-side scripting", "context-free grammars", "html validation", "cross-site scripting", "static analysis"], "combined": "Static approximation of dynamically generated Web pages No contact information provided yet. [[EENNDD]] server-side scripting; context-free grammars; html validation; cross-site scripting; static analysis"}, "Penghampiran statik halaman Web yang dihasilkan secara dinamik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] skrip sisi pelayan; tatabahasa bebas konteks; pengesahan html; skrip merentas laman web; analisis statik"], [{"string": "How semantics make better wikis No contact information provided yet.", "keywords": ["semantic annotation", "semantic wikis", "wikis", "semantic web", "information access"], "combined": "How semantics make better wikis No contact information provided yet. [[EENNDD]] semantic annotation; semantic wikis; wikis; semantic web; information access"}, "Bagaimana semantik menjadikan wiki lebih baik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] anotasi semantik; wiki semantik; wiki; web semantik; capaian maklumat"], [{"string": "Adaptive push-pull: disseminating dynamic web data An abstract is not available.", "keywords": ["dynamic data", "data dissemination", "resiliency", "scalability", "temporal coherency", "pull", "push", "world wide web"], "combined": "Adaptive push-pull: disseminating dynamic web data An abstract is not available. [[EENNDD]] dynamic data; data dissemination; resiliency; scalability; temporal coherency; pull; push; world wide web"}, "Push-pull adaptif: menyebarkan data web dinamik Abstrak tidak tersedia. [[EENNDD]] data dinamik; penyebaran data; ketahanan; skalabiliti; koherensi temporal; tarik; tolak; web seluruh dunia"], [{"string": "Transforming web contents into a storybook with dialogues and animations No contact information provided yet.", "keywords": ["dialogue", "animation", "information presentation", "agent", "media conversion"], "combined": "Transforming web contents into a storybook with dialogues and animations No contact information provided yet. [[EENNDD]] dialogue; animation; information presentation; agent; media conversion"}, "Mengubah kandungan web menjadi buku cerita dengan dialog dan animasi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] dialog; animasi; penyampaian maklumat; ejen; penukaran media"], [{"string": "Privacy in dynamic social networks Anonymization of social networks before they are published or shared has become an important research question. Recent work on anonymizing social networks has looked at privacy preserving techniques for publishing a single instance of the network. However, social networks evolve and a single instance is inadequate for analyzing the evolution of the social network or for performing any longitudinal data analysis. We study the problem of repeatedly publishing social network data as the network evolves, while preserving privacy of users. Publishing multiple instances of the same network independently has privacy risks, since stitching the information together may allow an adversary to identify users in the networks.", "keywords": ["re-publication", "dynamic networks"], "combined": "Privacy in dynamic social networks Anonymization of social networks before they are published or shared has become an important research question. Recent work on anonymizing social networks has looked at privacy preserving techniques for publishing a single instance of the network. However, social networks evolve and a single instance is inadequate for analyzing the evolution of the social network or for performing any longitudinal data analysis. We study the problem of repeatedly publishing social network data as the network evolves, while preserving privacy of users. Publishing multiple instances of the same network independently has privacy risks, since stitching the information together may allow an adversary to identify users in the networks. [[EENNDD]] re-publication; dynamic networks"}, "Privasi dalam rangkaian sosial yang dinamik Anonimisasi rangkaian sosial sebelum diterbitkan atau dikongsi telah menjadi persoalan kajian penting. Kerja terbaru dalam menganonimkan jejaring sosial telah melihat teknik menjaga privasi untuk menerbitkan satu contoh rangkaian. Walau bagaimanapun, rangkaian sosial berkembang dan satu contoh tidak mencukupi untuk menganalisis evolusi rangkaian sosial atau untuk melakukan analisis data membujur. Kami mengkaji masalah menerbitkan data rangkaian sosial berulang kali ketika rangkaian berkembang, sambil menjaga privasi pengguna. Menerbitkan beberapa contoh rangkaian yang sama secara berasingan mempunyai risiko privasi, kerana menyatukan maklumat bersama-sama dapat memungkinkan musuh mengenali pengguna dalam rangkaian. [[EENNDD]] penerbitan semula; rangkaian dinamik"], [{"string": "Detectives: detecting coalition hit inflation attacks in advertising networks streams Click fraud is jeopardizing the industry of Internet advertising. Internet advertising is crucial for the thriving of the entire Internet, since it allows producers to advertise their products, and hence contributes to the well being of e-commerce. Moreover, advertising supports the intellectual value of the Internet by covering the running expenses of publishing content. Some content publishers are dishonest, and use automation to generate traffic to defraud the advertisers. Similarly, some advertisers automate clicks on the advertisements of their competitors to deplete their competitors' advertising budgets. This paper describes the advertising network model, and focuses on the most sophisticated type of fraud, which involves coalitions among fraudsters. We build on several published theoretical results to devise the Similarity-Seeker algorithm that discovers coalitions made by pairs of fraudsters. We then generalize the solution to coalitions of arbitrary sizes. Before deploying our system on a real network, we conducted comprehensive experiments on data samples for proof of concept. The results were very accurate. We detected several coalitions, formed using various techniques, and spanning numerous sites. This reveals the generality of our model and approach.", "keywords": ["real data experiments", "similarity-sensitive sampling", "coalition fraud attacks", "cliques enumeration", "approximate set similarity", "click spam detection"], "combined": "Detectives: detecting coalition hit inflation attacks in advertising networks streams Click fraud is jeopardizing the industry of Internet advertising. Internet advertising is crucial for the thriving of the entire Internet, since it allows producers to advertise their products, and hence contributes to the well being of e-commerce. Moreover, advertising supports the intellectual value of the Internet by covering the running expenses of publishing content. Some content publishers are dishonest, and use automation to generate traffic to defraud the advertisers. Similarly, some advertisers automate clicks on the advertisements of their competitors to deplete their competitors' advertising budgets. This paper describes the advertising network model, and focuses on the most sophisticated type of fraud, which involves coalitions among fraudsters. We build on several published theoretical results to devise the Similarity-Seeker algorithm that discovers coalitions made by pairs of fraudsters. We then generalize the solution to coalitions of arbitrary sizes. Before deploying our system on a real network, we conducted comprehensive experiments on data samples for proof of concept. The results were very accurate. We detected several coalitions, formed using various techniques, and spanning numerous sites. This reveals the generality of our model and approach. [[EENNDD]] real data experiments; similarity-sensitive sampling; coalition fraud attacks; cliques enumeration; approximate set similarity; click spam detection"}, "Detektif: mengesan koalisi menyerang inflasi di rangkaian rangkaian iklan Penipuan klik membahayakan industri pengiklanan Internet. Pengiklanan internet sangat penting untuk berkembangnya seluruh Internet, kerana membolehkan pengeluar mengiklankan produk mereka, dan dengan itu menyumbang kepada kesejahteraan e-dagang. Selain itu, pengiklanan menyokong nilai intelektual Internet dengan menanggung perbelanjaan mengurus kandungan penerbitan. Beberapa penerbit kandungan tidak jujur, dan menggunakan automasi untuk menghasilkan lalu lintas untuk menipu pengiklan. Begitu juga, sebilangan pengiklan mengautomasikan klik pada iklan pesaing mereka untuk menghabiskan anggaran iklan pesaing mereka. Makalah ini menerangkan model rangkaian pengiklanan, dan memfokuskan pada jenis penipuan yang paling canggih, yang melibatkan gabungan di antara penipu. Kami membina beberapa hasil teori yang diterbitkan untuk merangka algoritma Similarity-Seeker yang menemui gabungan yang dibuat oleh pasangan penipu. Kami kemudian menggeneralisasikan penyelesaian untuk gabungan ukuran sewenang-wenangnya. Sebelum menggunakan sistem kami di rangkaian nyata, kami melakukan eksperimen menyeluruh pada sampel data untuk bukti konsep. Hasilnya sangat tepat. Kami mengesan beberapa gabungan, dibentuk menggunakan pelbagai teknik, dan merangkumi banyak laman web. Ini menunjukkan keunikan model dan pendekatan kami. [[EENNDD]] eksperimen data sebenar; persampelan sensitif kesamaan; serangan penipuan gabungan; penghitungan klek; kesamaan set anggaran; klik pengesanan spam"], [{"string": "Unsupervised learning of soft patterns for generating definitions from online news No contact information provided yet.", "keywords": ["definition generation", "information search and retrieval", "soft patterns", "definitional question answering", "unsupervised learning", "pseudo-relevance feedback"], "combined": "Unsupervised learning of soft patterns for generating definitions from online news No contact information provided yet. [[EENNDD]] definition generation; information search and retrieval; soft patterns; definitional question answering; unsupervised learning; pseudo-relevance feedback"}, "Pembelajaran corak lembut yang tidak diawasi untuk menghasilkan definisi dari berita dalam talian Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penjanaan definisi; pencarian dan pengambilan maklumat; corak lembut; menjawab soalan definisi; pembelajaran tanpa pengawasan; maklum balas berkaitan pseudo"], [{"string": "Human wayfinding in information networks Navigating information spaces is an essential part of our everyday lives, and in order to design efficient and user-friendly information systems, it is important to understand how humans navigate and find the information they are looking for. We perform a large-scale study of human wayfinding, in which, given a network of links between the concepts of Wikipedia, people play a game of finding a short path from a given start to a given target concept by following hyperlinks. What distinguishes our setup from other studies of human Web-browsing behavior is that in our case people navigate a graph of connections between concepts, and that the exact goal of the navigation is known ahead of time. We study more than 30,000 goal-directed human search paths and identify strategies people use when navigating information spaces. We find that human wayfinding, while mostly very efficient, differs from shortest paths in characteristic ways. Most subjects navigate through high-degree hubs in the early phase, while their search is guided by content features thereafter. We also observe a trade-off between simplicity and efficiency: conceptually simple solutions are more common but tend to be less efficient than more complex ones. Finally, we consider the task of predicting the target a user is trying to reach. We design a model and an efficient learning algorithm. Such predictive models of human wayfinding can be applied in intelligent browsing interfaces.", "keywords": ["information networks", "browsing", "wikipedia", "human computation", "wikispeedia"], "combined": "Human wayfinding in information networks Navigating information spaces is an essential part of our everyday lives, and in order to design efficient and user-friendly information systems, it is important to understand how humans navigate and find the information they are looking for. We perform a large-scale study of human wayfinding, in which, given a network of links between the concepts of Wikipedia, people play a game of finding a short path from a given start to a given target concept by following hyperlinks. What distinguishes our setup from other studies of human Web-browsing behavior is that in our case people navigate a graph of connections between concepts, and that the exact goal of the navigation is known ahead of time. We study more than 30,000 goal-directed human search paths and identify strategies people use when navigating information spaces. We find that human wayfinding, while mostly very efficient, differs from shortest paths in characteristic ways. Most subjects navigate through high-degree hubs in the early phase, while their search is guided by content features thereafter. We also observe a trade-off between simplicity and efficiency: conceptually simple solutions are more common but tend to be less efficient than more complex ones. Finally, we consider the task of predicting the target a user is trying to reach. We design a model and an efficient learning algorithm. Such predictive models of human wayfinding can be applied in intelligent browsing interfaces. [[EENNDD]] information networks; browsing; wikipedia; human computation; wikispeedia"}, "Pencarian manusia dalam rangkaian maklumat Menavigasi ruang maklumat adalah bahagian penting dalam kehidupan seharian kita, dan untuk merancang sistem maklumat yang cekap dan mesra pengguna, penting untuk memahami bagaimana manusia menavigasi dan mencari maklumat yang mereka cari. Kami melakukan kajian skala besar mengenai cara mencari manusia, di mana, dengan adanya rangkaian hubungan antara konsep Wikipedia, orang memainkan permainan mencari jalan pendek dari awal tertentu hingga konsep sasaran tertentu dengan mengikuti hyperlink. Apa yang membezakan penyediaan kami dari kajian lain mengenai tingkah laku melayari Web manusia adalah bahawa dalam kes kita orang menavigasi grafik hubungan antara konsep, dan bahawa tujuan navigasi yang tepat diketahui lebih awal. Kami mengkaji lebih daripada 30,000 jalan pencarian manusia yang diarahkan kepada tujuan dan mengenal pasti strategi yang digunakan orang ketika menavigasi ruang maklumat. Kami mendapati bahawa cara mencari manusia, walaupun kebanyakannya sangat cekap, berbeza dengan jalan terpendek dengan cara ciri. Sebilangan besar subjek menavigasi melalui hub darjah tinggi pada fasa awal, sementara pencarian mereka dipandu oleh ciri kandungan selepas itu. Kami juga melihat pertukaran antara kesederhanaan dan kecekapan: penyelesaian mudah secara konseptual lebih biasa tetapi cenderung kurang efisien daripada penyelesaian yang lebih kompleks. Akhirnya, kami menganggap tugas untuk meramalkan sasaran yang ingin dicapai oleh pengguna. Kami merancang model dan algoritma pembelajaran yang cekap. Model ramalan penemuan jalan manusia seperti itu dapat diterapkan dalam antara muka pelayaran pintar. [[EENNDD]] rangkaian maklumat; melayari; wikipedia; pengiraan manusia; wikispeedia"], [{"string": "A foundation for tool based mobility support for visually impaired web users No contact information provided yet.", "keywords": ["mobility support tool", "travel", "mobility", "visual impairment", "travel objects"], "combined": "A foundation for tool based mobility support for visually impaired web users No contact information provided yet. [[EENNDD]] mobility support tool; travel; mobility; visual impairment; travel objects"}, "Asas untuk sokongan mobiliti berasaskan alat untuk pengguna web cacat penglihatan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] alat sokongan mobiliti; perjalanan; mobiliti; masalah penglihatan; objek perjalanan"], [{"string": "User-centric content freshness metrics for search engines In order to return relevant search results, a search engine must keep its local repository synchronized to the Web, but it is usually impossible to attain perfect freshness. Hence, it is vital for a production search engine continually to monitor and improve repository freshness. Most previous freshness metrics, formulated in the context of developing better synchronization policies, focused on the web crawler while ignoring other parts of a search engine. But, the freshness of documents in a web crawler does not necessarily translate directly into the freshness of search results as seen by users. We propose metrics for measuring freshness from a user's perspective, which take into account the latency between when documents are crawled and when they are viewed by users, as well as the variation in user click and view frequency among different documents. We also describe a practical implementation of these metrics that were used in a production search engine.", "keywords": ["search engine", "crawling", "monitoring", "freshness", "latency", "metrics", "document age"], "combined": "User-centric content freshness metrics for search engines In order to return relevant search results, a search engine must keep its local repository synchronized to the Web, but it is usually impossible to attain perfect freshness. Hence, it is vital for a production search engine continually to monitor and improve repository freshness. Most previous freshness metrics, formulated in the context of developing better synchronization policies, focused on the web crawler while ignoring other parts of a search engine. But, the freshness of documents in a web crawler does not necessarily translate directly into the freshness of search results as seen by users. We propose metrics for measuring freshness from a user's perspective, which take into account the latency between when documents are crawled and when they are viewed by users, as well as the variation in user click and view frequency among different documents. We also describe a practical implementation of these metrics that were used in a production search engine. [[EENNDD]] search engine; crawling; monitoring; freshness; latency; metrics; document age"}, "Metrik kesegaran kandungan yang berpusatkan pengguna untuk enjin carian Untuk mengembalikan hasil carian yang relevan, mesin carian mesti memastikan repositori tempatannya disegerakkan ke Web, tetapi biasanya mustahil untuk mencapai kesegaran yang sempurna. Oleh itu, sangat penting bagi mesin carian pengeluaran untuk terus memantau dan meningkatkan kesegaran repositori. Sebilangan besar metrik kesegaran sebelumnya, yang dirumuskan dalam konteks mengembangkan dasar penyegerakan yang lebih baik, tertumpu pada perayap web sambil mengabaikan bahagian lain dari mesin pencari. Tetapi, kesegaran dokumen dalam perayap web tidak semestinya diterjemahkan secara langsung ke kesegaran hasil carian seperti yang dilihat oleh pengguna. Kami mencadangkan metrik untuk mengukur kesegaran dari perspektif pengguna, yang mempertimbangkan kependaman antara ketika dokumen dirayapi dan ketika mereka dilihat oleh pengguna, serta variasi klik pengguna dan frekuensi tontonan di antara dokumen yang berbeza. Kami juga menerangkan pelaksanaan praktikal metrik ini yang digunakan dalam mesin carian pengeluaran. [[EENNDD]] enjin carian; merangkak; pemantauan; kesegaran; kependaman; sukatan; usia dokumen"], [{"string": "Framework and algorithms for network bucket testing Bucket testing, also known as split testing, A/B testing, or 0/1 testing, is a widely used method for evaluating users' satisfaction with new features, products, or services. In order not to expose the whole user base to the new service, the mean user satisfaction rate is estimated by exposing the service only to a few uniformly chosen random users. In a recent work, Backstrom and Kleinberg, defined the notion of network bucket testing for social services. In this context, users' interactions are only valid for measurement if some minimal number of their friends are also given the service. The goal is to estimate the mean user satisfaction rate while providing the service to the least number of users. This constraint makes uniform sampling, which is optimal for the traditional case, grossly inefficient. In this paper we introduce a simple general framework for designing and evaluating sampling techniques for network bucket testing. The framework is constructed in a way that sampling algorithms are only required to generate sets of users to which the service should be provided. Given an algorithm, the framework produces an unbiased user satisfaction rate estimator and a corresponding variance bound for any network and any user satisfaction function. Furthermore, we present several simple sampling algorithms that are evaluated using both synthetic and real social networks. Our experiments corroborate the theoretical results and demonstrate the effectiveness of the proposed framework and algorithms.", "keywords": ["bucket testing", "network bucket testing", "a/b testing", "social networks", "nonnumerical algorithms and problems"], "combined": "Framework and algorithms for network bucket testing Bucket testing, also known as split testing, A/B testing, or 0/1 testing, is a widely used method for evaluating users' satisfaction with new features, products, or services. In order not to expose the whole user base to the new service, the mean user satisfaction rate is estimated by exposing the service only to a few uniformly chosen random users. In a recent work, Backstrom and Kleinberg, defined the notion of network bucket testing for social services. In this context, users' interactions are only valid for measurement if some minimal number of their friends are also given the service. The goal is to estimate the mean user satisfaction rate while providing the service to the least number of users. This constraint makes uniform sampling, which is optimal for the traditional case, grossly inefficient. In this paper we introduce a simple general framework for designing and evaluating sampling techniques for network bucket testing. The framework is constructed in a way that sampling algorithms are only required to generate sets of users to which the service should be provided. Given an algorithm, the framework produces an unbiased user satisfaction rate estimator and a corresponding variance bound for any network and any user satisfaction function. Furthermore, we present several simple sampling algorithms that are evaluated using both synthetic and real social networks. Our experiments corroborate the theoretical results and demonstrate the effectiveness of the proposed framework and algorithms. [[EENNDD]] bucket testing; network bucket testing; a/b testing; social networks; nonnumerical algorithms and problems"}, "Rangka kerja dan algoritma untuk pengujian bucket rangkaian Ujian bucket, juga dikenali sebagai ujian split, pengujian A / B, atau pengujian 0/1, adalah kaedah yang banyak digunakan untuk menilai kepuasan pengguna dengan fitur, produk, atau perkhidmatan baru. Agar tidak memaparkan seluruh pangkalan pengguna kepada perkhidmatan baru, tingkat kepuasan pengguna rata-rata dianggarkan dengan memperlihatkan perkhidmatan hanya kepada beberapa pengguna rawak yang dipilih secara seragam. Dalam karya baru-baru ini, Backstrom dan Kleinberg, mendefinisikan pengertian ujian baldi rangkaian untuk perkhidmatan sosial. Dalam konteks ini, interaksi pengguna hanya berlaku untuk pengukuran sekiranya sebilangan minimum rakan mereka juga diberi layanan. Tujuannya adalah untuk mengira kadar kepuasan pengguna rata-rata sambil memberikan perkhidmatan kepada bilangan pengguna paling sedikit. Kekangan ini menjadikan persampelan seragam, yang optimum untuk kes tradisional, sangat tidak efisien. Dalam makalah ini kami memperkenalkan kerangka umum sederhana untuk merancang dan menilai teknik pensampelan untuk pengujian baldi rangkaian. Kerangka ini dibina dengan cara bahawa algoritma pensampelan hanya diperlukan untuk menghasilkan set pengguna yang harus diberikan perkhidmatan. Dengan menggunakan algoritma, kerangka menghasilkan penghitung kadar kepuasan pengguna yang tidak berat sebelah dan varians yang sesuai untuk setiap rangkaian dan fungsi kepuasan pengguna. Selanjutnya, kami menyajikan beberapa algoritma persampelan mudah yang dinilai menggunakan rangkaian sosial sintetik dan nyata. Eksperimen kami mengesahkan hasil teori dan menunjukkan keberkesanan kerangka dan algoritma yang dicadangkan. [[EENNDD]] ujian baldi; ujian baldi rangkaian; ujian a / b; rangkaian sosial; algoritma dan masalah bukan berangka"], [{"string": "Disambiguating Web appearances of people in a social network No contact information provided yet.", "keywords": ["document clustering", "information search and retrieval", "link structure", "web appearance", "information bottleneck", "social network", "name disambiguation"], "combined": "Disambiguating Web appearances of people in a social network No contact information provided yet. [[EENNDD]] document clustering; information search and retrieval; link structure; web appearance; information bottleneck; social network; name disambiguation"}, "Menyelaraskan penampilan Web orang di rangkaian sosial Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengelompokan dokumen; pencarian dan pengambilan maklumat; struktur pautan; penampilan laman web; masalah maklumat; rangkaian sosial; disambiguasi nama"], [{"string": "Vinci: a service-oriented architecture for rapid development of web applications An abstract is not available.", "keywords": ["xml"], "combined": "Vinci: a service-oriented architecture for rapid development of web applications An abstract is not available. [[EENNDD]] xml"}, "Vinci: seni bina berorientasikan perkhidmatan untuk pengembangan aplikasi web yang pesat Abstrak tidak tersedia. [[EENNDD]] xml"], [{"string": "Towards the self-annotating web No contact information provided yet.", "keywords": ["semantic annotation", "information extraction", "natural language processing", "document and text editing", "semantic web", "metadata"], "combined": "Towards the self-annotating web No contact information provided yet. [[EENNDD]] semantic annotation; information extraction; natural language processing; document and text editing; semantic web; metadata"}, "Ke laman web anotasi diri Belum ada maklumat hubungan yang diberikan. [[EENNDD]] anotasi semantik; pengekstrakan maklumat; pemprosesan bahasa semula jadi; penyuntingan dokumen dan teks; web semantik; metadata"], [{"string": "Visibly pushdown automata for streaming XML We propose the study of visibly pushdown automata (VPA) for processing XML documents. VPAs are pushdown automata where the input determines the stack operation, and XML documents are naturally visibly pushdown with the VPA pushing onto the stack on open-tags and popping the stack on close-tags. In this paper we demonstrate the power and ease visibly pushdown automata give in the design of streaming algorithms for XML documents.", "keywords": ["typing", "query", "pushdown automata", "data description languages", "schema", "xml", "streaming algorithms"], "combined": "Visibly pushdown automata for streaming XML We propose the study of visibly pushdown automata (VPA) for processing XML documents. VPAs are pushdown automata where the input determines the stack operation, and XML documents are naturally visibly pushdown with the VPA pushing onto the stack on open-tags and popping the stack on close-tags. In this paper we demonstrate the power and ease visibly pushdown automata give in the design of streaming algorithms for XML documents. [[EENNDD]] typing; query; pushdown automata; data description languages; schema; xml; streaming algorithms"}, "Automata pushdown yang kelihatan untuk streaming XML Kami mencadangkan kajian automata pushdown (VPA) untuk memproses dokumen XML. VPA adalah pushdown automata di mana input menentukan operasi tumpukan, dan dokumen XML secara semula jadi pushdown dengan jelas VPA mendorong ke tumpukan pada tag terbuka dan memunculkan tumpukan pada tag dekat. Dalam makalah ini kami menunjukkan kekuatan dan kemudahan automatik pushdown yang jelas dalam reka bentuk streaming algoritma untuk dokumen XML. [[EENNDD]] menaip; pertanyaan; pushdown automata; bahasa penerangan data; skema; xml; algoritma penstriman"], [{"string": "Improving Web search efficiency via a locality based static pruning method No contact information provided yet.", "keywords": ["indexing", "information retrieval", "search engines", "web search", "pruning"], "combined": "Improving Web search efficiency via a locality based static pruning method No contact information provided yet. [[EENNDD]] indexing; information retrieval; search engines; web search; pruning"}, "Meningkatkan kecekapan carian Web melalui kaedah pemangkasan statik berdasarkan lokasi Tidak ada maklumat hubungan yang disediakan. [[EENNDD]] pengindeksan; pengambilan maklumat; enjin carian; carian sesawang; pemangkasan"], [{"string": "Recommendations to boost content spread in social networks Content sharing in social networks is a powerful mechanism for discovering content on the Internet. The degree to which content is disseminated within the network depends on the connectivity relationships among network nodes. Existing schemes for recommending connections in social networks are based on the number of common neighbors, similarity of user profiles, etc. However, such similarity-based connections do not consider the amount of content discovered.", "keywords": ["social networks", "recommendation", "content spread"], "combined": "Recommendations to boost content spread in social networks Content sharing in social networks is a powerful mechanism for discovering content on the Internet. The degree to which content is disseminated within the network depends on the connectivity relationships among network nodes. Existing schemes for recommending connections in social networks are based on the number of common neighbors, similarity of user profiles, etc. However, such similarity-based connections do not consider the amount of content discovered. [[EENNDD]] social networks; recommendation; content spread"}, "Cadangan untuk meningkatkan penyebaran kandungan di rangkaian sosial Perkongsian kandungan di rangkaian sosial adalah mekanisme yang kuat untuk menemui kandungan di Internet. Sejauh mana kandungan disebarkan dalam rangkaian bergantung pada hubungan kesalinghubungan antara nod rangkaian. Skema yang ada untuk mengesyorkan sambungan di rangkaian sosial didasarkan pada jumlah jiran yang sama, persamaan profil pengguna, dan lain-lain. Walau bagaimanapun, sambungan berdasarkan kesamaan tersebut tidak mempertimbangkan jumlah kandungan yang dijumpai. [[EENNDD]] rangkaian sosial; cadangan; penyebaran kandungan"], [{"string": "A decentralized CF approach based on cooperative agents No contact information provided yet.", "keywords": ["friend network", "distributed collaborative filtering", "p2p system"], "combined": "A decentralized CF approach based on cooperative agents No contact information provided yet. [[EENNDD]] friend network; distributed collaborative filtering; p2p system"}, "Pendekatan CF terdesentralisasi berdasarkan ejen koperasi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] rangkaian rakan; penapisan kolaboratif yang diedarkan; sistem p2p"], [{"string": "DOM-based content extraction of HTML documents No contact information provided yet.", "keywords": ["html documents", "dom trees", "accessibility", "content extraction", "reformatting", "speech rendering"], "combined": "DOM-based content extraction of HTML documents No contact information provided yet. [[EENNDD]] html documents; dom trees; accessibility; content extraction; reformatting; speech rendering"}, "Pengekstrakan kandungan berasaskan dokumen HTML berdasarkan DOM Belum ada maklumat hubungan. [[EENNDD]] dokumen html; pokok dom; kebolehcapaian; pengekstrakan kandungan; memformat semula; penyampaian pertuturan"], [{"string": "Meteor-s web service annotation framework No contact information provided yet.", "keywords": ["semantic web services", "ontology", "web services discovery", "wsdl", "semantic annotation of web services"], "combined": "Meteor-s web service annotation framework No contact information provided yet. [[EENNDD]] semantic web services; ontology; web services discovery; wsdl; semantic annotation of web services"}, "Rangka anotasi perkhidmatan web Meteor-s Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perkhidmatan web semantik; ontologi; penemuan perkhidmatan web; wsdl; anotasi semantik perkhidmatan web"], [{"string": "Accessibility: a Web engineering approach No contact information provided yet.", "keywords": ["semantic web", "web engineering", "accessibility", "visual impairment"], "combined": "Accessibility: a Web engineering approach No contact information provided yet. [[EENNDD]] semantic web; web engineering; accessibility; visual impairment"}, "Kebolehcapaian: pendekatan teknik Web Belum ada maklumat hubungan yang diberikan [[EENNDD]] web semantik; kejuruteraan web; kebolehcapaian; masalah penglihatan"], [{"string": "Partitioning of Web graphs by community topology No contact information provided yet.", "keywords": ["maximum flow algorithm", "graph partitioning", "web community"], "combined": "Partitioning of Web graphs by community topology No contact information provided yet. [[EENNDD]] maximum flow algorithm; graph partitioning; web community"}, "Pembahagian grafik Web oleh topologi komuniti Belum ada maklumat hubungan yang diberikan. [[EENNDD]] algoritma aliran maksimum; pembahagian graf; komuniti web"], [{"string": "Shilling recommender systems for fun and profit No contact information provided yet.", "keywords": ["collaborative filtering", "shilling", "recommender systems"], "combined": "Shilling recommender systems for fun and profit No contact information provided yet. [[EENNDD]] collaborative filtering; shilling; recommender systems"}, "Sistem pengesyoran shilling untuk keseronokan dan keuntungan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penapisan kolaboratif; shilling; sistem cadangan"], [{"string": "A user profile-based approach for personal information access: shaping your information portfolio No contact information provided yet.", "keywords": ["information retrieval", "user profile", "internet behavior", "language constructs and features", "system", "personal information access"], "combined": "A user profile-based approach for personal information access: shaping your information portfolio No contact information provided yet. [[EENNDD]] information retrieval; user profile; internet behavior; language constructs and features; system; personal information access"}, "Pendekatan berdasarkan profil pengguna untuk akses maklumat peribadi: membentuk portfolio maklumat anda Belum ada maklumat hubungan yang disediakan. [[EENNDD]] pengambilan maklumat; profil pengguna; tingkah laku internet; konstruk dan ciri bahasa; sistem; akses maklumat peribadi"], [{"string": "A browser for browsing the past web No contact information provided yet.", "keywords": ["web archive browsing", "web archives", "past web"], "combined": "A browser for browsing the past web No contact information provided yet. [[EENNDD]] web archive browsing; web archives; past web"}, "Penyemak imbas untuk melayari web yang lalu Belum ada maklumat hubungan yang diberikan. [[EENNDD]] melayari arkib web; arkib web; web yang lalu"], [{"string": "Network bucket testing Bucket testing, also known as A/B testing, is a practice that is widely used by on-line sites with large audiences: in a simple version of the methodology, one evaluates a new feature on the site by exposing it to a very small fraction of the total user population and measuring its effect on this exposed group. For traditional uses of this technique, uniform independent sampling of the population is often enough to produce an exposed group that can serve as a statistical proxy for the full population.", "keywords": ["bucket testing", "random walks", "a/b testing", "miscellaneous", "social networks"], "combined": "Network bucket testing Bucket testing, also known as A/B testing, is a practice that is widely used by on-line sites with large audiences: in a simple version of the methodology, one evaluates a new feature on the site by exposing it to a very small fraction of the total user population and measuring its effect on this exposed group. For traditional uses of this technique, uniform independent sampling of the population is often enough to produce an exposed group that can serve as a statistical proxy for the full population. [[EENNDD]] bucket testing; random walks; a/b testing; miscellaneous; social networks"}, "Ujian bucket rangkaian Ujian bucket, juga dikenali sebagai pengujian A / B, adalah praktik yang banyak digunakan oleh laman web dalam talian dengan khalayak besar: dalam versi metodologi yang mudah, seseorang menilai ciri baru di laman web ini dengan memaparkannya kepada sebahagian kecil daripada jumlah populasi pengguna dan mengukur kesannya pada kumpulan terdedah ini. Untuk penggunaan tradisional teknik ini, persampelan bebas yang seragam terhadap populasi seringkali cukup untuk menghasilkan kumpulan yang terdedah yang dapat berfungsi sebagai proksi statistik untuk populasi penuh. [[EENNDD]] ujian baldi; jalan rawak; ujian a / b; pelbagai; rangkaian sosial"], [{"string": "New objective functions for social collaborative filtering This paper examines the problem of social collaborative filtering (CF) to recommend items of interest to users in a social network setting. Unlike standard CF algorithms using relatively simple user and item features, recommendation in social networks poses the more complex problem of learning user preferences from a rich and complex set of user profile and interaction information. Many existing social CF methods have extended traditional CF matrix factorization, but have overlooked important aspects germane to the social setting. We propose a unified framework for social CF matrix factorization by introducing novel objective functions for training. Our new objective functions have three key features that address main drawbacks of existing approaches: (a) we fully exploit feature-based user similarity, (b) we permit direct learning of user-to-user information diffusion, and (c) we leverage co-preference (dis)agreement between two users to learn restricted areas of common interest. We evaluate these new social CF objectives, comparing them to each other and to a variety of (social) CF baselines, and analyze user behavior on live user trials in a custom-developed Facebook App involving data collected over five months from over 100 App users and their 37,000+ friends.", "keywords": ["collaborative filtering", "machine learning", "social networks"], "combined": "New objective functions for social collaborative filtering This paper examines the problem of social collaborative filtering (CF) to recommend items of interest to users in a social network setting. Unlike standard CF algorithms using relatively simple user and item features, recommendation in social networks poses the more complex problem of learning user preferences from a rich and complex set of user profile and interaction information. Many existing social CF methods have extended traditional CF matrix factorization, but have overlooked important aspects germane to the social setting. We propose a unified framework for social CF matrix factorization by introducing novel objective functions for training. Our new objective functions have three key features that address main drawbacks of existing approaches: (a) we fully exploit feature-based user similarity, (b) we permit direct learning of user-to-user information diffusion, and (c) we leverage co-preference (dis)agreement between two users to learn restricted areas of common interest. We evaluate these new social CF objectives, comparing them to each other and to a variety of (social) CF baselines, and analyze user behavior on live user trials in a custom-developed Facebook App involving data collected over five months from over 100 App users and their 37,000+ friends. [[EENNDD]] collaborative filtering; machine learning; social networks"}, "Fungsi objektif baru untuk penapisan kolaboratif sosial Makalah ini mengkaji masalah penapisan kolaboratif sosial (CF) untuk mengesyorkan item yang menarik kepada pengguna dalam pengaturan rangkaian sosial. Tidak seperti algoritma CF standard yang menggunakan ciri pengguna dan item yang agak mudah, cadangan dalam rangkaian sosial menimbulkan masalah pembelajaran pilihan pengguna yang lebih kompleks dari sekumpulan profil pengguna dan maklumat interaksi yang kaya dan kompleks. Banyak kaedah CF sosial yang ada telah memperluaskan pemfaktoran matriks CF tradisional, tetapi telah mengabaikan aspek-aspek penting hingga ke tahap sosial. Kami mencadangkan kerangka bersatu untuk pemfaktoran matriks CF sosial dengan memperkenalkan fungsi objektif baru untuk latihan. Fungsi objektif baru kami mempunyai tiga ciri utama yang menangani kelemahan utama pendekatan yang ada: (a) kami memanfaatkan sepenuhnya kesamaan pengguna berdasarkan ciri, (b) kami membenarkan pembelajaran langsung penyebaran maklumat pengguna-ke-pengguna, dan (c) kami memanfaatkan kesepakatan bersama (dis) perjanjian antara dua pengguna untuk mempelajari bidang kepentingan bersama yang terhad. Kami menilai objektif CF sosial baru ini, membandingkannya antara satu sama lain dan dengan pelbagai garis dasar CF (sosial), dan menganalisis tingkah laku pengguna pada percubaan pengguna langsung dalam Aplikasi Facebook yang dibangunkan khas yang melibatkan data yang dikumpulkan selama lima bulan dari lebih dari 100 pengguna Aplikasi dan 37,000+ rakan mereka. [[EENNDD]] penapisan kolaboratif; pembelajaran mesin; rangkaian sosial"], [{"string": "Using proportional transportation similarity with learned element semantics for XML document clustering No contact information provided yet.", "keywords": ["proportional transportation similarity", "xml document clustering"], "combined": "Using proportional transportation similarity with learned element semantics for XML document clustering No contact information provided yet. [[EENNDD]] proportional transportation similarity; xml document clustering"}, "Menggunakan persamaan pengangkutan berkadaran dengan semantik elemen yang dipelajari untuk pengelompokan dokumen XML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] persamaan pengangkutan berkadar; pengelompokan dokumen xml"], [{"string": "Ranking refinement and its application to information retrieval We consider the problem of ranking refinement, i.e., to improve the accuracy of an existing ranking function with a small set of labeled instances. We are, particularly, interested in learning a better ranking function using two complementary sources of information, ranking information given by the existing ranking function (i.e., a base ranker) and that obtained from users' feedbacks. This problem is very important in information retrieval where the feedback is gradually collected. The key challenge in combining the two sources of information arises from the fact that the ranking information presented by the base ranker tends to be imperfect and the ranking information obtained from users' feedbacks tends to be noisy. We present a novel boosting framework for ranking refinement that can effectively leverage the uses of the two sources of information. Our empirical study shows that the proposed algorithm is effective for ranking refinement, and furthermore significantly outperforms the baseline algorithms that incorporate the outputs from the base ranker as an additional feature.", "keywords": ["boosting", "incremental learning", "background information", "learning to rank"], "combined": "Ranking refinement and its application to information retrieval We consider the problem of ranking refinement, i.e., to improve the accuracy of an existing ranking function with a small set of labeled instances. We are, particularly, interested in learning a better ranking function using two complementary sources of information, ranking information given by the existing ranking function (i.e., a base ranker) and that obtained from users' feedbacks. This problem is very important in information retrieval where the feedback is gradually collected. The key challenge in combining the two sources of information arises from the fact that the ranking information presented by the base ranker tends to be imperfect and the ranking information obtained from users' feedbacks tends to be noisy. We present a novel boosting framework for ranking refinement that can effectively leverage the uses of the two sources of information. Our empirical study shows that the proposed algorithm is effective for ranking refinement, and furthermore significantly outperforms the baseline algorithms that incorporate the outputs from the base ranker as an additional feature. [[EENNDD]] boosting; incremental learning; background information; learning to rank"}, "Penyempurnaan peringkat dan penerapannya untuk pencarian maklumat Kami menganggap masalah penyempurnaan peringkat, iaitu, untuk meningkatkan ketepatan fungsi peringkat yang ada dengan sekumpulan contoh kecil yang berlabel. Kami, khususnya, berminat untuk mempelajari fungsi peringkat yang lebih baik menggunakan dua sumber maklumat yang saling melengkapi, maklumat peringkat yang diberikan oleh fungsi peringkat yang ada (iaitu, peringkat dasar) dan yang diperoleh dari maklum balas pengguna. Masalah ini sangat penting dalam pencarian maklumat di mana maklum balas dikumpulkan secara beransur-ansur. Cabaran utama dalam menggabungkan dua sumber maklumat tersebut timbul dari kenyataan bahawa maklumat peringkat yang disampaikan oleh pemeringkat asas cenderung tidak sempurna dan maklumat peringkat yang diperoleh dari maklum balas pengguna cenderung menjadi bising. Kami menyajikan kerangka penambahbaikan novel untuk penyempurnaan peringkat yang dapat memanfaatkan penggunaan dua sumber maklumat dengan berkesan. Kajian empirikal kami menunjukkan bahawa algoritma yang dicadangkan berkesan untuk penyempurnaan peringkat, dan lebih jauh melebihi algoritma garis dasar yang menggabungkan output dari peringkat dasar sebagai ciri tambahan. [[EENNDD]] meningkatkan; pembelajaran tambahan; maklumat latar belakang; belajar berpangkat"], [{"string": "Efficient search for peer-to-peer information retrieval using semantic small world No contact information provided yet.", "keywords": ["peer-to-peer", "small world", "semantic", "information retrieval"], "combined": "Efficient search for peer-to-peer information retrieval using semantic small world No contact information provided yet. [[EENNDD]] peer-to-peer; small world; semantic; information retrieval"}, "Pencarian cekap untuk mendapatkan maklumat peer-to-peer menggunakan dunia semantik kecil Belum ada maklumat hubungan yang diberikan. [[EENNDD]] rakan sebaya; dunia kecil; semantik; pengambilan maklumat"], [{"string": "Analyzing search engine advertising: firm behavior and cross-selling in electronic markets The phenomenon of sponsored search advertising is gaining ground as the largest source of revenues for search engines. Firms across different industries have are beginning to adopt this as the primary form of online advertising. This process works on an auction mechanism in which advertisers bid for different keywords, and final rank for a given keyword is allocated by the search engine. But how different are firm's actual bids from their optimal bids? Moreover, what are other ways in which firms can potentially benefit from sponsored search advertising? Based on the model and estimates from prior work [10], we conduct a number of policy simulations in order to investigate to what extent an advertiser can benefit from bidding optimally for its keywords. Further, we build a Hierarchical Bayesian modeling framework to explore the potential for cross-selling or spillovers effects from a given keyword advertisement across multiple product categories, and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that advertisers are not bidding optimally with respect to maximizing profits. We conduct a detailed analysis with product level variables to explore the extent of cross-selling opportunities across different categories from a given keyword advertisement. We find that there exists significant potential for cross-selling through search keyword advertisements in that consumers often end up buying products from other categories in addition to the product they were searching for. Latency (the time it takes for consumer to place a purchase order after clicking on the advertisement) and the presence of a brand name in the keyword are associated with consumer spending on product categories that are different from the one they were originally searching for on the Internet.", "keywords": ["search engines", "hierarchical bayesian modeling", "web 2.0", "paid search advertising", "electronic commerce", "online advertising"], "combined": "Analyzing search engine advertising: firm behavior and cross-selling in electronic markets The phenomenon of sponsored search advertising is gaining ground as the largest source of revenues for search engines. Firms across different industries have are beginning to adopt this as the primary form of online advertising. This process works on an auction mechanism in which advertisers bid for different keywords, and final rank for a given keyword is allocated by the search engine. But how different are firm's actual bids from their optimal bids? Moreover, what are other ways in which firms can potentially benefit from sponsored search advertising? Based on the model and estimates from prior work [10], we conduct a number of policy simulations in order to investigate to what extent an advertiser can benefit from bidding optimally for its keywords. Further, we build a Hierarchical Bayesian modeling framework to explore the potential for cross-selling or spillovers effects from a given keyword advertisement across multiple product categories, and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that advertisers are not bidding optimally with respect to maximizing profits. We conduct a detailed analysis with product level variables to explore the extent of cross-selling opportunities across different categories from a given keyword advertisement. We find that there exists significant potential for cross-selling through search keyword advertisements in that consumers often end up buying products from other categories in addition to the product they were searching for. Latency (the time it takes for consumer to place a purchase order after clicking on the advertisement) and the presence of a brand name in the keyword are associated with consumer spending on product categories that are different from the one they were originally searching for on the Internet. [[EENNDD]] search engines; hierarchical bayesian modeling; web 2.0; paid search advertising; electronic commerce; online advertising"}, "Menganalisis iklan mesin pencari: tingkah laku tegas dan penjualan silang di pasaran elektronik Fenomena iklan carian yang ditaja mendapat tempat sebagai sumber pendapatan terbesar untuk mesin pencari. Syarikat-syarikat di pelbagai industri mula mengadopsi ini sebagai bentuk utama iklan dalam talian. Proses ini berfungsi pada mekanisme lelang di mana pengiklan menawar kata kunci yang berbeza, dan peringkat akhir untuk kata kunci tertentu diperuntukkan oleh mesin pencari. Tetapi bagaimana berbeza tawaran sebenar syarikat daripada tawaran optimumnya? Lebih-lebih lagi, apakah cara lain di mana syarikat berpotensi mendapat keuntungan dari iklan carian yang ditaja? Berdasarkan model dan anggaran dari karya sebelumnya [10], kami melakukan sejumlah simulasi dasar untuk menyelidiki sejauh mana pengiklan dapat memperoleh keuntungan daripada membida kata kunci dengan optimum. Selanjutnya, kami membina kerangka pemodelan Bayerian Hierarkis untuk meneroka potensi kesan penjualan silang atau limpahan dari iklan kata kunci yang diberikan di beberapa kategori produk, dan menganggarkan model menggunakan kaedah Markov Chain Monte Carlo (MCMC). Analisis kami menunjukkan bahawa pengiklan tidak menawar secara optimum berkenaan dengan memaksimumkan keuntungan. Kami melakukan analisis terperinci dengan pemboleh ubah tahap produk untuk meneroka sejauh mana peluang penjualan silang di pelbagai kategori dari iklan kata kunci tertentu. Kami mendapati bahawa terdapat potensi yang besar untuk penjualan silang melalui iklan kata kunci carian kerana pengguna sering akhirnya membeli produk dari kategori lain sebagai tambahan kepada produk yang mereka cari. Latensi (masa yang diperlukan pengguna untuk membuat pesanan pembelian setelah mengklik iklan) dan kehadiran nama jenama dalam kata kunci dikaitkan dengan perbelanjaan pengguna untuk kategori produk yang berbeza daripada yang mereka cari pada awalnya di Internet. [[EENNDD]] enjin carian; pemodelan bayesian hierarki; laman web 2.0; iklan carian berbayar; perdagangan elektronik; iklan dalam talian"], [{"string": "A comparative analysis of web and peer-to-peer traffic Peer-to-Peer (P2P) applications continue to grow in popularity, and have reportedly overtaken Web applications as the single largest contributor to Internet traffic. Using traces collected from a large edge network, we conduct an extensive analysis of P2P traffic, compare P2P traffic with Web traffic, and discuss the implications of increased P2P traffic. In addition to studying the aggregate P2P traffic, we also analyze and compare the two main constituents of P2P traffic in our data, namely BitTorrent and Gnutella. The results presented in the paper may be used for generating synthetic workloads, gaining insights into the functioning of P2P applications, and developing network management strategies. For example, our results suggest that new models are necessary for Internet traffic. As a first step, we present flow-level distributional models for Web and P2P traffic that may be used in network simulation and emulation experiments.", "keywords": ["web", "network protocols", "traffic characterization", "traffic models", "peer-to-peer", "model development"], "combined": "A comparative analysis of web and peer-to-peer traffic Peer-to-Peer (P2P) applications continue to grow in popularity, and have reportedly overtaken Web applications as the single largest contributor to Internet traffic. Using traces collected from a large edge network, we conduct an extensive analysis of P2P traffic, compare P2P traffic with Web traffic, and discuss the implications of increased P2P traffic. In addition to studying the aggregate P2P traffic, we also analyze and compare the two main constituents of P2P traffic in our data, namely BitTorrent and Gnutella. The results presented in the paper may be used for generating synthetic workloads, gaining insights into the functioning of P2P applications, and developing network management strategies. For example, our results suggest that new models are necessary for Internet traffic. As a first step, we present flow-level distributional models for Web and P2P traffic that may be used in network simulation and emulation experiments. [[EENNDD]] web; network protocols; traffic characterization; traffic models; peer-to-peer; model development"}, "Analisis perbandingan aplikasi web dan trafik peer-to-peer Peer-to-Peer (P2P) terus bertambah popular, dan dilaporkan telah mengatasi aplikasi Web sebagai penyumbang tunggal terbesar untuk lalu lintas Internet. Dengan menggunakan jejak yang dikumpulkan dari rangkaian tepi yang besar, kami melakukan analisis luas mengenai lalu lintas P2P, membandingkan lalu lintas P2P dengan lalu lintas Web, dan membincangkan implikasi peningkatan lalu lintas P2P. Selain mengkaji lalu lintas P2P agregat, kami juga menganalisis dan membandingkan dua penyusun utama trafik P2P dalam data kami, iaitu BitTorrent dan Gnutella. Hasil yang disajikan dalam makalah ini dapat digunakan untuk menghasilkan beban kerja sintetik, mendapatkan wawasan tentang fungsi aplikasi P2P, dan mengembangkan strategi manajemen jaringan. Sebagai contoh, hasil kami menunjukkan bahawa model baru diperlukan untuk trafik Internet. Sebagai langkah pertama, kami menyajikan model distribusi tahap aliran untuk lalu lintas Web dan P2P yang dapat digunakan dalam eksperimen simulasi dan emulasi rangkaian. [[EENNDD]] web; protokol rangkaian; pencirian lalu lintas; model lalu lintas; rakan sebaya; pembangunan model"], [{"string": "Learning consensus opinion: mining data from a labeling game We consider the problem of identifying the consensus ranking for the results of a query, given preferences among those results from a set of individual users. Once consensus rankings are identified for a set of queries, these rankings can serve for both evaluation and training of retrieval and learning systems. We present a novel approach to collecting the individual user preferences over image-search results: we use a collaborative game in which players are rewarded for agreeing on which image result is best for a query. Our approach is distinct from other labeling games because we are able to elicit directly the preferences of interest with respect to image queries extracted from query logs. As a source of relevance judgments, this data provides a useful complement to click data. Furthermore, the data is free of positional biases and is collected by the game without the risk of frustrating users with non-relevant results; this risk is prevalent in standard mechanisms for debiasing clicks. We describe data collected over 34 days from a deployed version of this game that amounts to about 18 million expressed preferences between pairs. Finally, we present several approaches to modeling this data in order to extract the consensus rankings from the preferences and better sort the search results for targeted queries.", "keywords": ["preference judgments", "learning preferences"], "combined": "Learning consensus opinion: mining data from a labeling game We consider the problem of identifying the consensus ranking for the results of a query, given preferences among those results from a set of individual users. Once consensus rankings are identified for a set of queries, these rankings can serve for both evaluation and training of retrieval and learning systems. We present a novel approach to collecting the individual user preferences over image-search results: we use a collaborative game in which players are rewarded for agreeing on which image result is best for a query. Our approach is distinct from other labeling games because we are able to elicit directly the preferences of interest with respect to image queries extracted from query logs. As a source of relevance judgments, this data provides a useful complement to click data. Furthermore, the data is free of positional biases and is collected by the game without the risk of frustrating users with non-relevant results; this risk is prevalent in standard mechanisms for debiasing clicks. We describe data collected over 34 days from a deployed version of this game that amounts to about 18 million expressed preferences between pairs. Finally, we present several approaches to modeling this data in order to extract the consensus rankings from the preferences and better sort the search results for targeted queries. [[EENNDD]] preference judgments; learning preferences"}, "Belajar pendapat konsensus: melombong data dari permainan pelabelan Kami mempertimbangkan masalah mengenal pasti kedudukan konsensus untuk hasil pertanyaan, memandangkan keutamaan di antara hasil tersebut dari sekumpulan pengguna individu. Setelah peringkat konsensus dikenal pasti untuk satu set pertanyaan, peringkat ini dapat berfungsi untuk penilaian dan latihan pengambilan dan sistem pembelajaran. Kami menyajikan pendekatan baru untuk mengumpulkan pilihan pengguna secara individu berbanding hasil carian gambar: kami menggunakan permainan kolaboratif di mana pemain diberi ganjaran kerana bersetuju mengenai hasil gambar mana yang terbaik untuk pertanyaan. Pendekatan kami berbeza dengan permainan pelabelan lain kerana kami dapat memperoleh secara langsung pilihan minat sehubungan dengan pertanyaan gambar yang diekstrak dari log pertanyaan. Sebagai sumber penilaian relevan, data ini memberikan pelengkap berguna untuk mengklik data. Selanjutnya, data tersebut bebas dari bias kedudukan dan dikumpulkan oleh permainan tanpa risiko membuat pengguna kecewa dengan hasil yang tidak relevan; risiko ini berlaku dalam mekanisme standard untuk mengurangkan klik. Kami menerangkan data yang dikumpulkan selama 34 hari dari versi permainan yang digunakan yang berjumlah sekitar 18 juta pilihan yang dinyatakan antara pasangan. Akhirnya, kami menyajikan beberapa pendekatan untuk memodelkan data ini untuk mengekstrak kedudukan konsensus dari pilihan dan menyusun hasil carian dengan lebih baik untuk pertanyaan yang disasarkan. [[EENNDD]] penilaian pilihan; pilihan belajar"], [{"string": "A game based approach to assign geographical relevance to web images Geographical context is very important for images. Millions of images on the Web have been already assigned latitude and longitude information. Due to the rapid proliferation of such images with geographical context, it is still difficult to effectively search and browse them, since we do not have ways to decide their relevance. In this paper, we focus on the geographical relevance of images, which is defined as to what extent the main objects in an image match landmarks at the location where the image was taken. Recently, researchers have proposed to use game based approaches to label large scale data such as Web images. However, previous works have not shown the quality of collected game logs in detail and how the logs can improve existing applications. To answer these questions, we design and implement a Web-based and multi-player game to collect human knowledge while people are enjoying the game. Then we thoroughly analyze the game logs obtained during a three week study with 147 participants and propose methods to determine the image geographical relevance. In addition, we conduct an experiment to compare our methods with a commercial search engine. Experimental results show that our methods dramatically improve image search relevance. Furthermore, we show that we can derive geographically relevant objects and their salient portion in images, which is valuable for a number of applications such as image location recognition.", "keywords": ["image annotation", "image search", "geographical relevance", "human computation"], "combined": "A game based approach to assign geographical relevance to web images Geographical context is very important for images. Millions of images on the Web have been already assigned latitude and longitude information. Due to the rapid proliferation of such images with geographical context, it is still difficult to effectively search and browse them, since we do not have ways to decide their relevance. In this paper, we focus on the geographical relevance of images, which is defined as to what extent the main objects in an image match landmarks at the location where the image was taken. Recently, researchers have proposed to use game based approaches to label large scale data such as Web images. However, previous works have not shown the quality of collected game logs in detail and how the logs can improve existing applications. To answer these questions, we design and implement a Web-based and multi-player game to collect human knowledge while people are enjoying the game. Then we thoroughly analyze the game logs obtained during a three week study with 147 participants and propose methods to determine the image geographical relevance. In addition, we conduct an experiment to compare our methods with a commercial search engine. Experimental results show that our methods dramatically improve image search relevance. Furthermore, we show that we can derive geographically relevant objects and their salient portion in images, which is valuable for a number of applications such as image location recognition. [[EENNDD]] image annotation; image search; geographical relevance; human computation"}, "Pendekatan berasaskan permainan untuk menetapkan perkaitan geografi dengan gambar web Konteks geografi sangat penting untuk gambar. Berjuta-juta gambar di Web telah diberikan maklumat garis lintang dan garis bujur. Oleh kerana percambahan cepat gambar sedemikian dengan konteks geografi, masih sukar untuk mencari dan melihatnya secara berkesan, kerana kita tidak mempunyai cara untuk menentukan kesesuaiannya. Dalam makalah ini, kami memfokuskan pada relevansi geografi gambar, yang didefinisikan sejauh mana objek utama dalam gambar sesuai dengan tanda tempat di lokasi di mana gambar itu diambil. Baru-baru ini, para penyelidik telah mencadangkan untuk menggunakan pendekatan berdasarkan permainan untuk melabel data skala besar seperti gambar Web. Namun, karya sebelumnya tidak menunjukkan kualiti log permainan yang dikumpulkan secara terperinci dan bagaimana log dapat meningkatkan aplikasi yang ada. Untuk menjawab soalan-soalan ini, kami merancang dan melaksanakan permainan berasaskan Web dan berbilang pemain untuk mengumpulkan pengetahuan manusia semasa orang menikmati permainan ini. Kemudian kami menganalisis secara menyeluruh log permainan yang diperoleh selama tiga minggu kajian dengan 147 peserta dan mencadangkan kaedah untuk menentukan perkaitan geografi gambar. Di samping itu, kami menjalankan eksperimen untuk membandingkan kaedah kami dengan mesin carian komersial. Hasil eksperimen menunjukkan bahawa kaedah kami meningkatkan relevansi carian gambar secara dramatik. Selanjutnya, kami menunjukkan bahawa kami dapat memperoleh objek yang relevan secara geografi dan bahagiannya yang menonjol dalam gambar, yang sangat berharga untuk sebilangan aplikasi seperti pengenalan lokasi gambar. [[EENNDD]] anotasi gambar; carian gambar; perkaitan geografi; pengiraan manusia"], [{"string": "Structured objects in owl: representation and reasoning Applications of semantic technologies often require the representation of and reasoning with structured objects - that is, objects composed of parts connected in complex ways. Although OWL is a general and powerful language, its class descriptions and axioms cannot be used to describe arbitrarily connected structures. An OWL representation of structured objects can thus be underconstrained, which reduces the inferences that can be drawn and causes performance problems in reasoning. To address these problems, we extend OWL with description graphs, which allow for the description of structured objects in a simple and precise way. To represent conditional aspects of the domain, we also allow for SWRL-like rules over description graphs. Based on an observation about the nature of structured objects, we ensure decidability of our formalism. We also present a hypertableau-based decision procedure, which we implemented in the HermiT reasoner. To evaluate its performance, we have extracted description graphs from the GALEN and FMA ontologies, classified them successfully, and even detected a modeling error in GALEN.", "keywords": ["description logics", "owl", "structured objects"], "combined": "Structured objects in owl: representation and reasoning Applications of semantic technologies often require the representation of and reasoning with structured objects - that is, objects composed of parts connected in complex ways. Although OWL is a general and powerful language, its class descriptions and axioms cannot be used to describe arbitrarily connected structures. An OWL representation of structured objects can thus be underconstrained, which reduces the inferences that can be drawn and causes performance problems in reasoning. To address these problems, we extend OWL with description graphs, which allow for the description of structured objects in a simple and precise way. To represent conditional aspects of the domain, we also allow for SWRL-like rules over description graphs. Based on an observation about the nature of structured objects, we ensure decidability of our formalism. We also present a hypertableau-based decision procedure, which we implemented in the HermiT reasoner. To evaluate its performance, we have extracted description graphs from the GALEN and FMA ontologies, classified them successfully, and even detected a modeling error in GALEN. [[EENNDD]] description logics; owl; structured objects"}, "Objek berstruktur dalam burung hantu: perwakilan dan penaakulan Aplikasi teknologi semantik sering memerlukan perwakilan dan penaakulan dengan objek berstruktur - iaitu, objek yang terdiri daripada bahagian yang dihubungkan dengan cara yang kompleks. Walaupun OWL adalah bahasa umum dan kuat, deskripsi kelas dan aksioma tidak dapat digunakan untuk menggambarkan struktur yang disambungkan secara sewenang-wenang. Perwakilan OWL objek terstruktur dapat dikurangkan, yang dapat mengurangkan kesimpulan yang dapat ditarik dan menyebabkan masalah prestasi dalam penaakulan. Untuk mengatasi masalah ini, kami memperluas OWL dengan grafik keterangan, yang memungkinkan untuk menerangkan objek berstruktur dengan cara yang mudah dan tepat. Untuk mewakili aspek bersyarat dari domain, kami juga membenarkan peraturan seperti SWRL mengenai grafik keterangan. Berdasarkan pemerhatian mengenai sifat objek berstruktur, kami memastikan kebolehtentuan formalisme kami. Kami juga membentangkan prosedur keputusan berdasarkan hypertableau, yang kami laksanakan dengan alasan HermiT. Untuk menilai prestasinya, kami telah mengekstrak grafik keterangan dari ontologi GALEN dan FMA, berjaya mengklasifikasikannya, dan bahkan mengesan ralat pemodelan di GALEN. [[EENNDD]] logik keterangan; burung hantu; objek berstruktur"], [{"string": "Extracting query modifications from nonlinear SVMs No contact information provided yet.", "keywords": ["sensitivity analysis", "rule extraction", "support vector machine", "query modification"], "combined": "Extracting query modifications from nonlinear SVMs No contact information provided yet. [[EENNDD]] sensitivity analysis; rule extraction; support vector machine; query modification"}, "Mengambil pengubahsuaian pertanyaan dari SVM tidak linear Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] analisis kepekaan; pengekstrakan peraturan; mesin vektor sokongan; pengubahsuaian pertanyaan"], [{"string": "Duplicate detection in click streams No contact information provided yet.", "keywords": ["data streams", "advertising networks", "duplicate detection", "approximate queries"], "combined": "Duplicate detection in click streams No contact information provided yet. [[EENNDD]] data streams; advertising networks; duplicate detection; approximate queries"}, "Pengesanan pendua dalam aliran klik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] aliran data; rangkaian pengiklanan; pengesanan pendua; pertanyaan anggaran"], [{"string": "Efficient similarity joins for near duplicate detection With the increasing amount of data and the need to integrate data from multiple data sources, a challenging issue is to find near duplicate records efficiently. In this paper, we focus on efficient algorithms to find pairs of records such that their similarities are above a given threshold. Several existing algorithms rely on the prefix filtering principle to avoid computing similarity values for all possible pairs of records. We propose new filtering techniques by exploiting the ordering information; they are integrated into the existing methods and drastically reduce the candidate sizes and hence improve the efficiency. Experimental results show that our proposed algorithms can achieve up to 2.6x - 5x speed-up over previous algorithms on several real datasets and provide alternative solutions to the near duplicate Web page detection problem.", "keywords": ["similarity join", "near duplicate detection"], "combined": "Efficient similarity joins for near duplicate detection With the increasing amount of data and the need to integrate data from multiple data sources, a challenging issue is to find near duplicate records efficiently. In this paper, we focus on efficient algorithms to find pairs of records such that their similarities are above a given threshold. Several existing algorithms rely on the prefix filtering principle to avoid computing similarity values for all possible pairs of records. We propose new filtering techniques by exploiting the ordering information; they are integrated into the existing methods and drastically reduce the candidate sizes and hence improve the efficiency. Experimental results show that our proposed algorithms can achieve up to 2.6x - 5x speed-up over previous algorithms on several real datasets and provide alternative solutions to the near duplicate Web page detection problem. [[EENNDD]] similarity join; near duplicate detection"}, "Kesamaan yang cekap bergabung untuk pengesanan hampir pendua Dengan peningkatan jumlah data dan keperluan untuk mengintegrasikan data dari pelbagai sumber data, masalah yang mencabar adalah mencari rekod pendua yang hampir dengan cekap. Dalam makalah ini, kami memfokuskan pada algoritma yang cekap untuk mencari pasangan rekod sehingga kesamaannya melebihi ambang yang ditentukan. Beberapa algoritma yang ada bergantung pada prinsip penyaringan awalan untuk mengelakkan nilai kesamaan pengkomputeran untuk semua pasangan rekod yang mungkin. Kami mencadangkan teknik penyaringan baru dengan memanfaatkan maklumat pesanan; mereka digabungkan ke dalam kaedah yang ada dan secara drastik mengurangkan ukuran calon dan dengan itu meningkatkan kecekapan. Hasil eksperimen menunjukkan bahawa algoritma yang dicadangkan kami dapat mencapai peningkatan sehingga 2.6x - 5x berbanding algoritma sebelumnya pada beberapa set data sebenar dan memberikan penyelesaian alternatif untuk masalah pengesanan halaman Web yang hampir sama. [[EENNDD]] kesamaan bergabung; dekat pengesanan pendua"], [{"string": "Answering bounded continuous search queries in the world wide web Search queries applied to extract relevant information from the World Wide Web over a period of time may be denoted as continuous search queries. The improvement of continuous search queries may concern not only the quality of retrieved results but also the freshness of results, i.e. the time between the availability of a respective data object on the Web and the notification of a user by the search engine. In some cases a user should be notified immediately since the value of the respective information decreases quickly, as e.g. news about companies that affect the value of respective stocks, or sales offers for products that may no longer be available after a short period of time.", "keywords": ["monitoring search", "optimal stopping", "continuous queries"], "combined": "Answering bounded continuous search queries in the world wide web Search queries applied to extract relevant information from the World Wide Web over a period of time may be denoted as continuous search queries. The improvement of continuous search queries may concern not only the quality of retrieved results but also the freshness of results, i.e. the time between the availability of a respective data object on the Web and the notification of a user by the search engine. In some cases a user should be notified immediately since the value of the respective information decreases quickly, as e.g. news about companies that affect the value of respective stocks, or sales offers for products that may no longer be available after a short period of time. [[EENNDD]] monitoring search; optimal stopping; continuous queries"}, "Menjawab pertanyaan carian berterusan yang terikat di web seluruh dunia Pertanyaan carian yang digunakan untuk mengekstrak maklumat yang relevan dari World Wide Web dalam jangka masa tertentu dapat dilambangkan sebagai pertanyaan carian berterusan. Peningkatan pertanyaan carian berterusan mungkin tidak hanya menyangkut kualiti hasil yang diambil tetapi juga kesegaran hasil, yaitu waktu antara ketersediaan objek data masing-masing di Web dan pemberitahuan pengguna oleh mesin pencari. Dalam beberapa kes, pengguna harus diberitahu dengan segera kerana nilai maklumat masing-masing menurun dengan cepat, seperti berita mengenai syarikat yang mempengaruhi nilai saham masing-masing, atau tawaran penjualan untuk produk yang mungkin tidak lagi tersedia setelah jangka waktu yang singkat. [[EENNDD]] memantau carian; berhenti optimum; pertanyaan berterusan"], [{"string": "Mind the data skew: distributed inferencing by speeddating in elastic regions Semantic Web data exhibits very skewed frequency distributions among terms. Efficient large-scale distributed reasoning methods should maintain load-balance in the face of such highly skewed distribution of input data. We show that term-based partitioning, used by most distributed reasoning approaches, has limited scalability due to load-balancing problems.", "keywords": ["peer-to-peer", "self-organisation", "distributed", "reasoning"], "combined": "Mind the data skew: distributed inferencing by speeddating in elastic regions Semantic Web data exhibits very skewed frequency distributions among terms. Efficient large-scale distributed reasoning methods should maintain load-balance in the face of such highly skewed distribution of input data. We show that term-based partitioning, used by most distributed reasoning approaches, has limited scalability due to load-balancing problems. [[EENNDD]] peer-to-peer; self-organisation; distributed; reasoning"}, "Perhatikan kecenderungan data: inferens yang disebarkan dengan berkelajuan pantas di kawasan elastik Data Web Semantik menunjukkan pengedaran frekuensi yang sangat miring antara istilah. Kaedah penaakulan berskala besar yang cekap harus mengekalkan keseimbangan beban dalam menghadapi sebaran data input yang sangat miring. Kami menunjukkan bahawa pemisahan berdasarkan istilah, yang digunakan oleh kebanyakan pendekatan penaakulan terdistribusi, mempunyai skalabiliti yang terhad kerana masalah pengimbangan beban. [[EENNDD]] rakan sebaya; organisasi diri; diedarkan; penaakulan"], [{"string": "SPath: a path language for XML schema XML is increasingly being used as a typed data format, and therefore it becomes more important to gain access to the type system; very often this is an XML Schema. The XML Schema Path Language (SPath) presented in this paper provides access to XML Schema components by extending the well-known XPath language to also include the domain of XML Schemas. Using SPath, XML developers gain access to XML Schemas and thus can more easily develop software which is type- or schema-aware, and thus more robust.", "keywords": ["xml schema", "xpath", "xml", "spath"], "combined": "SPath: a path language for XML schema XML is increasingly being used as a typed data format, and therefore it becomes more important to gain access to the type system; very often this is an XML Schema. The XML Schema Path Language (SPath) presented in this paper provides access to XML Schema components by extending the well-known XPath language to also include the domain of XML Schemas. Using SPath, XML developers gain access to XML Schemas and thus can more easily develop software which is type- or schema-aware, and thus more robust. [[EENNDD]] xml schema; xpath; xml; spath"}, "SPath: bahasa jalan untuk skema XML XML semakin digunakan sebagai format data yang ditaip, dan oleh itu menjadi lebih penting untuk mendapatkan akses ke sistem jenis; selalunya ini adalah Skema XML. Bahasa Laluan Skema XML (SPath) yang disajikan dalam makalah ini memberikan akses ke komponen Skema XML dengan memperluas bahasa XPath yang terkenal untuk juga memasukkan domain Skema XML. Dengan menggunakan SPath, pembangun XML mendapat akses ke Skema XML dan dengan demikian dapat dengan lebih mudah mengembangkan perisian yang peka dengan jenis atau skema, dan dengan demikian lebih mantap. [[EENNDD]] skema xml; xpath; xml; spath"], [{"string": "Topic-oriented query expansion for web search No contact information provided yet.", "keywords": ["intercluster similarity", "term-term similarity matrix", "query expansion", "information bottleneck", "topic-oriented", "intracluster similarity"], "combined": "Topic-oriented query expansion for web search No contact information provided yet. [[EENNDD]] intercluster similarity; term-term similarity matrix; query expansion; information bottleneck; topic-oriented; intracluster similarity"}, "Perluasan pertanyaan berorientasikan topik untuk carian web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] persamaan interkluster; matriks kesamaan jangka masa; pengembangan pertanyaan; masalah maklumat; berorientasikan topik; persamaan antara kaum"], [{"string": "Consistency-preserving caching of dynamic database content With the growing use of dynamic web content generated from relational databases, traditional caching solutions for through put and latency improvements are ineffective. We describe a middleware layer called Ganesh that reduces the volume of data transmitted without semantic interpretation of queries or results. It achieves this reduction through the use of cryptographic hashing to detect similarities with previous results. These benefits do not require any compromise of the strict consistency semantics provided by the back-end database. Further, Ganesh does not require modifications to applications, web servers, or database servers, and works with closed-source applications and databases. Using two bench marks representative of dynamic web sites, measurements of our prototype show that it can increase end-to-end throughput by as much as two fold for non-data intensive applications and by as much as ten fold for dataintensive ones.", "keywords": ["content addressable storage", "bandwidth optimization", "systems", "wide area networks", "relational database systems", "database caching", "distributed systems"], "combined": "Consistency-preserving caching of dynamic database content With the growing use of dynamic web content generated from relational databases, traditional caching solutions for through put and latency improvements are ineffective. We describe a middleware layer called Ganesh that reduces the volume of data transmitted without semantic interpretation of queries or results. It achieves this reduction through the use of cryptographic hashing to detect similarities with previous results. These benefits do not require any compromise of the strict consistency semantics provided by the back-end database. Further, Ganesh does not require modifications to applications, web servers, or database servers, and works with closed-source applications and databases. Using two bench marks representative of dynamic web sites, measurements of our prototype show that it can increase end-to-end throughput by as much as two fold for non-data intensive applications and by as much as ten fold for dataintensive ones. [[EENNDD]] content addressable storage; bandwidth optimization; systems; wide area networks; relational database systems; database caching; distributed systems"}, "Penyimpanan cache kandungan pangkalan data dinamik Dengan semakin meningkatnya penggunaan kandungan web dinamik yang dihasilkan dari pangkalan data hubungan, penyelesaian cache tradisional untuk peningkatan put dan latency tidak berkesan. Kami menerangkan lapisan middleware bernama Ganesh yang mengurangkan jumlah data yang dihantar tanpa tafsiran semantik mengenai pertanyaan atau hasil. Ia mencapai pengurangan ini melalui penggunaan hash kriptografi untuk mengesan persamaan dengan hasil sebelumnya. Manfaat ini tidak memerlukan kompromi semantik konsistensi ketat yang disediakan oleh pangkalan data belakang. Selanjutnya, Ganesh tidak memerlukan modifikasi pada aplikasi, pelayan web, atau pelayan pangkalan data, dan bekerja dengan aplikasi dan pangkalan data sumber tertutup. Dengan menggunakan dua tanda aras yang mewakili laman web dinamik, pengukuran prototaip kami menunjukkan bahawa ia dapat meningkatkan throughput ujung ke hujung sebanyak dua kali ganda untuk aplikasi intensif bukan data dan sebanyak sepuluh kali lipat untuk yang lengkap. [[EENNDD]] penyimpanan yang boleh diatasi kandungan pengoptimuman lebar jalur; sistem; rangkaian kawasan luas; sistem pangkalan data hubungan; cache pangkalan data; sistem yang diedarkan"], [{"string": "Simple authentication for the web Automated email-based password reestablishment (EBPR) is an efficient, cost-effective means to deal with forgotten passwords. In this technique, email providers authenticate users on behalf of web sites. This method works because web sites trust email providers to deliver messages to their intended recipients. Simple Authentication for the Web (SAW) improves upon this basic approach to user authentication to create an alternative to password-based logins. SAW: 1) Removes the setup and management costs of passwords at sites that accept the risks of EBPR; 2) Provides single sign-on without a specialized identity provider; 3) Thwarts all passive attacks.", "keywords": ["web single sign-on", "password alternative"], "combined": "Simple authentication for the web Automated email-based password reestablishment (EBPR) is an efficient, cost-effective means to deal with forgotten passwords. In this technique, email providers authenticate users on behalf of web sites. This method works because web sites trust email providers to deliver messages to their intended recipients. Simple Authentication for the Web (SAW) improves upon this basic approach to user authentication to create an alternative to password-based logins. SAW: 1) Removes the setup and management costs of passwords at sites that accept the risks of EBPR; 2) Provides single sign-on without a specialized identity provider; 3) Thwarts all passive attacks. [[EENNDD]] web single sign-on; password alternative"}, "Pengesahan mudah untuk web Penyusunan semula kata laluan berasaskan e-mel automatik (EBPR) adalah kaedah yang cekap dan menjimatkan untuk menangani kata laluan terlupa. Dalam teknik ini, penyedia e-mel mengesahkan pengguna bagi pihak laman web. Kaedah ini berfungsi kerana laman web mempercayai penyedia e-mel untuk menyampaikan mesej kepada penerima yang dimaksudkan. Pengesahan Mudah untuk Web (SAW) meningkatkan pendekatan asas ini untuk pengesahan pengguna untuk membuat alternatif untuk log masuk berdasarkan kata laluan. SAW: 1) Menghapus kos penyediaan dan pengurusan kata laluan di laman web yang menerima risiko EBPR; 2) Menyediakan single sign-on tanpa penyedia identiti khusus; 3) Menggagalkan semua serangan pasif. [[EENNDD]] log masuk tunggal web; alternatif kata laluan"], [{"string": "EDUTELLA: a P2P networking infrastructure based on RDF No contact information provided yet.", "keywords": ["e-learning", "semantic web", "peer-to-peer"], "combined": "EDUTELLA: a P2P networking infrastructure based on RDF No contact information provided yet. [[EENNDD]] e-learning; semantic web; peer-to-peer"}, "EDUTELLA: infrastruktur rangkaian P2P berdasarkan RDF Belum ada maklumat hubungan yang diberikan. [[EENNDD]] e-pembelajaran; web semantik; rakan sebaya"], [{"string": "Visual diversification of image search results Due to the reliance on the textual information associated with an image, image search engines on the Web lack the discriminative power to deliver visually diverse search results. The textual descriptions are key to retrieve relevant results for a given user query, but at the same time provide little information about the rich image content.", "keywords": ["content analysis and indexing", "on-line information services", "image clustering", "visual diversity", "flickr"], "combined": "Visual diversification of image search results Due to the reliance on the textual information associated with an image, image search engines on the Web lack the discriminative power to deliver visually diverse search results. The textual descriptions are key to retrieve relevant results for a given user query, but at the same time provide little information about the rich image content. [[EENNDD]] content analysis and indexing; on-line information services; image clustering; visual diversity; flickr"}, "Kepelbagaian visual hasil carian gambar Kerana ketergantungan pada maklumat teks yang berkaitan dengan gambar, mesin pencari gambar di Web kekurangan daya diskriminatif untuk memberikan hasil carian yang beragam. Huraian teks adalah kunci untuk mendapatkan hasil yang relevan untuk pertanyaan pengguna tertentu, tetapi pada masa yang sama memberikan sedikit maklumat mengenai kandungan gambar yang kaya. [[EENNDD]] analisis dan pengindeksan kandungan; perkhidmatan maklumat dalam talian; pengelompokan gambar; kepelbagaian visual; flickr"], [{"string": "Semantic WS-agreement partner selection No contact information provided yet.", "keywords": ["multi-ontology service annotation", "semantic web service", "ontologies", "agreement matching", "information search and retrieval", "arl", "semantic policy matching", "wsdl-s", "ws-agreement", "snobase", "owl", "dynamic service selection"], "combined": "Semantic WS-agreement partner selection No contact information provided yet. [[EENNDD]] multi-ontology service annotation; semantic web service; ontologies; agreement matching; information search and retrieval; arl; semantic policy matching; wsdl-s; ws-agreement; snobase; owl; dynamic service selection"}, "Pemilihan rakan sepakat perjanjian WS Semantik Belum ada maklumat hubungan. [[EENNDD]] anotasi perkhidmatan pelbagai ontologi; perkhidmatan web semantik; ontologi; pemadanan perjanjian; pencarian dan pengambilan maklumat; arl; pemadanan dasar semantik; wsdl-s; ws-perjanjian; snobase; burung hantu; pemilihan perkhidmatan yang dinamik"], [{"string": "SecuBat: a web vulnerability scanner No contact information provided yet.", "keywords": ["automated vulnerability detection", "xss", "miscellaneous", "security and protection", "cross-site scripting", "crawling", "sql injection", "scanner"], "combined": "SecuBat: a web vulnerability scanner No contact information provided yet. [[EENNDD]] automated vulnerability detection; xss; miscellaneous; security and protection; cross-site scripting; crawling; sql injection; scanner"}, "SecuBat: pengimbas kerentanan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengesanan kerentanan automatik; xss; pelbagai; keselamatan dan perlindungan; skrip merentas laman web; merangkak; suntikan sql; pengimbas"], [{"string": "Predicting clicks: estimating the click-through rate for new ads Search engine advertising has become a significant element of the Web browsing experience. Choosing the right ads for the query and the order in which they are displayed greatly affects the probability that a user will see and click on each ad. This ranking has a strong impact on the revenue the search engine receives from the ads. Further, showing the user an ad that they prefer to click on improves user satisfaction. For these reasons, it is important to be able to accurately estimate the click-through rate of ads in the system. For ads that have been displayed repeatedly, this is empirically measurable, but for new ads, other means must be used. We show that we can use features of ads, terms, and advertisers to learn a model that accurately predicts the click-though rate for new ads. We also show that using our model improves the convergence and performance of an advertising system. As a result, our model increases both revenue and user satisfaction.", "keywords": ["ranking", "web advertising", "cpc", "click-through rate", "information search and retrieval", "learning", "ctr", "sponsored search", "paid search"], "combined": "Predicting clicks: estimating the click-through rate for new ads Search engine advertising has become a significant element of the Web browsing experience. Choosing the right ads for the query and the order in which they are displayed greatly affects the probability that a user will see and click on each ad. This ranking has a strong impact on the revenue the search engine receives from the ads. Further, showing the user an ad that they prefer to click on improves user satisfaction. For these reasons, it is important to be able to accurately estimate the click-through rate of ads in the system. For ads that have been displayed repeatedly, this is empirically measurable, but for new ads, other means must be used. We show that we can use features of ads, terms, and advertisers to learn a model that accurately predicts the click-though rate for new ads. We also show that using our model improves the convergence and performance of an advertising system. As a result, our model increases both revenue and user satisfaction. [[EENNDD]] ranking; web advertising; cpc; click-through rate; information search and retrieval; learning; ctr; sponsored search; paid search"}, "Meramalkan klik: menganggarkan kadar klik-tayang untuk iklan baru Pengiklanan mesin carian telah menjadi elemen penting dalam pengalaman melayari Web. Memilih iklan yang tepat untuk pertanyaan dan urutan di mana iklan tersebut dipaparkan sangat mempengaruhi kebarangkalian pengguna akan melihat dan mengklik setiap iklan. Peringkat ini mempunyai kesan yang kuat terhadap pendapatan yang diterima oleh mesin pencari dari iklan. Selanjutnya, menunjukkan kepada pengguna iklan yang mereka gemari untuk mengklik meningkatkan kepuasan pengguna. Atas sebab-sebab ini, penting untuk dapat menganggarkan kadar klik iklan yang tepat dalam sistem. Untuk iklan yang telah dipaparkan berulang kali, ini dapat diukur secara empirik, tetapi untuk iklan baru, cara lain mesti digunakan. Kami menunjukkan bahawa kami dapat menggunakan fitur iklan, istilah, dan pengiklan untuk mempelajari model yang memprediksi dengan tepat kadar klik untuk iklan baru. Kami juga menunjukkan bahawa menggunakan model kami dapat meningkatkan penumpuan dan prestasi sistem pengiklanan. Hasilnya, model kami meningkatkan pendapatan dan kepuasan pengguna. [[EENNDD]] kedudukan; pengiklanan web; cpc; kadar klik lalu; carian dan pengambilan maklumat; belajar; ctr; carian tajaan; carian berbayar"], [{"string": "Releasing search queries and clicks privately The question of how to publish an anonymized search log was brought to the forefront by a well-intentioned, but privacy-unaware AOL search log release. Since then a series of ad-hoc techniques have been proposed in the literature, though none are known to be provably private. In this paper, we take a major step towards a solution: we show how queries, clicks and their associated perturbed counts can be published in a manner that rigorously preserves privacy. Our algorithm is decidedly simple to state, but non-trivial to analyze. On the opposite side of privacy is the question of whether the data we can safely publish is of any use. Our findings offer a glimmer of hope: we demonstrate that a non-negligible fraction of queries and clicks can indeed be safely published via a collection of experiments on a real search log. In addition, we select an application, keyword generation, and show that the keyword suggestions generated from the perturbed data resemble those generated from the original data.", "keywords": ["query click graph", "data release", "differential privacy", "security, integrity, and protection", "search logs"], "combined": "Releasing search queries and clicks privately The question of how to publish an anonymized search log was brought to the forefront by a well-intentioned, but privacy-unaware AOL search log release. Since then a series of ad-hoc techniques have been proposed in the literature, though none are known to be provably private. In this paper, we take a major step towards a solution: we show how queries, clicks and their associated perturbed counts can be published in a manner that rigorously preserves privacy. Our algorithm is decidedly simple to state, but non-trivial to analyze. On the opposite side of privacy is the question of whether the data we can safely publish is of any use. Our findings offer a glimmer of hope: we demonstrate that a non-negligible fraction of queries and clicks can indeed be safely published via a collection of experiments on a real search log. In addition, we select an application, keyword generation, and show that the keyword suggestions generated from the perturbed data resemble those generated from the original data. [[EENNDD]] query click graph; data release; differential privacy; security, integrity, and protection; search logs"}, "Melepaskan pertanyaan carian dan klik secara peribadi Persoalan bagaimana menerbitkan log carian tanpa nama dibawa ke barisan hadapan oleh siaran log carian AOL yang disengajakan, tetapi tidak disengajakan. Sejak itu serangkaian teknik ad-hoc telah diusulkan dalam literatur, walaupun tidak ada yang diketahui bersifat peribadi. Dalam makalah ini, kami mengambil langkah besar menuju penyelesaian: kami menunjukkan bagaimana pertanyaan, klik dan jumlah yang berkaitan dengannya dapat diterbitkan dengan cara yang menjaga privasi dengan ketat. Algoritma kami jelas mudah dinyatakan, tetapi tidak sepele untuk dianalisis. Di sebalik privasi adalah persoalan sama ada data yang dapat kita terbitkan dengan selamat ada gunanya. Penemuan kami memberikan harapan yang cerah: kami menunjukkan bahawa sebilangan kecil pertanyaan dan klik yang tidak dapat diabaikan memang dapat diterbitkan dengan selamat melalui koleksi eksperimen pada log carian sebenar. Di samping itu, kami memilih aplikasi, pembuatan kata kunci, dan menunjukkan bahawa cadangan kata kunci yang dihasilkan dari data yang terganggu menyerupai yang dihasilkan dari data asal. [[EENNDD]] graf klik pertanyaan; pelepasan data; privasi berbeza; keselamatan, integriti, dan perlindungan; mencari log"], [{"string": "Information diffusion through blogspace No contact information provided yet.", "keywords": ["graph theory", "information propagation", "learning", "topic structure", "viruses", "topic characterization", "blogs", "viral propagation", "memes"], "combined": "Information diffusion through blogspace No contact information provided yet. [[EENNDD]] graph theory; information propagation; learning; topic structure; viruses; topic characterization; blogs; viral propagation; memes"}, "Penyebaran maklumat melalui ruang blog Belum ada maklumat hubungan yang diberikan [[EENNDD]] teori grafik; penyebaran maklumat; belajar; struktur topik; virus; pencirian topik; blog; penyebaran virus; meme"], [{"string": "Using graphics processors for high performance IR query processing Web search engines are facing formidable performance challenges due to data sizes and query loads. The major engines have to process tens of thousands of queries per second over tens of billions of documents. To deal with this heavy workload, such engines employ massively parallel systems consisting of thousands of machines. The significant cost of operating these systems has motivated a lot of recent research into more efficient query processing mechanisms. We investigate a new way to build such high performance IR systems using graphical processing units (GPUs). GPUs were originally designed to accelerate computer graphics applications through massive on-chip parallelism. Recently a number of researchers have studied how to use GPUs for other problem domains such as databases and scientific computing. Our contribution here is to design a basic system architecture for GPU-based high-performance IR, to develop suitable algorithms for subtasks such as inverted list compression, list intersection, and top-$k$ scoring, and to show how to achieve highly efficient query processing on GPU-based systems. Our experimental results for a prototype GPU-based system on $25.2$ million web pages indicate that significant gains in query processing performance can be obtained.", "keywords": ["search engines", "information search and retrieval", "gpu", "ir query processing", "index compression"], "combined": "Using graphics processors for high performance IR query processing Web search engines are facing formidable performance challenges due to data sizes and query loads. The major engines have to process tens of thousands of queries per second over tens of billions of documents. To deal with this heavy workload, such engines employ massively parallel systems consisting of thousands of machines. The significant cost of operating these systems has motivated a lot of recent research into more efficient query processing mechanisms. We investigate a new way to build such high performance IR systems using graphical processing units (GPUs). GPUs were originally designed to accelerate computer graphics applications through massive on-chip parallelism. Recently a number of researchers have studied how to use GPUs for other problem domains such as databases and scientific computing. Our contribution here is to design a basic system architecture for GPU-based high-performance IR, to develop suitable algorithms for subtasks such as inverted list compression, list intersection, and top-$k$ scoring, and to show how to achieve highly efficient query processing on GPU-based systems. Our experimental results for a prototype GPU-based system on $25.2$ million web pages indicate that significant gains in query processing performance can be obtained. [[EENNDD]] search engines; information search and retrieval; gpu; ir query processing; index compression"}, "Menggunakan pemproses grafik untuk pemprosesan pertanyaan IR berprestasi tinggi Mesin pencari web menghadapi cabaran prestasi yang hebat kerana ukuran data dan banyak permintaan. Enjin utama harus memproses puluhan ribu pertanyaan sesaat lebih dari puluhan bilion dokumen. Untuk mengatasi beban kerja yang berat ini, enjin sedemikian menggunakan sistem selari secara besar-besaran yang terdiri daripada ribuan mesin. Kos operasi sistem ini yang signifikan telah mendorong banyak penyelidikan terkini mengenai mekanisme pemprosesan pertanyaan yang lebih cekap. Kami menyiasat cara baru untuk membina sistem IR berprestasi tinggi seperti menggunakan unit pemprosesan grafik (GPU). GPU pada mulanya dirancang untuk mempercepat aplikasi grafik komputer melalui paralelisme cip besar. Baru-baru ini sebilangan penyelidik telah mengkaji bagaimana menggunakan GPU untuk domain masalah lain seperti pangkalan data dan pengkomputeran saintifik. Sumbangan kami di sini adalah untuk merancang seni bina sistem asas untuk IR berprestasi tinggi berasaskan GPU, untuk mengembangkan algoritma yang sesuai untuk subtugas seperti pemampatan senarai terbalik, persimpangan senarai, dan pemarkahan $ k $ teratas, dan untuk menunjukkan bagaimana mencapai prestasi yang sangat berkesan pemprosesan pertanyaan pada sistem berasaskan GPU. Hasil eksperimen kami untuk sistem berasaskan GPU prototaip pada halaman web $ 25.2 juta $ menunjukkan bahawa keuntungan yang signifikan dalam prestasi pemprosesan pertanyaan dapat diperoleh. [[EENNDD]] enjin carian; carian dan pengambilan maklumat; gpu; pemprosesan pertanyaan; pemampatan indeks"], [{"string": "Annotea: an open RDF infrastructure for shared Web annotations An abstract is not available.", "keywords": ["xpointer", "web", "world-wide web", "xml", "semantic web", "annotations", "rdf", "metadata"], "combined": "Annotea: an open RDF infrastructure for shared Web annotations An abstract is not available. [[EENNDD]] xpointer; web; world-wide web; xml; semantic web; annotations; rdf; metadata"}, "Annotea: infrastruktur RDF terbuka untuk anotasi Web yang dikongsi Abstrak tidak tersedia. [[EENNDD]] xpointer; laman web; laman web seluruh dunia; xml; web semantik; anotasi; rdf; metadata"], [{"string": "Anti-aliasing on the web No contact information provided yet.", "keywords": ["privacy", "aliases", "natural language processing", "learning", "personas", "alias detection", "bulletin boards", "clustering", "pseudonyms"], "combined": "Anti-aliasing on the web No contact information provided yet. [[EENNDD]] privacy; aliases; natural language processing; learning; personas; alias detection; bulletin boards; clustering; pseudonyms"}, "Anti-aliasing di web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] privasi; alias; pemprosesan bahasa semula jadi; belajar; personas; pengesanan alias; papan buletin; pengelompokan; nama samaran"], [{"string": "Service-oriented data denormalization for scalable web applications Many techniques have been proposed to scale web applications. However, the data interdependencies between the database queries and transactions issued by the applications limit their efficiency. We claim that major scalability improvements can be gained by restructuring the web application data into multiple independent data services with exclusive access to their private data store. While this restructuring does not provide performance gains by itself, the implied simplification of each database workload allows a much more efficient use of classical techniques. We illustrate the data denormalization process on three benchmark applications: TPC-W, RUBiS and RUBBoS. We deploy the resulting service-oriented implementation of TPC-W across an 85-node cluster and show that restructuring its data can provide at least an order of magnitude improvement in the maximum sustainable throughput compared to master-slave database replication, while preserving strong consistency and transactional properties.", "keywords": ["scalability", "web applications", "data denormalization", "distributed systems"], "combined": "Service-oriented data denormalization for scalable web applications Many techniques have been proposed to scale web applications. However, the data interdependencies between the database queries and transactions issued by the applications limit their efficiency. We claim that major scalability improvements can be gained by restructuring the web application data into multiple independent data services with exclusive access to their private data store. While this restructuring does not provide performance gains by itself, the implied simplification of each database workload allows a much more efficient use of classical techniques. We illustrate the data denormalization process on three benchmark applications: TPC-W, RUBiS and RUBBoS. We deploy the resulting service-oriented implementation of TPC-W across an 85-node cluster and show that restructuring its data can provide at least an order of magnitude improvement in the maximum sustainable throughput compared to master-slave database replication, while preserving strong consistency and transactional properties. [[EENNDD]] scalability; web applications; data denormalization; distributed systems"}, "Denormalisasi data berorientasikan perkhidmatan untuk aplikasi web yang berskala Banyak teknik telah dicadangkan untuk menskalakan aplikasi web. Namun, saling ketergantungan data antara pertanyaan pangkalan data dan transaksi yang dikeluarkan oleh aplikasi membatasi kecekapannya. Kami mendakwa bahawa peningkatan skalabiliti utama dapat diperoleh dengan menyusun semula data aplikasi web ke dalam beberapa perkhidmatan data bebas dengan akses eksklusif ke kedai data peribadi mereka. Walaupun penyusunan semula ini tidak memberikan peningkatan prestasi dengan sendirinya, penyederhanaan tersirat dari setiap beban kerja pangkalan data memungkinkan penggunaan teknik klasik yang jauh lebih efisien. Kami menggambarkan proses denormalisasi data pada tiga aplikasi penanda aras: TPC-W, RUBiS dan RUBBoS. Kami menggunakan implementasi TPC-W yang berorientasikan perkhidmatan yang dihasilkan di sekelompok 85-node dan menunjukkan bahawa menyusun semula datanya dapat memberikan sekurang-kurangnya urutan peningkatan besar dalam throughput lestari maksimum dibandingkan dengan replikasi pangkalan data master-slave, sambil menjaga konsistensi yang kuat dan sifat transaksional. [[EENNDD]] skalabiliti; aplikasi web; denormalisasi data; sistem yang diedarkan"], [{"string": "Domain-independent entity extraction from web search query logs Query logs of a Web search engine have been increasingly used as a vital source for data mining. This paper presents a study on large-scale domain-independent entity extraction from search query logs. We present a completely unsupervised method to extract entities by applying pattern-based heuristics and statistical measures. We compare against existing techniques that use Web documents as well as search logs, and show that we improve over the state of the art. We also provide an in-depth qualitative analysis outlining differences and commonalities between these methods.", "keywords": ["general", "data mining", "entity extraction", "query logs"], "combined": "Domain-independent entity extraction from web search query logs Query logs of a Web search engine have been increasingly used as a vital source for data mining. This paper presents a study on large-scale domain-independent entity extraction from search query logs. We present a completely unsupervised method to extract entities by applying pattern-based heuristics and statistical measures. We compare against existing techniques that use Web documents as well as search logs, and show that we improve over the state of the art. We also provide an in-depth qualitative analysis outlining differences and commonalities between these methods. [[EENNDD]] general; data mining; entity extraction; query logs"}, "Pengekstrakan entiti bebas domain dari log pertanyaan carian web Log pertanyaan dari enjin carian Web semakin banyak digunakan sebagai sumber penting untuk perlombongan data. Makalah ini membentangkan kajian mengenai pengekstrakan entiti bebas domain berskala besar dari log pertanyaan carian. Kami menyajikan kaedah yang sepenuhnya tidak diawasi untuk mengekstrak entiti dengan menerapkan kaedah heuristik dan statistik berdasarkan corak. Kami membandingkan dengan teknik yang ada yang menggunakan dokumen Web dan juga log carian, dan menunjukkan bahawa kami semakin maju dalam keadaan terkini. Kami juga memberikan analisis kualitatif mendalam yang menggariskan perbezaan dan persamaan antara kaedah ini. [[EENNDD]] umum; perlombongan data; pengekstrakan entiti; log pertanyaan"], [{"string": "Propagation of trust and distrust No contact information provided yet.", "keywords": ["web of trust", "information search and retrieval", "distrust", "trust propagation"], "combined": "Propagation of trust and distrust No contact information provided yet. [[EENNDD]] web of trust; information search and retrieval; distrust; trust propagation"}, "Penyebaran kepercayaan dan ketidakpercayaan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] laman web kepercayaan; pencarian dan pengambilan maklumat; tidak percaya; penyebaran kepercayaan"], [{"string": "Access control enforcement for conversation-based web services No contact information provided yet.", "keywords": ["transition systems", "web services", "access control", "conversations"], "combined": "Access control enforcement for conversation-based web services No contact information provided yet. [[EENNDD]] transition systems; web services; access control; conversations"}, "Penguatkuasaan kawalan akses untuk perkhidmatan web berasaskan perbualan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] sistem peralihan; perkhidmatan web; kawalan akses; perbualan"], [{"string": "An efficient and systematic method to generate xslt stylesheets for different wireless pervasive devices No contact information provided yet.", "keywords": ["xslt", "wml", "pervasive devices", "pda", "xml", "applicative programming"], "combined": "An efficient and systematic method to generate xslt stylesheets for different wireless pervasive devices No contact information provided yet. [[EENNDD]] xslt; wml; pervasive devices; pda; xml; applicative programming"}, "Kaedah yang cekap dan sistematik untuk menghasilkan helaian gaya xslt untuk peranti penyebar tanpa wayar yang berbeza Belum ada maklumat hubungan yang diberikan. [[EENNDD]] xslt; wml; peranti yang meluas; pda; xml; pengaturcaraan aplikatif"], [{"string": "Towards a multimedia formatting vocabulary No contact information provided yet.", "keywords": ["document transformation", "document preparation", "cuypers", "hypertext/hypermedia", "multimedia information systems", "hyper-media", "formatting objects", "multimedia"], "combined": "Towards a multimedia formatting vocabulary No contact information provided yet. [[EENNDD]] document transformation; document preparation; cuypers; hypertext/hypermedia; multimedia information systems; hyper-media; formatting objects; multimedia"}, "Ke arah perbendaharaan kata pemformatan multimedia Belum ada maklumat hubungan yang diberikan. [[EENNDD]] transformasi dokumen; penyediaan dokumen; cuypers; hiperteks / hipermedia; sistem maklumat multimedia; media hiper; memformat objek; multimedia"], [{"string": "SLA based profit optimization in web systems No contact information provided yet.", "keywords": ["utility function", "sla optimization", "quality of service", "resource allocation", "load balancing"], "combined": "SLA based profit optimization in web systems No contact information provided yet. [[EENNDD]] utility function; sla optimization; quality of service; resource allocation; load balancing"}, "Pengoptimuman keuntungan berdasarkan SLA dalam sistem web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] fungsi utiliti; pengoptimuman sla; kualiti sesuatu servis; peruntukan sumber; pengimbangan beban"], [{"string": "Clustering e-commerce search engines No contact information provided yet.", "keywords": ["document clustering", "search engine categorization"], "combined": "Clustering e-commerce search engines No contact information provided yet. [[EENNDD]] document clustering; search engine categorization"}, "Mengelompokkan enjin carian e-dagang Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengelompokan dokumen; pengkategorian enjin carian"], [{"string": "Pagerank for product image search In this paper, we cast the image-ranking problem into the task of identifying \"authority\" nodes on an inferred visual similarity graph and propose an algorithm to analyze the visual link structure that can be created among a group of images. Through an iterative procedure based on the PageRank computation, a numerical weight is assigned to each image; this measures its relative importance to the other images being considered. The incorporation of visual signals in this process differs from the majority of large-scale commercial-search engines in use today. Commercial search-engines often solely rely on the text clues of the pages in which images are embedded to rank images, and often entirely ignore the content of the images themselves as a ranking signal. To quantify the performance of our approach in a real-world system, we conducted a series of experiments based on the task of retrieving images for 2000 of the most popular products queries. Our experimental results show significant improvement, in terms of user satisfaction and relevancy, in comparison to the most recent Google Image Search results.", "keywords": ["pagerank", "graph theory", "information search and retrieval", "visual similarity"], "combined": "Pagerank for product image search In this paper, we cast the image-ranking problem into the task of identifying \"authority\" nodes on an inferred visual similarity graph and propose an algorithm to analyze the visual link structure that can be created among a group of images. Through an iterative procedure based on the PageRank computation, a numerical weight is assigned to each image; this measures its relative importance to the other images being considered. The incorporation of visual signals in this process differs from the majority of large-scale commercial-search engines in use today. Commercial search-engines often solely rely on the text clues of the pages in which images are embedded to rank images, and often entirely ignore the content of the images themselves as a ranking signal. To quantify the performance of our approach in a real-world system, we conducted a series of experiments based on the task of retrieving images for 2000 of the most popular products queries. Our experimental results show significant improvement, in terms of user satisfaction and relevancy, in comparison to the most recent Google Image Search results. [[EENNDD]] pagerank; graph theory; information search and retrieval; visual similarity"}, "Pagerank untuk carian gambar produk Dalam makalah ini, kami memasukkan masalah pemeringkatan gambar ke dalam tugas mengenal pasti simpul \"otoritas\" pada grafik kesamaan visual yang disimpulkan dan mencadangkan algoritma untuk menganalisis struktur pautan visual yang dapat dibuat di antara sekumpulan gambar . Melalui prosedur berulang berdasarkan perhitungan PageRank, suatu berat berangka diberikan untuk setiap gambar; ini mengukur kepentingan relatifnya dengan gambar lain yang sedang dipertimbangkan. Penggabungan isyarat visual dalam proses ini berbeza dengan sebilangan besar enjin carian komersial berskala besar yang digunakan sekarang. Mesin carian komersial hanya bergantung pada petunjuk teks halaman di mana gambar disisipkan untuk memberi peringkat gambar, dan sering kali sepenuhnya mengabaikan kandungan gambar itu sendiri sebagai isyarat peringkat. Untuk mengukur prestasi pendekatan kami dalam sistem dunia nyata, kami melakukan serangkaian eksperimen berdasarkan tugas mengambil gambar untuk 2000 pertanyaan produk yang paling popular. Hasil percubaan kami menunjukkan peningkatan yang ketara, dari segi kepuasan pengguna dan kesesuaian, berbanding dengan hasil Carian Imej Google yang paling baru. [[EENNDD]] pagerank; teori grafik; carian dan pengambilan maklumat; persamaan visual"], [{"string": "Semantic information portals No contact information provided yet.", "keywords": ["general", "information portals", "semantic web"], "combined": "Semantic information portals No contact information provided yet. [[EENNDD]] general; information portals; semantic web"}, "Portal maklumat semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] umum; portal maklumat; web semantik"], [{"string": "Tag ranking Social media sharing web sites like Flickr allow users to annotate images with free tags, which significantly facilitate Web image search and organization. However, the tags associated with an image generally are in a random order without any importance or relevance information, which limits the effectiveness of these tags in search and other applications. In this paper, we propose a tag ranking scheme, aiming to automatically rank the tags associated with a given image according to their relevance to the image content. We first estimate initial relevance scores for the tags based on probability density estimation, and then perform a random walk over a tag similarity graph to refine the relevance scores. Experimental results on a 50, 000 Flickr photo collection", "keywords": ["content analysis and indexing", "recommendation", "random walk", "tag ranking", "search", "flickr"], "combined": "Tag ranking Social media sharing web sites like Flickr allow users to annotate images with free tags, which significantly facilitate Web image search and organization. However, the tags associated with an image generally are in a random order without any importance or relevance information, which limits the effectiveness of these tags in search and other applications. In this paper, we propose a tag ranking scheme, aiming to automatically rank the tags associated with a given image according to their relevance to the image content. We first estimate initial relevance scores for the tags based on probability density estimation, and then perform a random walk over a tag similarity graph to refine the relevance scores. Experimental results on a 50, 000 Flickr photo collection [[EENNDD]] content analysis and indexing; recommendation; random walk; tag ranking; search; flickr"}, "Penarafan tag Laman sosial yang berkongsi laman web seperti Flickr membolehkan pengguna memberi komen gambar dengan tag percuma, yang memudahkan pencarian dan organisasi imej Web. Walau bagaimanapun, tag yang dikaitkan dengan gambar umumnya berada dalam urutan rawak tanpa ada kepentingan atau maklumat yang relevan, yang membatasi keberkesanan tag ini dalam carian dan aplikasi lain. Dalam makalah ini, kami mengusulkan skema pemeringkatan tag, yang bertujuan untuk secara automatik memberi peringkat tag yang terkait dengan gambar tertentu sesuai dengan relevansinya dengan isi gambar. Kami terlebih dahulu mengira skor relevansi awal untuk tag berdasarkan anggaran ketumpatan kebarangkalian, dan kemudian melakukan jalan rawak di atas grafik kesamaan tag untuk memperbaiki skor relevansi. Hasil eksperimen pada 50, 000 koleksi foto Flickr [[EENNDD]] analisis kandungan dan pengindeksan; cadangan; jalan rawak; kedudukan tag; cari; flickr"], [{"string": "Sync kit: a persistent client-side database caching toolkit for data intensive websites We introduce a client-server toolkit called Sync Kit that demonstrates how client-side database storage can improve the performance of data intensive websites. Sync Kit is designed to make use of the embedded relational database defined in the upcoming HTML5 standard to offload some data storage and processing from a web server onto the web browsers to which it serves content. Our toolkit provides various strategies for synchronizing relational database tables between the browser and the web server, along with a client-side template library so that portions web applications may be executed client-side. Unlike prior work in this area, Sync Kit persists both templates and data in the browser across web sessions, increasing the number of concurrent connections a server can handle by up to a factor of four versus that of a traditional server-only web stack and a factor of three versus a recent template caching approach.", "keywords": ["general", "cache", "web", "data base", "database", "client-side"], "combined": "Sync kit: a persistent client-side database caching toolkit for data intensive websites We introduce a client-server toolkit called Sync Kit that demonstrates how client-side database storage can improve the performance of data intensive websites. Sync Kit is designed to make use of the embedded relational database defined in the upcoming HTML5 standard to offload some data storage and processing from a web server onto the web browsers to which it serves content. Our toolkit provides various strategies for synchronizing relational database tables between the browser and the web server, along with a client-side template library so that portions web applications may be executed client-side. Unlike prior work in this area, Sync Kit persists both templates and data in the browser across web sessions, increasing the number of concurrent connections a server can handle by up to a factor of four versus that of a traditional server-only web stack and a factor of three versus a recent template caching approach. [[EENNDD]] general; cache; web; data base; database; client-side"}, "Sync kit: kit alat cache pangkalan data sisi pelanggan yang berterusan untuk laman web intensif data Kami memperkenalkan kit alat pelayan pelanggan yang disebut Sync Kit yang menunjukkan bagaimana penyimpanan pangkalan data sisi pelanggan dapat meningkatkan prestasi laman web intensif data. Sync Kit direka untuk menggunakan pangkalan data relasi tertanam yang ditentukan dalam standard HTML5 yang akan datang untuk memuatkan beberapa penyimpanan dan pemprosesan data dari pelayan web ke pelayar web yang menyajikan kandungannya. Kit alat kami menyediakan pelbagai strategi untuk menyinkronkan jadual pangkalan data hubungan antara penyemak imbas dan pelayan web, bersama dengan pustaka templat sisi pelanggan sehingga sebahagian aplikasi web dapat dijalankan dari sisi klien. Tidak seperti pekerjaan sebelumnya di bidang ini, Sync Kit tetap menggunakan templat dan data dalam penyemak imbas sepanjang sesi web, meningkatkan bilangan sambungan serentak yang dapat dikendalikan oleh pelayan hingga faktor empat berbanding dengan timbunan web pelayan tradisional dan faktor tiga berbanding pendekatan cache templat baru-baru ini. [[EENNDD]] umum; cache; laman web; pangkalan data; pangkalan data; pelanggan"], [{"string": "Using context- and content-based trust policies on the semantic web No contact information provided yet.", "keywords": ["trust mechanisms", "semantic web", "trust policies", "named graphs"], "combined": "Using context- and content-based trust policies on the semantic web No contact information provided yet. [[EENNDD]] trust mechanisms; semantic web; trust policies; named graphs"}, "Menggunakan dasar kepercayaan berasaskan konteks dan kandungan di web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] mekanisme kepercayaan; web semantik; dasar kepercayaan; graf yang dinamakan"], [{"string": "Anchor-based proximity measures We present a family of measures of proximity of an arbitrary node in a directed graph to a pre-specified subset of nodes, called the anchor. Our measures are based on three different propagation schemesand two different uses of the connectivity structure of the graph. We consider a web-specific application of the above measures with two disjoint anchors - good and bad web pages - and study the accuracy of these measures in this context.", "keywords": ["proximity", "link propagation", "harmonic rank", "miscellaneous"], "combined": "Anchor-based proximity measures We present a family of measures of proximity of an arbitrary node in a directed graph to a pre-specified subset of nodes, called the anchor. Our measures are based on three different propagation schemesand two different uses of the connectivity structure of the graph. We consider a web-specific application of the above measures with two disjoint anchors - good and bad web pages - and study the accuracy of these measures in this context. [[EENNDD]] proximity; link propagation; harmonic rank; miscellaneous"}, "Ukuran jarak berdasarkan jangkar Kami menyajikan sekumpulan ukuran jarak dari simpul sewenang-wenang dalam grafik yang diarahkan ke subset nod yang ditentukan sebelumnya, yang disebut jangkar. Ukuran kami berdasarkan tiga skema penyebaran yang berbeza dan dua penggunaan struktur sambungan grafik yang berbeza. Kami mempertimbangkan aplikasi khusus web untuk langkah-langkah di atas dengan dua jangkar yang terpisah - laman web yang baik dan buruk - dan mengkaji ketepatan langkah-langkah ini dalam konteks ini. [[EENNDD]] berdekatan; penyebaran pautan; peringkat harmonik; pelbagai"], [{"string": "Analysis of multimedia workloads with implications for internet streaming No contact information provided yet.", "keywords": ["internet"], "combined": "Analysis of multimedia workloads with implications for internet streaming No contact information provided yet. [[EENNDD]] internet"}, "Analisis beban kerja multimedia dengan implikasi untuk streaming internet Belum ada maklumat hubungan yang diberikan. [[EENNDD]] internet"], [{"string": "Constructing folksonomies from user-specified relations on flickr Automatic folksonomy construction from tags has attracted much attention recently. However, inferring hierarchical relations between concepts from tags has a drawback in that it is difficult to distinguish between more popular and more general concepts. Instead of tags we propose to use user-specified relations for learning folksonomy. We explore two statistical frameworks for aggregating many shallow individual hierarchies, expressed through the collection/set relations on the social photosharing site Flickr, into a common deeper folksonomy that reflects how a community organizes knowledge. Our approach addresses a number of challenges that arise while aggregating information from diverse users, namely noisy vocabulary, and variations in the granularity level of the concepts expressed. Our second contribution is a method for automatically evaluating learned folksonomy by comparing it to a reference taxonomy, e.g., the Web directory created by the Open Directory Project. Our empirical results suggest that user-specified relations are a good source of evidence for learning folksonomies.", "keywords": ["social information processing", "folksonomies", "taxonomies", "collective knowledge"], "combined": "Constructing folksonomies from user-specified relations on flickr Automatic folksonomy construction from tags has attracted much attention recently. However, inferring hierarchical relations between concepts from tags has a drawback in that it is difficult to distinguish between more popular and more general concepts. Instead of tags we propose to use user-specified relations for learning folksonomy. We explore two statistical frameworks for aggregating many shallow individual hierarchies, expressed through the collection/set relations on the social photosharing site Flickr, into a common deeper folksonomy that reflects how a community organizes knowledge. Our approach addresses a number of challenges that arise while aggregating information from diverse users, namely noisy vocabulary, and variations in the granularity level of the concepts expressed. Our second contribution is a method for automatically evaluating learned folksonomy by comparing it to a reference taxonomy, e.g., the Web directory created by the Open Directory Project. Our empirical results suggest that user-specified relations are a good source of evidence for learning folksonomies. [[EENNDD]] social information processing; folksonomies; taxonomies; collective knowledge"}, "Membina folksonomi dari hubungan yang ditentukan pengguna pada flickr Pembinaan folksonomi automatik dari tag telah menarik banyak perhatian baru-baru ini. Walau bagaimanapun, menyimpulkan hubungan hierarki antara konsep dari tag mempunyai kelemahan kerana sukar untuk membezakan antara konsep yang lebih popular dan lebih umum. Daripada teg, kami mencadangkan untuk menggunakan hubungan yang ditentukan pengguna untuk belajar folksonomi. Kami meneroka dua kerangka statistik untuk mengumpulkan banyak hierarki individu yang cetek, yang dinyatakan melalui pengumpulan / menetapkan hubungan di laman perkongsian foto sosial Flickr, menjadi sebuah folksonomi yang lebih mendalam yang mencerminkan bagaimana sebuah komuniti mengatur pengetahuan. Pendekatan kami menangani sejumlah cabaran yang timbul ketika mengumpulkan maklumat dari pelbagai pengguna, iaitu perbendaharaan kata yang bising, dan variasi dalam tahap butiran konsep yang dinyatakan. Sumbangan kedua kami adalah kaedah untuk menilai secara automatik folksonomi yang dipelajari dengan membandingkannya dengan taksonomi rujukan, misalnya, direktori Web yang dibuat oleh Open Directory Project. Hasil empirik kami menunjukkan bahawa hubungan yang ditentukan pengguna adalah sumber bukti yang baik untuk belajar folksonomies. [[EENNDD]] pemprosesan maklumat sosial; folksonomi; taksonomi; pengetahuan kolektif"], [{"string": "Declarative platform for data sourcing games Harnessing a crowd of users for the collection of mass data (data sourcing) has recently become a wide-spread practice. One effective technique is based on games as a tool that attracts the crowd to contribute useful facts. We focus here on the data management layer of such games, and observe that the development of this layer involves challenges such as dealing with probabilistic data, combined with recursive manipulation of this data. These challenges are difficult to address using current declarative data management framework works, and we thus propose here a novel such framework, and demonstrate its usefulness in expressing different aspects in the data management of Trivia-like games. We have implemented a system prototype with our novel data management framework at its core, and we highlight key issues in the system design, as well as our experimentations that indicate the usefulness and scalability of the approach.", "keywords": ["games", "databases", "crowdsourcing", "data description languages", "probabilistic"], "combined": "Declarative platform for data sourcing games Harnessing a crowd of users for the collection of mass data (data sourcing) has recently become a wide-spread practice. One effective technique is based on games as a tool that attracts the crowd to contribute useful facts. We focus here on the data management layer of such games, and observe that the development of this layer involves challenges such as dealing with probabilistic data, combined with recursive manipulation of this data. These challenges are difficult to address using current declarative data management framework works, and we thus propose here a novel such framework, and demonstrate its usefulness in expressing different aspects in the data management of Trivia-like games. We have implemented a system prototype with our novel data management framework at its core, and we highlight key issues in the system design, as well as our experimentations that indicate the usefulness and scalability of the approach. [[EENNDD]] games; databases; crowdsourcing; data description languages; probabilistic"}, "Platform deklaratif untuk permainan sumber data Memanfaatkan sekumpulan pengguna untuk pengumpulan data massa (sumber data) baru-baru ini menjadi amalan yang tersebar luas. Satu teknik yang berkesan adalah berdasarkan permainan sebagai alat yang menarik orang ramai untuk menyumbang fakta berguna. Kami memberi tumpuan di sini pada lapisan pengurusan data permainan seperti itu, dan melihat bahawa pengembangan lapisan ini melibatkan cabaran seperti menangani data probabilistik, digabungkan dengan manipulasi rekursif data ini. Cabaran-cabaran ini sukar ditangani dengan menggunakan kerangka kerja pengurusan data deklaratif semasa, dan dengan demikian kami mencadangkan kerangka baru seperti ini, dan menunjukkan kegunaannya dalam menyatakan aspek yang berbeza dalam pengurusan data permainan seperti Trivia. Kami telah menerapkan prototaip sistem dengan kerangka pengurusan data baru kami sebagai inti, dan kami menyoroti isu-isu penting dalam reka bentuk sistem, serta eksperimen kami yang menunjukkan kegunaan dan skalabilitas pendekatan. [[EENNDD]] permainan; pangkalan data; sumber orang ramai; bahasa penerangan data; kebarangkalian"], [{"string": "Partitioned multi-indexing: bringing order to social search To answer search queries on a social network rich with user-generated content, it is desirable to give a higher ranking to content that is closer to the individual issuing the query. Queries occur at nodes in the network, documents are also created by nodes in the same network, and the goal is to find the document that matches the query and is closest in network distance to the node issuing the query. In this paper, we present the \"Partitioned Multi-Indexing\" scheme, which provides an approximate solution to this problem. With m links in the network, after an offline ~O(m) pre-processing time, our scheme allows for social index operations (i.e., social search queries, as well as insertion and deletion of words into and from a document at any node), all in time ~O(1). Further, our scheme can be implemented on open source distributed streaming systems such as Yahoo! S4 or Twitter's Storm so that every social index operation takes ~O(1) processing time and network queries in the worst case, and just two network queries in the common case where the reverse index corresponding to the query keyword is much smaller than the memory available at any distributed compute node. Building on Das Sarma et al.'s approximate distance oracle, the worst-case approximation ratio of our scheme is ~O(1) for undirected networks. Our simulations on the social network Twitter as well as synthetic networks show that in practice, the approximation ratio is actually close to 1 for both directed and undirected networks. We believe that this work is the first demonstration of the feasibility of social search with real-time text updates at large scales.", "keywords": ["social search", "partitioned multi-indexing", "information search and retrieval", "real time", "scalable"], "combined": "Partitioned multi-indexing: bringing order to social search To answer search queries on a social network rich with user-generated content, it is desirable to give a higher ranking to content that is closer to the individual issuing the query. Queries occur at nodes in the network, documents are also created by nodes in the same network, and the goal is to find the document that matches the query and is closest in network distance to the node issuing the query. In this paper, we present the \"Partitioned Multi-Indexing\" scheme, which provides an approximate solution to this problem. With m links in the network, after an offline ~O(m) pre-processing time, our scheme allows for social index operations (i.e., social search queries, as well as insertion and deletion of words into and from a document at any node), all in time ~O(1). Further, our scheme can be implemented on open source distributed streaming systems such as Yahoo! S4 or Twitter's Storm so that every social index operation takes ~O(1) processing time and network queries in the worst case, and just two network queries in the common case where the reverse index corresponding to the query keyword is much smaller than the memory available at any distributed compute node. Building on Das Sarma et al.'s approximate distance oracle, the worst-case approximation ratio of our scheme is ~O(1) for undirected networks. Our simulations on the social network Twitter as well as synthetic networks show that in practice, the approximation ratio is actually close to 1 for both directed and undirected networks. We believe that this work is the first demonstration of the feasibility of social search with real-time text updates at large scales. [[EENNDD]] social search; partitioned multi-indexing; information search and retrieval; real time; scalable"}, "Berbilang pengindeksan berpecah: membawa pesanan ke carian sosial Untuk menjawab pertanyaan carian di rangkaian sosial yang kaya dengan kandungan yang dihasilkan pengguna, adalah wajar untuk memberi peringkat yang lebih tinggi kepada kandungan yang lebih dekat dengan individu yang mengeluarkan pertanyaan. Pertanyaan berlaku di node dalam rangkaian, dokumen juga dibuat oleh node di rangkaian yang sama, dan tujuannya adalah untuk mencari dokumen yang sesuai dengan pertanyaan dan paling dekat dalam jarak jaringan ke node yang mengeluarkan pertanyaan. Dalam makalah ini, kami membentangkan skema \"Partitioned Multi-Indexing\", yang memberikan jalan keluar untuk mengatasi masalah ini. Dengan pautan m dalam rangkaian, setelah waktu pra-pemprosesan ~ O (m) di luar talian, skema kami memungkinkan untuk operasi indeks sosial (iaitu, pertanyaan carian sosial, serta penyisipan dan penghapusan kata ke dalam dan dari dokumen di mana-mana nod ), sepanjang masa ~ O (1). Selanjutnya, skema kami dapat dilaksanakan pada sistem streaming yang diedarkan sumber terbuka seperti Yahoo! S4 atau Twitter's Storm sehingga setiap operasi indeks sosial memerlukan ~ O (1) masa memproses dan pertanyaan rangkaian dalam keadaan terburuk, dan hanya dua pertanyaan rangkaian dalam kes umum di mana indeks terbalik yang sesuai dengan kata kunci pertanyaan jauh lebih kecil daripada memori tersedia di mana-mana nod pengkomputeran yang diedarkan. Berdasarkan oracle jarak dekat Das Sarma et al., Nisbah penghampiran skema terburuk adalah ~ O (1) untuk rangkaian yang tidak diarahkan. Simulasi kami di rangkaian sosial Twitter dan juga rangkaian sintetik menunjukkan bahawa dalam praktiknya, nisbah penghampiran sebenarnya mendekati 1 untuk kedua-dua rangkaian yang diarahkan dan tidak diarahkan. Kami percaya bahawa karya ini adalah demonstrasi pertama mengenai kelayakan carian sosial dengan kemas kini teks masa nyata pada skala besar. [[EENNDD]] carian sosial; berbilang pengindeksan berpisah; carian dan pengambilan maklumat; masa sebenar; berskala"], [{"string": "BizCQ: using continual queries to cope with changes in business information exchange No contact information provided yet.", "keywords": ["content analysis and indexing", "b2b", "semantic web", "business-to-business", "continual query", "change response", "information quality"], "combined": "BizCQ: using continual queries to cope with changes in business information exchange No contact information provided yet. [[EENNDD]] content analysis and indexing; b2b; semantic web; business-to-business; continual query; change response; information quality"}, "BizCQ: menggunakan pertanyaan berterusan untuk mengatasi perubahan pertukaran maklumat perniagaan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] analisis dan pengindeksan kandungan; b2b; web semantik; perniagaan ke perniagaan; pertanyaan berterusan; menukar tindak balas; kualiti maklumat"], [{"string": "Beyond dwell time: estimating document relevance from cursor movements and other post-click searcher behavior Result clickthrough statistics and dwell time on clicked results have been shown valuable for inferring search result relevance, but the interpretation of these signals can vary substantially for different tasks and users. This paper shows that that post-click searcher behavior, such as cursor movement and scrolling, provides additional clues for better estimating document relevance. To this end, we identify patterns of examination and interaction behavior that correspond to viewing a relevant or non-relevant document, and design a new Post-Click Behavior (PCB) model to capture these patterns. To our knowledge, PCB is the first to successfully incorporate post-click searcher interactions such as cursor movements and scrolling on a landing page for estimating document relevance. We evaluate PCB on a dataset collected from a controlled user study that contains interactions gathered from hundreds of unique queries, result clicks, and page examinations. The experimental results show that PCB is significantly more effective than using page dwell time information alone, both for estimating the explicit judgments of each user, and for re-ranking the results using the estimated relevance.", "keywords": ["post-click search behavior", "relevance estimation"], "combined": "Beyond dwell time: estimating document relevance from cursor movements and other post-click searcher behavior Result clickthrough statistics and dwell time on clicked results have been shown valuable for inferring search result relevance, but the interpretation of these signals can vary substantially for different tasks and users. This paper shows that that post-click searcher behavior, such as cursor movement and scrolling, provides additional clues for better estimating document relevance. To this end, we identify patterns of examination and interaction behavior that correspond to viewing a relevant or non-relevant document, and design a new Post-Click Behavior (PCB) model to capture these patterns. To our knowledge, PCB is the first to successfully incorporate post-click searcher interactions such as cursor movements and scrolling on a landing page for estimating document relevance. We evaluate PCB on a dataset collected from a controlled user study that contains interactions gathered from hundreds of unique queries, result clicks, and page examinations. The experimental results show that PCB is significantly more effective than using page dwell time information alone, both for estimating the explicit judgments of each user, and for re-ranking the results using the estimated relevance. [[EENNDD]] post-click search behavior; relevance estimation"}, "Di luar masa senggang: menganggarkan perkaitan dokumen dari pergerakan kursor dan tingkah laku pencari pasca klik yang lain. Statistik klik lalu hasil dan masa penekanan pada hasil yang diklik telah terbukti berharga untuk menyimpulkan kesesuaian hasil carian, tetapi penafsiran isyarat ini boleh berbeza secara besar-besaran untuk tugas dan pengguna yang berbeza. . Makalah ini menunjukkan bahawa tingkah laku pencari pasca-klik, seperti pergerakan kursor dan tatal, memberikan petunjuk tambahan untuk menganggarkan relevansi dokumen dengan lebih baik. Untuk tujuan ini, kami mengenal pasti corak tingkah laku pemeriksaan dan interaksi yang sesuai dengan melihat dokumen yang relevan atau tidak berkaitan, dan merancang model Perilaku Pasca Klik (PCB) baru untuk menangkap corak ini. Setahu kami, PCB adalah yang pertama berjaya menggabungkan interaksi pencari pasca klik seperti pergerakan kursor dan menatal di halaman arahan untuk menganggar kesesuaian dokumen. Kami menilai PCB pada set data yang dikumpulkan dari kajian pengguna terkawal yang mengandungi interaksi yang dikumpulkan dari beratus-ratus pertanyaan unik, klik hasil, dan pemeriksaan halaman. Hasil eksperimen menunjukkan bahawa PCB secara signifikan lebih efektif daripada menggunakan maklumat masa tinggal halaman sahaja, baik untuk menganggarkan penilaian eksplisit setiap pengguna, dan untuk menilai semula hasilnya menggunakan anggaran yang relevan. [[EENNDD]] tingkah laku carian pasca klik; anggaran perkaitan"], [{"string": "Capturing RIA concepts in a web modeling language No contact information provided yet.", "keywords": ["computer-aided software engineering", "web site design", "web engineering", "rich internet applications"], "combined": "Capturing RIA concepts in a web modeling language No contact information provided yet. [[EENNDD]] computer-aided software engineering; web site design; web engineering; rich internet applications"}, "Menangkap konsep RIA dalam bahasa pemodelan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kejuruteraan perisian berbantukan komputer; reka bentuk laman web; kejuruteraan web; aplikasi internet yang kaya"], [{"string": "Selective hypertext induced topic search No contact information provided yet.", "keywords": ["searching", "topic distillation", "link analysis"], "combined": "Selective hypertext induced topic search No contact information provided yet. [[EENNDD]] searching; topic distillation; link analysis"}, "Pencarian topik yang disebabkan oleh hiperteks terpilih Belum ada maklumat hubungan yang diberikan. [[EENNDD]] mencari; penyulingan topik; analisis pautan"], [{"string": "Mining the search trails of surfing crowds: identifying relevant websites from user activity The paper proposes identifying relevant information sources from the history of combined searching and browsing behavior of many Web users. While it has been previously shown that user interactions with search engines can be employed to improve document ranking, browsing behavior that occurs beyond search result pages has been largely overlooked in prior work. The paper demonstrates that users' post-search browsing activity strongly reflects implicit endorsement of visited pages, which allows estimating topical relevance of Web resources by mining large-scale datasets of search trails. We present heuristic and probabilistic algorithms that rely on such datasets for suggesting authoritative websites for search queries. Experimental evaluation shows that exploiting complete post-search browsing trails outperforms alternatives in isolation (e.g., clickthrough logs), and yields accuracy improvements when employed as a feature in learning to rank for Web search.", "keywords": ["web search", "implicit feedback", "mining search and browsing logs", "learning from user behavior"], "combined": "Mining the search trails of surfing crowds: identifying relevant websites from user activity The paper proposes identifying relevant information sources from the history of combined searching and browsing behavior of many Web users. While it has been previously shown that user interactions with search engines can be employed to improve document ranking, browsing behavior that occurs beyond search result pages has been largely overlooked in prior work. The paper demonstrates that users' post-search browsing activity strongly reflects implicit endorsement of visited pages, which allows estimating topical relevance of Web resources by mining large-scale datasets of search trails. We present heuristic and probabilistic algorithms that rely on such datasets for suggesting authoritative websites for search queries. Experimental evaluation shows that exploiting complete post-search browsing trails outperforms alternatives in isolation (e.g., clickthrough logs), and yields accuracy improvements when employed as a feature in learning to rank for Web search. [[EENNDD]] web search; implicit feedback; mining search and browsing logs; learning from user behavior"}, "Melombong jejak pencarian orang ramai melayari: mengenal pasti laman web yang relevan dari aktiviti pengguna Makalah ini mencadangkan untuk mengenal pasti sumber maklumat yang relevan dari sejarah gabungan tingkah laku mencari dan melayari banyak pengguna Web. Walaupun sebelumnya telah ditunjukkan bahawa interaksi pengguna dengan mesin pencari dapat digunakan untuk meningkatkan peringkat dokumen, tingkah laku melayari yang terjadi di luar halaman hasil carian banyak diabaikan pada pekerjaan sebelumnya. Makalah ini menunjukkan bahawa aktiviti menyemak imbas pasca carian pengguna sangat mencerminkan sokongan tersirat dari halaman yang dikunjungi, yang memungkinkan untuk menganggarkan relevansi topikal sumber Web dengan melombong set data skala pencarian skala besar. Kami mengemukakan algoritma heuristik dan probabilistik yang bergantung pada set data tersebut untuk mencadangkan laman web yang berwibawa untuk pertanyaan carian. Penilaian eksperimental menunjukkan bahawa mengeksploitasi jejak penyemakan imbas pasca carian yang lengkap mengatasi alternatif secara terpisah (mis., Log klik-tayang), dan menghasilkan peningkatan ketepatan ketika digunakan sebagai fitur dalam belajar memberi peringkat untuk carian Web. [[EENNDD]] carian web; maklum balas tersirat; melombong carian dan melayari log; belajar dari tingkah laku pengguna"], [{"string": "Explorations in the use of semantic web technologies for product information management Master data refers to core business entities a company uses repeatedly across many business processes and systems (such as lists or hierarchies of customers, suppliers, accounts, products, or organizational units). Product information is the most important kind of master data and product information management (PIM) is becoming critical for modern enterprises because it provides a rich business context for various applications. Existing PIM systems are less flexible and scalable for on-demand business, as well as too weak to completely capture and use the semantics of master data. This paper explores how to use semantic web technologies to enhance a collaborative PIM system by simplifying modeling and representation while preserving enough dynamic flexibility. Furthermore, we build a semantic PIM system using one of the state-of-art ontology repositories and summarize the challenges we encountered based on our experimental results, especially on performance and scalability. We believe that our study and experiences are valuable for both semantic web community and master data management community.", "keywords": ["modeling", "general", "master data management", "ontology", "semantic web", "product information management"], "combined": "Explorations in the use of semantic web technologies for product information management Master data refers to core business entities a company uses repeatedly across many business processes and systems (such as lists or hierarchies of customers, suppliers, accounts, products, or organizational units). Product information is the most important kind of master data and product information management (PIM) is becoming critical for modern enterprises because it provides a rich business context for various applications. Existing PIM systems are less flexible and scalable for on-demand business, as well as too weak to completely capture and use the semantics of master data. This paper explores how to use semantic web technologies to enhance a collaborative PIM system by simplifying modeling and representation while preserving enough dynamic flexibility. Furthermore, we build a semantic PIM system using one of the state-of-art ontology repositories and summarize the challenges we encountered based on our experimental results, especially on performance and scalability. We believe that our study and experiences are valuable for both semantic web community and master data management community. [[EENNDD]] modeling; general; master data management; ontology; semantic web; product information management"}, "Penerokaan dalam penggunaan teknologi web semantik untuk pengurusan maklumat produk Data induk merujuk kepada entiti perniagaan teras yang digunakan syarikat berulang kali di banyak proses dan sistem perniagaan (seperti senarai atau hierarki pelanggan, pembekal, akaun, produk, atau unit organisasi). Maklumat produk adalah jenis data induk yang paling penting dan pengurusan maklumat produk (PIM) menjadi penting bagi perusahaan moden kerana menyediakan konteks perniagaan yang kaya untuk pelbagai aplikasi. Sistem PIM yang ada kurang fleksibel dan berskala untuk perniagaan berdasarkan permintaan, serta terlalu lemah untuk menangkap dan menggunakan semantik data induk sepenuhnya. Makalah ini meneroka bagaimana menggunakan teknologi web semantik untuk meningkatkan sistem PIM kolaboratif dengan mempermudah pemodelan dan perwakilan sambil mengekalkan fleksibiliti dinamik yang cukup. Selanjutnya, kami membina sistem PIM semantik menggunakan salah satu repositori ontologi canggih dan merangkum cabaran yang kami hadapi berdasarkan hasil eksperimen kami, terutama pada prestasi dan skalabilitas. Kami percaya bahawa kajian dan pengalaman kami sangat berharga bagi komuniti web semantik dan komuniti pengurusan data induk. [[EENNDD]] pemodelan; umum; pengurusan data induk; ontologi; web semantik; pengurusan maklumat produk"], [{"string": "Implementing physical hyperlinks using ubiquitous identifier resolution No contact information provided yet.", "keywords": ["physical hyperlinks", "ubiquitous computing", "identifier resolution", "nomadic computing", "mobile computing"], "combined": "Implementing physical hyperlinks using ubiquitous identifier resolution No contact information provided yet. [[EENNDD]] physical hyperlinks; ubiquitous computing; identifier resolution; nomadic computing; mobile computing"}, "Melaksanakan hyperlink fizikal menggunakan resolusi pengenal di mana-mana. Belum ada maklumat hubungan yang diberikan. [[EENNDD]] hyperlink fizikal; pengkomputeran di mana-mana; ketetapan pengecam; pengkomputeran nomad; pengkomputeran mudah alih"], [{"string": "Detection and analysis of drive-by-download attacks and malicious JavaScript code JavaScript is a browser scripting language that allows developers to create sophisticated client-side interfaces for web applications. However, JavaScript code is also used to carry out attacks against the user's browser and its extensions. These attacks usually result in the download of additional malware that takes complete control of the victim's platform, and are, therefore, called \"drive-by downloads.\" Unfortunately, the dynamic nature of the JavaScript language and its tight integration with the browser make it difficult to detect and block malicious JavaScript code.", "keywords": ["anomaly detection", "drive-by-download attacks", "web client exploits", "security and protection"], "combined": "Detection and analysis of drive-by-download attacks and malicious JavaScript code JavaScript is a browser scripting language that allows developers to create sophisticated client-side interfaces for web applications. However, JavaScript code is also used to carry out attacks against the user's browser and its extensions. These attacks usually result in the download of additional malware that takes complete control of the victim's platform, and are, therefore, called \"drive-by downloads.\" Unfortunately, the dynamic nature of the JavaScript language and its tight integration with the browser make it difficult to detect and block malicious JavaScript code. [[EENNDD]] anomaly detection; drive-by-download attacks; web client exploits; security and protection"}, "Pengesanan dan analisis serangan drive-by-download dan JavaScript kod JavaScript berbahaya adalah bahasa skrip penyemak imbas yang membolehkan pembangun membuat antara muka pelanggan yang canggih untuk aplikasi web. Walau bagaimanapun, kod JavaScript juga digunakan untuk melakukan serangan terhadap penyemak imbas pengguna dan pelanjutannya. Serangan ini biasanya menghasilkan muat turun malware tambahan yang mengendalikan sepenuhnya platform mangsa, dan oleh itu disebut \"muat turun drive-by.\" Malangnya, sifat bahasa JavaScript yang dinamik dan penyatuannya yang ketat dengan penyemak imbas menjadikannya sukar untuk mengesan dan menyekat kod JavaScript yang berbahaya. [[EENNDD]] pengesanan anomali; serangan pemacu demi muat turun; eksploitasi pelanggan web; keselamatan dan perlindungan"], [{"string": "A clustering method for web data with multi-type interrelated components Traditional clustering algorithms work on \"flat\" data, making the assumption that the data instances can only be represented by a set of homogeneous and uniform features. Many real world data, however, is heterogeneous in nature, comprising of multiple types of interrelated components. We present a clustering algorithm, K-SVMeans, that integrates the well known K-Means clustering with the highly popular Support Vector Machines(SVM) in order to utilize the richness of data. Our experimental results on authorship analysis of scientific publications show that K-SVMeans achieves better clustering performance than homogeneous data clustering.", "keywords": ["k-means", "k-svmeans", "online svm", "multi-type data clustering"], "combined": "A clustering method for web data with multi-type interrelated components Traditional clustering algorithms work on \"flat\" data, making the assumption that the data instances can only be represented by a set of homogeneous and uniform features. Many real world data, however, is heterogeneous in nature, comprising of multiple types of interrelated components. We present a clustering algorithm, K-SVMeans, that integrates the well known K-Means clustering with the highly popular Support Vector Machines(SVM) in order to utilize the richness of data. Our experimental results on authorship analysis of scientific publications show that K-SVMeans achieves better clustering performance than homogeneous data clustering. [[EENNDD]] k-means; k-svmeans; online svm; multi-type data clustering"}, "Kaedah pengelompokan untuk data web dengan komponen yang saling berkaitan pelbagai jenis Algoritma pengelompokan tradisional berfungsi pada data \"rata\", membuat andaian bahawa contoh data hanya dapat diwakili oleh sekumpulan ciri homogen dan seragam. Akan tetapi, banyak data dunia nyata bersifat heterogen, yang terdiri daripada pelbagai jenis komponen yang saling berkaitan. Kami menyajikan algoritma kluster, K-SVMeans, yang mengintegrasikan pengelompokan K-Means yang terkenal dengan Mesin Vektor Sokongan (SVM) yang sangat popular untuk memanfaatkan kekayaan data. Hasil eksperimen kami pada analisis kepengarangan penerbitan ilmiah menunjukkan bahawa K-SVMeans mencapai prestasi pengelompokan yang lebih baik daripada pengelompokan data homogen. [[EENNDD]] k-bermaksud; k-svmeans; svm dalam talian; pengelompokan data pelbagai jenis"], [{"string": "Web ontology segmentation: analysis, classification and use No contact information provided yet.", "keywords": ["ontology", "scalability", "semantic web", "owl", "segmentation"], "combined": "Web ontology segmentation: analysis, classification and use No contact information provided yet. [[EENNDD]] ontology; scalability; semantic web; owl; segmentation"}, "Segmentasi ontologi web: analisis, klasifikasi dan penggunaan Belum ada maklumat hubungan yang disediakan. [[EENNDD]] ontologi; skalabiliti; web semantik; burung hantu; pembahagian"], [{"string": "CloudGenius: decision support for web server cloud migration Cloud computing is the latest computing paradigm that delivers hardware and software resources as virtualized services in which users are free from the burden of worrying about the low-level system administration details. Migrating Web applications to Cloud services and integrating Cloud services into existing computing infrastructures is non-trivial. It leads to new challenges that often require innovation of paradigms and practices at all levels: technical, cultural, legal, regulatory, and social. The key problem in mapping Web applications to virtualized Cloud services is selecting the best and compatible mix of software images (e.g., Web server image) and infrastructure services to ensure that Quality of Service (QoS) targets of an application are achieved. The fact that, when selecting Cloud services, engineers must consider heterogeneous sets of criteria and complex dependencies between infrastructure services and software images, which are impossible to resolve manually, is a critical issue. To overcome these challenges, we present a framework (called CloudGenius) which automates the decision-making process based on a model and factors specifically for Web server migration to the Cloud. CloudGenius leverages a well known multi-criteria decision making technique, called Analytic Hierarchy Process, to automate the selection process based on a model, factors, and QoS parameters related to an application. An example application demonstrates the applicability of the theoretical CloudGenius approach. Moreover, we present an implementation of CloudGenius that has been validated through experiments.", "keywords": ["service selection", "decision support", "automation", "cloud computing", "criteria", "computer-aided software engineering", "factors", "selection algorithm", "migration process"], "combined": "CloudGenius: decision support for web server cloud migration Cloud computing is the latest computing paradigm that delivers hardware and software resources as virtualized services in which users are free from the burden of worrying about the low-level system administration details. Migrating Web applications to Cloud services and integrating Cloud services into existing computing infrastructures is non-trivial. It leads to new challenges that often require innovation of paradigms and practices at all levels: technical, cultural, legal, regulatory, and social. The key problem in mapping Web applications to virtualized Cloud services is selecting the best and compatible mix of software images (e.g., Web server image) and infrastructure services to ensure that Quality of Service (QoS) targets of an application are achieved. The fact that, when selecting Cloud services, engineers must consider heterogeneous sets of criteria and complex dependencies between infrastructure services and software images, which are impossible to resolve manually, is a critical issue. To overcome these challenges, we present a framework (called CloudGenius) which automates the decision-making process based on a model and factors specifically for Web server migration to the Cloud. CloudGenius leverages a well known multi-criteria decision making technique, called Analytic Hierarchy Process, to automate the selection process based on a model, factors, and QoS parameters related to an application. An example application demonstrates the applicability of the theoretical CloudGenius approach. Moreover, we present an implementation of CloudGenius that has been validated through experiments. [[EENNDD]] service selection; decision support; automation; cloud computing; criteria; computer-aided software engineering; factors; selection algorithm; migration process"}, "CloudGenius: sokongan keputusan untuk penghijrahan awan pelayan web Cloud pengkomputeran adalah paradigma pengkomputeran terbaru yang memberikan sumber perkakasan dan perisian sebagai perkhidmatan maya di mana pengguna bebas dari beban bimbang tentang perincian pentadbiran sistem tahap rendah. Memindahkan aplikasi Web ke perkhidmatan Cloud dan mengintegrasikan perkhidmatan Cloud ke dalam infrastruktur pengkomputeran yang ada adalah tidak remeh. Ini membawa kepada cabaran baru yang sering memerlukan inovasi paradigma dan amalan di semua peringkat: teknikal, budaya, undang-undang, peraturan, dan sosial. Masalah utama dalam memetakan aplikasi Web ke perkhidmatan Awan maya adalah memilih gabungan gambar perisian yang terbaik dan serasi (mis., Gambar pelayan Web) dan perkhidmatan infrastruktur untuk memastikan bahawa sasaran Kualiti Perkhidmatan (QoS) aplikasi tercapai. Fakta bahawa, ketika memilih perkhidmatan Cloud, jurutera mesti mempertimbangkan set kriteria yang berbeza dan ketergantungan yang kompleks antara perkhidmatan infrastruktur dan gambar perisian, yang mustahil diselesaikan secara manual, adalah masalah penting. Untuk mengatasi cabaran ini, kami menyajikan kerangka kerja (disebut CloudGenius) yang mengotomatisasi proses membuat keputusan berdasarkan model dan faktor-faktor khusus untuk penghijrahan pelayan Web ke Cloud. CloudGenius memanfaatkan teknik membuat keputusan multi-kriteria yang terkenal, yang disebut Proses Hierarki Analitik, untuk mengotomatisasi proses pemilihan berdasarkan model, faktor, dan parameter QoS yang berkaitan dengan aplikasi. Aplikasi contoh menunjukkan kebolehlaksanaan pendekatan CloudGenius secara teori. Selain itu, kami menyajikan pelaksanaan CloudGenius yang telah disahkan melalui eksperimen. [[EENNDD]] pemilihan perkhidmatan; sokongan keputusan; automasi; pengkomputeran awan; kriteria; kejuruteraan perisian berbantukan komputer; faktor; algoritma pemilihan; proses penghijrahan"], [{"string": "Privacy preserving frequency capping in internet banner advertising We describe an optimize-and-dispatch approach for delivering pay-per-impression advertisements in online advertising. The platform provider for an advertising network commits to showing advertisers' banner ads while capping the number of advertising message shown to a unique user as the user transitions through the network. The traditional approach for enforcing frequency caps has been to use cross-site cookies to track users. However,cross-site cookies and other tracking mechanisms can infringe on the user privacy. In this paper, we propose a novel linear programming approach that decides when to show an ad to the user based solely on the page currently viewed by the users. We show that the frequency caps are fulfilled in expectation. We show the efficacy of that approach using simulation results.", "keywords": ["user model", "privacy"], "combined": "Privacy preserving frequency capping in internet banner advertising We describe an optimize-and-dispatch approach for delivering pay-per-impression advertisements in online advertising. The platform provider for an advertising network commits to showing advertisers' banner ads while capping the number of advertising message shown to a unique user as the user transitions through the network. The traditional approach for enforcing frequency caps has been to use cross-site cookies to track users. However,cross-site cookies and other tracking mechanisms can infringe on the user privacy. In this paper, we propose a novel linear programming approach that decides when to show an ad to the user based solely on the page currently viewed by the users. We show that the frequency caps are fulfilled in expectation. We show the efficacy of that approach using simulation results. [[EENNDD]] user model; privacy"}, "Pembatasan frekuensi menjaga privasi dalam periklanan sepanduk internet Kami menerangkan pendekatan pengoptimalan dan pengiriman untuk menyampaikan iklan bayar per tayangan dalam iklan dalam talian. Penyedia platform untuk rangkaian pengiklanan berkomitmen untuk menunjukkan iklan sepanduk pengiklan sambil membatasi jumlah mesej iklan yang ditunjukkan kepada pengguna unik ketika pengguna beralih melalui rangkaian. Pendekatan tradisional untuk menegakkan had frekuensi adalah dengan menggunakan kuki silang untuk mengesan pengguna. Walau bagaimanapun, kuki lintas tapak dan mekanisme penjejakan lain boleh melanggar privasi pengguna. Dalam makalah ini, kami mencadangkan pendekatan pengaturcaraan linier baru yang memutuskan kapan untuk menampilkan iklan kepada pengguna hanya berdasarkan halaman yang dilihat oleh pengguna pada masa ini. Kami menunjukkan bahawa had frekuensi dipenuhi dengan jangkaan. Kami menunjukkan keberkesanan pendekatan itu menggunakan hasil simulasi. [[EENNDD]] model pengguna; privasi"], [{"string": "Processing link structures and linkbases on the web Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.", "keywords": ["xlink", "link modeling and processing", "hyperlink", "linkbase", "electronic publishing"], "combined": "Processing link structures and linkbases on the web Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references. [[EENNDD]] xlink; link modeling and processing; hyperlink; linkbase; electronic publishing"}, "Memproses struktur pautan dan pangkalan pautan di web Catatan: Kesalahan OCR mungkin terdapat dalam Senarai Rujukan ini yang diekstrak dari artikel teks lengkap. ACM memilih untuk mendedahkan Senarai lengkap dan bukan hanya rujukan yang betul dan berkaitan. [[EENNDD]] xlink; pemodelan dan pemprosesan pautan; pautan hiper; pautan pautan; penerbitan elektronik"], [{"string": "A link classification based approach to website topic hierarchy generation Hierarchical models are commonly used to organize a Website's content. A Website's content structure can be represented by a topic hierarchy, a directed tree rooted at a Website's homepage in which the vertices and edges correspond to Web pages and hyperlinks. In this work, we propose a new method for constructing the topic hierarchy of a Website. We model the Website's link structure using weighted directed graph, in which the edge weights are computed using a classifier that predicts if an edge connects a pair of nodes representing a topic and a sub-topic. We then pose the problem of building the topic hierarchy as finding the shortest-path tree and directed minimum spanning tree in the weighted graph. We've done extensive experiments using real Websites and obtained very promising results.", "keywords": ["content structure", "topioc hierarchy", "website mining"], "combined": "A link classification based approach to website topic hierarchy generation Hierarchical models are commonly used to organize a Website's content. A Website's content structure can be represented by a topic hierarchy, a directed tree rooted at a Website's homepage in which the vertices and edges correspond to Web pages and hyperlinks. In this work, we propose a new method for constructing the topic hierarchy of a Website. We model the Website's link structure using weighted directed graph, in which the edge weights are computed using a classifier that predicts if an edge connects a pair of nodes representing a topic and a sub-topic. We then pose the problem of building the topic hierarchy as finding the shortest-path tree and directed minimum spanning tree in the weighted graph. We've done extensive experiments using real Websites and obtained very promising results. [[EENNDD]] content structure; topioc hierarchy; website mining"}, "Pendekatan berdasarkan klasifikasi pautan ke generasi hierarki topik laman web Model hierarki biasanya digunakan untuk mengatur kandungan Laman Web. Struktur kandungan Laman Web dapat diwakili oleh hierarki topik, pohon yang diarahkan yang berakar pada laman utama laman web di mana bucu dan tepi sesuai dengan halaman Web dan pautan. Dalam karya ini, kami mencadangkan kaedah baru untuk membina hierarki topik Laman Web. Kami memodelkan struktur pautan Laman web menggunakan graf terarah berwajaran, di mana bobot tepi dihitung menggunakan pengkelasan yang meramalkan jika suatu tepi menghubungkan sepasang nod yang mewakili topik dan subtopik. Kami kemudian menimbulkan masalah membina hierarki topik sebagai mencari pokok jalan terpendek dan pokok rentang minimum terarah dalam grafik berwajaran. Kami telah melakukan eksperimen yang luas menggunakan Laman Web sebenar dan memperoleh hasil yang sangat menjanjikan. [[EENNDD]] struktur kandungan; hierarki topioc; perlombongan laman web"], [{"string": "CS AKTive space: representing computer science in the semantic web No contact information provided yet.", "keywords": ["semantic web challenge groups", "ontologies", "semantic web", "user interfaces"], "combined": "CS AKTive space: representing computer science in the semantic web No contact information provided yet. [[EENNDD]] semantic web challenge groups; ontologies; semantic web; user interfaces"}, "CS AKTive space: mewakili sains komputer di web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kumpulan cabaran web semantik; ontologi; web semantik; antara muka pengguna"], [{"string": "Meaning and the semantic web No contact information provided yet.", "keywords": ["meaning", "representation", "semantic web"], "combined": "Meaning and the semantic web No contact information provided yet. [[EENNDD]] meaning; representation; semantic web"}, "Makna dan web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] bermaksud; perwakilan; web semantik"], [{"string": "Mining web logs to improve website organization An abstract is not available.", "keywords": ["interaction styles"], "combined": "Mining web logs to improve website organization An abstract is not available. [[EENNDD]] interaction styles"}, "Melombong log web untuk meningkatkan organisasi laman web Abstrak tidak tersedia. [[EENNDD]] gaya interaksi"], [{"string": "TotalRank: ranking without damping Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.", "keywords": ["ranking", "pagerank", "link farms", "discrete mathematics", "probability and statistics", "kendall's \u03c4"], "combined": "TotalRank: ranking without damping Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references. [[EENNDD]] ranking; pagerank; link farms; discrete mathematics; probability and statistics; kendall's \u03c4"}, "TotalRank: ranking tanpa redaman Catatan: Kesalahan OCR mungkin terdapat dalam Daftar Rujukan ini yang diekstrak dari artikel teks lengkap. ACM memilih untuk mendedahkan Senarai lengkap dan bukan hanya rujukan yang betul dan berkaitan. [[EENNDD]] kedudukan; pagerank; pautan ladang; matematik diskrit; kebarangkalian dan statistik; kendall's \u03c4"], [{"string": "Hera presentation generator No contact information provided yet.", "keywords": ["design environment", "swis", "computer-aided software engineering", "wis", "semantic web", "rdf"], "combined": "Hera presentation generator No contact information provided yet. [[EENNDD]] design environment; swis; computer-aided software engineering; wis; semantic web; rdf"}, "Penjana persembahan Hera Belum ada maklumat hubungan yang diberikan. [[EENNDD]] persekitaran reka bentuk; swis; kejuruteraan perisian berbantukan komputer; bijaksana; web semantik; rdf"], [{"string": "Learning deterministic regular expressions for the inference of schemas from XML data Inferring an appropriate DTD or XML Schema Definition (XSD) for a given collection of XML documents essentially reduces to learning deterministic regular expressions from sets of positive example words. Unfortunately, there is no algorithm capable of learning the complete class of deterministic regular expressions from positive examples only, as we will show. The regular expressions occurring in practical DTDs and XSDs, however, are such that every alphabet symbol occurs only a small number of times. As such, in practice it suffices to learn the subclass of regular expressions in which each alphabet symbol occurs at most k times, for some small k. We refer to such expressions as k-occurrence regular expressions (k-OREs for short). Motivated by this observation, we provide a probabilistic algorithm that learns k-OREs for increasing values of k, and selects the one that best describes the sample based on a Minimum Description Length argument. The effectiveness of the method is empirically validated both on real world and synthetic data. Furthermore, the method is shown to be conservative over the simpler classes of expressions considered in previous work.", "keywords": ["document preparation", "learning", "schema inference", "xml", "regular expressions", "formal languages"], "combined": "Learning deterministic regular expressions for the inference of schemas from XML data Inferring an appropriate DTD or XML Schema Definition (XSD) for a given collection of XML documents essentially reduces to learning deterministic regular expressions from sets of positive example words. Unfortunately, there is no algorithm capable of learning the complete class of deterministic regular expressions from positive examples only, as we will show. The regular expressions occurring in practical DTDs and XSDs, however, are such that every alphabet symbol occurs only a small number of times. As such, in practice it suffices to learn the subclass of regular expressions in which each alphabet symbol occurs at most k times, for some small k. We refer to such expressions as k-occurrence regular expressions (k-OREs for short). Motivated by this observation, we provide a probabilistic algorithm that learns k-OREs for increasing values of k, and selects the one that best describes the sample based on a Minimum Description Length argument. The effectiveness of the method is empirically validated both on real world and synthetic data. Furthermore, the method is shown to be conservative over the simpler classes of expressions considered in previous work. [[EENNDD]] document preparation; learning; schema inference; xml; regular expressions; formal languages"}, "Mempelajari ekspresi reguler deterministik untuk inferensi skema dari data XML Menyimpulkan DTD atau XML Schema Definition (XSD) yang sesuai untuk koleksi dokumen XML yang diberikan pada dasarnya akan mengurangkan pembelajaran ekspresi reguler deterministik dari kumpulan kata contoh positif. Malangnya, tidak ada algoritma yang dapat mempelajari kelas lengkap ungkapan biasa deterministik dari contoh positif sahaja, seperti yang akan kami tunjukkan. Walau bagaimanapun, ungkapan biasa yang berlaku dalam DTD praktikal dan XSD sedemikian rupa sehingga setiap simbol abjad hanya berlaku sebilangan kecil kali. Oleh itu, dalam praktiknya cukup untuk mempelajari subkelas ungkapan biasa di mana setiap simbol abjad berlaku paling banyak k kali, untuk beberapa k kecil. Kami merujuk ungkapan seperti ungkapan biasa k-kejadian (k-OREs pendek). Didorong oleh pemerhatian ini, kami menyediakan algoritma probabilistik yang mempelajari k-ORE untuk meningkatkan nilai k, dan memilih yang paling tepat menggambarkan sampel berdasarkan argumen Panjang Huraian Minimum. Keberkesanan kaedah ini disahkan secara empirik baik di dunia nyata dan data sintetik. Selanjutnya, kaedah ini terbukti konservatif berbanding kelas ungkapan yang lebih sederhana yang dipertimbangkan dalam karya sebelumnya. [[EENNDD]] penyediaan dokumen; belajar; inferens skema; xml; ungkapan biasa; bahasa formal"], [{"string": "Adaptive record extraction from web pages We describe an adaptive method for extracting records from web pages. Our algorithm combines a weighted tree matching metric with clustering for obtaining data extraction patterns.We compare our method experimentally to the state-of-the-art, and show that our approach is very competitive for rigidly-structured records (such as product descriptions) and far superior for loosely-structured records (such as entrieson blogs).", "keywords": ["deep web", "data extraction"], "combined": "Adaptive record extraction from web pages We describe an adaptive method for extracting records from web pages. Our algorithm combines a weighted tree matching metric with clustering for obtaining data extraction patterns.We compare our method experimentally to the state-of-the-art, and show that our approach is very competitive for rigidly-structured records (such as product descriptions) and far superior for loosely-structured records (such as entrieson blogs). [[EENNDD]] deep web; data extraction"}, "Pengekstrakan rekod adaptif dari laman web Kami menerangkan kaedah penyesuaian untuk mengekstrak rekod dari laman web. Algoritma kami menggabungkan metrik pemadanan pokok berbobot dengan pengelompokan untuk mendapatkan corak pengekstrakan data. Kami membandingkan kaedah kami secara eksperimental dengan yang terkini, dan menunjukkan bahawa pendekatan kami sangat kompetitif untuk rekod berstruktur kaku (seperti keterangan produk) dan jauh lebih unggul untuk rekod berstruktur longgar (seperti entri di blog). [[EENNDD]] web dalam; pengekstrakan data"], [{"string": "Information retrieval in P2P networks using genetic algorithm No contact information provided yet.", "keywords": ["genetic algorithm", "query routing", "longest path problem", "p2p"], "combined": "Information retrieval in P2P networks using genetic algorithm No contact information provided yet. [[EENNDD]] genetic algorithm; query routing; longest path problem; p2p"}, "Pengambilan maklumat dalam rangkaian P2P menggunakan algoritma genetik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] algoritma genetik; penghalaan pertanyaan; masalah jalan terpanjang; p2p"], [{"string": "Decentralized orchestration of composite web services No contact information provided yet.", "keywords": ["code partitioning", "decentralized orchestration", "composite web services", "bpel4ws"], "combined": "Decentralized orchestration of composite web services No contact information provided yet. [[EENNDD]] code partitioning; decentralized orchestration; composite web services; bpel4ws"}, "Orkestrasi perkhidmatan web komposit yang terdesentralisasi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pembahagian kod; orkestrasi terdesentralisasi; perkhidmatan web komposit; bpel4ws"], [{"string": "XHTML meta data profiles No contact information provided yet.", "keywords": ["link relationships", "reuse", "html", "class names", "schema", "meta data", "xmdp", "www", "profiles", "xhtml", "lowercase semantic web", "xfn", "world wide web", "microformats"], "combined": "XHTML meta data profiles No contact information provided yet. [[EENNDD]] link relationships; reuse; html; class names; schema; meta data; xmdp; www; profiles; xhtml; lowercase semantic web; xfn; world wide web; microformats"}, "Profil data meta XHTML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] menghubungkan hubungan; menggunakan semula; html; nama kelas; skema; data meta; xmdp; www; profil; xhtml; web semantik huruf kecil; xfn; laman web seluruh dunia; mikroformat"], [{"string": "A method for modeling uncertainty in semantic web taxonomies No contact information provided yet.", "keywords": ["ontology", "knowledge representation formalisms and methods", "uncertainty", "semantic web"], "combined": "A method for modeling uncertainty in semantic web taxonomies No contact information provided yet. [[EENNDD]] ontology; knowledge representation formalisms and methods; uncertainty; semantic web"}, "Kaedah untuk memodelkan ketidaktentuan dalam taksonomi web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ontologi; formalisme dan kaedah perwakilan pengetahuan; ketidakpastian; web semantik"], [{"string": "Optimal marketing strategies over social networks We discuss the use of social networks in implementing viral marketing strategies. While influence maximization has been studied in this context (see Chapter 24 of [10]), we study revenue maximization, arguably, a more natural objective. In our model, a buyer's decision to buy an item is influenced by the set of other buyers that own the item and the price at which the item is offered.", "keywords": ["general", "pricing", "monetizing social networks", "marketing", "submodular maximization"], "combined": "Optimal marketing strategies over social networks We discuss the use of social networks in implementing viral marketing strategies. While influence maximization has been studied in this context (see Chapter 24 of [10]), we study revenue maximization, arguably, a more natural objective. In our model, a buyer's decision to buy an item is influenced by the set of other buyers that own the item and the price at which the item is offered. [[EENNDD]] general; pricing; monetizing social networks; marketing; submodular maximization"}, "Strategi pemasaran optimum melalui rangkaian sosial Kami membincangkan penggunaan rangkaian sosial dalam melaksanakan strategi pemasaran viral. Walaupun memaksimumkan pengaruh telah dikaji dalam konteks ini (lihat Bab 24 dari [10]), kami mengkaji pemaksimalan pendapatan, boleh dikatakan, objektif yang lebih semula jadi. Dalam model kami, keputusan pembeli untuk membeli item dipengaruhi oleh sekumpulan pembeli lain yang memiliki item tersebut dan harga di mana item tersebut ditawarkan. [[EENNDD]] umum; harga; mengewangkan rangkaian sosial; Pemasaran; pemaksimuman submodular"], [{"string": "An agent system reasoning about the web and the user No contact information provided yet.", "keywords": ["distributed artificial intelligence", "information retrieval", "agents", "answer set programming", "logic programming"], "combined": "An agent system reasoning about the web and the user No contact information provided yet. [[EENNDD]] distributed artificial intelligence; information retrieval; agents; answer set programming; logic programming"}, "Sistem ejen memberi alasan mengenai web dan pengguna Belum ada maklumat hubungan yang diberikan. [[EENNDD]] mengedarkan kecerdasan buatan; pengambilan maklumat; ejen; pengaturcaraan set jawapan; pengaturcaraan logik"], [{"string": "Extrapolation methods for accelerating PageRank computations No contact information provided yet.", "keywords": ["pagerank", "eigenvalues and eigenvectors", "eigenvector computation", "link analysis"], "combined": "Extrapolation methods for accelerating PageRank computations No contact information provided yet. [[EENNDD]] pagerank; eigenvalues and eigenvectors; eigenvector computation; link analysis"}, "Kaedah ekstrapolasi untuk mempercepat pengiraan PageRank Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pagerank; nilai eigen dan vektor eigen; pengiraan eigenvector; analisis pautan"], [{"string": "A comprehensive comparative study on term weighting schemes for text categorization with support vector machines No contact information provided yet.", "keywords": ["document preparation", "svm", "term weighting schemes", "categorization", "text"], "combined": "A comprehensive comparative study on term weighting schemes for text categorization with support vector machines No contact information provided yet. [[EENNDD]] document preparation; svm; term weighting schemes; categorization; text"}, "Kajian perbandingan komprehensif mengenai skema pemberat jangka untuk pengkategorian teks dengan mesin vektor sokongan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penyediaan dokumen; svm; skema pemberat jangka masa; pengkategorian; teks"], [{"string": "Web data integration using approximate string join No contact information provided yet.", "keywords": ["approximate string join", "data integration"], "combined": "Web data integration using approximate string join No contact information provided yet. [[EENNDD]] approximate string join; data integration"}, "Penyatuan data web menggunakan perkiraan rentetan gabung. Belum ada maklumat hubungan [[EENNDD]] anggaran rentetan bergabung; penyatuan data"], [{"string": "Cooperative leases: scalable consistency maintenance in content distribution networks No contact information provided yet.", "keywords": ["dynamic data", "data dissemination", "scalability", "content distribution networks", "world wide web", "leases", "push", "data consistency", "pullc"], "combined": "Cooperative leases: scalable consistency maintenance in content distribution networks No contact information provided yet. [[EENNDD]] dynamic data; data dissemination; scalability; content distribution networks; world wide web; leases; push; data consistency; pullc"}, "Pajakan koperasi: penyelenggaraan konsistensi berskala dalam rangkaian pengedaran kandungan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] data dinamik; penyebaran data; skalabiliti; rangkaian pengedaran kandungan; laman web seluruh dunia; pajakan; tolak; ketekalan data; tarikan"], [{"string": "Highlighting disputed claims on the web We describe Dispute Finder, a browser extension that alerts a user when information they read online is disputed by a source that they might trust. Dispute Finder examines the text on the page that the user is browsing and highlights any phrases that resemble known disputed claims. If a user clicks on a highlighted phrase then Dispute Finder shows them a list of articles that support other points of view.", "keywords": ["annotation", "web", "decision support", "miscellaneous", "argumentation", "cscw", "sensemaking"], "combined": "Highlighting disputed claims on the web We describe Dispute Finder, a browser extension that alerts a user when information they read online is disputed by a source that they might trust. Dispute Finder examines the text on the page that the user is browsing and highlights any phrases that resemble known disputed claims. If a user clicks on a highlighted phrase then Dispute Finder shows them a list of articles that support other points of view. [[EENNDD]] annotation; web; decision support; miscellaneous; argumentation; cscw; sensemaking"}, "Menyoroti tuntutan yang dipertikaikan di web Kami menerangkan Dispute Finder, pelanjutan penyemak imbas yang memberi amaran kepada pengguna apabila maklumat yang mereka baca dalam talian dipertikaikan oleh sumber yang mungkin mereka percayai. Dispute Finder memeriksa teks di halaman yang dilayari oleh pengguna dan menyoroti setiap frasa yang menyerupai tuntutan yang dipertikaikan. Sekiranya pengguna mengklik frasa yang diserlahkan, maka Dispute Finder menunjukkan kepada mereka senarai artikel yang menyokong sudut pandangan lain. [[EENNDD]] anotasi; laman web; sokongan keputusan; pelbagai; perbalahan; cscw; membuat pertimbangan"], [{"string": "Leveraging auxiliary text terms for automatic image annotation This paper proposes a novel algorithm to annotate web images by automatically aligning the images with their most relevant auxiliary text terms. First, the DOM-based web page segmentation is performed to extract images and their most relevant auxiliary text blocks. Second, automatic image clustering is used to partition the web images into a set of groups according to their visual similarity contexts, which significantly reduces the uncertainty on the relatedness between the images and their auxiliary terms. The semantics of the visually-similar images in the same cluster are then described by the same ranked list of terms which frequently co-occur in their text blocks. Finally, a relevance re-ranking process is performed over a term correlation network to further refine the ranked term list. Our experiments on a large-scale database of web pages have provided very positive results.", "keywords": ["relevance re-ranking", "automatic image annotation", "image-text alignment"], "combined": "Leveraging auxiliary text terms for automatic image annotation This paper proposes a novel algorithm to annotate web images by automatically aligning the images with their most relevant auxiliary text terms. First, the DOM-based web page segmentation is performed to extract images and their most relevant auxiliary text blocks. Second, automatic image clustering is used to partition the web images into a set of groups according to their visual similarity contexts, which significantly reduces the uncertainty on the relatedness between the images and their auxiliary terms. The semantics of the visually-similar images in the same cluster are then described by the same ranked list of terms which frequently co-occur in their text blocks. Finally, a relevance re-ranking process is performed over a term correlation network to further refine the ranked term list. Our experiments on a large-scale database of web pages have provided very positive results. [[EENNDD]] relevance re-ranking; automatic image annotation; image-text alignment"}, "Memanfaatkan istilah teks tambahan untuk anotasi gambar automatik Makalah ini mencadangkan algoritma baru untuk memberi penjelasan pada gambar web dengan menyelaraskan gambar secara automatik dengan istilah teks tambahan yang paling relevan. Pertama, segmentasi halaman web berasaskan DOM dilakukan untuk mengekstrak gambar dan blok teks tambahan yang paling relevan. Kedua, pengelompokan gambar automatik digunakan untuk membagi gambar web ke dalam sekumpulan kumpulan mengikut konteks kesamaan visual mereka, yang secara signifikan mengurangkan ketidakpastian hubungan antara gambar dan istilah tambahannya. Semantik gambar yang serupa dengan visual dalam kelompok yang sama kemudian dijelaskan oleh senarai istilah yang sama peringkat yang sering berlaku dalam blok teksnya. Akhirnya, proses pemeringkatan semula relevansi dilakukan melalui rangkaian korelasi istilah untuk lebih menyempurnakan senarai istilah yang diperingkat. Eksperimen kami pada pangkalan data laman web berskala besar memberikan hasil yang sangat positif. [[EENNDD]] kedudukan semula relevan; anotasi gambar automatik; penjajaran teks-gambar"], [{"string": "Turning portlets into services: the consumer profile Portlets strive to play at the front end the same role that Web services currently enjoy at the back end, namely, enablers of application assembly through reusable services. However, it is well-known in the component community that, the larger the component, the more reduced the reuse. Hence, the coarse-grained nature of portlets (they encapsulate also the presentation layer) can jeopardize this vision of portlets as reusable services. To avoid this situation, this work proposes a perspective shift in portlet development by introducing the notion of Consumer Profile. While the user profile characterizes the end user (e.g. age, name, etc), the Consumer Profile captures the idiosyncrasies of the organization through which the portlet is being delivered (e.g. the portal owner) as far as the portlet functionality is concerned. The user profile can be dynamic and hence, requires the portlet to be customized at runtime. By contrast, the Consumer Profile is known at registration time, and it is not always appropriate/possible to consider it at runtime. Rather, it is better to customize the code at development time, and produce an organization-specific portlet which built-in, custom functionality. In this scenario, we no longer have a portlet but a family of portlets, and the portlet provider becomes the \"assembly line\" of this family. This work promotes this vision by introducing an organization-aware, WSRPcompliant architecture that let portlet consumers registry and handle \"family portlets\" in the same way that \"traditional portlets\". In so doing, portlets are nearer to become truly reusable services.", "keywords": ["reusable software", "wsrp", "product lines", "soa", "adaptability", "portals", "portlets"], "combined": "Turning portlets into services: the consumer profile Portlets strive to play at the front end the same role that Web services currently enjoy at the back end, namely, enablers of application assembly through reusable services. However, it is well-known in the component community that, the larger the component, the more reduced the reuse. Hence, the coarse-grained nature of portlets (they encapsulate also the presentation layer) can jeopardize this vision of portlets as reusable services. To avoid this situation, this work proposes a perspective shift in portlet development by introducing the notion of Consumer Profile. While the user profile characterizes the end user (e.g. age, name, etc), the Consumer Profile captures the idiosyncrasies of the organization through which the portlet is being delivered (e.g. the portal owner) as far as the portlet functionality is concerned. The user profile can be dynamic and hence, requires the portlet to be customized at runtime. By contrast, the Consumer Profile is known at registration time, and it is not always appropriate/possible to consider it at runtime. Rather, it is better to customize the code at development time, and produce an organization-specific portlet which built-in, custom functionality. In this scenario, we no longer have a portlet but a family of portlets, and the portlet provider becomes the \"assembly line\" of this family. This work promotes this vision by introducing an organization-aware, WSRPcompliant architecture that let portlet consumers registry and handle \"family portlets\" in the same way that \"traditional portlets\". In so doing, portlets are nearer to become truly reusable services. [[EENNDD]] reusable software; wsrp; product lines; soa; adaptability; portals; portlets"}, "Mengubah portlet menjadi perkhidmatan: Portlet profil pengguna berusaha untuk memainkan peranan yang sama seperti yang dinikmati oleh perkhidmatan Web pada bahagian belakang, iaitu, pemboleh pemasangan aplikasi melalui perkhidmatan yang boleh digunakan semula. Walau bagaimanapun, adalah terkenal dalam komuniti komponen bahawa, semakin besar komponennya, semakin berkurang penggunaannya kembali. Oleh itu, sifat portlet yang kasar (mereka merangkumi lapisan persembahan) boleh membahayakan visi portlet ini sebagai perkhidmatan yang boleh digunakan semula. Untuk mengelakkan keadaan ini, karya ini mencadangkan perubahan perspektif dalam pengembangan portlet dengan memperkenalkan konsep Profil Pengguna. Walaupun profil pengguna mencirikan pengguna akhir (mis. Umur, nama, dan lain-lain), Profil Pengguna menangkap keanehan organisasi di mana portlet dihantar (mis. Pemilik portal) berkenaan dengan fungsi portlet. Profil pengguna boleh dinamik dan oleh itu, memerlukan portlet disesuaikan pada waktu runtime. Sebaliknya, Profil Pengguna diketahui pada waktu pendaftaran, dan tidak selalu wajar / mungkin untuk mempertimbangkannya pada waktu runtime. Sebaliknya, lebih baik menyesuaikan kod pada waktu pembangunan, dan menghasilkan portlet khusus organisasi yang mempunyai fungsi khusus yang terbina dalam. Dalam senario ini, kita tidak lagi mempunyai portlet tetapi keluarga portlet, dan penyedia portlet menjadi \"barisan pemasangan\" keluarga ini. Karya ini mempromosikan visi ini dengan memperkenalkan seni bina yang sesuai dengan organisasi, WSRP yang memungkinkan pengguna portlet mendaftar dan menangani \"portlet keluarga\" dengan cara yang sama seperti \"portlet tradisional\". Dengan berbuat demikian, portlet lebih dekat untuk menjadi perkhidmatan yang boleh digunakan semula. [[EENNDD]] perisian yang boleh digunakan semula; wsrp; barisan produk; soa; kebolehsuaian; portal; bahagian kecil"], [{"string": "Distributed web retrieval In the ocean of Web data, Web search engines are the primary way to access content. As the data is on the order of petabytes, current search engines are very large centralized systems based on replicated clusters. Web data, however, is always evolving. The number of Web sites continues to grow rapidly (over 270 millions at the beginning of 2011) and there are currently more than 20 billion indexed pages. On the other hand, Internet users are above one billion and hundreds of million of queries are issued each day. In the near future, centralized systems are likely to become less effective against such a data-query load, thus suggesting the need of fully distributed search engines. Such engines need to maintain high quality answers, fast response time, high query throughput, high availability and scalability; in spite of network latency and scattered data. In this tutorial we present the architecture of current search engines and we explore the main challenges behind the design of all the processes of a distributed Web retrieval system crawling, indexing, and query processing.", "keywords": ["indexing", "crawling", "web search", "query processing", "distributed systems"], "combined": "Distributed web retrieval In the ocean of Web data, Web search engines are the primary way to access content. As the data is on the order of petabytes, current search engines are very large centralized systems based on replicated clusters. Web data, however, is always evolving. The number of Web sites continues to grow rapidly (over 270 millions at the beginning of 2011) and there are currently more than 20 billion indexed pages. On the other hand, Internet users are above one billion and hundreds of million of queries are issued each day. In the near future, centralized systems are likely to become less effective against such a data-query load, thus suggesting the need of fully distributed search engines. Such engines need to maintain high quality answers, fast response time, high query throughput, high availability and scalability; in spite of network latency and scattered data. In this tutorial we present the architecture of current search engines and we explore the main challenges behind the design of all the processes of a distributed Web retrieval system crawling, indexing, and query processing. [[EENNDD]] indexing; crawling; web search; query processing; distributed systems"}, "Pengambilan web yang diedarkan Di lautan data Web, enjin carian Web adalah cara utama untuk mengakses kandungan. Oleh kerana datanya mengikut urutan petabyte, mesin carian semasa adalah sistem terpusat yang sangat besar berdasarkan kluster yang direplikasi. Data web, bagaimanapun, selalu berkembang. Jumlah laman web terus berkembang pesat (lebih dari 270 juta pada awal tahun 2011) dan pada masa ini terdapat lebih dari 20 bilion halaman yang diindeks. Sebaliknya, pengguna Internet melebihi satu bilion dan ratusan juta pertanyaan dikeluarkan setiap hari. Dalam waktu dekat, sistem terpusat cenderung menjadi kurang efektif terhadap beban permintaan data, sehingga menunjukkan keperluan mesin pencari yang diedarkan sepenuhnya. Enjin sedemikian perlu mengekalkan jawapan berkualiti tinggi, masa tindak balas yang cepat, throughput pertanyaan yang tinggi, ketersediaan dan skalabiliti yang tinggi; walaupun latensi rangkaian dan data tersebar. Dalam tutorial ini kami menyajikan seni bina mesin pencari semasa dan kami meneroka cabaran utama di sebalik reka bentuk semua proses sistem pengambilan Web yang diedarkan merangkak, mengindeks, dan memproses pertanyaan. [[EENNDD]] pengindeksan; merangkak; carian sesawang; pemprosesan pertanyaan; sistem yang diedarkan"], [{"string": "Document recommendation in social tagging services Social tagging services allow users to annotate various online resources with freely chosen keywords (tags). They not only facilitate the users in finding and organizing online resources, but also provide meaningful collaborative semantic data which can potentially be exploited by recommender systems. Traditional studies on recommender systems focused on user rating data, while recently social tagging data is becoming more and more prevalent. How to perform resource recommendation based on tagging data is an emerging research topic. In this paper we consider the problem of document (e.g. Web pages, research papers) recommendation using purely tagging data. That is, we only have data containing users, tags, documents and the relationships among them. We propose a novel graph-based representation learning algorithm for this purpose. The users, tags and documents are represented in the same semantic space in which two related objects are close to each other. For a given user, we recommend those documents that are sufficiently close to him/her. Experimental results on two data sets crawled from Del.icio.us and CiteULike show that our algorithm can generate promising recommendations and outperforms traditional recommendation algorithms.", "keywords": ["social tagging", "information search and retrieval", "recommender systems"], "combined": "Document recommendation in social tagging services Social tagging services allow users to annotate various online resources with freely chosen keywords (tags). They not only facilitate the users in finding and organizing online resources, but also provide meaningful collaborative semantic data which can potentially be exploited by recommender systems. Traditional studies on recommender systems focused on user rating data, while recently social tagging data is becoming more and more prevalent. How to perform resource recommendation based on tagging data is an emerging research topic. In this paper we consider the problem of document (e.g. Web pages, research papers) recommendation using purely tagging data. That is, we only have data containing users, tags, documents and the relationships among them. We propose a novel graph-based representation learning algorithm for this purpose. The users, tags and documents are represented in the same semantic space in which two related objects are close to each other. For a given user, we recommend those documents that are sufficiently close to him/her. Experimental results on two data sets crawled from Del.icio.us and CiteULike show that our algorithm can generate promising recommendations and outperforms traditional recommendation algorithms. [[EENNDD]] social tagging; information search and retrieval; recommender systems"}, "Dokumen cadangan dalam perkhidmatan penandaan sosial Perkhidmatan penandaan sosial membolehkan pengguna memberi anotasi pelbagai sumber dalam talian dengan kata kunci (tag) yang dipilih secara bebas. Mereka bukan sahaja memudahkan pengguna dalam mencari dan mengatur sumber dalam talian, tetapi juga memberikan data semantik kolaboratif yang bermakna yang berpotensi dimanfaatkan oleh sistem pengesyoran. Kajian tradisional mengenai sistem pengesyoran tertumpu pada data penarafan pengguna, sementara baru-baru ini data penandaan sosial semakin berleluasa. Cara melaksanakan cadangan sumber berdasarkan penandaan data adalah topik penyelidikan yang muncul. Dalam makalah ini kami mempertimbangkan masalah cadangan dokumen (mis. Halaman Web, makalah penyelidikan) menggunakan data penandaan semata-mata. Maksudnya, kita hanya mempunyai data yang mengandungi pengguna, tag, dokumen dan hubungan di antara mereka. Kami mencadangkan algoritma pembelajaran representasi berasaskan grafik baru untuk tujuan ini. Pengguna, tag dan dokumen ditunjukkan dalam ruang semantik yang sama di mana dua objek yang berkaitan saling berdekatan. Untuk pengguna tertentu, kami mengesyorkan dokumen-dokumen yang cukup dekat dengannya. Hasil eksperimen pada dua set data yang dirangkak dari Del.icio.us dan CiteULike menunjukkan bahawa algoritma kami dapat menghasilkan cadangan yang menjanjikan dan mengatasi algoritma cadangan tradisional. [[EENNDD]] penandaan sosial; carian dan pengambilan maklumat; sistem cadangan"], [{"string": "Community gravity: measuring bidirectional effects by trust and rating on online social networks Several attempts have been made to analyze customer behavior on online E-commerce sites. Some studies particularly emphasize the social networks of customers. Users' reviews and ratings of a product exert effects on other consumers' purchasing behavior. Whether a user refers to other users' ratings depends on the trust accorded by a user to the reviewer. On the other hand, the trust that is felt by a user for another user correlates with the similarity of two users' ratings. This bidirectional interaction that involves trust and rating is an important aspect of understanding consumer behavior in online communities because it suggests clustering of similar users and the evolution of strong communities. This paper presents a theoretical model along with analyses of an actual online E-commerce site. We analyzed a large community site in Japan: @cosme. The noteworthy characteristics of @cosme are that users can bookmark their trusted users; in addition, they can post their own ratings of products, which facilitates our analyses of the ratings' bidirectional effects on trust and ratings. We describe an overview of the data in @cosme, analyses of effects from trust to rating and vice versa, and our proposition of a measure of community gravity, which measures how strongly a user might be attracted to a community. Our study is based on the @cosme dataset in addition to the Epinions dataset. It elucidates important insights and proposes a potentially important measure for mining online social networks.", "keywords": ["social networks", "online community", "rating", "trust"], "combined": "Community gravity: measuring bidirectional effects by trust and rating on online social networks Several attempts have been made to analyze customer behavior on online E-commerce sites. Some studies particularly emphasize the social networks of customers. Users' reviews and ratings of a product exert effects on other consumers' purchasing behavior. Whether a user refers to other users' ratings depends on the trust accorded by a user to the reviewer. On the other hand, the trust that is felt by a user for another user correlates with the similarity of two users' ratings. This bidirectional interaction that involves trust and rating is an important aspect of understanding consumer behavior in online communities because it suggests clustering of similar users and the evolution of strong communities. This paper presents a theoretical model along with analyses of an actual online E-commerce site. We analyzed a large community site in Japan: @cosme. The noteworthy characteristics of @cosme are that users can bookmark their trusted users; in addition, they can post their own ratings of products, which facilitates our analyses of the ratings' bidirectional effects on trust and ratings. We describe an overview of the data in @cosme, analyses of effects from trust to rating and vice versa, and our proposition of a measure of community gravity, which measures how strongly a user might be attracted to a community. Our study is based on the @cosme dataset in addition to the Epinions dataset. It elucidates important insights and proposes a potentially important measure for mining online social networks. [[EENNDD]] social networks; online community; rating; trust"}, "Komuniti masyarakat: mengukur kesan dua arah dengan kepercayaan dan penilaian di rangkaian sosial dalam talian Beberapa percubaan telah dilakukan untuk menganalisis tingkah laku pelanggan di laman web E-dagang dalam talian. Beberapa kajian khususnya menekankan rangkaian sosial pelanggan. Ulasan dan penilaian pengguna terhadap produk memberi kesan kepada tingkah laku pembelian pengguna lain. Sama ada pengguna merujuk kepada penilaian pengguna lain bergantung pada kepercayaan yang diberikan oleh pengguna kepada pengulas. Sebaliknya, kepercayaan yang dirasakan oleh pengguna untuk pengguna lain berkorelasi dengan persamaan penilaian dua pengguna. Interaksi dua arah ini yang melibatkan kepercayaan dan penilaian adalah aspek penting untuk memahami tingkah laku pengguna dalam komuniti dalam talian kerana ini menunjukkan pengumpulan pengguna serupa dan evolusi komuniti yang kuat. Makalah ini menyajikan model teori bersama dengan analisis laman web E-dagang dalam talian yang sebenarnya. Kami menganalisis laman komuniti besar di Jepun: @cosme. Ciri-ciri @cosme yang perlu diperhatikan ialah pengguna dapat menandakan pengguna yang dipercayai; di samping itu, mereka dapat menyiarkan penilaian produk mereka sendiri, yang memudahkan analisis kami mengenai kesan dua arah penilaian terhadap kepercayaan dan penilaian. Kami menerangkan gambaran keseluruhan data di @cosme, analisis kesan dari kepercayaan hingga penilaian dan sebaliknya, dan cadangan kami mengenai ukuran graviti komuniti, yang mengukur seberapa kuat pengguna mungkin tertarik pada komuniti. Kajian kami berdasarkan pada set data @cosme sebagai tambahan kepada set data Epinions. Ini menjelaskan pandangan penting dan mencadangkan langkah yang berpotensi penting untuk melombong rangkaian sosial dalam talian. [[EENNDD]] rangkaian sosial; komuniti dalam talian; penilaian; kepercayaan"], [{"string": "The chatty web: emergent semantics through gossiping No contact information provided yet.", "keywords": ["data translation", "semantic integration", "self-organization", "semantic agreements"], "combined": "The chatty web: emergent semantics through gossiping No contact information provided yet. [[EENNDD]] data translation; semantic integration; self-organization; semantic agreements"}, "Web yang cerewet: semantik baru muncul melalui gosip Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] terjemahan data; integrasi semantik; organisasi diri; perjanjian semantik"], [{"string": "Robust methodologies for modeling web click distributions Metrics such as click counts are vital to online businesses but their measurement has been problematic due to inclusion of high variance robot traffic. We posit that by applying statistical methods more rigorous than have been employed to date that we can build a robust model of thedistribution of clicks following which we can set probabilistically sound thresholds to address outliers and robots. Prior research in this domain has used inappropriate statistical methodology to model distributions and current industrial practice eschews this research for conservative ad-hoc click-level thresholds. Prevailing belief is that such distributions are scale-free power law distributions but using more rigorous statistical methods we find the best description of the data is instead provided by a scale-sensitive Zipf-Mandelbrot mixture distribution. Our results are based on ten data sets from various verticals in the Yahoo domain. Since mixture models can overfit the data we take care to use the BIC log-likelihood method which penalizes overly complex models. Using a mixture model in the web activity domain makes sense because there are likely multiple classes of users. In particular, we have noticed that there is a significantly large set of \"users\" that visit the Yahoo portal exactly once a day. We surmise these may be robots testing internet connectivity by pinging the Yahoo main website.", "keywords": ["distribution fitting"], "combined": "Robust methodologies for modeling web click distributions Metrics such as click counts are vital to online businesses but their measurement has been problematic due to inclusion of high variance robot traffic. We posit that by applying statistical methods more rigorous than have been employed to date that we can build a robust model of thedistribution of clicks following which we can set probabilistically sound thresholds to address outliers and robots. Prior research in this domain has used inappropriate statistical methodology to model distributions and current industrial practice eschews this research for conservative ad-hoc click-level thresholds. Prevailing belief is that such distributions are scale-free power law distributions but using more rigorous statistical methods we find the best description of the data is instead provided by a scale-sensitive Zipf-Mandelbrot mixture distribution. Our results are based on ten data sets from various verticals in the Yahoo domain. Since mixture models can overfit the data we take care to use the BIC log-likelihood method which penalizes overly complex models. Using a mixture model in the web activity domain makes sense because there are likely multiple classes of users. In particular, we have noticed that there is a significantly large set of \"users\" that visit the Yahoo portal exactly once a day. We surmise these may be robots testing internet connectivity by pinging the Yahoo main website. [[EENNDD]] distribution fitting"}, "Metodologi yang kuat untuk memodelkan taburan klik web Metrik seperti jumlah klik sangat penting bagi perniagaan dalam talian tetapi pengukurannya bermasalah kerana penyertaan lalu lintas robot varians tinggi. Kami berpendapat bahawa dengan menggunakan kaedah statistik yang lebih ketat daripada yang telah digunakan hingga kini, kita dapat membina model penyebaran klik yang kuat berikut yang mana kita dapat menetapkan ambang suara yang mungkin untuk menangani pengguna dan robot. Penyelidikan sebelumnya dalam domain ini telah menggunakan metodologi statistik yang tidak sesuai untuk memodelkan pengedaran dan amalan industri semasa menghindari penyelidikan ini untuk ambang tahap klik ad-hoc konservatif. Kepercayaan yang berlaku adalah bahawa sebaran tersebut adalah pengedaran undang-undang kuasa bebas skala tetapi dengan menggunakan kaedah statistik yang lebih ketat, kami dapati penerangan terbaik mengenai data tersebut sebaliknya disediakan oleh pengedaran campuran Zipf-Mandelbrot yang peka terhadap skala. Hasil kami berdasarkan sepuluh set data dari pelbagai menegak dalam domain Yahoo. Oleh kerana model campuran dapat melampaui data, kami berhati-hati menggunakan kaedah kemungkinan log BIC yang menghukum model yang terlalu kompleks. Menggunakan model campuran dalam domain aktiviti web masuk akal kerana kemungkinan terdapat banyak kelas pengguna. Khususnya, kami perhatikan bahawa terdapat banyak \"pengguna\" yang mengunjungi portal Yahoo sekali sehari. Kami menduga ini mungkin robot yang menguji kesambungan internet dengan melakukan ping di laman web utama Yahoo. [[EENNDD]] pengagihan yang sesuai"], [{"string": "Fluid annotations through open hypermedia: using and extending emerging web standards Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.", "keywords": ["xlink", "rdf", "xpointer", "web augmentation with open hypermedia", "annotea", "annotations", "fluid documents"], "combined": "Fluid annotations through open hypermedia: using and extending emerging web standards Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references. [[EENNDD]] xlink; rdf; xpointer; web augmentation with open hypermedia; annotea; annotations; fluid documents"}, "Anotasi cecair melalui hipermedia terbuka: menggunakan dan memperluas piawaian web yang baru muncul Catatan: Kesalahan OCR mungkin terdapat dalam Senarai Rujukan ini yang diekstrak dari artikel teks lengkap. ACM memilih untuk mendedahkan Senarai lengkap dan bukan hanya rujukan yang betul dan berkaitan. [[EENNDD]] xlink; rdf; xpointer; pembesaran web dengan hipermedia terbuka; annotea; anotasi; dokumen cecair"], [{"string": "Extracting data records from the web using tag path clustering Fully automatic methods that extract lists of objects from the Web have been studied extensively. Record extraction, the first step of this object extraction process, identifies a set of Web page segments, each of which represents an individual object (e.g., a product). State-of-the-art methods suffice for simple search, but they often fail to handle more complicated or noisy Web page structures due to a key limitation -- their greedy manner of identifying a list of records through pairwise comparison (i.e., similarity match) of consecutive segments. This paper introduces a new method for record extraction that captures a list of objects in a more robust way based on a holistic analysis of a Web page. The method focuses on how a distinct tag path appears repeatedly in the DOM tree of the Web document. Instead of comparing a pair of individual segments, it compares a pair of tag path occurrence patterns (called visual signals) to estimate how likely these two tag paths represent the same list of objects. The paper introduces a similarity measure that captures how closely the visual signals appear and interleave. Clustering of tag paths is then performed based on this similarity measure, and sets of tag paths that form the structure of data records are extracted. Experiments show that this method achieves higher accuracy than previous methods.", "keywords": ["information extraction", "clustering", "data record extraction"], "combined": "Extracting data records from the web using tag path clustering Fully automatic methods that extract lists of objects from the Web have been studied extensively. Record extraction, the first step of this object extraction process, identifies a set of Web page segments, each of which represents an individual object (e.g., a product). State-of-the-art methods suffice for simple search, but they often fail to handle more complicated or noisy Web page structures due to a key limitation -- their greedy manner of identifying a list of records through pairwise comparison (i.e., similarity match) of consecutive segments. This paper introduces a new method for record extraction that captures a list of objects in a more robust way based on a holistic analysis of a Web page. The method focuses on how a distinct tag path appears repeatedly in the DOM tree of the Web document. Instead of comparing a pair of individual segments, it compares a pair of tag path occurrence patterns (called visual signals) to estimate how likely these two tag paths represent the same list of objects. The paper introduces a similarity measure that captures how closely the visual signals appear and interleave. Clustering of tag paths is then performed based on this similarity measure, and sets of tag paths that form the structure of data records are extracted. Experiments show that this method achieves higher accuracy than previous methods. [[EENNDD]] information extraction; clustering; data record extraction"}, "Mengekstrak rekod data dari web menggunakan pengelompokan jalur tag Kaedah automatik sepenuhnya yang mengekstrak senarai objek dari Web telah dipelajari secara meluas. Rekod pengekstrakan, langkah pertama proses pengekstrakan objek ini, mengenal pasti sekumpulan segmen halaman Web, yang masing-masing mewakili objek individu (mis., Produk). Kaedah canggih cukup untuk carian mudah, tetapi mereka sering gagal menangani struktur halaman Web yang lebih rumit atau bising kerana batasan utama - cara tamak mereka mengenal pasti senarai rekod melalui perbandingan berpasangan (iaitu, kesamaan padanan segmen berturut-turut. Makalah ini memperkenalkan kaedah baru untuk pengambilan rekod yang menangkap senarai objek dengan cara yang lebih mantap berdasarkan analisis menyeluruh dari laman Web. Kaedah ini memfokuskan pada bagaimana jalur tag yang berbeza muncul berulang kali di pohon DOM dokumen Web. Daripada membandingkan sepasang segmen individu, ia membandingkan sepasang corak kejadian jalur tag (disebut isyarat visual) untuk menganggarkan kemungkinan dua jalur tag ini mewakili senarai objek yang sama. Makalah ini memperkenalkan ukuran kesamaan yang dapat menangkap seberapa dekat isyarat visual muncul dan interleave. Pengelompokan jalur tag kemudian dilakukan berdasarkan ukuran kesamaan ini, dan set jalur tag yang membentuk struktur catatan data diekstrak. Eksperimen menunjukkan bahawa kaedah ini mencapai ketepatan yang lebih tinggi daripada kaedah sebelumnya. [[EENNDD]] pengekstrakan maklumat; pengelompokan; pengekstrakan rekod data"], [{"string": "Semantic email No contact information provided yet.", "keywords": ["satisfiability", "miscellaneous", "formal model", "semantic web", "decision-theoretic"], "combined": "Semantic email No contact information provided yet. [[EENNDD]] satisfiability; miscellaneous; formal model; semantic web; decision-theoretic"}, "E-mel semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kepuasan; pelbagai; model formal; web semantik; keputusan-teori"], [{"string": "Measuring the similarity between implicit semantic relations from the web Measuring the similarity between semantic relations that hold among entities is an important and necessary step in various Web related tasks such as relation extraction, information retrieval and analogy detection. For example, consider the case in which a person knows a pair of entities (e.g. Google, YouTube), between which a particular relation holds (e.g. acquisition). The person is interested in retrieving other such pairs with similar relations (e.g. Microsoft, Powerset). Existing keyword-based search engines cannot be applied directly in this case because, in keyword-based search, the goal is to retrieve documents that are relevant to the words used in a query -- not necessarily to the relations implied by a pair of words. We propose a relational similarity measure, using a Web search engine, to compute the similarity between semantic relations implied by two pairs of words. Our method has three components: representing the various semantic relations that exist between a pair of words using automatically extracted lexical patterns, clustering the extracted lexical patterns to identify the different patterns that express a particular semantic relation, and measuring the similarity between semantic relations using a metric learning approach. We evaluate the proposed method in two tasks: classifying semantic relations between named entities, and solving word-analogy questions. The proposed method outperforms all baselines in a relation classification task with a statistically significant average precision score of 0.74. Moreover, it reduces the time taken by Latent Relational Analysis to process 374 word-analogy questions from 9 days to less than 6 hours, with an SAT score of 51%.", "keywords": ["relational similarity", "natural language processing", "web mining"], "combined": "Measuring the similarity between implicit semantic relations from the web Measuring the similarity between semantic relations that hold among entities is an important and necessary step in various Web related tasks such as relation extraction, information retrieval and analogy detection. For example, consider the case in which a person knows a pair of entities (e.g. Google, YouTube), between which a particular relation holds (e.g. acquisition). The person is interested in retrieving other such pairs with similar relations (e.g. Microsoft, Powerset). Existing keyword-based search engines cannot be applied directly in this case because, in keyword-based search, the goal is to retrieve documents that are relevant to the words used in a query -- not necessarily to the relations implied by a pair of words. We propose a relational similarity measure, using a Web search engine, to compute the similarity between semantic relations implied by two pairs of words. Our method has three components: representing the various semantic relations that exist between a pair of words using automatically extracted lexical patterns, clustering the extracted lexical patterns to identify the different patterns that express a particular semantic relation, and measuring the similarity between semantic relations using a metric learning approach. We evaluate the proposed method in two tasks: classifying semantic relations between named entities, and solving word-analogy questions. The proposed method outperforms all baselines in a relation classification task with a statistically significant average precision score of 0.74. Moreover, it reduces the time taken by Latent Relational Analysis to process 374 word-analogy questions from 9 days to less than 6 hours, with an SAT score of 51%. [[EENNDD]] relational similarity; natural language processing; web mining"}, "Mengukur persamaan antara hubungan semantik tersirat dari web Mengukur kesamaan antara hubungan semantik yang berlaku di antara entiti adalah langkah penting dan perlu dalam pelbagai tugas berkaitan Web seperti pengekstrakan hubungan, pengambilan maklumat dan pengesanan analogi. Sebagai contoh, pertimbangkan kes di mana seseorang mengetahui sepasang entiti (mis. Google, YouTube), di mana hubungan tersebut berlaku (mis. Pemerolehan). Orang itu berminat untuk mendapatkan pasangan lain dengan hubungan yang serupa (mis. Microsoft, Powerset). Enjin carian berdasarkan kata kunci yang ada tidak dapat diterapkan secara langsung dalam hal ini kerana, dalam carian berdasarkan kata kunci, tujuannya adalah untuk mendapatkan dokumen yang relevan dengan kata-kata yang digunakan dalam pertanyaan - tidak semestinya untuk hubungan yang tersirat oleh sepasang kata . Kami mencadangkan ukuran kesamaan hubungan, menggunakan mesin carian Web, untuk menghitung persamaan antara hubungan semantik yang disiratkan oleh dua pasang kata. Kaedah kami mempunyai tiga komponen: mewakili pelbagai hubungan semantik yang ada di antara sepasang kata menggunakan corak leksikal yang diekstrak secara automatik, mengelompokkan corak leksikal yang diekstrak untuk mengenal pasti corak yang berbeza yang menyatakan hubungan semantik tertentu, dan mengukur persamaan antara hubungan semantik menggunakan pendekatan pembelajaran metrik. Kami menilai kaedah yang dicadangkan dalam dua tugas: mengklasifikasikan hubungan semantik antara entiti bernama, dan menyelesaikan soalan analogi kata. Kaedah yang dicadangkan mengatasi semua garis dasar dalam tugas klasifikasi hubungan dengan skor ketepatan purata yang signifikan secara statistik sebanyak 0.74. Lebih-lebih lagi, ini mengurangkan masa yang diambil oleh Latent Relational Analysis untuk memproses 374 soalan analogi kata dari 9 hari hingga kurang dari 6 jam, dengan skor SAT 51%. [[EENNDD]] persamaan hubungan; pemprosesan bahasa semula jadi; perlombongan web"], [{"string": "Browsing fatigue in handhelds: semantic bookmarking spells relief No contact information provided yet.", "keywords": ["handheld device content adaptation", "web page partitioning", "semantic bookmarking"], "combined": "Browsing fatigue in handhelds: semantic bookmarking spells relief No contact information provided yet. [[EENNDD]] handheld device content adaptation; web page partitioning; semantic bookmarking"}, "Melayari keletihan di tangan: penanda buku semantik melegakan mantra Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penyesuaian kandungan peranti genggam; pembahagian laman web; penanda buku semantik"], [{"string": "Document hierarchies from text and links Hierarchical taxonomies provide a multi-level view of large document collections, allowing users to rapidly drill down to fine-grained distinctions in topics of interest. We show that automatically induced taxonomies can be made more robust by combining text with relational links. The underlying mechanism is a Bayesian generative model in which a latent hierarchical structure explains the observed data --- thus, finding hierarchical groups of documents with similar word distributions and dense network connections. As a nonparametric Bayesian model, our approach does not require pre-specification of the branching factor at each non-terminal, but finds the appropriate level of detail directly from the data. Unlike many prior latent space models of network structure, the complexity of our approach does not grow quadratically in the number of documents, enabling application to networks with more than ten thousand nodes. Experimental results on hypertext and citation network corpora demonstrate the advantages of our hierarchical, multimodal approach.", "keywords": ["topic models", "learning", "hierarchical clustering", "bayesian generative models", "stochastic block models"], "combined": "Document hierarchies from text and links Hierarchical taxonomies provide a multi-level view of large document collections, allowing users to rapidly drill down to fine-grained distinctions in topics of interest. We show that automatically induced taxonomies can be made more robust by combining text with relational links. The underlying mechanism is a Bayesian generative model in which a latent hierarchical structure explains the observed data --- thus, finding hierarchical groups of documents with similar word distributions and dense network connections. As a nonparametric Bayesian model, our approach does not require pre-specification of the branching factor at each non-terminal, but finds the appropriate level of detail directly from the data. Unlike many prior latent space models of network structure, the complexity of our approach does not grow quadratically in the number of documents, enabling application to networks with more than ten thousand nodes. Experimental results on hypertext and citation network corpora demonstrate the advantages of our hierarchical, multimodal approach. [[EENNDD]] topic models; learning; hierarchical clustering; bayesian generative models; stochastic block models"}, "Hierarki dokumen dari teks dan pautan Taksonomi hierarki memberikan pandangan pelbagai peringkat mengenai koleksi dokumen yang besar, yang membolehkan pengguna meneliti perbezaan yang jelas dalam topik yang menarik. Kami menunjukkan bahawa taksonomi yang dihasilkan secara automatik dapat dibuat lebih mantap dengan menggabungkan teks dengan pautan hubungan. Mekanisme yang mendasari adalah model generatif Bayes di mana struktur hierarki laten menerangkan data yang diperhatikan --- dengan demikian, mencari kumpulan dokumen hierarki dengan pengedaran kata yang serupa dan sambungan rangkaian yang padat. Sebagai model Bayesian nonparametrik, pendekatan kami tidak memerlukan pra-spesifikasi faktor percabangan pada setiap terminal bukan terminal, tetapi menemui tahap perincian yang sesuai secara langsung dari data. Tidak seperti banyak model ruang rangkaian laten sebelumnya, kerumitan pendekatan kami tidak bertambah secara kuadratik dalam jumlah dokumen, yang memungkinkan aplikasi ke rangkaian dengan lebih dari sepuluh ribu nod. Hasil eksperimen pada korporat rangkaian hiperteks dan kutipan menunjukkan kelebihan pendekatan hirarki, multimodal kami. [[EENNDD]] model topik; belajar; pengelompokan hierarki; model generatif bayesian; model blok stokastik"], [{"string": "Three-level caching for efficient query processing in large Web search engines No contact information provided yet.", "keywords": ["web search", "inverted index", "miscellaneous", "caching", "metrics"], "combined": "Three-level caching for efficient query processing in large Web search engines No contact information provided yet. [[EENNDD]] web search; inverted index; miscellaneous; caching; metrics"}, "Cache tiga peringkat untuk pemprosesan pertanyaan yang cekap di enjin carian Web yang besar Belum ada maklumat hubungan yang diberikan. [[EENNDD]] carian web; indeks terbalik; pelbagai; caching; sukatan"], [{"string": "An adaptive ontology-based approach to identify correlation between publications In this paper, we propose an adaptive ontology-based approach for related paper identification, to meet most researchers' practical needs. By searching ontology, we can return a diverse set of papers that are explicitly and implicitly related to an input paper. Moreover, our approach does not rely on known ontology. Instead, we build and update ontology for a collection with any domain of interest. Being independent from known ontology, our approach is much more adaptive for different domains.", "keywords": ["correlation of publications", "adaptive approach", "information search and retrieval", "ontology-based"], "combined": "An adaptive ontology-based approach to identify correlation between publications In this paper, we propose an adaptive ontology-based approach for related paper identification, to meet most researchers' practical needs. By searching ontology, we can return a diverse set of papers that are explicitly and implicitly related to an input paper. Moreover, our approach does not rely on known ontology. Instead, we build and update ontology for a collection with any domain of interest. Being independent from known ontology, our approach is much more adaptive for different domains. [[EENNDD]] correlation of publications; adaptive approach; information search and retrieval; ontology-based"}, "Pendekatan berdasarkan ontologi adaptif untuk mengenal pasti korelasi antara penerbitan Dalam makalah ini, kami mencadangkan pendekatan berasaskan ontologi adaptif untuk pengenalan kertas yang berkaitan, untuk memenuhi keperluan praktikal kebanyakan penyelidik. Dengan mencari ontologi, kita dapat mengembalikan sekumpulan makalah yang berkait secara eksplisit dan implisit dengan kertas input. Lebih-lebih lagi, pendekatan kami tidak bergantung pada ontologi yang diketahui. Sebaliknya, kami membina dan mengemas kini ontologi untuk koleksi dengan domain minat apa pun. Menjadi bebas dari ontologi yang diketahui, pendekatan kami jauh lebih adaptif untuk pelbagai domain. [[EENNDD]] korelasi penerbitan; pendekatan adaptif; pencarian dan pengambilan maklumat; berasaskan ontologi"], [{"string": "Efficient web change monitoring with page digest No contact information provided yet.", "keywords": ["document storage", "systems and software", "scalability", "web document monitoring"], "combined": "Efficient web change monitoring with page digest No contact information provided yet. [[EENNDD]] document storage; systems and software; scalability; web document monitoring"}, "Pemantauan perubahan web yang cekap dengan pencernaan halaman Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penyimpanan dokumen; sistem dan perisian; skalabiliti; pemantauan dokumen web"], [{"string": "Certified email with a light on-line trusted third party: design and implementation No contact information provided yet.", "keywords": ["security and protection"], "combined": "Certified email with a light on-line trusted third party: design and implementation No contact information provided yet. [[EENNDD]] security and protection"}, "E-mel yang diperakui dengan pihak ketiga yang dipercayai secara on-line: reka bentuk dan pelaksanaan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] keselamatan dan perlindungan"], [{"string": "Text-based video blogging No contact information provided yet.", "keywords": ["vlog", "tvml", "blog", "ape", "web-casting"], "combined": "Text-based video blogging No contact information provided yet. [[EENNDD]] vlog; tvml; blog; ape; web-casting"}, "Blogging video berasaskan teks Belum ada maklumat hubungan yang diberikan. [[EENNDD]] vlog; tvml; blog; beruk; penghantaran laman web"], [{"string": "Simplifying friendlist management Online social networks like Facebook allow users to connect, communicate, and share content. The popularity of these services has lead to an information overload for their users; the task of simply keeping track of different interactions has become daunting. To reduce this burden, sites like Facebook allows the user to group friends into specific lists, known as friendlists, aggregating the interactions and content from all friends in each friendlist. While this approach greatly reduces the burden on the user, it still forces the user to create and populate the friendlists themselves and, worse, makes the user responsible for maintaining the membership of their friendlists over time. We show that friendlists often have a strong correspondence to the structure of the social network, implying that friendlists may be automatically inferred by leveraging the social network structure. We present a demonstration of Friendlist Manager, a Facebook application that proposes friendlists to the user based on the structure of their local social network, allows the user to tweak the proposed friendlists, and then automatically creates the friendlists for the user.", "keywords": ["privacy", "on-line information services", "communications applications", "social applications", "facebook", "online social networks"], "combined": "Simplifying friendlist management Online social networks like Facebook allow users to connect, communicate, and share content. The popularity of these services has lead to an information overload for their users; the task of simply keeping track of different interactions has become daunting. To reduce this burden, sites like Facebook allows the user to group friends into specific lists, known as friendlists, aggregating the interactions and content from all friends in each friendlist. While this approach greatly reduces the burden on the user, it still forces the user to create and populate the friendlists themselves and, worse, makes the user responsible for maintaining the membership of their friendlists over time. We show that friendlists often have a strong correspondence to the structure of the social network, implying that friendlists may be automatically inferred by leveraging the social network structure. We present a demonstration of Friendlist Manager, a Facebook application that proposes friendlists to the user based on the structure of their local social network, allows the user to tweak the proposed friendlists, and then automatically creates the friendlists for the user. [[EENNDD]] privacy; on-line information services; communications applications; social applications; facebook; online social networks"}, "Memudahkan pengurusan senarai rakan Rangkaian sosial dalam talian seperti Facebook membolehkan pengguna menyambung, berkomunikasi, dan berkongsi kandungan. Populariti perkhidmatan ini menyebabkan maklumat berlebihan kepada pengguna mereka; tugas untuk hanya mengesan interaksi yang berbeza telah menjadi menakutkan. Untuk mengurangkan beban ini, laman web seperti Facebook membolehkan pengguna mengumpulkan rakan ke dalam senarai tertentu, yang dikenali sebagai senarai rakan, menggabungkan interaksi dan kandungan dari semua rakan dalam setiap senarai rakan. Walaupun pendekatan ini sangat mengurangkan beban pengguna, ia tetap memaksa pengguna untuk membuat dan mengisi senarai teman itu sendiri, dan lebih buruk lagi, menjadikan pengguna bertanggungjawab untuk mengekalkan keahlian senarai rakan mereka dari masa ke masa. Kami menunjukkan bahawa senarai rakan sering mempunyai korelasi yang kuat terhadap struktur rangkaian sosial, yang menyiratkan bahawa senarai teman dapat disimpulkan secara automatik dengan memanfaatkan struktur rangkaian sosial. Kami menyajikan demonstrasi Pengurus Senarai Rakan, aplikasi Facebook yang mencadangkan senarai teman kepada pengguna berdasarkan struktur rangkaian sosial tempatan mereka, membolehkan pengguna mengubah suai senarai teman yang dicadangkan, dan kemudian secara automatik membuat daftar teman untuk pengguna. [[EENNDD]] privasi; perkhidmatan maklumat dalam talian; aplikasi komunikasi; aplikasi sosial; facebook; rangkaian sosial dalam talian"], [{"string": "Extending the compatibility notion for abstract WS-BPEL processes WS-BPEL defines a standard for executable processes. Executable processes are business processes which can be automated through an IT infrastructure. The WS-BPEL specification also introduces the concept of abstract processes: In contrast to their executable siblings, abstract processes are not executable and can have parts where business logic is disguised. Nevertheless, the WS-BPEL specification introduces a notion of compatibility between such an under-specified abstract process and a fully specified executable one. Basically, this compatibility notion defines a set of syntactical rules that can be augmented or restricted by profiles. So far, there exist two of such profiles: the Abstract Process Profile for Observable Behavior and the Abstract Process Profile for Templates. None of these profiles defines a concept of behavioral equivalence. Therefore, both profiles are too strict with respect to the rules they impose when deciding whether an executable process is compatible to an abstract one. In this paper, we propose a novel profile that extends the existing Abstract Process Profile for Observable Behavior by defining a behavioral relationship. We also show that our novel profile allows for more flexibility when deciding whether an executable and an abstract process are compatible.", "keywords": ["petri nets", "ws-bpel", "compliance", "abstract profile"], "combined": "Extending the compatibility notion for abstract WS-BPEL processes WS-BPEL defines a standard for executable processes. Executable processes are business processes which can be automated through an IT infrastructure. The WS-BPEL specification also introduces the concept of abstract processes: In contrast to their executable siblings, abstract processes are not executable and can have parts where business logic is disguised. Nevertheless, the WS-BPEL specification introduces a notion of compatibility between such an under-specified abstract process and a fully specified executable one. Basically, this compatibility notion defines a set of syntactical rules that can be augmented or restricted by profiles. So far, there exist two of such profiles: the Abstract Process Profile for Observable Behavior and the Abstract Process Profile for Templates. None of these profiles defines a concept of behavioral equivalence. Therefore, both profiles are too strict with respect to the rules they impose when deciding whether an executable process is compatible to an abstract one. In this paper, we propose a novel profile that extends the existing Abstract Process Profile for Observable Behavior by defining a behavioral relationship. We also show that our novel profile allows for more flexibility when deciding whether an executable and an abstract process are compatible. [[EENNDD]] petri nets; ws-bpel; compliance; abstract profile"}, "Memperluas konsep keserasian untuk proses WS-BPEL abstrak WS-BPEL menentukan standard untuk proses yang boleh dilaksanakan. Proses yang dapat dilaksanakan adalah proses perniagaan yang dapat dilakukan secara automatik melalui infrastruktur IT. Spesifikasi WS-BPEL juga memperkenalkan konsep proses abstrak: Berbeza dengan saudara kandungnya yang dapat dilaksanakan, proses abstrak tidak dapat dilaksanakan dan boleh mempunyai bahagian di mana logik perniagaan menyamar. Walaupun demikian, spesifikasi WS-BPEL memperkenalkan konsep keserasian antara proses abstrak yang tidak ditentukan dan proses yang dapat dilaksanakan sepenuhnya. Pada dasarnya, tanggapan keserasian ini menentukan sekumpulan peraturan sintaksis yang dapat ditambah atau dibatasi oleh profil. Setakat ini, terdapat dua profil tersebut: Profil Proses Abstrak untuk Tingkah Laku yang Dapat Diperhatikan dan Profil Proses Abstrak untuk Templat. Tiada profil ini menentukan konsep kesetaraan tingkah laku. Oleh itu, kedua-dua profil terlalu ketat berkenaan dengan peraturan yang mereka tetapkan ketika memutuskan sama ada proses yang dapat dilaksanakan sesuai dengan yang abstrak. Dalam makalah ini, kami mencadangkan profil baru yang memperluas Profil Proses Abstrak yang ada untuk Tingkah Laku yang Dapat Diperhatikan dengan menentukan hubungan tingkah laku. Kami juga menunjukkan bahawa profil novel kami memungkinkan lebih banyak fleksibiliti ketika memutuskan sama ada proses yang boleh dilaksanakan dan abstrak serasi. [[EENNDD]] jaring petri; ws-bpel; pematuhan; profil abstrak"], [{"string": "Adding semantics to rosettaNet specifications No contact information provided yet.", "keywords": ["electronic data interchange", "b2b integration", "xml", "rosettanet", "ontologysing"], "combined": "Adding semantics to rosettaNet specifications No contact information provided yet. [[EENNDD]] electronic data interchange; b2b integration; xml; rosettanet; ontologysing"}, "Menambah semantik ke spesifikasi rosettaNet Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pertukaran data elektronik; penyatuan b2b; xml; rosettanet; ontologi"], [{"string": "Automatic construction of a context-aware sentiment lexicon: an optimization approach The explosion of Web opinion data has made essential the need for automatic tools to analyze and understand people's sentiments toward different topics. In most sentiment analysis applications, the sentiment lexicon plays a central role. However, it is well known that there is no universally optimal sentiment lexicon since the polarity of words is sensitive to the topic domain. Even worse, in the same domain the same word may indicate different polarities with respect to different aspects. For example, in a laptop review, \"large\" is negative for the battery aspect while being positive for the screen aspect. In this paper, we focus on the problem of learning a sentiment lexicon that is not only domain specific but also dependent on the aspect in context given an unlabeled opinionated text collection. We propose a novel optimization framework that provides a unified and principled way to combine different sources of information for learning such a context-dependent sentiment lexicon. Experiments on two data sets (hotel reviews and customer feedback surveys on printers) show that our approach can not only identify new sentiment words specific to the given domain but also determine the different polarities of a word depending on the aspect in context. In further quantitative evaluation, our method is proved to be effective in constructing a high quality lexicon by comparing with a human annotated gold standard. In addition, using the learned context-dependent sentiment lexicon improved the accuracy in an aspect-level sentiment classification task.", "keywords": ["sentiment analysis", "sentiment lexicon", "optimization", "miscellaneous", "opinion mining"], "combined": "Automatic construction of a context-aware sentiment lexicon: an optimization approach The explosion of Web opinion data has made essential the need for automatic tools to analyze and understand people's sentiments toward different topics. In most sentiment analysis applications, the sentiment lexicon plays a central role. However, it is well known that there is no universally optimal sentiment lexicon since the polarity of words is sensitive to the topic domain. Even worse, in the same domain the same word may indicate different polarities with respect to different aspects. For example, in a laptop review, \"large\" is negative for the battery aspect while being positive for the screen aspect. In this paper, we focus on the problem of learning a sentiment lexicon that is not only domain specific but also dependent on the aspect in context given an unlabeled opinionated text collection. We propose a novel optimization framework that provides a unified and principled way to combine different sources of information for learning such a context-dependent sentiment lexicon. Experiments on two data sets (hotel reviews and customer feedback surveys on printers) show that our approach can not only identify new sentiment words specific to the given domain but also determine the different polarities of a word depending on the aspect in context. In further quantitative evaluation, our method is proved to be effective in constructing a high quality lexicon by comparing with a human annotated gold standard. In addition, using the learned context-dependent sentiment lexicon improved the accuracy in an aspect-level sentiment classification task. [[EENNDD]] sentiment analysis; sentiment lexicon; optimization; miscellaneous; opinion mining"}, "Pembinaan automatik leksikon sentimen yang peka konteks: pendekatan pengoptimuman Ledakan data pendapat Web menjadikan keperluan alat automatik penting untuk menganalisis dan memahami sentimen orang terhadap topik yang berbeza. Dalam kebanyakan aplikasi analisis sentimen, leksikon sentimen memainkan peranan penting. Walau bagaimanapun, sudah diketahui bahawa tidak ada leksikon sentimen yang optimum secara universal kerana kekutuban kata sensitif terhadap domain topik. Lebih teruk lagi, dalam domain yang sama kata yang sama dapat menunjukkan polariti yang berbeza sehubungan dengan aspek yang berbeza. Sebagai contoh, dalam tinjauan komputer riba, \"besar\" adalah negatif untuk aspek bateri sementara positif untuk aspek skrin. Dalam makalah ini, kami memfokuskan pada masalah mempelajari leksikon sentimen yang tidak hanya domain khusus tetapi juga bergantung pada aspek dalam konteks yang diberikan koleksi teks yang tidak berlabel. Kami mencadangkan kerangka pengoptimuman baru yang menyediakan cara yang bersatu dan berprinsip untuk menggabungkan pelbagai sumber maklumat untuk belajar seperti leksikon sentimen yang bergantung pada konteks. Eksperimen pada dua set data (tinjauan hotel dan tinjauan maklum balas pelanggan pada pencetak) menunjukkan bahawa pendekatan kami bukan sahaja dapat mengenal pasti kata-kata sentimen baru yang khusus untuk domain yang diberikan tetapi juga menentukan polariti yang berbeza dari satu perkataan bergantung pada aspek dalam konteks. Dalam penilaian kuantitatif lebih lanjut, kaedah kami terbukti berkesan dalam membina leksikon berkualiti tinggi dengan membandingkan dengan piawaian emas anotasi manusia. Di samping itu, menggunakan leksikon sentimen yang bergantung pada konteks yang dipelajari meningkatkan ketepatan dalam tugas klasifikasi sentimen tahap-aspek. [[EENNDD]] analisis sentimen; leksikon sentimen; pengoptimuman; pelbagai; perlombongan pendapat"], [{"string": "A unified framework for name disambiguation Name ambiguity problem has been a challenging issue for a long history. In this paper, we intend to make a thorough investigation of the whole problem. Specifically, we formalize the name disambiguation problem in a unified framework. The framework can incorporate both attribute and relationship into a probabilistic model. We explore a dynamic approach for automatically estimating the person number K and employ an adaptive distance measure to estimate the distance between objects. Experimental results show that our proposed framework can significantly outperform the baseline method.", "keywords": ["digital library", "information search and retrieval", "learning", "probabilistic model", "database applications", "name disambiguation"], "combined": "A unified framework for name disambiguation Name ambiguity problem has been a challenging issue for a long history. In this paper, we intend to make a thorough investigation of the whole problem. Specifically, we formalize the name disambiguation problem in a unified framework. The framework can incorporate both attribute and relationship into a probabilistic model. We explore a dynamic approach for automatically estimating the person number K and employ an adaptive distance measure to estimate the distance between objects. Experimental results show that our proposed framework can significantly outperform the baseline method. [[EENNDD]] digital library; information search and retrieval; learning; probabilistic model; database applications; name disambiguation"}, "Kerangka bersatu untuk disambiguasi nama Masalah kesamaran nama telah menjadi masalah yang mencabar untuk sejarah yang panjang. Dalam makalah ini, kami bermaksud untuk membuat penyelidikan menyeluruh mengenai keseluruhan masalah. Secara khusus, kami memformalkan masalah disambiguasi nama dalam kerangka yang disatukan. Kerangka kerja dapat menggabungkan sifat dan hubungan ke dalam model probabilistik. Kami meneroka pendekatan dinamik untuk secara automatik mengira bilangan orang K dan menggunakan ukuran jarak adaptif untuk menganggarkan jarak antara objek. Hasil eksperimen menunjukkan bahawa kerangka kerja yang dicadangkan kami dapat mengatasi kaedah asas dengan ketara. [[EENNDD]] perpustakaan digital; pencarian dan pengambilan maklumat; belajar; model kebarangkalian; aplikasi pangkalan data; disambiguasi nama"], [{"string": "Lightweight automatic face annotation in media pages Labeling human faces in images contained in Web media stories enables enriching the user experience offered by media sites. We propose a lightweight framework for automatic image annotation that exploits named entities mentioned in the article to significantly boost the accuracy of face recognition. While previous works in the area labor to train comprehensive offline visual models for a pre-defined universe of candidates, our approach models the people mentioned in a given story on the y, using a standard Web image search engine as an image sampling mechanism. We overcome multiple sources of noise introduced by this ad-hoc process, to build a fast and robust end-to-end system from off-the-shelf error-prone text analysis and machine vision components. In experiments conducted on approximately 900 faces depicted in 500 stories from a major celebrity news website, we were able to correctly label 81.5% of the faces while mislabeling 14.8% of them.", "keywords": ["applications", "web search", "face recognition", "text analysis", "electronic publishing", "machine learning"], "combined": "Lightweight automatic face annotation in media pages Labeling human faces in images contained in Web media stories enables enriching the user experience offered by media sites. We propose a lightweight framework for automatic image annotation that exploits named entities mentioned in the article to significantly boost the accuracy of face recognition. While previous works in the area labor to train comprehensive offline visual models for a pre-defined universe of candidates, our approach models the people mentioned in a given story on the y, using a standard Web image search engine as an image sampling mechanism. We overcome multiple sources of noise introduced by this ad-hoc process, to build a fast and robust end-to-end system from off-the-shelf error-prone text analysis and machine vision components. In experiments conducted on approximately 900 faces depicted in 500 stories from a major celebrity news website, we were able to correctly label 81.5% of the faces while mislabeling 14.8% of them. [[EENNDD]] applications; web search; face recognition; text analysis; electronic publishing; machine learning"}, "Anotasi wajah automatik ringan di halaman media Melabel wajah manusia dalam gambar yang terdapat dalam cerita media Web memungkinkan memperkaya pengalaman pengguna yang ditawarkan oleh laman web media. Kami mencadangkan kerangka ringan untuk anotasi gambar automatik yang memanfaatkan entiti bernama yang disebutkan dalam artikel untuk meningkatkan ketepatan pengecaman wajah dengan ketara. Walaupun sebelumnya bekerja di kawasan ini untuk melatih model visual luar talian yang komprehensif untuk calon yang sudah ditentukan sebelumnya, pendekatan kami memodelkan orang-orang yang disebutkan dalam kisah tertentu di y, menggunakan mesin carian imej Web standard sebagai mekanisme pengambilan gambar. Kami mengatasi pelbagai sumber kebisingan yang diperkenalkan oleh proses ad-hoc ini, untuk membina sistem hujung-ke-hujung yang pantas dan kuat dari analisis teks yang rawan kesalahan dan komponen penglihatan mesin. Dalam eksperimen yang dilakukan pada kira-kira 900 wajah yang digambarkan dalam 500 cerita dari laman web berita selebriti utama, kami dapat melabel 81.5% wajah dengan betul sambil melabel 14.8% daripadanya dengan salah. [[EENNDD]] aplikasi; carian sesawang; pengecaman wajah; analisis teks; penerbitan elektronik; pembelajaran mesin"], [{"string": "An adaptive, fast, and safe XML parser based on byte sequences memorization No contact information provided yet.", "keywords": ["xml parsers", "sax", "automata"], "combined": "An adaptive, fast, and safe XML parser based on byte sequences memorization No contact information provided yet. [[EENNDD]] xml parsers; sax; automata"}, "Penghurai XML yang adaptif, pantas, dan selamat berdasarkan penghafalan urutan bait Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] penghurai xml; saks; automata"], [{"string": "Efficient interactive fuzzy keyword search Traditional information systems return answers after a user submits a complete query. Users often feel \"left in the dark\" when they have limited knowledge about the underlying data, and have to use a try-and-see approach for finding information. A recent trend of supporting autocomplete in these systems is a first step towards solving this problem. In this paper, we study a new information-access paradigm, called \"interactive, fuzzy search,\" in which the system searches the underlying data \"on the fly\" as the user types in query keywords. It extends autocomplete interfaces by (1) allowing keywords to appear in multiple attributes (in an arbitrary order) of the underlying data; and (2) finding relevant records that have keywords matching query keywords approximately. This framework allows users to explore data as they type, even in the presence of minor errors. We study research challenges in this framework for large amounts of data. Since each keystroke of the user could invoke a query on the backend, we need efficient algorithms to process each query within milliseconds. We develop various incremental-search algorithms using previously computed and cached results in order to achieve an interactive speed. We have deployed several real prototypes using these techniques. One of them has been deployed to support interactive search on the UC Irvine people directory, which has been used regularly and well received by users due to its friendly interface and high efficiency.", "keywords": ["autocomplete", "interactive search", "fuzzy search"], "combined": "Efficient interactive fuzzy keyword search Traditional information systems return answers after a user submits a complete query. Users often feel \"left in the dark\" when they have limited knowledge about the underlying data, and have to use a try-and-see approach for finding information. A recent trend of supporting autocomplete in these systems is a first step towards solving this problem. In this paper, we study a new information-access paradigm, called \"interactive, fuzzy search,\" in which the system searches the underlying data \"on the fly\" as the user types in query keywords. It extends autocomplete interfaces by (1) allowing keywords to appear in multiple attributes (in an arbitrary order) of the underlying data; and (2) finding relevant records that have keywords matching query keywords approximately. This framework allows users to explore data as they type, even in the presence of minor errors. We study research challenges in this framework for large amounts of data. Since each keystroke of the user could invoke a query on the backend, we need efficient algorithms to process each query within milliseconds. We develop various incremental-search algorithms using previously computed and cached results in order to achieve an interactive speed. We have deployed several real prototypes using these techniques. One of them has been deployed to support interactive search on the UC Irvine people directory, which has been used regularly and well received by users due to its friendly interface and high efficiency. [[EENNDD]] autocomplete; interactive search; fuzzy search"}, "Pencarian kata kunci kabur interaktif yang cekap Sistem maklumat tradisional mengembalikan jawapan setelah pengguna mengemukakan pertanyaan yang lengkap. Pengguna sering merasa \"ditinggalkan dalam kegelapan\" ketika mereka mempunyai pengetahuan terhad tentang data yang mendasari, dan harus menggunakan pendekatan cuba-lihat untuk mencari maklumat. Trend baru menyokong pelengkap automatik dalam sistem ini adalah langkah pertama untuk menyelesaikan masalah ini. Dalam makalah ini, kami mempelajari paradigma akses maklumat baru, yang disebut \"carian interaktif, kabur,\" di mana sistem mencari data yang mendasari \"dengan cepat\" sebagai jenis pengguna dalam kata kunci pertanyaan. Ia memperluas antara muka pelengkap automatik dengan (1) yang membolehkan kata kunci muncul dalam pelbagai atribut (dalam urutan sewenang-wenang) dari data yang mendasari; dan (2) mencari rekod yang relevan yang mempunyai kata kunci yang hampir sama dengan kata kunci pertanyaan. Rangka kerja ini membolehkan pengguna meneroka data ketika mereka mengetik, walaupun terdapat kesalahan kecil. Kami mengkaji cabaran penyelidikan dalam kerangka ini untuk sejumlah besar data. Oleh kerana setiap penekanan tombol pengguna dapat meminta pertanyaan di backend, kami memerlukan algoritma yang cekap untuk memproses setiap pertanyaan dalam milisaat. Kami mengembangkan pelbagai algoritma carian tambahan menggunakan hasil yang dihitung dan di-cache sebelumnya untuk mencapai kelajuan interaktif. Kami telah menggunakan beberapa prototaip sebenar menggunakan teknik ini. Salah satu daripadanya telah digunakan untuk mendukung pencarian interaktif di direktori orang UC Irvine, yang telah digunakan secara berkala dan diterima dengan baik oleh pengguna kerana antara muka yang mesra dan kecekapan tinggi. [[EENNDD]] pelengkapan automatik; carian interaktif; carian kabur"], [{"string": "Trust-serv: model-driven lifecycle management of trust negotiation policies for web services No contact information provided yet.", "keywords": ["conceptual modeling", "on-line information services", "lifecycle management", "social issues", "security and protection", "web services", "trust negotiation"], "combined": "Trust-serv: model-driven lifecycle management of trust negotiation policies for web services No contact information provided yet. [[EENNDD]] conceptual modeling; on-line information services; lifecycle management; social issues; security and protection; web services; trust negotiation"}, "Trust-serv: pengurusan kitaran hidup berdasarkan model dasar perundingan kepercayaan untuk perkhidmatan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pemodelan konsep; perkhidmatan maklumat dalam talian; pengurusan kitaran hayat; Masalah sosial; keselamatan dan perlindungan; perkhidmatan web; perundingan kepercayaan"], [{"string": "A comparison of implicit and explicit links for web page classification No contact information provided yet.", "keywords": ["query log", "miscellaneous", "implicit link", "web page classification", "virtual document", "explicit link"], "combined": "A comparison of implicit and explicit links for web page classification No contact information provided yet. [[EENNDD]] query log; miscellaneous; implicit link; web page classification; virtual document; explicit link"}, "Perbandingan pautan tersirat dan eksplisit untuk klasifikasi halaman web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] log pertanyaan; pelbagai; pautan tersirat; pengelasan laman web; dokumen maya; pautan eksplisit"], [{"string": "Generalized link suggestions via web site clustering Proactive link suggestion leads to improved user experience by allowing users to reach relevant information with fewer clicks, fewer pages to read, or simply faster because the right pages are prefetched just in time. In this paper we tackle two new scenarios for link suggestion, which were not covered in prior work owing to scarcity of historical browsing data. In the web search scenario, we propose a method for generating quick links - additional entry points into Web sites, which are shown for top search results for navigational queries - for tail sites, for which little browsing statistics is available. Beyond Web search, we also propose a method for link suggestion in general web browsing, effectively anticipating the next link to be followed by the user. Our approach performs clustering of Web sites in order to aggregate information across multiple sites, and enables relevant link suggestion for virtually any site, including tail sites and brand new sites for which little historical data is available. Empirical evaluation confirms the validity of our method using editorially labeled data as well as real-life search and browsing data from a major US search engine.", "keywords": ["quick links", "tail sites", "information search and retrieval", "link suggestion", "assisted browsing", "website clustering"], "combined": "Generalized link suggestions via web site clustering Proactive link suggestion leads to improved user experience by allowing users to reach relevant information with fewer clicks, fewer pages to read, or simply faster because the right pages are prefetched just in time. In this paper we tackle two new scenarios for link suggestion, which were not covered in prior work owing to scarcity of historical browsing data. In the web search scenario, we propose a method for generating quick links - additional entry points into Web sites, which are shown for top search results for navigational queries - for tail sites, for which little browsing statistics is available. Beyond Web search, we also propose a method for link suggestion in general web browsing, effectively anticipating the next link to be followed by the user. Our approach performs clustering of Web sites in order to aggregate information across multiple sites, and enables relevant link suggestion for virtually any site, including tail sites and brand new sites for which little historical data is available. Empirical evaluation confirms the validity of our method using editorially labeled data as well as real-life search and browsing data from a major US search engine. [[EENNDD]] quick links; tail sites; information search and retrieval; link suggestion; assisted browsing; website clustering"}, "Cadangan pautan umum melalui pengelompokan laman web Cadangan pautan proaktif membawa kepada peningkatan pengalaman pengguna dengan membolehkan pengguna mencapai maklumat yang relevan dengan lebih sedikit klik, lebih sedikit halaman untuk dibaca, atau lebih cepat kerana halaman yang tepat diprapetap tepat pada waktunya. Dalam makalah ini kami menangani dua senario baru untuk cadangan pautan, yang tidak diliputi dalam pekerjaan sebelumnya kerana kekurangan data penyemakan sejarah. Dalam senario carian web, kami mencadangkan kaedah untuk menghasilkan pautan cepat - titik masuk tambahan ke laman web, yang ditunjukkan untuk hasil carian teratas untuk pertanyaan navigasi - untuk laman web ekor, di mana terdapat sedikit statistik penyemakan imbas. Di luar carian Web, kami juga mencadangkan kaedah untuk cadangan pautan dalam penyemakan imbas web secara umum, dengan berkesan menjangkakan pautan seterusnya yang akan diikuti oleh pengguna. Pendekatan kami melakukan pengelompokan laman Web untuk mengumpulkan maklumat di beberapa laman web, dan memungkinkan cadangan pautan yang relevan untuk hampir mana-mana laman web, termasuk laman ekor dan laman web baru yang hanya sedikit data sejarah yang tersedia. Penilaian empirikal mengesahkan kesahihan kaedah kami menggunakan data berlabel editorial serta carian dan carian data sebenar dari enjin carian utama AS. [[EENNDD]] pautan pantas; laman web ekor; carian dan pengambilan maklumat; cadangan pautan; penyemakan imbas dibantu; pengelompokan laman web"], [{"string": "Reliable QoS monitoring based on client feedback Service-level agreements (SLAs) establish a contract between service providersand clients concerning Quality of Service (QoS) parameters. Without properpenalties, service providers have strong incentives to deviate from theadvertised QoS, causing losses to the clients. Reliable QoS monitoring (andproper penalties computed on the basis of delivered QoS) are thereforeessential for the trustworthiness of a service-oriented environment. In thispaper, we present a novel QoS monitoring mechanism based on quality ratings from theclients. A reputation mechanism collects the ratings and computes theactual quality delivered to the clients. The mechanism provides incentives forthe clients to report honestly, and pays special attention to minimizing costand overhead1.", "keywords": ["service-oriented computing", "distributed artificial intelligence", "service-level agreement", "incentive compatibility", "reputation mechanism", "quality-of-service"], "combined": "Reliable QoS monitoring based on client feedback Service-level agreements (SLAs) establish a contract between service providersand clients concerning Quality of Service (QoS) parameters. Without properpenalties, service providers have strong incentives to deviate from theadvertised QoS, causing losses to the clients. Reliable QoS monitoring (andproper penalties computed on the basis of delivered QoS) are thereforeessential for the trustworthiness of a service-oriented environment. In thispaper, we present a novel QoS monitoring mechanism based on quality ratings from theclients. A reputation mechanism collects the ratings and computes theactual quality delivered to the clients. The mechanism provides incentives forthe clients to report honestly, and pays special attention to minimizing costand overhead1. [[EENNDD]] service-oriented computing; distributed artificial intelligence; service-level agreement; incentive compatibility; reputation mechanism; quality-of-service"}, "Pemantauan QoS yang boleh dipercayai berdasarkan maklum balas pelanggan Perjanjian tingkat perkhidmatan (SLA) menetapkan kontrak antara penyedia perkhidmatan dan pelanggan mengenai parameter Quality of Service (QoS). Tanpa harga yang sewajarnya, penyedia perkhidmatan mempunyai insentif yang kuat untuk menyimpang dari QoS yang diiklankan, menyebabkan kerugian kepada pelanggan. Oleh itu, pemantauan QoS yang boleh dipercayai (dan penalti yang betul yang dihitung berdasarkan QoS yang dihantar) adalah penting untuk mempercayai persekitaran berorientasikan perkhidmatan. Dalam makalah ini, kami menyajikan mekanisme pemantauan QoS novel berdasarkan penilaian kualiti dari pelanggan. Mekanisme reputasi mengumpulkan penilaian dan mengira kualiti sebenar yang disampaikan kepada pelanggan. Mekanisme ini memberi insentif kepada klien untuk melaporkan dengan jujur, dan memberi perhatian khusus untuk meminimumkan kos dan overhead1. [[EENNDD]] pengkomputeran berorientasikan perkhidmatan; kecerdasan buatan yang diedarkan; perjanjian tahap perkhidmatan; keserasian insentif; mekanisme reputasi; kualiti sesuatu servis"], [{"string": "XL: an XML programming language for web service specification and composition No contact information provided yet.", "keywords": ["web service", "language classifications", "xml", "programming language"], "combined": "XL: an XML programming language for web service specification and composition No contact information provided yet. [[EENNDD]] web service; language classifications; xml; programming language"}, "XL: bahasa pengaturcaraan XML untuk spesifikasi dan komposisi perkhidmatan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perkhidmatan web; pengelasan bahasa; xml; bahasa pengaturcaraan"], [{"string": "Proximity within paragraph: a measure to enhance document retrieval performance No contact information provided yet.", "keywords": ["information search and retrieval", "proximity measure", "ranking algorithm"], "combined": "Proximity within paragraph: a measure to enhance document retrieval performance No contact information provided yet. [[EENNDD]] information search and retrieval; proximity measure; ranking algorithm"}, "Kedekatan dalam perenggan: langkah untuk meningkatkan prestasi pengambilan dokumen Belum ada maklumat hubungan yang diberikan. [[EENNDD]] carian dan pengambilan maklumat; ukuran jarak; algoritma pemeringkatan"], [{"string": "Thumbs-up: a game for playing to rank search results Human computation is an effective way to channel human effort spent playing games to solving computational problems that are easy for humans but difficult for computers to automate. We propose Thumbs-Up, a new game for human computation with the purpose of playing to rank search result. Our experience from users shows that Thumbs-Up is not only fun to play, but produces more relevant rankings than both a major search engine and optimal rank aggregation using the Kemeny rule.", "keywords": ["search engine", "online games", "rank aggregation", "games with a purpose", "human computation", "relevance"], "combined": "Thumbs-up: a game for playing to rank search results Human computation is an effective way to channel human effort spent playing games to solving computational problems that are easy for humans but difficult for computers to automate. We propose Thumbs-Up, a new game for human computation with the purpose of playing to rank search result. Our experience from users shows that Thumbs-Up is not only fun to play, but produces more relevant rankings than both a major search engine and optimal rank aggregation using the Kemeny rule. [[EENNDD]] search engine; online games; rank aggregation; games with a purpose; human computation; relevance"}, "Thumbs-up: permainan untuk bermain untuk memberi peringkat hasil carian Pengiraan manusia adalah kaedah yang berkesan untuk menyalurkan usaha manusia yang menghabiskan masa bermain permainan untuk menyelesaikan masalah komputasi yang mudah bagi manusia tetapi sukar untuk komputer automatik. Kami mencadangkan Thumbs-Up, permainan baru untuk pengiraan manusia dengan tujuan bermain untuk memberi peringkat hasil carian. Pengalaman kami dari pengguna menunjukkan bahawa Thumbs-Up tidak hanya menyeronokkan untuk dimainkan, tetapi menghasilkan kedudukan yang lebih relevan daripada kedua-dua enjin carian utama dan agregasi peringkat yang optimum menggunakan peraturan Kemeny. [[EENNDD]] enjin carian; permainan dalam talian; pengagregatan peringkat; permainan dengan tujuan; pengiraan manusia; kesesuaian"], [{"string": "Providing session management as core business service It is extremely hard for a global organization with services over multiple channels to capture a consistent and unified view of its data, services, and interactions. While SOA and web services are addressing integration and interoperability problems, it is painful for an operational organization with legacy systems to quickly switch to service-based methods. We need methods to combine advantages of traditional (i.e. web, desktop, or mobile) application development environments and service-based deployments.In this paper, we focus on the design and implementation of session management as a core service to support business processes and go beyond application-specific sessions and web sessions. We develop local session components for different platforms and complement them with a remote \"session service\" that is independent of applications and platforms. We aim to close the gap between the two worlds by combining their performance, availability and interoperability advantages.", "keywords": ["data serialization", "session service", "multi-channel integration"], "combined": "Providing session management as core business service It is extremely hard for a global organization with services over multiple channels to capture a consistent and unified view of its data, services, and interactions. While SOA and web services are addressing integration and interoperability problems, it is painful for an operational organization with legacy systems to quickly switch to service-based methods. We need methods to combine advantages of traditional (i.e. web, desktop, or mobile) application development environments and service-based deployments.In this paper, we focus on the design and implementation of session management as a core service to support business processes and go beyond application-specific sessions and web sessions. We develop local session components for different platforms and complement them with a remote \"session service\" that is independent of applications and platforms. We aim to close the gap between the two worlds by combining their performance, availability and interoperability advantages. [[EENNDD]] data serialization; session service; multi-channel integration"}, "Menyediakan pengurusan sesi sebagai perkhidmatan perniagaan teras Sangat sukar bagi organisasi global dengan perkhidmatan melalui pelbagai saluran untuk menangkap pandangan yang konsisten dan bersatu mengenai data, perkhidmatan, dan interaksinya. Walaupun SOA dan perkhidmatan web menangani masalah integrasi dan interoperabilitas, menyakitkan bagi organisasi operasi dengan sistem warisan untuk beralih ke kaedah berasaskan perkhidmatan dengan cepat. Kami memerlukan kaedah untuk menggabungkan kelebihan persekitaran pembangunan aplikasi tradisional (seperti web, desktop, atau mudah alih) dan penyebaran berdasarkan perkhidmatan. Dalam makalah ini, kami memfokuskan pada reka bentuk dan pelaksanaan pengurusan sesi sebagai perkhidmatan teras untuk menyokong proses perniagaan dan terus berjalan di luar sesi khusus aplikasi dan sesi web. Kami mengembangkan komponen sesi tempatan untuk platform yang berbeza dan melengkapkannya dengan \"perkhidmatan sesi\" jarak jauh yang tidak bergantung pada aplikasi dan platform. Kami bertujuan untuk merapatkan jurang antara kedua dunia dengan menggabungkan prestasi, ketersediaan dan kelebihan operasi. [[EENNDD]] siri data; perkhidmatan sesi; integrasi berbilang saluran"], [{"string": "The Eigentrust algorithm for reputation management in P2P networks No contact information provided yet.", "keywords": ["reputation", "distributed eigenvector computation", "peer-to-peer"], "combined": "The Eigentrust algorithm for reputation management in P2P networks No contact information provided yet. [[EENNDD]] reputation; distributed eigenvector computation; peer-to-peer"}, "Algoritma Eigentrust untuk pengurusan reputasi dalam rangkaian P2P Belum ada maklumat hubungan yang diberikan. [[EENNDD]] reputasi; pengiraan eigenvektor yang diedarkan; rakan sebaya"], [{"string": "A quality framework for web site quality: user satisfaction and quality assurance No contact information provided yet.", "keywords": ["best practices", "web site quality", "handicapped persons/special needs", "quality assurance", "standards"], "combined": "A quality framework for web site quality: user satisfaction and quality assurance No contact information provided yet. [[EENNDD]] best practices; web site quality; handicapped persons/special needs; quality assurance; standards"}, "Kerangka kualiti untuk kualiti laman web: kepuasan pengguna dan jaminan kualiti Belum ada maklumat hubungan yang diberikan. [[EENNDD]] amalan terbaik; kualiti laman web; orang kurang upaya / keperluan khas; jaminan kualiti; piawaian"], [{"string": "A more precise model for web retrieval No contact information provided yet.", "keywords": ["internet", "web retrieval", "latency", "dependency", "model"], "combined": "A more precise model for web retrieval No contact information provided yet. [[EENNDD]] internet; web retrieval; latency; dependency; model"}, "Model yang lebih tepat untuk pengambilan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] internet; pengambilan web; kependaman; pergantungan; model"], [{"string": "Do not crawl in the DUST: different URLs with similar text No contact information provided yet.", "keywords": ["rules", "similarity", "duplicates", "mining"], "combined": "Do not crawl in the DUST: different URLs with similar text No contact information provided yet. [[EENNDD]] rules; similarity; duplicates; mining"}, "Jangan merangkak di DUST: URL berbeza dengan teks yang serupa Belum ada maklumat hubungan yang diberikan. [[EENNDD]] peraturan; persamaan; pendua; perlombongan"], [{"string": "WEBCAP: a capacity planning tool for web resource management No contact information provided yet.", "keywords": ["scheduling", "performance evaluation", "capacity-planning", "multimedia"], "combined": "WEBCAP: a capacity planning tool for web resource management No contact information provided yet. [[EENNDD]] scheduling; performance evaluation; capacity-planning; multimedia"}, "WEBCAP: alat perancangan keupayaan untuk pengurusan sumber web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penjadualan; penilaian prestasi; perancangan keupayaan; multimedia"], [{"string": "Event synchronization for interactive cyberdrama generation on the web: a distributed approach No contact information provided yet.", "keywords": ["interactive storytelling", "cyberdrama generation", "web-based multiplayer games", "computer-based entertainment"], "combined": "Event synchronization for interactive cyberdrama generation on the web: a distributed approach No contact information provided yet. [[EENNDD]] interactive storytelling; cyberdrama generation; web-based multiplayer games; computer-based entertainment"}, "Penyegerakan acara untuk penjanaan cyberdrama interaktif di web: pendekatan tersebar Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] penceritaan interaktif; generasi siber; permainan berbilang pemain berasaskan web; hiburan berasaskan komputer"], [{"string": "OpenRuleBench: an analysis of the performance of rule engines The Semantic Web initiative has led to an upsurge of the interest in rules as a general and powerful way of processing, combining, and analyzing semantic information. Since several of the technologies underlying rule-based systems are already quite mature, it is important to understand how such systems might perform on the Web scale. OpenRuleBench is a suite of benchmarks for analyzing the performance and scalability of different rule engines. Currently the study spans five different technologies and eleven systems, but OpenRuleBench is an open community resource, and contributions from the community are welcome. In this paper, we describe the tested systems and technologies, the methodology used in testing, and analyze the results.", "keywords": ["openrulebench", "semantic web", "performance evaluation", "rule systems", "benchmark"], "combined": "OpenRuleBench: an analysis of the performance of rule engines The Semantic Web initiative has led to an upsurge of the interest in rules as a general and powerful way of processing, combining, and analyzing semantic information. Since several of the technologies underlying rule-based systems are already quite mature, it is important to understand how such systems might perform on the Web scale. OpenRuleBench is a suite of benchmarks for analyzing the performance and scalability of different rule engines. Currently the study spans five different technologies and eleven systems, but OpenRuleBench is an open community resource, and contributions from the community are welcome. In this paper, we describe the tested systems and technologies, the methodology used in testing, and analyze the results. [[EENNDD]] openrulebench; semantic web; performance evaluation; rule systems; benchmark"}, "OpenRuleBench: analisis prestasi enjin peraturan Inisiatif Semantik Web telah menyebabkan peningkatan minat terhadap peraturan sebagai cara umum dan kuat untuk memproses, menggabungkan, dan menganalisis maklumat semantik. Oleh kerana beberapa teknologi yang mendasari sistem berasaskan peraturan sudah cukup matang, penting untuk memahami bagaimana sistem tersebut dapat berfungsi pada skala Web. OpenRuleBench adalah sekumpulan penanda aras untuk menganalisis prestasi dan skalabiliti mesin peraturan yang berbeza. Pada masa ini kajian merangkumi lima teknologi dan sebelas sistem yang berbeza, tetapi OpenRuleBench adalah sumber komuniti terbuka, dan sumbangan dari komuniti sangat dialu-alukan. Dalam makalah ini, kami menerangkan sistem dan teknologi yang diuji, metodologi yang digunakan dalam pengujian, dan menganalisis hasilnya. [[EENNDD]] openrulebench; web semantik; penilaian prestasi; sistem peraturan; penanda aras"], [{"string": "Retaining personal expression for social search Web is being extensively used for personal expression, which includes ratings, reviews, recommendations, blogs. This user created content, e.g. book review on Amazon.com, becomes the property of the website, and the user often does not have easy access to it. In some cases, user's feedback may get averaged with feedback from other users e.g. ratings of a video. We argue that the creator of such content needs to be able to retain (a link to) her created content. We introduce the concept of MEB which is a user controlled store of such retained links. A MEB allows a user to access/share all the reviews she has given on different websites. With this capability users can allow their friends to search through their feedback. Searching through one's social network allows harnessing the power of social networks where known relationships provide the context &amp; trust necessary to interpret feedback.", "keywords": ["systems and software", "2.0", "communications applications", "social networks", "user content", "web2.0", "social", "search"], "combined": "Retaining personal expression for social search Web is being extensively used for personal expression, which includes ratings, reviews, recommendations, blogs. This user created content, e.g. book review on Amazon.com, becomes the property of the website, and the user often does not have easy access to it. In some cases, user's feedback may get averaged with feedback from other users e.g. ratings of a video. We argue that the creator of such content needs to be able to retain (a link to) her created content. We introduce the concept of MEB which is a user controlled store of such retained links. A MEB allows a user to access/share all the reviews she has given on different websites. With this capability users can allow their friends to search through their feedback. Searching through one's social network allows harnessing the power of social networks where known relationships provide the context &amp; trust necessary to interpret feedback. [[EENNDD]] systems and software; 2.0; communications applications; social networks; user content; web2.0; social; search"}, ""], [{"string": "Are web pages characterized by color? No contact information provided yet.", "keywords": ["color", "general", "miscellaneous", "contents of web page"], "combined": "Are web pages characterized by color? No contact information provided yet. [[EENNDD]] color; general; miscellaneous; contents of web page"}, "Adakah laman web dicirikan oleh warna? Belum ada maklumat hubungan yang diberikan. [[EENNDD]] warna; umum; pelbagai; kandungan laman web"], [{"string": "Combining classifiers to identify online databases We address the problem of identifying the domain of onlinedatabases. More precisely, given a set F of Web forms automaticallygathered by a focused crawler and an online databasedomain D, our goal is to select from F only the formsthat are entry points to databases in D. Having a set ofWebforms that serve as entry points to similar online databasesis a requirement for many applications and techniques thataim to extract and integrate hidden-Web information, suchas meta-searchers, online database directories, hidden-Webcrawlers, and form-schema matching and merging.We propose a new strategy that automatically and accuratelyclassifies online databases based on features that canbe easily extracted from Web forms. By judiciously partitioningthe space of form features, this strategy allows theuse of simpler classifiers that can be constructed using learningtechniques that are better suited for the features of eachpartition. Experiments using real Web data in a representativeset of domains show that the use of different classifiersleads to high accuracy, precision and recall. This indicatesthat our modular classifier composition provides an effectiveand scalable solution for classifying online databases.", "keywords": ["hidden web", "learning classifiers", "online database directories", "hierarchical classifiers", "web crawlers"], "combined": "Combining classifiers to identify online databases We address the problem of identifying the domain of onlinedatabases. More precisely, given a set F of Web forms automaticallygathered by a focused crawler and an online databasedomain D, our goal is to select from F only the formsthat are entry points to databases in D. Having a set ofWebforms that serve as entry points to similar online databasesis a requirement for many applications and techniques thataim to extract and integrate hidden-Web information, suchas meta-searchers, online database directories, hidden-Webcrawlers, and form-schema matching and merging.We propose a new strategy that automatically and accuratelyclassifies online databases based on features that canbe easily extracted from Web forms. By judiciously partitioningthe space of form features, this strategy allows theuse of simpler classifiers that can be constructed using learningtechniques that are better suited for the features of eachpartition. Experiments using real Web data in a representativeset of domains show that the use of different classifiersleads to high accuracy, precision and recall. This indicatesthat our modular classifier composition provides an effectiveand scalable solution for classifying online databases. [[EENNDD]] hidden web; learning classifiers; online database directories; hierarchical classifiers; web crawlers"}, "Menggabungkan pengklasifikasi untuk mengenal pasti pangkalan data dalam talian Kami menangani masalah mengenal pasti domain onlinesatabases. Lebih tepatnya, memandangkan satu set F bentuk Web yang dikumpulkan secara automatik oleh perayap fokus dan pangkalan data dalam talian, D, tujuan kami adalah untuk memilih dari F hanya bentuk yang menjadi pintu masuk ke pangkalan data di D. Mempunyai satu set Borang Web yang berfungsi sebagai pintu masuk untuk serupa pangkalan data dalam talian adalah keperluan untuk banyak aplikasi dan teknik yang menuntut untuk mengekstrak dan mengintegrasikan maklumat Web tersembunyi, seperti pencari meta, direktori pangkalan data dalam talian, penyusun Web-tersembunyi, dan pemadanan dan penggabungan skema bentuk. Kami mencadangkan strategi baru yang mengklasifikasikan secara dalam talian secara automatik dan tepat pangkalan data berdasarkan ciri yang dapat dengan mudah diekstrak dari bentuk Web. Dengan mempartisi ruang bentuk ciri dengan bijak, strategi ini membolehkan penggunaan pengelasan lebih mudah yang boleh dibina menggunakan teknik pembelajaran yang lebih sesuai untuk ciri-ciri setiap bahagian. Eksperimen menggunakan data Web sebenar dalam kumpulan domain menunjukkan bahawa penggunaan pengkelasan yang berbeza membawa kepada ketepatan, ketepatan dan penarikan balik yang tinggi. Ini menunjukkan bahawa komposisi pengkelasan modular kami menyediakan penyelesaian berkesan dan boleh diskalakan untuk mengklasifikasikan pangkalan data dalam talian. [[EENNDD]] web tersembunyi; pengkelasan pembelajaran; direktori pangkalan data dalam talian; pengkelasan hierarki; perayap web"], [{"string": "SCTP: an innovative transport layer protocol for the web No contact information provided yet.", "keywords": ["web transport", "web applications", "internet", "standards", "fault-tolerance", "sctp", "stream control transmission protocol", "head-of-line blocking", "transport layer service"], "combined": "SCTP: an innovative transport layer protocol for the web No contact information provided yet. [[EENNDD]] web transport; web applications; internet; standards; fault-tolerance; sctp; stream control transmission protocol; head-of-line blocking; transport layer service"}, "SCTP: protokol lapisan pengangkutan yang inovatif untuk web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengangkutan web; aplikasi web; internet; standard; toleransi kesalahan; sctp; protokol penghantaran aliran kawalan; sekatan head-of-line; perkhidmatan lapisan pengangkutan"], [{"string": "WISER: a web-based interactive route search system for smartphones Many smartphones, nowadays, use GPS to detect the location of the user, and can use the Internet to interact with remote location-based services. These two capabilities support online navigation that incorporates search. In this demo we presents WISER---a system for Web-based Interactive Search en Route. In the system, users perform route search by providing (1) a target location, and (2) search terms that specify types of geographic entities to be visited.", "keywords": ["path planning", "route search", "navigation", "android"], "combined": "WISER: a web-based interactive route search system for smartphones Many smartphones, nowadays, use GPS to detect the location of the user, and can use the Internet to interact with remote location-based services. These two capabilities support online navigation that incorporates search. In this demo we presents WISER---a system for Web-based Interactive Search en Route. In the system, users perform route search by providing (1) a target location, and (2) search terms that specify types of geographic entities to be visited. [[EENNDD]] path planning; route search; navigation; android"}, "WISER: sistem carian laluan interaktif berasaskan web untuk telefon pintar. Banyak telefon pintar, pada masa kini, menggunakan GPS untuk mengesan lokasi pengguna, dan dapat menggunakan Internet untuk berinteraksi dengan perkhidmatan berdasarkan lokasi jarak jauh. Kedua keupayaan ini menyokong navigasi dalam talian yang menggabungkan carian. Dalam demo ini kami mempersembahkan WISER --- sistem untuk Carian Interaktif berasaskan Web dalam Laluan. Dalam sistem, pengguna melakukan pencarian rute dengan menyediakan (1) lokasi sasaran, dan (2) istilah pencarian yang menentukan jenis entitas geografi yang akan dikunjungi. [[EENNDD]] perancangan jalan; carian laluan; pelayaran; android"], [{"string": "Dtwiki: a disconnection and intermittency tolerant wiki Wikis have proven to be a valuable tool for collaboration and content generation on the web. Simple semantics and ease-of-use make wiki systems well suited for meeting many emerging region needs in the areas of education, collaboration and local content generation. Despite their usefulness, current wiki software does not work well in the network environments found in emerging regions. For example, it is common to have long-lasting network partitions due to cost, power and poor connectivity. Network partitions make a traditional centralized wiki architecture unusable due to the unavailability of the central server. Existing solutions towards addressing connectivity problems include web-caching proxies and snapshot distribution. While proxies and snapshots allow wiki data to be read while disconnected, they prevent users from contributing updates back to the wiki.", "keywords": ["wiki", "general", "developing regions", "delay-tolerant networking"], "combined": "Dtwiki: a disconnection and intermittency tolerant wiki Wikis have proven to be a valuable tool for collaboration and content generation on the web. Simple semantics and ease-of-use make wiki systems well suited for meeting many emerging region needs in the areas of education, collaboration and local content generation. Despite their usefulness, current wiki software does not work well in the network environments found in emerging regions. For example, it is common to have long-lasting network partitions due to cost, power and poor connectivity. Network partitions make a traditional centralized wiki architecture unusable due to the unavailability of the central server. Existing solutions towards addressing connectivity problems include web-caching proxies and snapshot distribution. While proxies and snapshots allow wiki data to be read while disconnected, they prevent users from contributing updates back to the wiki. [[EENNDD]] wiki; general; developing regions; delay-tolerant networking"}, "Dtwiki: Wiki Wiki yang bertolak ansur dan terputus hubungan telah terbukti menjadi alat yang berharga untuk kolaborasi dan penghasilan kandungan di web. Semantik yang sederhana dan kemudahan penggunaan menjadikan sistem wiki sangat sesuai untuk memenuhi banyak keperluan wilayah yang baru muncul dalam bidang pendidikan, kolaborasi dan penghasilan kandungan tempatan. Walaupun bermanfaat, perisian wiki semasa tidak berfungsi dengan baik di persekitaran rangkaian yang terdapat di kawasan yang baru muncul. Sebagai contoh, kebiasaan mempunyai partisi rangkaian yang tahan lama kerana kos, kuasa dan sambungan yang lemah. Partisi rangkaian menjadikan seni bina wiki terpusat tradisional tidak dapat digunakan kerana ketiadaan pelayan pusat. Penyelesaian yang ada untuk mengatasi masalah penyambungan termasuk proksi cache web dan pengedaran snapshot. Walaupun proksi dan tangkapan gambar membolehkan data wiki dibaca semasa terputus, mereka menghalang pengguna untuk menyumbang kemas kini kembali ke wiki. [[EENNDD]] wiki; umum; wilayah membangun; rangkaian tahan lama"], [{"string": "Echoes of power: language effects and power differences in social interaction Understanding social interaction within groups is key to analyzing online communities. Most current work focuses on structural properties: who talks to whom, and how such interactions form larger network structures. The interactions themselves, however, generally take place in the form of natural language --- either spoken or written --- and one could reasonably suppose that signals manifested in language might also provide information about roles, status, and other aspects of the group's dynamics. To date, however, finding domain-independent language-based signals has been a challenge.", "keywords": ["accommodation", "linguistic convergence", "language", "dependence", "linguistic style", "social and behavioral sciences", "relations", "power", "online communities", "social status", "coordination"], "combined": "Echoes of power: language effects and power differences in social interaction Understanding social interaction within groups is key to analyzing online communities. Most current work focuses on structural properties: who talks to whom, and how such interactions form larger network structures. The interactions themselves, however, generally take place in the form of natural language --- either spoken or written --- and one could reasonably suppose that signals manifested in language might also provide information about roles, status, and other aspects of the group's dynamics. To date, however, finding domain-independent language-based signals has been a challenge. [[EENNDD]] accommodation; linguistic convergence; language; dependence; linguistic style; social and behavioral sciences; relations; power; online communities; social status; coordination"}, "Gema kuasa: kesan bahasa dan perbezaan daya dalam interaksi sosial Memahami interaksi sosial dalam kumpulan adalah kunci untuk menganalisis komuniti dalam talian. Sebilangan besar karya semasa menumpukan pada sifat struktur: siapa yang bercakap dengan siapa, dan bagaimana interaksi sedemikian membentuk struktur rangkaian yang lebih besar. Interaksi itu sendiri, bagaimanapun, secara amnya berlaku dalam bentuk bahasa semula jadi --- baik yang dituturkan atau yang ditulis --- dan seseorang boleh menganggap bahawa isyarat yang ditunjukkan dalam bahasa juga dapat memberikan maklumat mengenai peranan, status, dan aspek lain dari kumpulan dinamik. Namun, setakat ini, mencari isyarat berdasarkan bahasa yang bebas domain telah menjadi satu cabaran. [[EENNDD]] penginapan; penumpuan linguistik; bahasa; pergantungan; gaya linguistik; sains sosial dan tingkah laku; hubungan; kuasa; komuniti dalam talian; status sosial; penyelarasan"], [{"string": "Nearest-neighbor caching for content-match applications Motivated by contextual advertising systems and other web applications involving efficiency-accuracy tradeoffs, we study similarity caching. Here, a cache hit is said to occur if the requested item is similar but not necessarily equal to some cached item. We study two objectives that dictate the efficiency-accuracy tradeoff and provide our caching policies for these objectives. By conducting extensive experiments on real data we show similarity caching can significantly improve the efficiency of contextual advertising systems, with minimal impact on accuracy. Inspired by the above, we propose a simple generative model that embodies two fundamental characteristics of page requests arriving to advertising systems, namely, long-range dependences and similarities. We provide theoretical bounds on the gains of similarity caching in this model and demonstrate these gains empirically by fitting the actual data to the model.", "keywords": ["content-match", "caching", "miscellaneous", "nearest-neighbor"], "combined": "Nearest-neighbor caching for content-match applications Motivated by contextual advertising systems and other web applications involving efficiency-accuracy tradeoffs, we study similarity caching. Here, a cache hit is said to occur if the requested item is similar but not necessarily equal to some cached item. We study two objectives that dictate the efficiency-accuracy tradeoff and provide our caching policies for these objectives. By conducting extensive experiments on real data we show similarity caching can significantly improve the efficiency of contextual advertising systems, with minimal impact on accuracy. Inspired by the above, we propose a simple generative model that embodies two fundamental characteristics of page requests arriving to advertising systems, namely, long-range dependences and similarities. We provide theoretical bounds on the gains of similarity caching in this model and demonstrate these gains empirically by fitting the actual data to the model. [[EENNDD]] content-match; caching; miscellaneous; nearest-neighbor"}, "Cache terdekat untuk aplikasi pencocokan kandungan Dimotivasi oleh sistem iklan kontekstual dan aplikasi web lain yang melibatkan pertukaran ketepatan kecekapan, kami mengkaji persamaan caching. Di sini, hit cache dikatakan berlaku jika item yang diminta serupa tetapi tidak semestinya sama dengan beberapa item cache. Kami mengkaji dua objektif yang menentukan pertukaran kecekapan-kecekapan dan menyediakan dasar cache kami untuk objektif tersebut. Dengan melakukan eksperimen yang meluas pada data sebenar, kami menunjukkan bahawa persamaan caching dapat meningkatkan kecekapan sistem periklanan kontekstual dengan ketara, dengan kesan minimum pada ketepatan. Diilhamkan oleh perkara di atas, kami mencadangkan model generatif sederhana yang merangkumi dua ciri asas permintaan halaman yang sampai ke sistem pengiklanan, iaitu, pergantungan jarak jauh dan kesamaan. Kami memberikan batasan teoretis mengenai keuntungan persamaan dalam model ini dan menunjukkan keuntungan ini secara empirik dengan memasangkan data sebenar dengan model tersebut. [[EENNDD]] padanan kandungan; caching; pelbagai; jiran terdekat"], [{"string": "Speeding up adaptation of web service compositions using expiration times Web processes must often operate in volatile environments where the quality of service parameters of the participating service providers change during the life time of the process. In order to remain optimal, the Web process must adapt to these changes. Adaptation requires knowledge about the parameter changes of each of the service providers and using this knowledge to determine whether the Web process should make a different more optimal decision. Previously, we defined a mechanism called the value of changed information which measures the impact of expected changes in the service parameters on the Web process, thereby offering a way to query and incorporate those changes that are useful and cost-efficient. However, computing the value of changed information incurs a substantial computational overhead. In this paper, we use service expiration times obtained from pre-defined service level agreements to reduce the computational overhead of adaptation. We formalize the intuition that services whose parameters have not expired need not be considered for querying for revised information. Using two realistic scenarios, we illustrate our approach and demonstrate the associated computational savings.", "keywords": ["volatile environments", "adaptation", "web services", "expiration times"], "combined": "Speeding up adaptation of web service compositions using expiration times Web processes must often operate in volatile environments where the quality of service parameters of the participating service providers change during the life time of the process. In order to remain optimal, the Web process must adapt to these changes. Adaptation requires knowledge about the parameter changes of each of the service providers and using this knowledge to determine whether the Web process should make a different more optimal decision. Previously, we defined a mechanism called the value of changed information which measures the impact of expected changes in the service parameters on the Web process, thereby offering a way to query and incorporate those changes that are useful and cost-efficient. However, computing the value of changed information incurs a substantial computational overhead. In this paper, we use service expiration times obtained from pre-defined service level agreements to reduce the computational overhead of adaptation. We formalize the intuition that services whose parameters have not expired need not be considered for querying for revised information. Using two realistic scenarios, we illustrate our approach and demonstrate the associated computational savings. [[EENNDD]] volatile environments; adaptation; web services; expiration times"}, "Mempercepat penyesuaian komposisi perkhidmatan web menggunakan masa tamat Proses Web mesti sering beroperasi dalam persekitaran yang tidak stabil di mana parameter kualiti perkhidmatan penyedia perkhidmatan yang berpartisipasi berubah selama masa proses berlangsung. Agar tetap optimum, proses Web mesti menyesuaikan diri dengan perubahan ini. Penyesuaian memerlukan pengetahuan mengenai perubahan parameter setiap penyedia perkhidmatan dan menggunakan pengetahuan ini untuk menentukan sama ada proses Web harus membuat keputusan yang lebih optimum dan berbeza. Sebelumnya, kami mendefinisikan mekanisme yang disebut nilai maklumat berubah yang mengukur kesan perubahan yang diharapkan dalam parameter perkhidmatan pada proses Web, dengan itu menawarkan cara untuk membuat pertanyaan dan memasukkan perubahan yang berguna dan menjimatkan kos. Walau bagaimanapun, mengira nilai maklumat yang berubah menghasilkan overhead komputasi yang besar. Dalam makalah ini, kami menggunakan masa tamat perkhidmatan yang diperoleh dari perjanjian tahap perkhidmatan yang telah ditentukan untuk mengurangkan overhed penyesuaian komputasi. Kami memformalkan intuisi bahawa perkhidmatan yang parameternya tidak luput tidak perlu dipertimbangkan untuk meminta maklumat yang disemak. Dengan menggunakan dua senario realistik, kami menggambarkan pendekatan kami dan menunjukkan penjimatan komputasi yang berkaitan. [[EENNDD]] persekitaran yang tidak menentu; adaptasi; perkhidmatan web; masa tamat"], [{"string": "Efficient structural joins with on-the-fly indexing No contact information provided yet.", "keywords": ["structural joins", "containment queries", "xml", "information search and retrieval"], "combined": "Efficient structural joins with on-the-fly indexing No contact information provided yet. [[EENNDD]] structural joins; containment queries; xml; information search and retrieval"}, "Struktur yang cekap bergabung dengan pengindeksan on-the-fly Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] struktur bergabung; pertanyaan pembendungan; xml; pencarian dan pencarian maklumat"], [{"string": "Scaling personalized web search No contact information provided yet.", "keywords": ["web search", "graph theory", "pagerank"], "combined": "Scaling personalized web search No contact information provided yet. [[EENNDD]] web search; graph theory; pagerank"}, "Menimbang carian web diperibadikan Belum ada maklumat hubungan yang diberikan [[EENNDD]] carian web; teori grafik; pagerank"], [{"string": "Crawling a country: better strategies than breadth-first for web page ordering No contact information provided yet.", "keywords": ["web page importance", "web crawler", "scheduling policy", "performance evaluation"], "combined": "Crawling a country: better strategies than breadth-first for web page ordering No contact information provided yet. [[EENNDD]] web page importance; web crawler; scheduling policy; performance evaluation"}, "Merangkak negara: strategi yang lebih baik daripada yang pertama untuk pesanan halaman web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kepentingan laman web; perangkak web; polisi penjadualan; penilaian prestasi"], [{"string": "Effective web-scale crawling through website analysis No contact information provided yet.", "keywords": ["netsifter", "uima", "sampling", "software architectures", "webfountain", "crawling"], "combined": "Effective web-scale crawling through website analysis No contact information provided yet. [[EENNDD]] netsifter; uima; sampling; software architectures; webfountain; crawling"}, "Penjelajahan skala web yang berkesan melalui analisis laman web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] lebih bersih; uima; persampelan; seni bina perisian; sumber web; merangkak"], [{"string": "b-Bit minwise hashing This paper establishes the theoretical framework of b-bit minwise hashing. The original minwise hashing method has become a standard technique for estimating set similarity (e.g., resemblance) with applications in information retrieval, data management, computational advertising, etc.", "keywords": ["similarity estimation", "hashing"], "combined": "b-Bit minwise hashing This paper establishes the theoretical framework of b-bit minwise hashing. The original minwise hashing method has become a standard technique for estimating set similarity (e.g., resemblance) with applications in information retrieval, data management, computational advertising, etc. [[EENNDD]] similarity estimation; hashing"}, "b-bit hashing minhing Kertas ini menetapkan kerangka teori hashing b-bit minwise. Kaedah hashing minwise yang asli telah menjadi teknik standard untuk mengira kesamaan set (mis., Kemiripan) dengan aplikasi dalam pengambilan maklumat, pengurusan data, iklan komputasi, dan lain-lain [[EENNDD]] anggaran kesamaan; hash"], [{"string": "Evaluation of edge caching/offloading for dynamic content delivery No contact information provided yet.", "keywords": ["edge caching", "dynamic content", "offloading", "performance evaluation"], "combined": "Evaluation of edge caching/offloading for dynamic content delivery No contact information provided yet. [[EENNDD]] edge caching; dynamic content; offloading; performance evaluation"}, "Penilaian cache tepi / pemunggahan untuk penyampaian kandungan dinamik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] cache tepi; kandungan dinamik; pemunggahan; penilaian prestasi"], [{"string": "Finding specification pages according to attributes No contact information provided yet.", "keywords": ["web search", "specification finding", "information search and retrieval", "attribute acquisition"], "combined": "Finding specification pages according to attributes No contact information provided yet. [[EENNDD]] web search; specification finding; information search and retrieval; attribute acquisition"}, "Mencari halaman spesifikasi mengikut atribut Belum ada maklumat hubungan yang diberikan. [[EENNDD]] carian web; penemuan spesifikasi; pencarian dan pengambilan maklumat; pemerolehan atribut"], [{"string": "MemoSpace: a visualization tool for web navigation No contact information provided yet.", "keywords": ["memospace", "graphical user interfaces", "navigation"], "combined": "MemoSpace: a visualization tool for web navigation No contact information provided yet. [[EENNDD]] memospace; graphical user interfaces; navigation"}, "MemoSpace: alat visualisasi untuk navigasi web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] memospace; antara muka pengguna grafik; pelayaran"], [{"string": "Dynamic coordination of information management services for processing dynamic web content No contact information provided yet.", "keywords": ["web information management systems", "dynamic service coordination", "scalable component-based software systems", "dynamic web content", "patterns", "semantic interoperability"], "combined": "Dynamic coordination of information management services for processing dynamic web content No contact information provided yet. [[EENNDD]] web information management systems; dynamic service coordination; scalable component-based software systems; dynamic web content; patterns; semantic interoperability"}, "Penyelarasan dinamik perkhidmatan pengurusan maklumat untuk memproses kandungan web dinamik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] sistem pengurusan maklumat web; penyelarasan perkhidmatan yang dinamik; sistem perisian berasaskan komponen berskala; kandungan web dinamik; corak; interoperabiliti semantik"], [{"string": "REST-based management of loosely coupled services Applications increasingly make use of the distributed platform that the World Wide Web provides - be it as a Software-as-a-Service such as salesforce.com, an application infrastructure such as facebook.com, or a computing infrastructure such as a \"cloud\". A common characteristic of applications of this kind is that they are deployed on infrastructure or make use of components that reside in different management domains. Current service management approaches and systems, however, often rely on a centrally managed configuration management database (CMDB), which is the basis for centrally orchestrated service management processes, in particular change management and incident management. The distribution of management responsibility of WWW based applications requires a decentralized approach to service management. This paper proposes an approach of decentralized service management based on distributed configuration management and service process co-ordination, making use RESTful access to configuration information and ATOM-based distribution of updates as a novel foundation for service management processes.", "keywords": ["general", "service management", "discovery", "loosely coupled systems", "rest"], "combined": "REST-based management of loosely coupled services Applications increasingly make use of the distributed platform that the World Wide Web provides - be it as a Software-as-a-Service such as salesforce.com, an application infrastructure such as facebook.com, or a computing infrastructure such as a \"cloud\". A common characteristic of applications of this kind is that they are deployed on infrastructure or make use of components that reside in different management domains. Current service management approaches and systems, however, often rely on a centrally managed configuration management database (CMDB), which is the basis for centrally orchestrated service management processes, in particular change management and incident management. The distribution of management responsibility of WWW based applications requires a decentralized approach to service management. This paper proposes an approach of decentralized service management based on distributed configuration management and service process co-ordination, making use RESTful access to configuration information and ATOM-based distribution of updates as a novel foundation for service management processes. [[EENNDD]] general; service management; discovery; loosely coupled systems; rest"}, "Pengurusan berasaskan REST terhadap perkhidmatan yang digabungkan secara longgar Aplikasi semakin menggunakan platform diedarkan yang disediakan oleh World Wide Web - sama ada sebagai Perisian-sebagai-a-Perkhidmatan seperti salesforce.com, infrastruktur aplikasi seperti facebook.com, atau infrastruktur pengkomputeran seperti \"awan\". Ciri umum aplikasi seperti ini adalah mereka digunakan pada infrastruktur atau menggunakan komponen yang berada dalam domain pengurusan yang berbeza. Pendekatan dan sistem pengurusan perkhidmatan semasa, bagaimanapun, sering bergantung pada pangkalan data pengurusan konfigurasi yang dikendalikan secara terpusat (CMDB), yang merupakan asas untuk proses pengurusan perkhidmatan yang diatur secara terpusat, khususnya pengurusan perubahan dan pengurusan kejadian. Pembahagian tanggungjawab pengurusan aplikasi berasaskan WWW memerlukan pendekatan yang terdesentralisasi untuk pengurusan perkhidmatan. Makalah ini mencadangkan pendekatan pengurusan perkhidmatan yang terdesentralisasi berdasarkan pengelolaan konfigurasi terdistribusi dan koordinasi proses perkhidmatan, menggunakan akses RESTful ke maklumat konfigurasi dan penyebaran kemas kini berasaskan ATOM sebagai asas baru untuk proses pengurusan perkhidmatan. [[EENNDD]] umum; pengurusan perkhidmatan; penemuan; sistem berpasangan longgar; berehat"], [{"string": "E-learning personalization based on itineraries and long-term navigational behavior No contact information provided yet.", "keywords": ["data mining", "personalization", "computer uses in education", "navigational patterns", "scorm", "user interfaces", "e-learning"], "combined": "E-learning personalization based on itineraries and long-term navigational behavior No contact information provided yet. [[EENNDD]] data mining; personalization; computer uses in education; navigational patterns; scorm; user interfaces; e-learning"}, "Pemperibadian e-pembelajaran berdasarkan jadual perjalanan dan tingkah laku navigasi jangka panjang Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlombongan data; pemperibadian; penggunaan komputer dalam pendidikan; corak pelayaran; ribut; antara muka pengguna; e-pembelajaran"], [{"string": "Measuring a commercial content delivery network Content delivery networks (CDNs) have become a crucial part of the modern Web infrastructure. This paper studies the performance of the leading content delivery provider - Akamai. It measures the performance of the current Akamai platform and considers a key architectural question faced by both CDN designers and their prospective customers: whether the co-location approach to CDN platforms adopted by Akamai, which tries to deploy servers in numerous Internet locations, brings inherent performance benefits over a more consolidated data center approach pursued by other influential CDNs such as Limelight. We believe the methodology we developed for this study will be useful for other researchers in the CDN arena.", "keywords": ["akamai", "content delivery networks", "cdn performance"], "combined": "Measuring a commercial content delivery network Content delivery networks (CDNs) have become a crucial part of the modern Web infrastructure. This paper studies the performance of the leading content delivery provider - Akamai. It measures the performance of the current Akamai platform and considers a key architectural question faced by both CDN designers and their prospective customers: whether the co-location approach to CDN platforms adopted by Akamai, which tries to deploy servers in numerous Internet locations, brings inherent performance benefits over a more consolidated data center approach pursued by other influential CDNs such as Limelight. We believe the methodology we developed for this study will be useful for other researchers in the CDN arena. [[EENNDD]] akamai; content delivery networks; cdn performance"}, "Mengukur rangkaian penghantaran kandungan komersial Rangkaian penghantaran kandungan (CDN) telah menjadi bahagian penting dari infrastruktur Web moden. Makalah ini mengkaji prestasi penyedia penyampaian kandungan terkemuka - Akamai. Ini mengukur prestasi platform Akamai semasa dan mempertimbangkan persoalan seni bina utama yang dihadapi oleh kedua-dua pereka CDN dan calon pelanggan mereka: adakah pendekatan lokasi bersama platform CDN yang diadopsi oleh Akamai, yang cuba menyebarkan pelayan di banyak lokasi Internet, membawa wujud faedah prestasi berbanding pendekatan pusat data yang lebih disatukan yang dilaksanakan oleh CDN lain yang berpengaruh seperti Limelight. Kami yakin metodologi yang kami kembangkan untuk kajian ini akan berguna untuk penyelidik lain di arena CDN. [[EENNDD]] akamai; rangkaian penghantaran kandungan; prestasi cdn"], [{"string": "Web object retrieval The primary function of current Web search engines is essentially relevance ranking at the document level. However, myriad structured information about real-world objects is embedded in static Web pages and online Web databases. Document-level information retrieval can unfortunately lead to highly inaccurate relevance ranking in answering object-oriented queries. In this paper, we propose a paradigm shift to enable searching at the object level. In traditional information retrieval models, documents are taken as the retrieval units and the content of a document is considered reliable. However, this reliability assumption is no longer valid in the object retrieval context when multiple copies of information about the same object typically exist. These copies may be inconsistent because of diversity of Web site qualities and the limited performance of current information extraction techniques. If we simply combine the noisy and inaccurate attribute information extracted from different sources, we may not be able to achieve satisfactory retrieval performance. In this paper, we propose several language models for Web object retrieval, namely an unstructured object retrieval model, a structured object retrieval model, and a hybrid model with both structured and unstructured retrieval features. We test these models on a paper search engine and compare their performances. We conclude that the hybrid model is the superior by taking into account the extraction errors at varying levels.", "keywords": ["information extraction", "language model", "information retrieval", "web objects"], "combined": "Web object retrieval The primary function of current Web search engines is essentially relevance ranking at the document level. However, myriad structured information about real-world objects is embedded in static Web pages and online Web databases. Document-level information retrieval can unfortunately lead to highly inaccurate relevance ranking in answering object-oriented queries. In this paper, we propose a paradigm shift to enable searching at the object level. In traditional information retrieval models, documents are taken as the retrieval units and the content of a document is considered reliable. However, this reliability assumption is no longer valid in the object retrieval context when multiple copies of information about the same object typically exist. These copies may be inconsistent because of diversity of Web site qualities and the limited performance of current information extraction techniques. If we simply combine the noisy and inaccurate attribute information extracted from different sources, we may not be able to achieve satisfactory retrieval performance. In this paper, we propose several language models for Web object retrieval, namely an unstructured object retrieval model, a structured object retrieval model, and a hybrid model with both structured and unstructured retrieval features. We test these models on a paper search engine and compare their performances. We conclude that the hybrid model is the superior by taking into account the extraction errors at varying levels. [[EENNDD]] information extraction; language model; information retrieval; web objects"}, "Pengambilan objek web Fungsi utama mesin pencari Web semasa pada dasarnya adalah kedudukan relevan pada peringkat dokumen. Walau bagaimanapun, pelbagai maklumat tersusun mengenai objek dunia nyata disertakan dalam laman web statik dan pangkalan data Web dalam talian. Pengambilan maklumat peringkat dokumen sayangnya dapat menyebabkan peringkat relevansi yang sangat tidak tepat dalam menjawab pertanyaan berorientasikan objek. Dalam makalah ini, kami mencadangkan pergeseran paradigma untuk membolehkan pencarian pada tahap objek. Dalam model pengambilan maklumat tradisional, dokumen diambil sebagai unit pengambilan dan kandungan dokumen dianggap boleh dipercayai. Walau bagaimanapun, anggapan kebolehpercayaan ini tidak lagi berlaku dalam konteks pengambilan objek apabila terdapat banyak salinan maklumat mengenai objek yang sama. Salinan ini mungkin tidak konsisten kerana kepelbagaian kualiti laman web dan prestasi teknik pengekstrakan maklumat yang terhad. Sekiranya kita hanya menggabungkan maklumat atribut yang bising dan tidak tepat yang diambil dari sumber yang berlainan, kita mungkin tidak dapat mencapai prestasi pengambilan yang memuaskan. Dalam makalah ini, kami mencadangkan beberapa model bahasa untuk pengambilan objek Web, yaitu model pengambilan objek tidak berstruktur, model pengambilan objek berstruktur, dan model hibrid dengan kedua-dua ciri pengambilan berstruktur dan tidak terstruktur. Kami menguji model ini pada mesin carian kertas dan membandingkan penampilannya. Kami menyimpulkan bahawa model hibrid adalah yang unggul dengan mengambil kira kesalahan pengekstrakan pada tahap yang berbeza-beza. [[EENNDD]] pengekstrakan maklumat; model bahasa; pengambilan maklumat; objek web"], [{"string": "EP-SPARQL: a unified language for event processing and stream reasoning Streams of events appear increasingly today in various Web applications such as blogs, feeds, sensor data streams, geospatial information, on-line financial data, etc. Event Processing (EP) is concerned with timely detection of compound events within streams of simple events. State-of-the-art EP provides on-the-fly analysis of event streams, but cannot combine streams with background knowledge and cannot perform reasoning tasks. On the other hand, semantic tools can effectively handle background knowledge and perform reasoning thereon, but cannot deal with rapidly changing data provided by event streams.", "keywords": ["streams", "complex event processing", "semantic web", "rule systems", "logic programming", "etalis"], "combined": "EP-SPARQL: a unified language for event processing and stream reasoning Streams of events appear increasingly today in various Web applications such as blogs, feeds, sensor data streams, geospatial information, on-line financial data, etc. Event Processing (EP) is concerned with timely detection of compound events within streams of simple events. State-of-the-art EP provides on-the-fly analysis of event streams, but cannot combine streams with background knowledge and cannot perform reasoning tasks. On the other hand, semantic tools can effectively handle background knowledge and perform reasoning thereon, but cannot deal with rapidly changing data provided by event streams. [[EENNDD]] streams; complex event processing; semantic web; rule systems; logic programming; etalis"}, "EP-SPARQL: bahasa yang disatukan untuk memproses peristiwa dan penaakulan aliran. Aliran peristiwa muncul semakin hari ini dalam pelbagai aplikasi Web seperti blog, suapan, aliran data sensor, maklumat geospasial, data kewangan dalam talian, dll. Pemrosesan Acara (EP) adalah prihatin dengan pengesanan kejadian kompaun tepat pada masanya dalam aliran peristiwa sederhana. EP yang canggih menyediakan analisis aliran acara secara langsung, tetapi tidak dapat menggabungkan aliran dengan pengetahuan latar belakang dan tidak dapat melakukan tugas penaakulan. Sebaliknya, alat semantik dapat menangani pengetahuan latar belakang dengan berkesan dan melakukan penaakulan di atasnya, tetapi tidak dapat menangani data yang berubah dengan cepat yang disediakan oleh aliran peristiwa. [[EENNDD]] aliran; pemprosesan acara yang kompleks; web semantik; sistem peraturan; pengaturcaraan logik; etalis"], [{"string": "Bootstrapping ontology alignment methods with APFEL An abstract is not available.", "keywords": ["mapping", "interoperability", "ontology", "matching", "alignment", "machine learning"], "combined": "Bootstrapping ontology alignment methods with APFEL An abstract is not available. [[EENNDD]] mapping; interoperability; ontology; matching; alignment; machine learning"}, "Kaedah penjajaran ontologi bootstrap dengan APFEL Abstrak tidak tersedia. [[EENNDD]] pemetaan; saling kendali; ontologi; padanan; penjajaran; pembelajaran mesin"], [{"string": "Social and semantics analysis via non-negative matrix factorization Social media such as Web forum often have dense interactions between user and content where network models are often appropriate for analysis. Joint non-negative matrix factorization model of participation and content data can be viewed as a bipartite graph model between users and media and is proposed for analysis social media. The factorizations allow simultaneous automatic discovery of leaders and sub-communities in the Web forum as well as the core latent topics in the forum. Results on topic detection of Web forums and cluster analysis show that social features are highly effective for forum analysis.", "keywords": ["algorithm", "hypertext/hypermedia", "graph theory", "learning"], "combined": "Social and semantics analysis via non-negative matrix factorization Social media such as Web forum often have dense interactions between user and content where network models are often appropriate for analysis. Joint non-negative matrix factorization model of participation and content data can be viewed as a bipartite graph model between users and media and is proposed for analysis social media. The factorizations allow simultaneous automatic discovery of leaders and sub-communities in the Web forum as well as the core latent topics in the forum. Results on topic detection of Web forums and cluster analysis show that social features are highly effective for forum analysis. [[EENNDD]] algorithm; hypertext/hypermedia; graph theory; learning"}, "Analisis sosial dan semantik melalui pemfaktoran matriks bukan negatif Media sosial seperti forum Web sering mempunyai interaksi yang padat antara pengguna dan kandungan di mana model rangkaian sering sesuai untuk analisis. Model faktorisasi matriks non-negatif bersama untuk penyertaan dan data kandungan dapat dilihat sebagai model grafik bipartit antara pengguna dan media dan dicadangkan untuk analisis media sosial. Faktor-faktor itu membolehkan penemuan automatik pemimpin dan sub-komuniti secara automatik dalam forum Web serta topik-topik teras terpendam dalam forum. Hasil pengesanan topik forum Web dan analisis kluster menunjukkan bahawa ciri sosial sangat berkesan untuk analisis forum. [[EENNDD]] algoritma; hiperteks / hipermedia; teori grafik; belajar"], [{"string": "Deriving knowledge from figures for digital libraries Figures in digital documents contain important information. Current digital libraries do not summarize and index information available within figures for document retrieval. We present our system on automatic categorization of figures and extraction of data from 2-D plots. A machine-learning based method is used to categorize figures into a set of predefined types based on image features. An automated algorithm is designed to extract data values from solid line curves in 2-D plots. The semantic type of figures and extracted data values from 2-D plots can be integrated with textual information within documents to provide more effective document retrieval services for digital library users. Experimental evaluation has demonstrated that our system can produce results suitable for real-world use.", "keywords": ["information search and retrieval", "figures", "feature extraction", "machine learning"], "combined": "Deriving knowledge from figures for digital libraries Figures in digital documents contain important information. Current digital libraries do not summarize and index information available within figures for document retrieval. We present our system on automatic categorization of figures and extraction of data from 2-D plots. A machine-learning based method is used to categorize figures into a set of predefined types based on image features. An automated algorithm is designed to extract data values from solid line curves in 2-D plots. The semantic type of figures and extracted data values from 2-D plots can be integrated with textual information within documents to provide more effective document retrieval services for digital library users. Experimental evaluation has demonstrated that our system can produce results suitable for real-world use. [[EENNDD]] information search and retrieval; figures; feature extraction; machine learning"}, "Memperoleh pengetahuan dari angka untuk perpustakaan digital Angka dalam dokumen digital mengandungi maklumat penting. Perpustakaan digital semasa tidak merangkum dan mengindeks maklumat yang terdapat dalam angka untuk pengambilan dokumen. Kami menyajikan sistem kami mengenai pengkategorian angka dan pengekstrakan data dari plot 2-D secara automatik. Kaedah berasaskan pembelajaran mesin digunakan untuk mengkategorikan angka menjadi sekumpulan jenis yang ditentukan berdasarkan ciri gambar. Algoritma automatik dirancang untuk mengekstrak nilai data dari lengkung garis pepejal dalam plot 2-D. Jenis semantik angka dan nilai data yang diekstrak dari plot 2-D dapat disatukan dengan maklumat teks dalam dokumen untuk menyediakan perkhidmatan pengambilan dokumen yang lebih berkesan untuk pengguna perpustakaan digital. Penilaian eksperimental telah menunjukkan bahawa sistem kami dapat menghasilkan hasil yang sesuai untuk penggunaan di dunia nyata. [[EENNDD]] carian dan pengambilan maklumat; angka; pengekstrakan ciri; pembelajaran mesin"], [{"string": "Track globally, deliver locally: improving content delivery networks by tracking geographic social cascades Providers such as YouTube offer easy access to multimedia content to millions, generating high bandwidth and storage demand on the Content Delivery Networks they rely upon. More and more, the diffusion of this content happens on online social networks such as Facebook and Twitter, where social cascades can be observed when users increasingly repost links they have received from others.", "keywords": ["information propagation", "online social networks", "content delivery networks", "on-line information services"], "combined": "Track globally, deliver locally: improving content delivery networks by tracking geographic social cascades Providers such as YouTube offer easy access to multimedia content to millions, generating high bandwidth and storage demand on the Content Delivery Networks they rely upon. More and more, the diffusion of this content happens on online social networks such as Facebook and Twitter, where social cascades can be observed when users increasingly repost links they have received from others. [[EENNDD]] information propagation; online social networks; content delivery networks; on-line information services"}, "Jejak secara global, hantar secara tempatan: meningkatkan rangkaian penyampaian kandungan dengan mengesan lata sosial geografi Penyedia seperti YouTube menawarkan akses mudah ke kandungan multimedia kepada berjuta-juta orang, menghasilkan lebar jalur yang tinggi dan permintaan penyimpanan pada Rangkaian Penghantaran Kandungan yang mereka andalkan. Semakin banyak, penyebaran kandungan ini berlaku di rangkaian sosial dalam talian seperti Facebook dan Twitter, di mana lata sosial dapat diperhatikan apabila pengguna semakin menyiarkan semula pautan yang mereka terima dari orang lain. [[EENNDD]] penyebaran maklumat; rangkaian sosial dalam talian; rangkaian penghantaran kandungan; perkhidmatan maklumat dalam talian"], [{"string": "Visually guided bottom-up table detection and segmentation in web documents No contact information provided yet.", "keywords": ["document capture", "systems and software", "web information extraction", "table detection"], "combined": "Visually guided bottom-up table detection and segmentation in web documents No contact information provided yet. [[EENNDD]] document capture; systems and software; web information extraction; table detection"}, "Pengesanan dan pemisahan jadual bawah-atas yang dipandu secara visual dalam dokumen web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] tangkapan dokumen; sistem dan perisian; pengekstrakan maklumat web; pengesanan jadual"], [{"string": "Spatio-temporal models for estimating click-through rate We propose novel spatio-temporal models to estimate click-through rates in the context of content recommendation. We track article CTR at a fixed location over time through a dynamic Gamma-Poisson model and combine information from correlated locations through dynamic linear regressions, significantly improving on per-location model. Our models adjust for user fatigue through an exponential tilt to the first-view CTR (probability of click on first article exposure) that is based only on user-specific repeat-exposure features. We illustrate our approach on data obtained from a module (Today Module) published regularly on Yahoo! Front Page and demonstrate significant improvement over commonly used baseline methods. Large scale simulation experiments to study the performance of our models under different scenarios provide encouraging results. Throughout, all modeling assumptions are validated via rigorous exploratory data analysis.", "keywords": ["ctr positional correlation", "content recommendation", "on-line information services"], "combined": "Spatio-temporal models for estimating click-through rate We propose novel spatio-temporal models to estimate click-through rates in the context of content recommendation. We track article CTR at a fixed location over time through a dynamic Gamma-Poisson model and combine information from correlated locations through dynamic linear regressions, significantly improving on per-location model. Our models adjust for user fatigue through an exponential tilt to the first-view CTR (probability of click on first article exposure) that is based only on user-specific repeat-exposure features. We illustrate our approach on data obtained from a module (Today Module) published regularly on Yahoo! Front Page and demonstrate significant improvement over commonly used baseline methods. Large scale simulation experiments to study the performance of our models under different scenarios provide encouraging results. Throughout, all modeling assumptions are validated via rigorous exploratory data analysis. [[EENNDD]] ctr positional correlation; content recommendation; on-line information services"}, "Model spatio-temporal untuk menganggarkan kadar klik-tayang Kami mencadangkan model spatio-temporal baru untuk menganggarkan kadar klik-tayang dalam konteks cadangan kandungan. Kami melacak RKT artikel di lokasi tetap dari masa ke masa melalui model Gamma-Poisson yang dinamis dan menggabungkan maklumat dari lokasi berkorelasi melalui regresi linier dinamik, dengan ketara meningkatkan model per lokasi. Model kami menyesuaikan diri dengan keletihan pengguna melalui kecenderungan eksponensial ke CTR pandangan pertama (kebarangkalian klik pada pendedahan artikel pertama) yang hanya berdasarkan pada ciri pendedahan berulang khusus pengguna. Kami menggambarkan pendekatan kami terhadap data yang diperoleh dari modul (Modul Hari Ini) yang diterbitkan secara berkala di Yahoo! Halaman Depan dan menunjukkan peningkatan yang ketara berbanding kaedah asas yang biasa digunakan. Eksperimen simulasi berskala besar untuk mengkaji prestasi model kami dalam senario yang berbeza memberikan hasil yang memberangsangkan. Sepanjang, semua andaian pemodelan disahkan melalui analisis data penerokaan yang ketat. [[EENNDD]] korelasi kedudukan ctr; cadangan kandungan; perkhidmatan maklumat dalam talian"], [{"string": "Towards domain-independent information extraction from web tables Traditionally, information extraction from web tables has focused on small, more or less homogeneous corpora, often based on assumptions about the use of", "keywords": ["web tables", "information extraction", "information search and retrieval", "visual analysis", "web page representation", "web mining"], "combined": "Towards domain-independent information extraction from web tables Traditionally, information extraction from web tables has focused on small, more or less homogeneous corpora, often based on assumptions about the use of [[EENNDD]] web tables; information extraction; information search and retrieval; visual analysis; web page representation; web mining"}, "Ke arah pengekstrakan maklumat yang tidak bergantung pada domain dari jadual web Secara tradisinya, pengekstrakan maklumat dari jadual web telah memberi tumpuan kepada korporat kecil, lebih atau kurang homogen, sering berdasarkan andaian mengenai penggunaan jadual web [[EENNDD]]; pengekstrakan maklumat; pencarian dan pengambilan maklumat; analisis visual; perwakilan laman web; perlombongan web"], [{"string": "Efficient pagerank approximation via graph aggregation No contact information provided yet.", "keywords": ["search engines", "information search and retrieval", "web information retrieval", "link analysis"], "combined": "Efficient pagerank approximation via graph aggregation No contact information provided yet. [[EENNDD]] search engines; information search and retrieval; web information retrieval; link analysis"}, "Penghampiran pagerank yang cekap melalui pengagregatan grafik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] enjin carian; pencarian dan pengambilan maklumat; pengambilan maklumat web; analisis pautan"], [{"string": "Scalable techniques for memory-efficient CDN simulations No contact information provided yet.", "keywords": ["content distribution networks", "approximate data structures", "simulation", "web proxy cache"], "combined": "Scalable techniques for memory-efficient CDN simulations No contact information provided yet. [[EENNDD]] content distribution networks; approximate data structures; simulation; web proxy cache"}, "Teknik berskala untuk simulasi CDN yang cekap memori Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] rangkaian pengedaran kandungan; struktur data anggaran; simulasi; cache proksi web"], [{"string": "To randomize or not to randomize: space optimal summaries for hyperlink analysis No contact information provided yet.", "keywords": ["link-analysis", "data streams", "probabilistic algorithms", "information search and retrieval", "scalability", "similarity search"], "combined": "To randomize or not to randomize: space optimal summaries for hyperlink analysis No contact information provided yet. [[EENNDD]] link-analysis; data streams; probabilistic algorithms; information search and retrieval; scalability; similarity search"}, "Untuk membuat rawak atau tidak secara rawak: ringkasan ruang yang optimum untuk analisis hiperpautan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] analisis pautan; aliran data; algoritma probabilistik; pencarian dan pengambilan maklumat; skalabiliti; carian kesamaan"], [{"string": "Semantic similarity between search engine queries using temporal correlation No contact information provided yet.", "keywords": ["search engines", "query stream analysis", "information search and retrieval", "semantic similarity among queries"], "combined": "Semantic similarity between search engine queries using temporal correlation No contact information provided yet. [[EENNDD]] search engines; query stream analysis; information search and retrieval; semantic similarity among queries"}, "Kesamaan semantik antara pertanyaan enjin carian menggunakan korelasi temporal Belum ada maklumat hubungan yang diberikan. [[EENNDD]] enjin carian; analisis aliran pertanyaan; pencarian dan pengambilan maklumat; persamaan semantik antara pertanyaan"], [{"string": "Index structures and algorithms for querying distributed RDF repositories No contact information provided yet.", "keywords": ["optimization", "rdf querying", "index structures"], "combined": "Index structures and algorithms for querying distributed RDF repositories No contact information provided yet. [[EENNDD]] optimization; rdf querying; index structures"}, "Struktur indeks dan algoritma untuk meminta repositori RDF yang diedarkan Tidak ada maklumat hubungan yang disediakan. [[EENNDD]] pengoptimuman; pertanyaan rdf; struktur indeks"], [{"string": "From user-centric web traffic data to usage data No contact information provided yet.", "keywords": ["systems and software", "user-centric traffic data", "usage data", "internet uses", "web usage mining", "traffic analysis"], "combined": "From user-centric web traffic data to usage data No contact information provided yet. [[EENNDD]] systems and software; user-centric traffic data; usage data; internet uses; web usage mining; traffic analysis"}, "Dari data trafik web yang berpusatkan pengguna hingga data penggunaan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] sistem dan perisian; data lalu lintas yang berpusatkan pengguna; data penggunaan; penggunaan internet; perlombongan penggunaan web; analisis lalu lintas"], [{"string": "Building a companion website in the semantic web No contact information provided yet.", "keywords": ["textbook", "semantic web", "bloom's taxonomy", "electronic publishing", "companion website"], "combined": "Building a companion website in the semantic web No contact information provided yet. [[EENNDD]] textbook; semantic web; bloom's taxonomy; electronic publishing; companion website"}, "Membina laman web pendamping dalam web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] buku teks; web semantik; taksonomi mekar; penerbitan elektronik; laman web pendamping"], [{"string": "Summarizing email conversations with clue words Accessing an ever increasing number of emails, possibly on small mobile devices, has become a major problem for many users. Email summarization is a promising way to solve this problem. In this paper, we propose a new framework for email summarization. One novelty is to use a fragment quotation graph to try to capture an email conversation. The second novelty is to use clue words to measure the importance of sentences in conversation summarization. Based on clue words and their scores, we propose a method called CWS, which is capable of producing a summary of any length as requested by the user. We provide a comprehensive comparison of CWS with various existing methods on the Enron data set. Preliminary results suggest that CWS provides better summaries than existing methods.", "keywords": ["email summarization", "text mining"], "combined": "Summarizing email conversations with clue words Accessing an ever increasing number of emails, possibly on small mobile devices, has become a major problem for many users. Email summarization is a promising way to solve this problem. In this paper, we propose a new framework for email summarization. One novelty is to use a fragment quotation graph to try to capture an email conversation. The second novelty is to use clue words to measure the importance of sentences in conversation summarization. Based on clue words and their scores, we propose a method called CWS, which is capable of producing a summary of any length as requested by the user. We provide a comprehensive comparison of CWS with various existing methods on the Enron data set. Preliminary results suggest that CWS provides better summaries than existing methods. [[EENNDD]] email summarization; text mining"}, "Meringkaskan perbualan e-mel dengan kata-kata petunjuk Mengakses bilangan e-mel yang semakin meningkat, mungkin pada peranti mudah alih kecil, telah menjadi masalah besar bagi banyak pengguna. Ringkasan e-mel adalah cara yang menjanjikan untuk menyelesaikan masalah ini. Dalam makalah ini, kami mencadangkan kerangka baru untuk ringkasan e-mel. Satu kebaharuan adalah menggunakan grafik petikan fragmen untuk cuba menangkap perbualan e-mel. Kebaruan kedua adalah menggunakan kata petunjuk untuk mengukur kepentingan ayat dalam ringkasan perbualan. Berdasarkan kata petunjuk dan skornya, kami mencadangkan kaedah yang disebut CWS, yang mampu menghasilkan ringkasan panjang seperti yang diminta oleh pengguna. Kami memberikan perbandingan komprehensif CWS dengan pelbagai kaedah yang ada pada set data Enron. Hasil awal menunjukkan bahawa CWS memberikan ringkasan yang lebih baik daripada kaedah yang ada. [[EENNDD]] ringkasan e-mel; perlombongan teks"], [{"string": "A high-performance interpretive approach to schema-directed parsing XML delivers key advantages in interoperability due to its flexibility, expressiveness, and platform-neutrality. As XML has become a performance-critical aspect of the next generation of business computing infrastructure, however, it has become increasingly clear that XML parsing often carries a heavy performance penalty, and that current, widely-used parsing technologies are unable to meet the performance demands of an XML-based computing infrastructure. Several efforts have been made to address this performance gap through the use of grammar-based parser generation. While the performance of generated parsers has been significantly improved, adoption of the technology has been hindered by the complexity of compiling and deploying the generated parsers. Through careful analysis of the operations required for parsing and validation, we have devised a set of specialized byte codes, designed for the task of XML parsing and validation. These byte codes are designed to engender the benefits of fine-grained composition of parsing and validation that make existing compiled parsers fast, while being coarse-grained enough to minimize interpreter overhead. This technique of using an interpretive,validating parser balances the need for performance against the requirements of simple tooling and robust scalable infrastructure. Our approach is demonstrated with a specialized schema compiler, used to generate byte codes which in turn drive an interpretive parser. With almost as little tooling and deployment complexity as a traditional interpretive parser, the byte code-driven parser usually demonstrates performance within 20% of the fastest fully compiled solutions.", "keywords": ["schema", "xml", "compiler", "interpreter"], "combined": "A high-performance interpretive approach to schema-directed parsing XML delivers key advantages in interoperability due to its flexibility, expressiveness, and platform-neutrality. As XML has become a performance-critical aspect of the next generation of business computing infrastructure, however, it has become increasingly clear that XML parsing often carries a heavy performance penalty, and that current, widely-used parsing technologies are unable to meet the performance demands of an XML-based computing infrastructure. Several efforts have been made to address this performance gap through the use of grammar-based parser generation. While the performance of generated parsers has been significantly improved, adoption of the technology has been hindered by the complexity of compiling and deploying the generated parsers. Through careful analysis of the operations required for parsing and validation, we have devised a set of specialized byte codes, designed for the task of XML parsing and validation. These byte codes are designed to engender the benefits of fine-grained composition of parsing and validation that make existing compiled parsers fast, while being coarse-grained enough to minimize interpreter overhead. This technique of using an interpretive,validating parser balances the need for performance against the requirements of simple tooling and robust scalable infrastructure. Our approach is demonstrated with a specialized schema compiler, used to generate byte codes which in turn drive an interpretive parser. With almost as little tooling and deployment complexity as a traditional interpretive parser, the byte code-driven parser usually demonstrates performance within 20% of the fastest fully compiled solutions. [[EENNDD]] schema; xml; compiler; interpreter"}, "Pendekatan interpretif berprestasi tinggi terhadap penghuraian XML yang diarahkan secara skema memberikan kelebihan utama dalam operasi kerana fleksibiliti, ekspresif, dan berkecuali platform. Oleh kerana XML telah menjadi aspek kritikal terhadap prestasi infrastruktur pengkomputeran perniagaan generasi berikutnya, namun, semakin jelas bahawa penghuraian XML sering membawa hukuman prestasi yang berat, dan teknologi penghuraian yang digunakan sekarang tidak dapat memenuhi prestasi tuntutan infrastruktur pengkomputeran berasaskan XML. Beberapa usaha telah dilakukan untuk mengatasi jurang prestasi ini melalui penggunaan generasi penghurai berdasarkan tatabahasa. Walaupun prestasi penghurai yang dihasilkan telah ditingkatkan secara signifikan, penerapan teknologi telah terhalang oleh kerumitan penyusunan dan penggunaan parser yang dihasilkan. Melalui analisis yang teliti terhadap operasi yang diperlukan untuk menghurai dan mengesahkan, kami telah membuat satu set kod byte khusus, yang dirancang untuk tugas penguraian dan pengesahan XML. Kod bait ini direka untuk memberi faedah komposisi penghuraian halus dan pengesahan yang menjadikan penghurai kompilasi yang ada cepat, sementara cukup kasar untuk meminimumkan overpreter. Teknik menggunakan parser yang menafsirkan dan mengesahkan ini menyeimbangkan keperluan prestasi berbanding keperluan perkakas sederhana dan infrastruktur berskala yang kuat. Pendekatan kami ditunjukkan dengan penyusun skema khusus, digunakan untuk menghasilkan kod bait yang seterusnya mendorong penghurai tafsiran. Dengan kerumitan perkakas dan penyebaran yang hampir sama dengan penghurai tafsiran tradisional, penghurai berdasarkan kod byte biasanya menunjukkan prestasi dalam 20% daripada penyelesaian yang disusun sepenuhnya paling cepat. [[EENNDD]] skema; xml; penyusun; jurubahasa"], [{"string": "Identifying enrichment candidates in textbooks Many textbooks written in emerging countries lack clear and adequate coverage of important concepts. We propose a technological solution for algorithmically identifying those sections of a book that are not well written and could benefit from better exposition. We provide a decision model based on the syntactic complexity of writing and the dispersion of key concepts. The model parameters are learned using a tune set which is algorithmically generated using a versioned authoritative web resource as a proxy. We evaluate the proposed methodology over a corpus of Indian textbooks which demonstrates its effectiveness in identifying enrichment candidates.", "keywords": ["information search and retrieval", "readability", "dispersion", "education", "textbooks", "concepts"], "combined": "Identifying enrichment candidates in textbooks Many textbooks written in emerging countries lack clear and adequate coverage of important concepts. We propose a technological solution for algorithmically identifying those sections of a book that are not well written and could benefit from better exposition. We provide a decision model based on the syntactic complexity of writing and the dispersion of key concepts. The model parameters are learned using a tune set which is algorithmically generated using a versioned authoritative web resource as a proxy. We evaluate the proposed methodology over a corpus of Indian textbooks which demonstrates its effectiveness in identifying enrichment candidates. [[EENNDD]] information search and retrieval; readability; dispersion; education; textbooks; concepts"}, "Mengenal pasti calon pengayaan dalam buku teks Banyak buku teks yang ditulis di negara-negara membangun tidak mempunyai liputan konsep yang jelas dan mencukupi. Kami mencadangkan penyelesaian teknologi untuk mengenal pasti algoritma bahagian-bahagian buku yang tidak ditulis dengan baik dan boleh mendapat manfaat daripada paparan yang lebih baik. Kami menyediakan model keputusan berdasarkan kerumitan sintaksis penulisan dan penyebaran konsep utama. Parameter model dipelajari menggunakan set lagu yang dihasilkan secara algoritma menggunakan sumber web berwibawa versi sebagai proksi. Kami menilai metodologi yang dicadangkan melalui sejumlah buku teks India yang menunjukkan keberkesanannya dalam mengenal pasti calon pengayaan. [[EENNDD]] carian dan pengambilan maklumat; kebolehbacaan; penyebaran; pendidikan; buku teks; konsep"], [{"string": "Collective context-aware topic models for entity disambiguation A crucial step in adding structure to unstructured data is to identify references to entities and disambiguate them. Such disambiguated references can help enhance readability and draw similarities across different pieces of running text in an automated fashion. Previous research has tackled this problem by first forming a catalog of entities from a knowledge base, such as Wikipedia, and then using this catalog to disambiguate references in unseen text. However, most of the previously proposed models either do not use all text in the knowledge base, potentially missing out on discriminative features, or do not exploit word-entity proximity to learn high-quality catalogs. In this work, we propose topic models that keep track of the context of every word in the knowledge base; so that words appearing within the same context as an entity are more likely to be associated with that entity. Thus, our topic models utilize all text present in the knowledge base and help learn high-quality catalogs. Our models also learn groups of co-occurring entities thus enabling collective disambiguation. Unlike most previous topic models, our models are non-parametric and do not require the user to specify the exact number of groups present in the knowledge base. In experiments performed on an extract of Wikipedia containing almost 60,000 references, our models outperform SVM-based baselines by as much as 18% in terms of disambiguation accuracy translating to an increment of almost 11,000 correctly disambiguated references.", "keywords": ["miscellaneous", "topic models", "entity disambiguation"], "combined": "Collective context-aware topic models for entity disambiguation A crucial step in adding structure to unstructured data is to identify references to entities and disambiguate them. Such disambiguated references can help enhance readability and draw similarities across different pieces of running text in an automated fashion. Previous research has tackled this problem by first forming a catalog of entities from a knowledge base, such as Wikipedia, and then using this catalog to disambiguate references in unseen text. However, most of the previously proposed models either do not use all text in the knowledge base, potentially missing out on discriminative features, or do not exploit word-entity proximity to learn high-quality catalogs. In this work, we propose topic models that keep track of the context of every word in the knowledge base; so that words appearing within the same context as an entity are more likely to be associated with that entity. Thus, our topic models utilize all text present in the knowledge base and help learn high-quality catalogs. Our models also learn groups of co-occurring entities thus enabling collective disambiguation. Unlike most previous topic models, our models are non-parametric and do not require the user to specify the exact number of groups present in the knowledge base. In experiments performed on an extract of Wikipedia containing almost 60,000 references, our models outperform SVM-based baselines by as much as 18% in terms of disambiguation accuracy translating to an increment of almost 11,000 correctly disambiguated references. [[EENNDD]] miscellaneous; topic models; entity disambiguation"}, "Model topik sedar konteks kolektif untuk disambiguasi entiti Langkah penting dalam menambahkan struktur ke data tidak berstruktur adalah dengan mengenal pasti rujukan kepada entiti dan menghilangkannya. Rujukan yang tidak jelas ini dapat membantu meningkatkan kebolehbacaan dan menarik kesamaan pada pelbagai teks yang dikendalikan secara automatik. Penyelidikan sebelumnya telah mengatasi masalah ini dengan terlebih dahulu membentuk katalog entiti dari pangkalan pengetahuan, seperti Wikipedia, dan kemudian menggunakan katalog ini untuk menghilangkan rujukan dalam teks yang tidak kelihatan. Walau bagaimanapun, kebanyakan model yang dicadangkan sebelumnya sama ada tidak menggunakan semua teks dalam pangkalan pengetahuan, berpotensi kehilangan ciri diskriminatif, atau tidak memanfaatkan kedekatan kata-kata untuk belajar katalog berkualiti tinggi. Dalam karya ini, kami mencadangkan model topik yang mengikuti konteks setiap perkataan dalam pangkalan pengetahuan; sehingga kata-kata yang muncul dalam konteks yang sama dengan entiti lebih cenderung dikaitkan dengan entiti itu. Oleh itu, model topik kami menggunakan semua teks yang terdapat di pangkalan pengetahuan dan membantu mempelajari katalog berkualiti tinggi. Model kami juga mempelajari kumpulan entiti bersama sehingga memungkinkan penyingkiran kolektif. Tidak seperti kebanyakan model topik sebelumnya, model kami bukan parametrik dan tidak memerlukan pengguna untuk menentukan bilangan kumpulan yang tepat yang terdapat di pangkalan pengetahuan. Dalam eksperimen yang dilakukan pada ekstrak Wikipedia yang mengandungi hampir 60,000 rujukan, model kami mengungguli garis dasar berdasarkan SVM sebanyak 18% dari segi ketepatan disambiguasi yang diterjemahkan menjadi kenaikan hampir 11,000 rujukan yang tidak jelas. [[EENNDD]] pelbagai; model topik; disambiguasi entiti"], [{"string": "Semi-automated adaptation of service interactions In today's Web, many functionality-wise similar Web services are offered through heterogeneous interfaces (operation definitions) and business protocols (ordering constraints defined on legal operation invocation sequences). The typical approach to enable interoperation in such a heterogeneous setting is through developing adapters. There have been approaches for classifying possible mismatches between service interfaces and business protocols to facilitate adapter development. However, the hard job is that of identifying, given two service specifications, the actual mismatches between their interfaces and business protocols.", "keywords": ["service adaptation", "service interface matching", "service protocol adaptation", "web services"], "combined": "Semi-automated adaptation of service interactions In today's Web, many functionality-wise similar Web services are offered through heterogeneous interfaces (operation definitions) and business protocols (ordering constraints defined on legal operation invocation sequences). The typical approach to enable interoperation in such a heterogeneous setting is through developing adapters. There have been approaches for classifying possible mismatches between service interfaces and business protocols to facilitate adapter development. However, the hard job is that of identifying, given two service specifications, the actual mismatches between their interfaces and business protocols. [[EENNDD]] service adaptation; service interface matching; service protocol adaptation; web services"}, "Penyesuaian interaktif perkhidmatan separa automatik Dalam Web hari ini, banyak perkhidmatan Web yang serupa dengan fungsi yang ditawarkan melalui antara muka heterogen (definisi operasi) dan protokol perniagaan (batasan pesanan yang ditentukan pada urutan permintaan operasi undang-undang). Pendekatan khas untuk membolehkan interoperasi dalam keadaan yang heterogen adalah melalui pengembangan adaptor. Terdapat pendekatan untuk mengklasifikasikan kemungkinan ketidakcocokan antara antara muka perkhidmatan dan protokol perniagaan untuk memudahkan pengembangan penyesuai. Namun, pekerjaan yang sukar adalah mengenal pasti, diberikan dua spesifikasi perkhidmatan, ketidakcocokan sebenar antara antara muka dan protokol perniagaan mereka. [[EENNDD]] penyesuaian perkhidmatan; padanan antara muka perkhidmatan; penyesuaian protokol perkhidmatan; perkhidmatan web"], [{"string": "Model based engineering of learning situations for adaptive web based educational systems No contact information provided yet.", "keywords": ["models and metamodels", "uml language", "design tools and techniques", "specification of educational applications", "requirements/specifications", "user/machine systems", "architectures and designs for web-based learning delivery environments"], "combined": "Model based engineering of learning situations for adaptive web based educational systems No contact information provided yet. [[EENNDD]] models and metamodels; uml language; design tools and techniques; specification of educational applications; requirements/specifications; user/machine systems; architectures and designs for web-based learning delivery environments"}, "Kejuruteraan berdasarkan model situasi pembelajaran untuk sistem pendidikan berasaskan web adaptif Belum ada maklumat hubungan yang diberikan. [[EENNDD]] model dan model; bahasa uml; alat dan teknik reka bentuk; spesifikasi aplikasi pendidikan; keperluan / spesifikasi; sistem pengguna / mesin; seni bina dan reka bentuk untuk persekitaran penyampaian pembelajaran berasaskan web"], [{"string": "Graffiti: node labeling in heterogeneous networks We introduce a multi-label classification model and algorithm for labeling heterogeneous networks, where nodes belong to different types and different types have different sets of classification labels. We present a graph-based approach which models the mutual influence between nodes in the network as a random walk. When viewing class labels as \"colors\", the random surfer is \"spraying\" different node types with different color palettes; hence the name Graffiti. We demonstrate the performance gains of our method by comparing it to three state-of-the-art techniques for graph-based classification.", "keywords": ["web 2.0 ir", "graph classification"], "combined": "Graffiti: node labeling in heterogeneous networks We introduce a multi-label classification model and algorithm for labeling heterogeneous networks, where nodes belong to different types and different types have different sets of classification labels. We present a graph-based approach which models the mutual influence between nodes in the network as a random walk. When viewing class labels as \"colors\", the random surfer is \"spraying\" different node types with different color palettes; hence the name Graffiti. We demonstrate the performance gains of our method by comparing it to three state-of-the-art techniques for graph-based classification. [[EENNDD]] web 2.0 ir; graph classification"}, "Graffiti: pelabelan nod dalam rangkaian heterogen Kami memperkenalkan model dan algoritma klasifikasi pelbagai label untuk pelabelan rangkaian heterogen, di mana node tergolong dalam pelbagai jenis dan pelbagai jenis mempunyai set label klasifikasi yang berbeza. Kami menyajikan pendekatan berdasarkan grafik yang memodelkan pengaruh bersama antara nod dalam rangkaian sebagai jalan rawak. Apabila melihat label kelas sebagai \"warna\", surfer rawak adalah \"menyemburkan\" jenis nod yang berlainan dengan palet warna yang berbeza; oleh itu nama Graffiti. Kami menunjukkan peningkatan prestasi kaedah kami dengan membandingkannya dengan tiga teknik canggih untuk klasifikasi berdasarkan grafik. [[EENNDD]] web 2.0 ir; pengelasan grafik"], [{"string": "Measurement-calibrated graph models for social network experiments Access to realistic, complex graph datasets is critical to research on social networking systems and applications. Simulations on graph data provide critical evaluation of new systems and applications ranging from community detection to spam filtering and social web search. Due to the high time and resource costs of gathering real graph datasets through direct measurements, researchers are anonymizing and sharing a small number of valuable datasets with the community. However, performing experiments using shared real datasets faces three key disadvantages: concerns that graphs can be de-anonymized to reveal private information, increasing costs of distributing large datasets, and that a small number of available social graphs limits the statistical confidence in the results.", "keywords": ["social networking", "model validation and analysis", "graph models"], "combined": "Measurement-calibrated graph models for social network experiments Access to realistic, complex graph datasets is critical to research on social networking systems and applications. Simulations on graph data provide critical evaluation of new systems and applications ranging from community detection to spam filtering and social web search. Due to the high time and resource costs of gathering real graph datasets through direct measurements, researchers are anonymizing and sharing a small number of valuable datasets with the community. However, performing experiments using shared real datasets faces three key disadvantages: concerns that graphs can be de-anonymized to reveal private information, increasing costs of distributing large datasets, and that a small number of available social graphs limits the statistical confidence in the results. [[EENNDD]] social networking; model validation and analysis; graph models"}, "Model grafik pengukuran yang dikalibrasi untuk eksperimen rangkaian sosial Akses ke set data grafik yang realistik dan kompleks sangat penting untuk meneliti sistem dan aplikasi rangkaian sosial. Simulasi pada data grafik memberikan penilaian kritikal terhadap sistem dan aplikasi baru, mulai dari pengesanan komuniti hingga penapisan spam dan carian web sosial. Oleh kerana kos masa dan sumber daya yang tinggi untuk mengumpulkan set data grafik sebenar melalui pengukuran langsung, penyelidik membuat anonim dan berkongsi sebilangan kecil set data berharga dengan masyarakat. Walau bagaimanapun, melakukan eksperimen menggunakan set data nyata yang dikongsi menghadapi tiga kelemahan utama: kebimbangan bahawa grafik dapat dinyahnonimkan untuk mendedahkan maklumat peribadi, meningkatkan kos menyebarkan set data yang besar, dan sebilangan kecil grafik sosial yang ada membatasi keyakinan statistik terhadap hasilnya. [[EENNDD]] rangkaian sosial; pengesahan dan analisis model; model grafik"], [{"string": "P-TAG: large scale automatic generation of personalized annotation tags for the web The success of the Semantic Web depends on the availability of Web pages annotated with metadata. Free form metadata or tags, as used in social bookmarking and folksonomies, have become more and more popular and successful. Such tags are relevant keywords associated with or assigned to a piece of information (e.g., a Web page), describing the item and enabling keyword-based classification. In this paper we propose P-TAG, a method which automatically generates personalized tags for Web pages. Upon browsing a Web page, P-TAG produces keywords relevant both to its textual content, but also to the data residing on the surfer's Desktop, thus expressing a personalized viewpoint. Empirical evaluations with several algorithms pursuing this approach showed very promising results. We are therefore very confident that such a user oriented automatic tagging approach can provide large scale personalized metadata annotations as an important step towards realizing the Semantic Web.", "keywords": ["personalization", "content analysis and indexing", "on-line information services", "user desktop", "web annotations", "tagging"], "combined": "P-TAG: large scale automatic generation of personalized annotation tags for the web The success of the Semantic Web depends on the availability of Web pages annotated with metadata. Free form metadata or tags, as used in social bookmarking and folksonomies, have become more and more popular and successful. Such tags are relevant keywords associated with or assigned to a piece of information (e.g., a Web page), describing the item and enabling keyword-based classification. In this paper we propose P-TAG, a method which automatically generates personalized tags for Web pages. Upon browsing a Web page, P-TAG produces keywords relevant both to its textual content, but also to the data residing on the surfer's Desktop, thus expressing a personalized viewpoint. Empirical evaluations with several algorithms pursuing this approach showed very promising results. We are therefore very confident that such a user oriented automatic tagging approach can provide large scale personalized metadata annotations as an important step towards realizing the Semantic Web. [[EENNDD]] personalization; content analysis and indexing; on-line information services; user desktop; web annotations; tagging"}, "P-TAG: penjanaan tag anotasi peribadi automatik berskala besar untuk web Kejayaan Web Semantik bergantung pada ketersediaan halaman Web yang diberi anotasi dengan metadata. Metadata atau tag bentuk percuma, seperti yang digunakan dalam penanda buku sosial dan folksonomies, telah menjadi semakin popular dan berjaya. Teg seperti itu adalah kata kunci yang relevan yang berkaitan dengan atau diberikan pada sekeping maklumat (mis., Halaman Web), menerangkan item tersebut dan memungkinkan pengkelasan berdasarkan kata kunci. Dalam makalah ini kami mencadangkan P-TAG, kaedah yang secara automatik menghasilkan tag yang diperibadikan untuk halaman Web. Semasa melayari laman web, P-TAG menghasilkan kata kunci yang relevan dengan kandungan teksnya, tetapi juga dengan data yang berada di Desktop surfer, sehingga menyatakan sudut pandang yang dipersonalisasi. Penilaian empirikal dengan beberapa algoritma yang mengikuti pendekatan ini menunjukkan hasil yang sangat menjanjikan. Oleh itu, kami sangat yakin bahawa pendekatan penandaan automatik yang berorientasikan pengguna seperti ini dapat memberikan anotasi metadata yang diperibadikan secara besar-besaran sebagai langkah penting untuk merealisasikan Semantik Web. [[EENNDD]] pemperibadian; analisis kandungan dan pengindeksan; perkhidmatan maklumat dalam talian; desktop pengguna; anotasi web; penandaan"], [{"string": "The discoverability of the web Previous studies have highlighted the high arrival rate of new contenton the web. We study the extent to which this new content can beefficiently discovered by a crawler. Our study has two parts. First,we study the inherent difficulty of the discovery problem using amaximum cover formulation, under an assumption of perfect estimates oflikely sources of links to new content. Second, we relax thisassumption and study a more realistic setting in which algorithms mustuse historical statistics to estimate which pages are most likely toyield links to new content. We recommend a simple algorithm thatperforms comparably to all approaches we consider.We measure the emphoverhead of discovering new content, defined asthe average number of fetches required to discover one new page. Weshow first that with perfect foreknowledge of where to explore forlinks to new content, it is possible to discover 90% of all newcontent with under 3% overhead, and 100% of new content with 9%overhead. But actual algorithms, which do not have access to perfectforeknowledge, face a more difficult task: one quarter of new contentis simply not amenable to efficient discovery. Of the remaining threequarters, 80% of new content during a given week may be discoveredwith 160% overhead if content is recrawled fully on a monthly basis.", "keywords": ["greedy", "miscellaneous", "set cover", "discovery", "max cover", "crawling"], "combined": "The discoverability of the web Previous studies have highlighted the high arrival rate of new contenton the web. We study the extent to which this new content can beefficiently discovered by a crawler. Our study has two parts. First,we study the inherent difficulty of the discovery problem using amaximum cover formulation, under an assumption of perfect estimates oflikely sources of links to new content. Second, we relax thisassumption and study a more realistic setting in which algorithms mustuse historical statistics to estimate which pages are most likely toyield links to new content. We recommend a simple algorithm thatperforms comparably to all approaches we consider.We measure the emphoverhead of discovering new content, defined asthe average number of fetches required to discover one new page. Weshow first that with perfect foreknowledge of where to explore forlinks to new content, it is possible to discover 90% of all newcontent with under 3% overhead, and 100% of new content with 9%overhead. But actual algorithms, which do not have access to perfectforeknowledge, face a more difficult task: one quarter of new contentis simply not amenable to efficient discovery. Of the remaining threequarters, 80% of new content during a given week may be discoveredwith 160% overhead if content is recrawled fully on a monthly basis. [[EENNDD]] greedy; miscellaneous; set cover; discovery; max cover; crawling"}, "Kebolehtemuan web Kajian terdahulu telah menunjukkan tahap kedatangan kandungan baru yang tinggi di web. Kami mengkaji sejauh mana kandungan baru ini dapat dijumpai oleh perayap. Kajian kami mempunyai dua bahagian. Pertama, kami mengkaji kesukaran yang wujud dari masalah penemuan dengan menggunakan rumusan penutup yang luar biasa, dengan anggapan anggaran sempurna mengenai sumber pautan ke kandungan baru. Kedua, kita melonggarkan andaian ini dan mengkaji tetapan yang lebih realistik di mana algoritma mesti menggunakan statistik sejarah untuk menganggarkan halaman mana yang kemungkinan besar menjadi pautan mainan ke kandungan baru. Kami mengesyorkan algoritma mudah yang berfungsi setanding dengan semua pendekatan yang kami pertimbangkan. Kami mengukur kemajuan mencari kandungan baru, yang ditakrifkan sebagai jumlah pengambilan yang diperlukan untuk menemui satu halaman baru. Weshow terlebih dahulu bahawa dengan pengetahuan awal mengenai tempat untuk menjelajahi tautan ke kandungan baru, adalah mungkin untuk menemui 90% semua kandungan baru dengan overhead di bawah 3%, dan 100% kandungan baru dengan overhead 9%. Tetapi algoritma sebenar, yang tidak mempunyai akses untuk mengetahui dengan sempurna, menghadapi tugas yang lebih sukar: satu perempat kandungan baru tidak dapat diterima dengan berkesan. Dari tiga markas selebihnya, 80% kandungan baru pada minggu tertentu dapat ditemui dengan 160% overhead jika kandungan dikumpulkan semula sepenuhnya setiap bulan. [[EENNDD]] tamak; pelbagai; set penutup; penemuan; penutup maksimum; merangkak"], [{"string": "The WT10G dataset and the evolution of the web No contact information provided yet.", "keywords": ["standard datasets", "hypertext/hypermedia", "miscellaneous", "rate of change", "web evolution"], "combined": "The WT10G dataset and the evolution of the web No contact information provided yet. [[EENNDD]] standard datasets; hypertext/hypermedia; miscellaneous; rate of change; web evolution"}, "Set data WT10G dan evolusi web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] set data standard; hiperteks / hipermedia; pelbagai; kadar perubahan; evolusi web"], [{"string": "Dynamic and graphical web page breakpoints Breakpoints are perhaps the quintessential feature of a de-bugger: they allow a developer to stop time and study the program state. Breakpoints are typically specified by selecting a line of source code. For large, complex, web pages with multiple developers, the relevant source line for a given user interface problem may not be known to the developer. In this paper we describe the implementation of breakpoints in dynamically created source, and on error messages, network events, DOMmutation, DOMobject property changes, and CSS style rule updates. Adding these domain-specific breakpoints to a general-purpose debugger for Javascript allows the developer to initiate the debugging process via Web page abstractions rather than lower level source code views. The breakpoints are implemented in the open source Fire-bug project, version 1.5, for the Firefox Web browser.", "keywords": ["html", "css", "web", "javascript", "dynamic", "firebug", "debugging", "breakpoints"], "combined": "Dynamic and graphical web page breakpoints Breakpoints are perhaps the quintessential feature of a de-bugger: they allow a developer to stop time and study the program state. Breakpoints are typically specified by selecting a line of source code. For large, complex, web pages with multiple developers, the relevant source line for a given user interface problem may not be known to the developer. In this paper we describe the implementation of breakpoints in dynamically created source, and on error messages, network events, DOMmutation, DOMobject property changes, and CSS style rule updates. Adding these domain-specific breakpoints to a general-purpose debugger for Javascript allows the developer to initiate the debugging process via Web page abstractions rather than lower level source code views. The breakpoints are implemented in the open source Fire-bug project, version 1.5, for the Firefox Web browser. [[EENNDD]] html; css; web; javascript; dynamic; firebug; debugging; breakpoints"}, "Breakpoints halaman web dinamik dan grafik Breakpoints mungkin merupakan ciri utama dari de-bugger: mereka membenarkan pembangun menghentikan masa dan mempelajari keadaan program. Titik putus biasanya ditentukan dengan memilih barisan kod sumber. Untuk laman web yang besar dan kompleks dengan pelbagai pembangun, garis sumber yang relevan untuk masalah antara muka pengguna tertentu mungkin tidak diketahui oleh pembangun. Dalam makalah ini kami menjelaskan pelaksanaan breakpoint di sumber yang dibuat secara dinamis, dan pada pesan ralat, peristiwa rangkaian, DOMmutasi, perubahan properti DOMobject, dan kemas kini peraturan gaya CSS. Menambah breakpoint khusus domain ini ke debugger tujuan umum untuk Javascript membolehkan pembangun memulakan proses debug melalui abstraksi halaman Web dan bukannya paparan kod sumber tahap yang lebih rendah. Titik putus dilaksanakan dalam projek Fire-bug sumber terbuka, versi 1.5, untuk penyemak imbas Web Firefox. [[EENNDD]] html; css; laman web; javascript; dinamik; penyekat api; penyahpepijatan; titik putus"], [{"string": "Milgram-routing in social networks We demonstrate how a recent model of social networks (\"Affiliation Networks\", [21]) offers powerful cues in local routing within social networks, a theme made famous by sociologist Milgram's \"six degrees of separation\" experiments. This model posits the existence of an \"interest space\" that underlies a social network; we prove that in networks produced by this model, not only do short paths exist among all pairs of nodes but natural local routing algorithms can discover them effectively. Specifically, we show that local routing can discover paths of length O(log2 n) to targets chosen uniformly at random, and paths of length O(1) to targets chosen with probability proportional to their degrees. Experiments on the co-authorship graph derived from DBLP data confirm our theoretical results, and shed light into the power of one step of lookahead in routing algorithms for social networks.", "keywords": ["social networks", "milgram's experiment", "affiliation networks"], "combined": "Milgram-routing in social networks We demonstrate how a recent model of social networks (\"Affiliation Networks\", [21]) offers powerful cues in local routing within social networks, a theme made famous by sociologist Milgram's \"six degrees of separation\" experiments. This model posits the existence of an \"interest space\" that underlies a social network; we prove that in networks produced by this model, not only do short paths exist among all pairs of nodes but natural local routing algorithms can discover them effectively. Specifically, we show that local routing can discover paths of length O(log2 n) to targets chosen uniformly at random, and paths of length O(1) to targets chosen with probability proportional to their degrees. Experiments on the co-authorship graph derived from DBLP data confirm our theoretical results, and shed light into the power of one step of lookahead in routing algorithms for social networks. [[EENNDD]] social networks; milgram's experiment; affiliation networks"}, "Perutean Milgram di rangkaian sosial Kami menunjukkan bagaimana model rangkaian sosial baru-baru ini (\"Jaringan Gabungan\", [21]) menawarkan petunjuk kuat dalam perutean tempatan dalam rangkaian sosial, tema yang terkenal oleh eksperimen \"enam darjah pemisahan\" Milgram. Model ini menunjukkan adanya \"ruang minat\" yang mendasari rangkaian sosial; kami membuktikan bahawa dalam rangkaian yang dihasilkan oleh model ini, jalan pintas tidak hanya wujud di antara semua pasangan nod tetapi algoritma penghalaan semula jadi tempatan dapat menjumpainya dengan berkesan. Secara khusus, kami menunjukkan bahawa perutean tempatan dapat menemui jalan panjang O (log2 n) ke sasaran yang dipilih secara seragam secara rawak, dan jalur panjang O (1) ke sasaran yang dipilih dengan kebarangkalian sebanding dengan darjahnya. Eksperimen pada grafik kepengarangan bersama yang berasal dari data DBLP mengesahkan hasil teori kami, dan menjelaskan kekuatan satu langkah lookahead dalam merutekan algoritma untuk rangkaian sosial. [[EENNDD]] rangkaian sosial; eksperimen milgram; rangkaian gabungan"], [{"string": "EntityTagger: automatically tagging entities with descriptive phrases We consider the problem of entity tagging: given one or more named entities from a specific domain, the goal is to automatically associate descriptive phrases, referred to as etags (entity tags), to each entity. Consider a product catalog containing product names and possibly short descriptions. For a product in the catalog, say Ricoh G600 Digital Camera, we want to associate etags such as \"water resistant\", \"rugged\" and \"outdoor\" to it, even though its name or description does not mention those phrases. Entity tagging can enable more effective search over entities. We propose to leverage signals in web documents to perform such tagging. We develop techniques to perform such tagging in a domain independent manner while ensuring high precision and high recall.", "keywords": ["entity tagging", "tag association", "tag discovery", "miscellaneous"], "combined": "EntityTagger: automatically tagging entities with descriptive phrases We consider the problem of entity tagging: given one or more named entities from a specific domain, the goal is to automatically associate descriptive phrases, referred to as etags (entity tags), to each entity. Consider a product catalog containing product names and possibly short descriptions. For a product in the catalog, say Ricoh G600 Digital Camera, we want to associate etags such as \"water resistant\", \"rugged\" and \"outdoor\" to it, even though its name or description does not mention those phrases. Entity tagging can enable more effective search over entities. We propose to leverage signals in web documents to perform such tagging. We develop techniques to perform such tagging in a domain independent manner while ensuring high precision and high recall. [[EENNDD]] entity tagging; tag association; tag discovery; miscellaneous"}, "EntityTagger: menandakan entiti secara automatik dengan frasa deskriptif Kami menganggap masalah pemberian tag entiti: diberikan satu atau lebih entiti bernama dari domain tertentu, tujuannya adalah secara automatik mengaitkan frasa deskriptif, disebut sebagai etag (tag entiti), ke setiap entiti. Pertimbangkan katalog produk yang mengandungi nama produk dan kemungkinan penerangan pendek. Untuk produk dalam katalog, katakan Kamera Digital Ricoh G600, kami ingin mengaitkan etag seperti \"tahan air\", \"lasak\" dan \"luaran\" dengannya, walaupun nama atau keterangannya tidak menyebut frasa tersebut. Penandaan entiti dapat membolehkan carian lebih berkesan berbanding entiti. Kami mencadangkan untuk memanfaatkan isyarat dalam dokumen web untuk melakukan penandaan seperti itu. Kami mengembangkan teknik untuk melakukan penandaan sedemikian secara bebas domain sambil memastikan ketepatan tinggi dan penarikan balik yang tinggi. [[EENNDD]] penandaan entiti; persatuan tag; penemuan tag; pelbagai"], [{"string": "A search engine for natural language applications No contact information provided yet.", "keywords": ["corpus", "search engine", "indexing", "language", "content analysis and indexing", "variables", "information extraction", "query", "information search and retrieval"], "combined": "A search engine for natural language applications No contact information provided yet. [[EENNDD]] corpus; search engine; indexing; language; content analysis and indexing; variables; information extraction; query; information search and retrieval"}, "Enjin carian untuk aplikasi bahasa semula jadi Belum ada maklumat hubungan. [[EENNDD]] korpus; enjin carian; pengindeksan; bahasa; analisis kandungan dan pengindeksan; pemboleh ubah; pengekstrakan maklumat; pertanyaan; pencarian dan pencarian maklumat"], [{"string": "Estimating the web robot population In this research, capture-recapture (CR) models are used to estimate the population of web robots based on web server access logs from different websites. Each robot is considered as an individual randomly surfing the web and each website is considered as a trap that records the visitation of robots. We use maximum likelihood estimator to fit the observation data. Results show that there are 3,860 identifiable robot User-Agent strings and 780,760 IP addresses being used by web robots around the world. We also examine the origination of the named robots by their IP addresses. The results suggest that over 50% of web robot IP addresses are from United States and China.", "keywords": ["web robot population"], "combined": "Estimating the web robot population In this research, capture-recapture (CR) models are used to estimate the population of web robots based on web server access logs from different websites. Each robot is considered as an individual randomly surfing the web and each website is considered as a trap that records the visitation of robots. We use maximum likelihood estimator to fit the observation data. Results show that there are 3,860 identifiable robot User-Agent strings and 780,760 IP addresses being used by web robots around the world. We also examine the origination of the named robots by their IP addresses. The results suggest that over 50% of web robot IP addresses are from United States and China. [[EENNDD]] web robot population"}, "Mengira populasi robot web Dalam penyelidikan ini, model penangkapan semula (CR) digunakan untuk menganggarkan populasi robot web berdasarkan log akses pelayan web dari laman web yang berbeza. Setiap robot dianggap sebagai individu melayari laman web secara rawak dan setiap laman web dianggap sebagai perangkap yang merekodkan lawatan robot. Kami menggunakan penganggar kemungkinan maksimum untuk memenuhi data pemerhatian. Hasil kajian menunjukkan bahawa terdapat 3,860 rentetan Pengguna-Ejen robot yang dapat dikenal pasti dan 780,760 alamat IP yang digunakan oleh robot web di seluruh dunia. Kami juga memeriksa asal-usul robot bernama dengan alamat IP mereka. Hasilnya menunjukkan bahawa lebih daripada 50% alamat IP robot web berasal dari Amerika Syarikat dan China. [[EENNDD]] populasi robot web"], [{"string": "Video quality estimation for internet streaming An abstract is not available.", "keywords": ["video quality", "network measurement", "streaming", "network protocols"], "combined": "Video quality estimation for internet streaming An abstract is not available. [[EENNDD]] video quality; network measurement; streaming; network protocols"}, "Anggaran kualiti video untuk streaming internet Abstrak tidak tersedia. [[EENNDD]] kualiti video; pengukuran rangkaian; penstriman; protokol rangkaian"], [{"string": "A web middleware architecture for dynamic customization of content for wireless clients No contact information provided yet.", "keywords": ["wireless", "proxy", "general", "middleware", "http", "mobile code"], "combined": "A web middleware architecture for dynamic customization of content for wireless clients No contact information provided yet. [[EENNDD]] wireless; proxy; general; middleware; http; mobile code"}, "Senibina middleware web untuk penyesuaian kandungan dinamik untuk klien tanpa wayar Belum ada maklumat hubungan yang diberikan. [[EENNDD]] tanpa wayar; proksi; umum; alat tengah; http; kod mudah alih"], [{"string": "Spam attacks: p2p to the rescue No contact information provided yet.", "keywords": ["spam filtering", "security and protection", "structured p2p", "reputation", "distributed systems"], "combined": "Spam attacks: p2p to the rescue No contact information provided yet. [[EENNDD]] spam filtering; security and protection; structured p2p; reputation; distributed systems"}, "Serangan spam: p2p untuk menyelamatkan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penapisan spam; keselamatan dan perlindungan; p2p berstruktur; reputasi; sistem yang diedarkan"], [{"string": "A fast XPATH evaluation technique with the facility of updates No contact information provided yet.", "keywords": ["biaxes path expression", "xpath query optimization", "dewey indexing", "xml", "updates"], "combined": "A fast XPATH evaluation technique with the facility of updates No contact information provided yet. [[EENNDD]] biaxes path expression; xpath query optimization; dewey indexing; xml; updates"}, "Teknik penilaian XPATH yang pantas dengan kemudahan kemas kini Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ungkapan jalur biaxes; pengoptimuman pertanyaan xpath; pengindeksan dewey; xml; kemas kini"], [{"string": "Traffic characterization and internet usage in rural Africa While Internet connectivity has reached a significant part of the world's population, those living in rural areas of the developing world are still largely disconnected. Recent efforts have provided Internet connectivity to a growing number of remote locations, yet Internet traffic demands cause many of these networks to fail to deliver basic quality of service needed for simple applications. For an in-depth investigation of the problem, we gather and analyze network traces from a rural wireless network in Macha, Zambia. We supplement our analysis with on-site interviews from Macha, Zambia and Dwesa, South Africa, another rural community that hosts a local wireless network. The results reveal that Internet traffic in rural Africa differs significantly from the developed world. We observe dominance of web-based traffic, as opposed to peer-to-peer traffic common in urban areas. Application-wise, online social networks are the most popular, while the majority of bandwidth is consumed by large operating system updates. Our analysis also uncovers numerous network anomalies, such as significant malware traffic. Finally, we find a strong feedback loop between network performance and user behavior. Based on our findings, we conclude with a discussion of new directions in network design that take into account both technical and social factors.", "keywords": ["interviews", "rural networks", "applications", "internet usage"], "combined": "Traffic characterization and internet usage in rural Africa While Internet connectivity has reached a significant part of the world's population, those living in rural areas of the developing world are still largely disconnected. Recent efforts have provided Internet connectivity to a growing number of remote locations, yet Internet traffic demands cause many of these networks to fail to deliver basic quality of service needed for simple applications. For an in-depth investigation of the problem, we gather and analyze network traces from a rural wireless network in Macha, Zambia. We supplement our analysis with on-site interviews from Macha, Zambia and Dwesa, South Africa, another rural community that hosts a local wireless network. The results reveal that Internet traffic in rural Africa differs significantly from the developed world. We observe dominance of web-based traffic, as opposed to peer-to-peer traffic common in urban areas. Application-wise, online social networks are the most popular, while the majority of bandwidth is consumed by large operating system updates. Our analysis also uncovers numerous network anomalies, such as significant malware traffic. Finally, we find a strong feedback loop between network performance and user behavior. Based on our findings, we conclude with a discussion of new directions in network design that take into account both technical and social factors. [[EENNDD]] interviews; rural networks; applications; internet usage"}, "Pencirian lalu lintas dan penggunaan internet di luar bandar Afrika Walaupun hubungan Internet telah mencapai sebahagian besar populasi dunia, mereka yang tinggal di kawasan luar bandar di dunia membangun masih terputus. Usaha baru-baru ini telah menyediakan sambungan Internet ke sejumlah lokasi terpencil, namun tuntutan lalu lintas internet menyebabkan banyak rangkaian ini gagal memberikan kualiti asas perkhidmatan yang diperlukan untuk aplikasi sederhana. Untuk penyiasatan mendalam mengenai masalah ini, kami mengumpulkan dan menganalisis jejak rangkaian dari rangkaian tanpa wayar luar bandar di Macha, Zambia. Kami melengkapkan analisis kami dengan wawancara di lokasi dari Macha, Zambia dan Dwesa, Afrika Selatan, komuniti luar bandar lain yang menjadi tuan rumah rangkaian tanpa wayar tempatan. Hasil kajian menunjukkan bahawa lalu lintas Internet di luar bandar Afrika jauh berbeza dengan negara maju. Kami melihat dominasi lalu lintas berasaskan web, berbanding lalu lintas peer-to-peer yang biasa terjadi di kawasan bandar. Aplikasi sosial, rangkaian sosial dalam talian adalah yang paling popular, sementara lebar jalur digunakan oleh kemas kini sistem operasi yang besar. Analisis kami juga menemui banyak anomali rangkaian, seperti lalu lintas malware yang ketara. Akhirnya, kami menemui gelung maklum balas yang kuat antara prestasi rangkaian dan tingkah laku pengguna. Berdasarkan penemuan kami, kami menyimpulkan dengan perbincangan mengenai arah baru dalam reka bentuk rangkaian yang mengambil kira faktor teknikal dan sosial. [[EENNDD]] temu ramah; rangkaian luar bandar; permohonan; penggunaan internet"], [{"string": "Generating maps of web pages using cellular automata No contact information provided yet.", "keywords": ["cellular automata", "visualization", "unsupervised clustering", "web pages"], "combined": "Generating maps of web pages using cellular automata No contact information provided yet. [[EENNDD]] cellular automata; visualization; unsupervised clustering; web pages"}, "Menjana peta laman web menggunakan automata selular Belum ada maklumat hubungan yang diberikan. [[EENNDD]] automata selular; visualisasi; pengelompokan tanpa pengawasan; laman sesawang"], [{"string": "WCAG formalization with W3C standards No contact information provided yet.", "keywords": ["wcag", "xquery", "xpointer", "hypertext/hypermedia", "miscellaneous", "user interfaces", "xpath", "wai"], "combined": "WCAG formalization with W3C standards No contact information provided yet. [[EENNDD]] wcag; xquery; xpointer; hypertext/hypermedia; miscellaneous; user interfaces; xpath; wai"}, "Formalisasi WCAG dengan standard W3C Belum ada maklumat hubungan yang diberikan. [[EENNDD]] wcag; xquery; xpointer; hiperteks / hipermedia; pelbagai; antara muka pengguna; xpath; wai"], [{"string": "Distributing private data in challenged network environments Developing countries face significant challenges in network access, making even simple network tasks unpleasant. Many standard techniques - caching and predictive prefetching - help somewhat, but provide little or no assistance for personal data that is needed only by a single user. Sulula addresses this problem by leveraging the near-ubiquity of cellular phones able to send and receive simple SMS messages. Rather than visit a kiosk and fetch data on demand - a tiresome process at best - users request a future visit. If capacity exists, the kiosk can schedule secure retrieval of that user's data, saving time and more efficiently utilizing the kiosk's limited connectivity. When the user arrives at a provisioned kiosk, she need only obtain the session key on-demand, and thereafter has instant access. In addition, Sulula allows users to schedule data uploads. Experimental results show significant gains for the end user, saving tens of minutes of time for a typical email/news reading session. We also describe a small, ongoing deployment in-country for proof-of-concept, lessons learned from that experience, and provide a discussion on pricing and marketplace issues that remain to be addressed to make the system viable for developing-world access.", "keywords": ["ethiopia", "personal data", "developing regions", "caching", "band-width", "latency", "www access", "prefetching", "sms", "limited connectivity"], "combined": "Distributing private data in challenged network environments Developing countries face significant challenges in network access, making even simple network tasks unpleasant. Many standard techniques - caching and predictive prefetching - help somewhat, but provide little or no assistance for personal data that is needed only by a single user. Sulula addresses this problem by leveraging the near-ubiquity of cellular phones able to send and receive simple SMS messages. Rather than visit a kiosk and fetch data on demand - a tiresome process at best - users request a future visit. If capacity exists, the kiosk can schedule secure retrieval of that user's data, saving time and more efficiently utilizing the kiosk's limited connectivity. When the user arrives at a provisioned kiosk, she need only obtain the session key on-demand, and thereafter has instant access. In addition, Sulula allows users to schedule data uploads. Experimental results show significant gains for the end user, saving tens of minutes of time for a typical email/news reading session. We also describe a small, ongoing deployment in-country for proof-of-concept, lessons learned from that experience, and provide a discussion on pricing and marketplace issues that remain to be addressed to make the system viable for developing-world access. [[EENNDD]] ethiopia; personal data; developing regions; caching; band-width; latency; www access; prefetching; sms; limited connectivity"}, "Mengedarkan data peribadi di persekitaran rangkaian yang dicabar Negara-negara membangun menghadapi cabaran besar dalam akses rangkaian, menjadikan tugas rangkaian yang sederhana tidak menyenangkan. Banyak teknik standard - caching dan predetching prefetching - agak membantu, tetapi memberikan sedikit atau tanpa bantuan untuk data peribadi yang hanya diperlukan oleh satu pengguna. Sulula mengatasi masalah ini dengan memanfaatkan hampir semua telefon bimbit yang dapat menghantar dan menerima mesej SMS ringkas. Daripada mengunjungi kiosk dan mengambil data berdasarkan permintaan - proses yang melelahkan pada tahap terbaik - pengguna meminta lawatan masa depan. Sekiranya kapasiti ada, kios dapat menjadualkan pengambilan data pengguna dengan selamat, menjimatkan masa dan lebih cekap menggunakan sambungan terhad kiosk. Apabila pengguna tiba di kios yang disediakan, dia hanya perlu mendapatkan kunci sesi atas permintaan, dan setelah itu mempunyai akses segera. Di samping itu, Sulula membolehkan pengguna menjadualkan muat naik data. Hasil eksperimen menunjukkan keuntungan yang signifikan bagi pengguna akhir, menjimatkan puluhan minit masa untuk sesi membaca e-mel / berita biasa. Kami juga menerangkan penerapan kecil di negara untuk bukti bukti konsep, pelajaran yang diperoleh dari pengalaman itu, dan memberikan perbincangan mengenai harga dan isu-isu pasaran yang masih harus ditangani untuk menjadikan sistem ini layak untuk akses dunia berkembang. [[EENNDD]] etiopia; data peribadi; wilayah membangun; caching; lebar jalur; kependaman; akses www; prakiraan; sms; kesambungan terhad"], [{"string": "Identifying primary content from web pages and its application to web search ranking Web pages are usually highly structured documents. In some documents, content with different functionality is laid out in blocks, some merely supporting the main discourse. In other documents, there may be several blocks of unrelated main content. Indexing a web page as if it were a linear document can cause problems because of the diverse nature of its content. If the retrieval function treats all blocks of the web page equally without attention to structure, it may lead to irrelevant query matches. In this paper, we describe how content quality of different blocks of a web page can be utilized to improve a retrieval function. Our method is based on segmenting a web page into semantically coherent blocks and learning a predictor of segment content quality. We also describe how to use segment content quality estimates as weights in the BM25F formulation. Experimental results show our method improves relevance of retrieved results by as much as 4.5% compared to BM25F that treats the body of a web page as a single section, and by a larger margin of over 9% for difficult queries.", "keywords": ["information search and retrieval", "page structure", "content quality models", "segmentation", "search"], "combined": "Identifying primary content from web pages and its application to web search ranking Web pages are usually highly structured documents. In some documents, content with different functionality is laid out in blocks, some merely supporting the main discourse. In other documents, there may be several blocks of unrelated main content. Indexing a web page as if it were a linear document can cause problems because of the diverse nature of its content. If the retrieval function treats all blocks of the web page equally without attention to structure, it may lead to irrelevant query matches. In this paper, we describe how content quality of different blocks of a web page can be utilized to improve a retrieval function. Our method is based on segmenting a web page into semantically coherent blocks and learning a predictor of segment content quality. We also describe how to use segment content quality estimates as weights in the BM25F formulation. Experimental results show our method improves relevance of retrieved results by as much as 4.5% compared to BM25F that treats the body of a web page as a single section, and by a larger margin of over 9% for difficult queries. [[EENNDD]] information search and retrieval; page structure; content quality models; segmentation; search"}, "Mengenal pasti kandungan utama dari laman web dan aplikasinya ke peringkat carian web Halaman web biasanya merupakan dokumen yang sangat berstruktur. Dalam beberapa dokumen, isi dengan fungsi yang berbeda disusun dalam beberapa blok, beberapa hanya menyokong wacana utama. Dalam dokumen lain, mungkin terdapat beberapa blok kandungan utama yang tidak berkaitan. Mengindeks laman web seolah-olah dokumen linier dapat menimbulkan masalah kerana sifat isinya yang pelbagai. Sekiranya fungsi pengambilan memperlakukan semua blok laman web secara sama rata tanpa perhatian terhadap struktur, ia boleh menyebabkan padanan pertanyaan yang tidak relevan. Dalam makalah ini, kami menerangkan bagaimana kualiti kandungan blok laman web yang berbeza dapat digunakan untuk meningkatkan fungsi pengambilan. Kaedah kami didasarkan pada membagi halaman web menjadi blok koheren semantik dan mempelajari peramal kualiti kandungan segmen. Kami juga menerangkan bagaimana menggunakan anggaran kualiti kandungan segmen sebagai bobot dalam formulasi BM25F. Hasil eksperimen menunjukkan kaedah kami meningkatkan relevansi hasil yang diperoleh sebanyak 4.5% berbanding BM25F yang memperlakukan badan laman web sebagai satu bahagian, dan dengan margin lebih besar lebih dari 9% untuk pertanyaan sukar. [[EENNDD]] carian dan pengambilan maklumat; struktur halaman; model kualiti kandungan; segmentasi; cari"], [{"string": "Search engines and their public interfaces: which apis are the most synchronized? Researchers of commercial search engines often collect datausing the application programming interface (API) or by\"scraping\" results from the web user interface (WUI), butanecdotal evidence suggests the interfaces produce differentresults. We provide the first in-depth quantitative analysisof the results produced by the Google, MSN and Yahoo APIand WUI interfaces. After submitting a variety of queriesto the interfaces for 5 months, we found significant discrepanciesin several categories. Our findings suggest that theAPI indexes are not older, but they are probably smaller for Google and Yahoo. Researchers may use our findings tobetter understand the differences between the interfaces andchoose the best API for their particular types of queries.", "keywords": ["search engine", "api"], "combined": "Search engines and their public interfaces: which apis are the most synchronized? Researchers of commercial search engines often collect datausing the application programming interface (API) or by\"scraping\" results from the web user interface (WUI), butanecdotal evidence suggests the interfaces produce differentresults. We provide the first in-depth quantitative analysisof the results produced by the Google, MSN and Yahoo APIand WUI interfaces. After submitting a variety of queriesto the interfaces for 5 months, we found significant discrepanciesin several categories. Our findings suggest that theAPI indexes are not older, but they are probably smaller for Google and Yahoo. Researchers may use our findings tobetter understand the differences between the interfaces andchoose the best API for their particular types of queries. [[EENNDD]] search engine; api"}, "Enjin carian dan antara muka awam mereka: apis mana yang paling diselaraskan? Penyelidik mesin pencari komersial sering mengumpulkan data menggunakan antara muka pengaturcaraan aplikasi (API) atau dengan \"mengikis\" hasil dari antara muka pengguna web (WUI), tetapi bukti-bukti butanecdotal menunjukkan antara muka menghasilkan hasil yang berbeza. Kami memberikan analisis kuantitatif mendalam pertama hasil yang dihasilkan oleh antara muka Google, MSN dan Yahoo API dan WUI. Setelah menghantar pelbagai pertanyaan untuk antara muka selama 5 bulan, kami mendapati terdapat perbezaan yang signifikan dalam beberapa kategori. Hasil kajian kami menunjukkan bahawa indeksAPI tidak lebih tua, tetapi mungkin lebih kecil untuk Google dan Yahoo. Penyelidik boleh menggunakan penemuan kami untuk mengetahui perbezaan antara antara muka dan memilih API terbaik untuk jenis pertanyaan mereka. [[EENNDD]] enjin carian; api"], [{"string": "A hybrid approach for searching in the semantic web No contact information provided yet.", "keywords": ["ontologies", "semantic associations", "network analysis", "information search and retrieval", "spread activation algorithms", "semantic web", "semantic search"], "combined": "A hybrid approach for searching in the semantic web No contact information provided yet. [[EENNDD]] ontologies; semantic associations; network analysis; information search and retrieval; spread activation algorithms; semantic web; semantic search"}, "Pendekatan hibrid untuk mencari di web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ontologi; persatuan semantik; analisis rangkaian; pencarian dan pengambilan maklumat; algoritma pengaktifan penyebaran; web semantik; carian semantik"], [{"string": "Web page rank prediction with markov models In this paper we propose a method for predicting the ranking position of a Web page. Assuming a set of successive past top-k rankings, we study the evolution of Web pages in terms of ranking trend sequences used for Markov Models training, which are in turn used to predict future rankings. The predictions are highly accurate for all experimental setups and similarity measures.", "keywords": ["markov models", "ranking prediction"], "combined": "Web page rank prediction with markov models In this paper we propose a method for predicting the ranking position of a Web page. Assuming a set of successive past top-k rankings, we study the evolution of Web pages in terms of ranking trend sequences used for Markov Models training, which are in turn used to predict future rankings. The predictions are highly accurate for all experimental setups and similarity measures. [[EENNDD]] markov models; ranking prediction"}, "Ramalan peringkat laman web dengan model markov Dalam makalah ini kami mencadangkan kaedah untuk meramalkan kedudukan kedudukan halaman Web. Dengan mengandaikan satu set peringkat top-k berturut-turut, kami mengkaji evolusi laman web dari segi urutan trend peringkat yang digunakan untuk latihan Model Markov, yang pada gilirannya digunakan untuk meramalkan peringkat masa depan. Ramalan ini sangat tepat untuk semua penyediaan eksperimen dan ukuran kesamaan. [[EENNDD]] model markov; ramalan peringkat"], [{"string": "Upgrading relational legacy data to the semantic web No contact information provided yet.", "keywords": ["database-to-ontology mappings", "database applications", "semantic web", "upgrade", "relational databases"], "combined": "Upgrading relational legacy data to the semantic web No contact information provided yet. [[EENNDD]] database-to-ontology mappings; database applications; semantic web; upgrade; relational databases"}, "Meningkatkan data warisan hubungan ke web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pemetaan pangkalan data ke ontologi; aplikasi pangkalan data; web semantik; naik taraf; pangkalan data hubungan"], [{"string": "Gossip based streaming No contact information provided yet.", "keywords": ["overlay networks", "general", "streaming", "multicast"], "combined": "Gossip based streaming No contact information provided yet. [[EENNDD]] overlay networks; general; streaming; multicast"}, "Penstriman berasaskan gosip Belum ada maklumat hubungan yang diberikan. [[EENNDD]] rangkaian tindanan; umum; penstriman; pelbagai siaran"], [{"string": "Leveraging interlingual classification to improve web search In this paper we address the problem of improving accuracy of web search in a smaller, data-limited search market (search language) using behavioral data from a larger, data-rich market (assist language). Specifically, we use interlingual classification to infer the search language query's intent using the assist language click-through data. We use these improved estimates of query intent, along with the query intent based on the search language data, to compute features that encode the similarity between a search result (URL) and the query. These features are subsequently fed into the ranking model to improve the relevance ranking of the documents. Our experimental results on German and French languages show the effectiveness of using assist language behavioral data especially, when the search language queries have small click-through data.", "keywords": ["text classification", "web search", "information search and retrieval", "machine translation", "cross-lingual information retrieval"], "combined": "Leveraging interlingual classification to improve web search In this paper we address the problem of improving accuracy of web search in a smaller, data-limited search market (search language) using behavioral data from a larger, data-rich market (assist language). Specifically, we use interlingual classification to infer the search language query's intent using the assist language click-through data. We use these improved estimates of query intent, along with the query intent based on the search language data, to compute features that encode the similarity between a search result (URL) and the query. These features are subsequently fed into the ranking model to improve the relevance ranking of the documents. Our experimental results on German and French languages show the effectiveness of using assist language behavioral data especially, when the search language queries have small click-through data. [[EENNDD]] text classification; web search; information search and retrieval; machine translation; cross-lingual information retrieval"}, "Memanfaatkan klasifikasi interlingual untuk meningkatkan carian web Dalam makalah ini kami menangani masalah peningkatan ketepatan carian web di pasar carian (data carian) yang lebih kecil dan terhad menggunakan data tingkah laku dari pasar yang lebih besar dan kaya data (bahasa bantu). Secara khusus, kami menggunakan klasifikasi interlingual untuk menyimpulkan maksud pertanyaan bahasa carian menggunakan data klik-tayang bahasa bantu. Kami menggunakan anggaran peningkatan permintaan kueri ini, bersama dengan maksud pertanyaan berdasarkan data bahasa carian, untuk menghitung ciri yang menyandikan kesamaan antara hasil carian (URL) dan pertanyaan. Ciri-ciri ini kemudian dimasukkan ke dalam model peringkat untuk meningkatkan kedudukan relevan dokumen. Hasil percubaan kami terhadap bahasa Jerman dan Perancis menunjukkan keberkesanan penggunaan data tingkah laku bahasa bantu terutamanya, apabila pertanyaan bahasa carian mempunyai data klik-tayang yang kecil. [[EENNDD]] pengelasan teks; carian sesawang; carian dan pengambilan maklumat; terjemahan mesin; pengambilan maklumat lintas bahasa"], [{"string": "Rewriting queries on SPARQL views The problem of answering SPARQL queries over virtual SPARQL views is commonly encountered in a number of settings, including while enforcing security policies to access RDF data, or when integrating RDF data from disparate sources. We approach this problem by rewriting SPARQL queries over the views to equivalent queries over the underlying RDF data, thus avoiding the costs entailed by view materialization and maintenance. We show that SPARQL query rewriting combines the most challenging aspects of rewriting for the relational and XML cases: like the relational case, SPARQL query rewriting requires synthesizing multiple views; like the XML case, the size of the rewritten query is exponential to the size of the query and the views. In this paper, we present the first native query rewriting algorithm for SPARQL. For an input SPARQL query over a set of virtual SPARQL views, the rewritten query resembles a union of conjunctive queries and can be of exponential size. We propose optimizations over the basic rewriting algorithm to (i) minimize each conjunctive query in the union; (ii) eliminate conjunctive queries with empty results from evaluation; and (iii) efficiently prune out big portions of the search space of empty rewritings. The experiments, performed on two RDF stores, show that our algorithms are scalable and independent of the underlying RDF stores. Furthermore, our optimizations have order of magnitude improvements over the basic rewriting algorithm in both the rewriting size and evaluation time.", "keywords": ["rewriting", "sparql views", "sparql query"], "combined": "Rewriting queries on SPARQL views The problem of answering SPARQL queries over virtual SPARQL views is commonly encountered in a number of settings, including while enforcing security policies to access RDF data, or when integrating RDF data from disparate sources. We approach this problem by rewriting SPARQL queries over the views to equivalent queries over the underlying RDF data, thus avoiding the costs entailed by view materialization and maintenance. We show that SPARQL query rewriting combines the most challenging aspects of rewriting for the relational and XML cases: like the relational case, SPARQL query rewriting requires synthesizing multiple views; like the XML case, the size of the rewritten query is exponential to the size of the query and the views. In this paper, we present the first native query rewriting algorithm for SPARQL. For an input SPARQL query over a set of virtual SPARQL views, the rewritten query resembles a union of conjunctive queries and can be of exponential size. We propose optimizations over the basic rewriting algorithm to (i) minimize each conjunctive query in the union; (ii) eliminate conjunctive queries with empty results from evaluation; and (iii) efficiently prune out big portions of the search space of empty rewritings. The experiments, performed on two RDF stores, show that our algorithms are scalable and independent of the underlying RDF stores. Furthermore, our optimizations have order of magnitude improvements over the basic rewriting algorithm in both the rewriting size and evaluation time. [[EENNDD]] rewriting; sparql views; sparql query"}, "Menulis semula pertanyaan pada pandangan SPARQL Masalah menjawab pertanyaan SPARQL berbanding pandangan SPARQL maya biasanya dihadapi dalam sejumlah pengaturan, termasuk ketika menerapkan kebijakan keamanan untuk mengakses data RDF, atau ketika mengintegrasikan data RDF dari sumber yang berbeza. Kami mendekati masalah ini dengan menulis semula pertanyaan SPARQL daripada pandangan kepada pertanyaan yang setara dengan data RDF yang mendasari, sehingga mengelakkan kos yang diperlukan oleh pemeliharaan dan penyelenggaraan pandangan. Kami menunjukkan bahawa penulisan semula pertanyaan SPARQL menggabungkan aspek penulisan semula yang paling mencabar untuk kes relasional dan XML: seperti kes hubungan, penulisan semula pertanyaan SPARQL memerlukan mensintesis pelbagai pandangan; seperti kes XML, ukuran pertanyaan yang ditulis semula sesuai dengan ukuran pertanyaan dan pandangan. Dalam makalah ini, kami membentangkan algoritma penulisan semula pertanyaan asli pertama untuk SPARQL. Untuk pertanyaan input SPARQL di atas sekumpulan pandangan SPARQL maya, pertanyaan yang ditulis semula menyerupai penyatuan pertanyaan konjungtif dan dapat berukuran eksponensial. Kami mencadangkan pengoptimuman terhadap algoritma penulisan semula asas untuk (i) meminimumkan setiap pertanyaan konjungtif dalam kesatuan; (ii) menghilangkan pertanyaan konjungtif dengan hasil kosong dari penilaian; dan (iii) memangkas bahagian carian ruang tulis yang kosong dengan cekap. Eksperimen, yang dilakukan di dua kedai RDF, menunjukkan bahawa algoritma kami boleh diskalakan dan tidak bergantung kepada kedai RDF yang mendasari. Tambahan pula, pengoptimuman kami mempunyai peningkatan yang besar berbanding algoritma penulisan semula asas dalam ukuran penulisan semula dan masa penilaian. [[EENNDD]] menulis semula; pandangan sparql; pertanyaan sparql"], [{"string": "Generating query substitutions No contact information provided yet.", "keywords": ["query rewriting", "sponsored search", "query substitution", "paraphrasing"], "combined": "Generating query substitutions No contact information provided yet. [[EENNDD]] query rewriting; sponsored search; query substitution; paraphrasing"}, "Menjana penggantian pertanyaan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] menulis semula pertanyaan; carian tajaan; penggantian pertanyaan; perangkaan"], [{"string": "The paths more taken: matching DOM trees to search logs for accurate webpage clustering An unsupervised clustering of the webpages on a website is a primary requirement for most wrapper induction and automated data extraction methods. Since page content can vary drastically across pages of one cluster (e.g., all product pages on amazon.com), traditional clustering methods typically use some distance function between the DOM trees representing a pair of webpages. However, without knowing which portions of the DOM tree are \"important,\" such distance functions might discriminate between similar pages based on trivial features (e.g., differing number of reviews on two product pages), or club together distinct types of pages based on superficial features present in the DOM trees of both (e.g., matching footer/copyright), leading to poor clustering performance.", "keywords": ["miscellaneous", "wrapper induction", "search logs", "clustering"], "combined": "The paths more taken: matching DOM trees to search logs for accurate webpage clustering An unsupervised clustering of the webpages on a website is a primary requirement for most wrapper induction and automated data extraction methods. Since page content can vary drastically across pages of one cluster (e.g., all product pages on amazon.com), traditional clustering methods typically use some distance function between the DOM trees representing a pair of webpages. However, without knowing which portions of the DOM tree are \"important,\" such distance functions might discriminate between similar pages based on trivial features (e.g., differing number of reviews on two product pages), or club together distinct types of pages based on superficial features present in the DOM trees of both (e.g., matching footer/copyright), leading to poor clustering performance. [[EENNDD]] miscellaneous; wrapper induction; search logs; clustering"}, "Jalan yang lebih diambil: mencocokkan pokok DOM untuk mencari log untuk pengelompokan halaman web yang tepat Penggabungan halaman web yang tidak diawasi di laman web adalah syarat utama untuk kebanyakan kaedah induksi pembungkus dan pengekstrakan data automatik. Oleh kerana kandungan halaman boleh berubah secara drastik di halaman satu kluster (mis., Semua halaman produk di amazon.com), kaedah pengelompokan tradisional biasanya menggunakan beberapa fungsi jarak antara pohon DOM yang mewakili sepasang halaman web. Walau bagaimanapun, tanpa mengetahui bahagian pokok DOM mana yang \"penting\", fungsi jarak seperti itu mungkin membeza-bezakan halaman serupa berdasarkan ciri-ciri sepele (contohnya, jumlah ulasan yang berbeza pada dua halaman produk), atau menggabungkan pelbagai jenis halaman berdasarkan dangkal ciri-ciri yang terdapat di pokok DOM keduanya (misalnya, footer / hak cipta yang sepadan), yang menyebabkan prestasi pengelompokan yang buruk. [[EENNDD]] pelbagai; aruhan pembungkus; log carian; pengelompokan"], [{"string": "Multi-objective ranking of comments on web With the explosion of information on any topic, the need for ranking is becoming very critical. Ranking typically depends on several aspects. Products, for example, have several aspects like price, recency, rating, etc. Product ranking has to bring the \"best\" product which is recent and highly rated. Hence ranking has to satisfy multiple objectives. In this paper, we explore multi-objective ranking of comments using Hodge decomposition. While Hodge decomposition produces a globally consistent ranking, a globally inconsistent component is also present. We propose an active learning strategy for the reduction of this component. Finally, we develop techniques for online Hodge decomposition. We experimentally validate the ideas presented in this paper.", "keywords": ["hodge decomposition", "active learning", "multi-objective ranking", "miscellaneous"], "combined": "Multi-objective ranking of comments on web With the explosion of information on any topic, the need for ranking is becoming very critical. Ranking typically depends on several aspects. Products, for example, have several aspects like price, recency, rating, etc. Product ranking has to bring the \"best\" product which is recent and highly rated. Hence ranking has to satisfy multiple objectives. In this paper, we explore multi-objective ranking of comments using Hodge decomposition. While Hodge decomposition produces a globally consistent ranking, a globally inconsistent component is also present. We propose an active learning strategy for the reduction of this component. Finally, we develop techniques for online Hodge decomposition. We experimentally validate the ideas presented in this paper. [[EENNDD]] hodge decomposition; active learning; multi-objective ranking; miscellaneous"}, "Penarafan pelbagai objektif di laman web Dengan ledakan maklumat mengenai topik apa pun, keperluan untuk peringkat menjadi sangat penting. Peringkat biasanya bergantung pada beberapa aspek. Produk, misalnya, mempunyai beberapa aspek seperti harga, kebelakangan, penilaian, dan lain-lain. Kedudukan produk harus membawa produk \"terbaik\" yang terkini dan dinilai tinggi. Oleh itu kedudukan mesti memenuhi pelbagai objektif. Dalam makalah ini, kami meneroka peringkat komen pelbagai objektif menggunakan penguraian Hodge. Walaupun penguraian Hodge menghasilkan peringkat yang konsisten secara global, komponen yang tidak konsisten secara global juga ada. Kami mencadangkan strategi pembelajaran aktif untuk pengurangan komponen ini. Akhirnya, kami mengembangkan teknik untuk penguraian Hodge dalam talian. Kami secara eksperimen mengesahkan idea yang dikemukakan dalam makalah ini. [[EENNDD]] penguraian hodge; pembelajaran aktif; peringkat pelbagai objektif; pelbagai"], [{"string": "Sig.ma: live views on the web of data We demonstrate Sig.ma, both a service and an end user application to access the Web of Data as an integrated information space.", "keywords": ["rdfa", "web of data", "semantic web", "aggregated search"], "combined": "Sig.ma: live views on the web of data We demonstrate Sig.ma, both a service and an end user application to access the Web of Data as an integrated information space. [[EENNDD]] rdfa; web of data; semantic web; aggregated search"}, "Sig.ma: paparan langsung di web data Kami menunjukkan Sig.ma, baik perkhidmatan dan aplikasi pengguna akhir untuk mengakses Web Data sebagai ruang maklumat bersepadu. [[EENNDD]] rdfa; web data; web semantik; carian agregat"], [{"string": "Finding advertising keywords on web pages No contact information provided yet.", "keywords": ["information extraction", "advertising", "miscellaneous", "keyword extraction"], "combined": "Finding advertising keywords on web pages No contact information provided yet. [[EENNDD]] information extraction; advertising; miscellaneous; keyword extraction"}, "Mencari kata kunci iklan di laman web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengekstrakan maklumat; mengiklankan; pelbagai; pengekstrakan kata kunci"], [{"string": "Toward tighter integration of web search with a geographic information system No contact information provided yet.", "keywords": ["web mining", "web-gis integration", "local web search"], "combined": "Toward tighter integration of web search with a geographic information system No contact information provided yet. [[EENNDD]] web mining; web-gis integration; local web search"}, "Ke arah penyatuan carian web yang lebih ketat dengan sistem maklumat geografi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlombongan web; integrasi web-gis; carian web tempatan"], [{"string": "Resource management for scalable disconnected access to Web services An abstract is not available.", "keywords": ["internet"], "combined": "Resource management for scalable disconnected access to Web services An abstract is not available. [[EENNDD]] internet"}, "Pengurusan sumber untuk akses terputus yang boleh diskalakan ke perkhidmatan Web Abstrak tidak tersedia. [[EENNDD]] internet"], [{"string": "Building an open source meta-search engine No contact information provided yet.", "keywords": ["open source", "information search and retrieval", "meta search engines"], "combined": "Building an open source meta-search engine No contact information provided yet. [[EENNDD]] open source; information search and retrieval; meta search engines"}, "Membina enjin carian meta sumber terbuka [[EENNDD]] sumber terbuka; pencarian dan pengambilan maklumat; enjin carian meta"], [{"string": "Characterization of a large web site population with implications for content delivery No contact information provided yet.", "keywords": ["applications", "cookie", "content distribution", "web caching", "workload characterization", "http"], "combined": "Characterization of a large web site population with implications for content delivery No contact information provided yet. [[EENNDD]] applications; cookie; content distribution; web caching; workload characterization; http"}, "Pencirian populasi laman web yang besar dengan implikasi penyampaian kandungan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] aplikasi; kuki; pengedaran kandungan; caching web; pencirian beban kerja; http"], [{"string": "Bid optimization for broad match ad auctions Ad auctions in sponsored search support \"broad match\" that allows an advertiser to target a large number of queries while bidding only on a limited number. While giving more expressiveness to advertisers, this feature makes it challenging to optimize bids to maximize their returns: choosing to bid on a query as a broad match because it provides high profit results in one bidding for related queries which may yield low or even negative profits.", "keywords": ["sponsored search", "analysis of algorithms and problem complexity", "optimal bidding", "ad auctions", "bid optimization"], "combined": "Bid optimization for broad match ad auctions Ad auctions in sponsored search support \"broad match\" that allows an advertiser to target a large number of queries while bidding only on a limited number. While giving more expressiveness to advertisers, this feature makes it challenging to optimize bids to maximize their returns: choosing to bid on a query as a broad match because it provides high profit results in one bidding for related queries which may yield low or even negative profits. [[EENNDD]] sponsored search; analysis of algorithms and problem complexity; optimal bidding; ad auctions; bid optimization"}, "Pengoptimuman bid untuk lelongan iklan pencocokan luas Lelang iklan dalam sokongan carian tajaan \"pencocokan luas\" yang membolehkan pengiklan menargetkan sebilangan besar pertanyaan sambil menawar hanya pada jumlah yang terhad. Walaupun memberi lebih ekspresif kepada pengiklan, ciri ini menjadikannya sukar untuk mengoptimumkan bida untuk memaksimumkan pulangan mereka: memilih untuk mengajukan pertanyaan sebagai padanan luas kerana memberikan hasil keuntungan yang tinggi dalam satu penawaran untuk pertanyaan berkaitan yang mungkin menghasilkan keuntungan rendah atau bahkan negatif . [[EENNDD]] carian yang ditaja; analisis algoritma dan kerumitan masalah; pembidaan optimum; lelongan iklan; pengoptimuman bidaan"], [{"string": "Autonomous resource provisioning for multi-service web applications Dynamic resource provisioning aims at maintaining the end-to-end response time of a web application within a pre-defined SLA. Although the topic has been well studied for monolithic applications, provisioning resources for applications composed of multiple services remains a challenge. When the SLA is violated, one must decide which service(s) should be reprovisioned for optimal effect. We propose to assign an SLA only to the front-end service. Other services are not given any particular response time objectives. Services are autonomously responsible for their own provisioning operations and collaboratively negotiate performance objectives with each other to decide the provisioning service(s). We demonstrate through extensive experiments that our system can add/remove/shift both servers and caches within an entire multi-service application under varying workloads to meet the SLA target and improve resource utilization.", "keywords": ["resource provisioning", "multi-service application"], "combined": "Autonomous resource provisioning for multi-service web applications Dynamic resource provisioning aims at maintaining the end-to-end response time of a web application within a pre-defined SLA. Although the topic has been well studied for monolithic applications, provisioning resources for applications composed of multiple services remains a challenge. When the SLA is violated, one must decide which service(s) should be reprovisioned for optimal effect. We propose to assign an SLA only to the front-end service. Other services are not given any particular response time objectives. Services are autonomously responsible for their own provisioning operations and collaboratively negotiate performance objectives with each other to decide the provisioning service(s). We demonstrate through extensive experiments that our system can add/remove/shift both servers and caches within an entire multi-service application under varying workloads to meet the SLA target and improve resource utilization. [[EENNDD]] resource provisioning; multi-service application"}, "Penyediaan sumber autonomi untuk aplikasi web pelbagai perkhidmatan Penyediaan sumber dinamik bertujuan untuk mengekalkan masa respons akhir ke akhir aplikasi web dalam SLA yang telah ditentukan. Walaupun topik ini telah dipelajari dengan baik untuk aplikasi monolitik, penyediaan sumber untuk aplikasi yang terdiri daripada pelbagai perkhidmatan tetap menjadi cabaran. Apabila SLA dilanggar, seseorang harus memutuskan perkhidmatan mana yang harus disediakan semula untuk mendapatkan kesan yang optimum. Kami mencadangkan untuk menetapkan SLA hanya untuk perkhidmatan front-end. Perkhidmatan lain tidak diberikan objektif masa tindak balas tertentu. Perkhidmatan bertanggungjawab secara automatik untuk operasi penyediaan mereka sendiri dan secara kolaboratif merundingkan objektif prestasi antara satu sama lain untuk memutuskan perkhidmatan penyediaan. Kami menunjukkan melalui eksperimen yang luas bahawa sistem kami dapat menambahkan / membuang / mengalihkan kedua-dua pelayan dan cache dalam keseluruhan aplikasi berbilang perkhidmatan di bawah bebanan kerja yang berbeza-beza untuk memenuhi sasaran SLA dan meningkatkan penggunaan sumber. [[EENNDD]] penyediaan sumber; aplikasi pelbagai perkhidmatan"], [{"string": "Privacy-enhancing personalized web search Personalized web search is a promising way to improve search quality by customizing search results for people with individual information goals. However, users are uncomfortable with exposing private preference information to search engines. On the other hand, privacy is not absolute, and often can be compromised if there is a gain in service or profitability to the user. Thus, a balance must be struck between search quality and privacy protection. This paper presents a scalable way for users to automatically build rich user profiles. These profiles summarize a user.s interests into a hierarchical organization according to specific interests. Two parameters for specifying privacy requirements are proposed to help the user to choose the content and degree of detail of the profile information that is exposed to the search engine. Experiments showed that the user profile improved search quality when compared to standard MSN rankings. More importantly, results verified our hypothesis that a significant improvement on search quality can be achieved by only sharing some higher-level user profile information, which is potentially less sensitive than detailed personal information.", "keywords": ["personalized search", "privacy", "hierarchical user profile"], "combined": "Privacy-enhancing personalized web search Personalized web search is a promising way to improve search quality by customizing search results for people with individual information goals. However, users are uncomfortable with exposing private preference information to search engines. On the other hand, privacy is not absolute, and often can be compromised if there is a gain in service or profitability to the user. Thus, a balance must be struck between search quality and privacy protection. This paper presents a scalable way for users to automatically build rich user profiles. These profiles summarize a user.s interests into a hierarchical organization according to specific interests. Two parameters for specifying privacy requirements are proposed to help the user to choose the content and degree of detail of the profile information that is exposed to the search engine. Experiments showed that the user profile improved search quality when compared to standard MSN rankings. More importantly, results verified our hypothesis that a significant improvement on search quality can be achieved by only sharing some higher-level user profile information, which is potentially less sensitive than detailed personal information. [[EENNDD]] personalized search; privacy; hierarchical user profile"}, "Pencarian laman web peribadi yang meningkatkan privasi Carian web yang diperibadikan adalah cara yang menjanjikan untuk meningkatkan kualiti carian dengan menyesuaikan hasil carian untuk orang yang mempunyai tujuan maklumat individu. Namun, pengguna tidak selesa dengan mendedahkan maklumat pilihan peribadi ke enjin carian. Sebaliknya, privasi tidak mutlak, dan sering dapat dikompromikan jika terdapat keuntungan dalam perkhidmatan atau keuntungan kepada pengguna. Oleh itu, keseimbangan mesti dicapai antara kualiti carian dan perlindungan privasi. Makalah ini menyajikan cara berskala bagi pengguna untuk membina profil pengguna kaya secara automatik. Profil ini merangkum minat pengguna menjadi organisasi hierarki mengikut minat tertentu. Dua parameter untuk menentukan keperluan privasi dicadangkan untuk membantu pengguna memilih kandungan dan tahap perincian maklumat profil yang terkena mesin pencari. Eksperimen menunjukkan bahawa profil pengguna meningkatkan kualiti carian jika dibandingkan dengan kedudukan MSN standard. Lebih penting lagi, hasil mengesahkan hipotesis kami bahawa peningkatan kualiti carian yang ketara dapat dicapai dengan hanya berkongsi beberapa maklumat profil pengguna tahap tinggi, yang berpotensi kurang sensitif daripada maklumat peribadi terperinci. [[EENNDD]] carian diperibadikan; privasi; profil pengguna hierarki"], [{"string": "Texquery: a full-text search extension to xquery No contact information provided yet.", "keywords": ["full-text search", "xquery", "miscellaneous"], "combined": "Texquery: a full-text search extension to xquery No contact information provided yet. [[EENNDD]] full-text search; xquery; miscellaneous"}, "Texquery: sambungan carian teks penuh ke xquery Belum ada maklumat hubungan yang diberikan. [[EENNDD]] carian teks penuh; xquery; pelbagai"], [{"string": "CWS: a comparative web search system No contact information provided yet.", "keywords": ["search engine", "on-line information services", "information search and retrieval", "comparative web search", "clustering", "keyphrase extraction"], "combined": "CWS: a comparative web search system No contact information provided yet. [[EENNDD]] search engine; on-line information services; information search and retrieval; comparative web search; clustering; keyphrase extraction"}, "CWS: sistem carian web perbandingan Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] enjin carian; perkhidmatan maklumat dalam talian; pencarian dan pengambilan maklumat; carian web perbandingan; pengelompokan; pengekstrakan papan kekunci"], [{"string": "A word at a time: computing word relatedness using temporal semantic analysis Computing the degree of semantic relatedness of words is a key functionality of many language applications such as search, clustering, and disambiguation. Previous approaches to computing semantic relatedness mostly used static language resources, while essentially ignoring their temporal aspects. We believe that a considerable amount of relatedness information can also be found in studying patterns of word usage over time. Consider, for instance, a newspaper archive spanning many years. Two words such as \"war\" and \"peace\" might rarely co-occur in the same articles, yet their patterns of use over time might be similar. In this paper, we propose a new semantic relatedness model, Temporal Semantic Analysis (TSA), which captures this temporal information. The previous state of the art method, Explicit Semantic Analysis (ESA), represented word semantics as a vector of concepts. TSA uses a more refined representation, where each concept is no longer scalar, but is instead represented as time series over a corpus of temporally-ordered documents. To the best of our knowledge, this is the first attempt to incorporate temporal evidence into models of semantic relatedness. Empirical evaluation shows that TSA provides consistent improvements over the state of the art ESA results on multiple benchmarks.", "keywords": ["semantic analysis", "content analysis and indexing", "temporal dynamics", "temporal semantics", "word relatedness", "semantic similarity"], "combined": "A word at a time: computing word relatedness using temporal semantic analysis Computing the degree of semantic relatedness of words is a key functionality of many language applications such as search, clustering, and disambiguation. Previous approaches to computing semantic relatedness mostly used static language resources, while essentially ignoring their temporal aspects. We believe that a considerable amount of relatedness information can also be found in studying patterns of word usage over time. Consider, for instance, a newspaper archive spanning many years. Two words such as \"war\" and \"peace\" might rarely co-occur in the same articles, yet their patterns of use over time might be similar. In this paper, we propose a new semantic relatedness model, Temporal Semantic Analysis (TSA), which captures this temporal information. The previous state of the art method, Explicit Semantic Analysis (ESA), represented word semantics as a vector of concepts. TSA uses a more refined representation, where each concept is no longer scalar, but is instead represented as time series over a corpus of temporally-ordered documents. To the best of our knowledge, this is the first attempt to incorporate temporal evidence into models of semantic relatedness. Empirical evaluation shows that TSA provides consistent improvements over the state of the art ESA results on multiple benchmarks. [[EENNDD]] semantic analysis; content analysis and indexing; temporal dynamics; temporal semantics; word relatedness; semantic similarity"}, "Satu kata pada satu masa: pengkomputeran keterkaitan kata menggunakan analisis semantik temporal Mengira tahap keterkaitan semantik kata adalah fungsi utama banyak aplikasi bahasa seperti pencarian, pengelompokan, dan disambiguasi. Pendekatan sebelumnya untuk pengkomputeran berkaitan semantik kebanyakan menggunakan sumber bahasa statik, sementara pada dasarnya mengabaikan aspek temporal mereka. Kami percaya bahawa sejumlah besar keterkaitan juga terdapat dalam mengkaji corak penggunaan kata dari masa ke masa. Pertimbangkan, misalnya, arkib surat khabar yang telah berlangsung bertahun-tahun. Dua perkataan seperti \"perang\" dan \"perdamaian\" mungkin jarang berlaku dalam artikel yang sama, namun corak penggunaannya dari masa ke masa mungkin serupa. Dalam makalah ini, kami mencadangkan model hubungan semantik baru, Analisis Semantik Temporal (TSA), yang menangkap maklumat temporal ini. Kaedah canggih sebelumnya, Analisis Semantik Eksplisit (ESA), mewakili semantik kata sebagai vektor konsep. TSA menggunakan perwakilan yang lebih halus, di mana setiap konsep tidak lagi skalar, tetapi sebaliknya diwakili sebagai siri masa di atas sejumlah dokumen yang dipesan secara sementara. Sepengetahuan kami, ini adalah percubaan pertama untuk memasukkan bukti temporal ke dalam model hubungan semantik. Penilaian empirikal menunjukkan bahawa TSA memberikan peningkatan yang konsisten terhadap hasil ESA yang canggih pada pelbagai tanda aras. [[EENNDD]] analisis semantik; analisis kandungan dan pengindeksan; dinamika temporal; semantik temporal; perkaitan perkataan; persamaan semantik"], [{"string": "Earthquake shakes Twitter users: real-time event detection by social sensors Twitter, a popular microblogging service, has received much attention recently. An important characteristic of Twitter is its real-time nature. For example, when an earthquake occurs, people make many Twitter posts (tweets) related to the earthquake, which enables detection of earthquake occurrence promptly, simply by observing the tweets. As described in this paper, we investigate the real-time interaction of events such as earthquakes in Twitter and propose an algorithm to monitor tweets and to detect a target event. To detect a target event, we devise a classifier of tweets based on features such as the keywords in a tweet, the number of words, and their context. Subsequently, we produce a probabilistic spatiotemporal model for the target event that can find the center and the trajectory of the event location. We consider each Twitter user as a sensor and apply Kalman filtering and particle filtering, which are widely used for location estimation in ubiquitous/pervasive computing. The particle filter works better than other comparable methods for estimating the centers of earthquakes and the trajectories of typhoons. As an application, we construct an earthquake reporting system in Japan. Because of the numerous earthquakes and the large number of Twitter users throughout the country, we can detect an earthquake with high probability (96% of earthquakes of Japan Meteorological Agency (JMA) seismic intensity scale 3 or more are detected) merely by monitoring tweets. Our system detects earthquakes promptly and sends e-mails to registered users. Notification is delivered much faster than the announcements that are broadcast by the JMA.", "keywords": ["social sensor", "location estimation", "event detection", "twitter", "earthquake"], "combined": "Earthquake shakes Twitter users: real-time event detection by social sensors Twitter, a popular microblogging service, has received much attention recently. An important characteristic of Twitter is its real-time nature. For example, when an earthquake occurs, people make many Twitter posts (tweets) related to the earthquake, which enables detection of earthquake occurrence promptly, simply by observing the tweets. As described in this paper, we investigate the real-time interaction of events such as earthquakes in Twitter and propose an algorithm to monitor tweets and to detect a target event. To detect a target event, we devise a classifier of tweets based on features such as the keywords in a tweet, the number of words, and their context. Subsequently, we produce a probabilistic spatiotemporal model for the target event that can find the center and the trajectory of the event location. We consider each Twitter user as a sensor and apply Kalman filtering and particle filtering, which are widely used for location estimation in ubiquitous/pervasive computing. The particle filter works better than other comparable methods for estimating the centers of earthquakes and the trajectories of typhoons. As an application, we construct an earthquake reporting system in Japan. Because of the numerous earthquakes and the large number of Twitter users throughout the country, we can detect an earthquake with high probability (96% of earthquakes of Japan Meteorological Agency (JMA) seismic intensity scale 3 or more are detected) merely by monitoring tweets. Our system detects earthquakes promptly and sends e-mails to registered users. Notification is delivered much faster than the announcements that are broadcast by the JMA. [[EENNDD]] social sensor; location estimation; event detection; twitter; earthquake"}, "Gempa menggegarkan pengguna Twitter: pengesanan peristiwa masa nyata oleh sensor sosial Twitter, perkhidmatan microblogging yang popular, telah mendapat banyak perhatian baru-baru ini. Ciri penting Twitter adalah sifat masa nyata. Sebagai contoh, ketika gempa terjadi, orang membuat banyak catatan Twitter (tweet) yang berkaitan dengan gempa, yang memungkinkan pengesanan kejadian gempa dengan segera, hanya dengan memerhatikan tweet tersebut. Seperti yang dijelaskan dalam makalah ini, kami menyelidiki interaksi masa nyata peristiwa seperti gempa bumi di Twitter dan mencadangkan algoritma untuk memantau tweet dan untuk mengesan peristiwa sasaran. Untuk mengesan peristiwa sasaran, kami merancang pengkelasan tweet berdasarkan ciri seperti kata kunci dalam tweet, jumlah kata, dan konteksnya. Selepas itu, kami menghasilkan model spatiotemporal probabilistik untuk peristiwa sasaran yang dapat mencari pusat dan lintasan lokasi acara. Kami menganggap setiap pengguna Twitter sebagai sensor dan menerapkan penyaringan Kalman dan penyaringan partikel, yang banyak digunakan untuk perkiraan lokasi dalam pengkomputeran di mana-mana / meresap. Penapis zarah berfungsi lebih baik daripada kaedah setanding lain untuk menganggarkan pusat gempa bumi dan lintasan taufan. Sebagai aplikasi, kami membina sistem pelaporan gempa di Jepun. Kerana gempa bumi yang banyak dan jumlah pengguna Twitter yang besar di seluruh negara, kita dapat mengesan gempa dengan kebarangkalian yang tinggi (96% gempa bumi Badan Meteorologi Jepun (JMA) skala intensiti seismik 3 atau lebih dikesan) hanya dengan memantau tweet. Sistem kami mengesan gempa bumi dengan segera dan menghantar e-mel kepada pengguna yang berdaftar. Pemberitahuan disampaikan lebih cepat daripada pengumuman yang disiarkan oleh JMA. [[EENNDD]] sensor sosial; anggaran lokasi; pengesanan peristiwa; twitter; gempa bumi"], [{"string": "Performance of compressed inverted list caching in search engines Due to the rapid growth in the size of the web, web search engines are facing enormous performance challenges. The larger engines in particular have to be able to process tens of thousands of queries per second on tens of billions of documents, making query throughput a critical issue. To satisfy this heavy workload, search engines use a variety of performance optimizations including index compression, caching, and early termination.", "keywords": ["search engines", "inverted index", "index compression", "index caching"], "combined": "Performance of compressed inverted list caching in search engines Due to the rapid growth in the size of the web, web search engines are facing enormous performance challenges. The larger engines in particular have to be able to process tens of thousands of queries per second on tens of billions of documents, making query throughput a critical issue. To satisfy this heavy workload, search engines use a variety of performance optimizations including index compression, caching, and early termination. [[EENNDD]] search engines; inverted index; index compression; index caching"}, "Prestasi cache senarai terbalik yang dimampatkan di mesin pencari Kerana pertumbuhan saiz web yang pesat, mesin carian web menghadapi cabaran prestasi yang sangat besar. Enjin yang lebih besar khususnya harus dapat memproses puluhan ribu pertanyaan sesaat pada puluhan bilion dokumen, menjadikan throughput pertanyaan menjadi masalah penting. Untuk memenuhi beban kerja yang berat ini, mesin pencari menggunakan pelbagai pengoptimuman prestasi termasuk pemampatan indeks, caching, dan penamatan awal. [[EENNDD]] enjin carian; indeks terbalik; pemampatan indeks; caching indeks"], [{"string": "ASDL: a wide spectrum language for designing web services No contact information provided yet.", "keywords": ["computational model", "asdl", "wide spectrum", "web services", "models of computation", "semantics of programming languages", "formal languages"], "combined": "ASDL: a wide spectrum language for designing web services No contact information provided yet. [[EENNDD]] computational model; asdl; wide spectrum; web services; models of computation; semantics of programming languages; formal languages"}, "ASDL: bahasa spektrum luas untuk merancang perkhidmatan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] model pengiraan; asdl; spektrum luas; perkhidmatan web; model pengiraan; semantik bahasa pengaturcaraan; bahasa formal"], [{"string": "SOFIE: a self-organizing framework for information extraction This paper presents SOFIE, a system for automated ontology extension. SOFIE can parse natural language documents, extract ontological facts from them and link the facts into an ontology. SOFIE uses logical reasoning on the existing knowledge and on the new knowledge in order to disambiguate words to their most probable meaning, to reason on the meaning of text patterns and to take into account world knowledge axioms. This allows SOFIE to check the plausibility of hypotheses and to avoid inconsistencies with the ontology. The framework of SOFIE unites the paradigms of pattern matching, word sense disambiguation and ontological reasoning in one unified model. Our experiments show that SOFIE delivers high-quality output, even from unstructured Internet documents.", "keywords": ["ontology", "general", "information extraction", "automated reasoning"], "combined": "SOFIE: a self-organizing framework for information extraction This paper presents SOFIE, a system for automated ontology extension. SOFIE can parse natural language documents, extract ontological facts from them and link the facts into an ontology. SOFIE uses logical reasoning on the existing knowledge and on the new knowledge in order to disambiguate words to their most probable meaning, to reason on the meaning of text patterns and to take into account world knowledge axioms. This allows SOFIE to check the plausibility of hypotheses and to avoid inconsistencies with the ontology. The framework of SOFIE unites the paradigms of pattern matching, word sense disambiguation and ontological reasoning in one unified model. Our experiments show that SOFIE delivers high-quality output, even from unstructured Internet documents. [[EENNDD]] ontology; general; information extraction; automated reasoning"}, "SOFIE: kerangka penyusunan diri untuk pengekstrakan maklumat Kertas ini membentangkan SOFIE, sistem untuk pengembangan ontologi automatik. SOFIE dapat menghuraikan dokumen bahasa semula jadi, mengambil fakta ontologi daripadanya dan menghubungkan fakta tersebut menjadi ontologi. SOFIE menggunakan penaakulan logik pada pengetahuan yang ada dan pengetahuan baru untuk menghilangkan kata-kata dengan makna yang paling mungkin, untuk memberi alasan mengenai makna corak teks dan mengambil kira aksioma pengetahuan dunia. Ini membolehkan SOFIE memeriksa kemungkinan hipotesis dan untuk mengelakkan percanggahan dengan ontologi. Kerangka SOFIE menyatukan paradigma pemadanan corak, disambiguasi pengertian kata dan penaakulan ontologi dalam satu model bersatu. Eksperimen kami menunjukkan bahawa SOFIE memberikan output berkualiti tinggi, walaupun dari dokumen Internet yang tidak berstruktur. [[EENNDD]] ontologi; umum; pengekstrakan maklumat; penaakulan automatik"], [{"string": "Detecting nepotistic links by language model disagreement No contact information provided yet.", "keywords": ["language modeling", "link spam", "information search and retrieval", "anchor text"], "combined": "Detecting nepotistic links by language model disagreement No contact information provided yet. [[EENNDD]] language modeling; link spam; information search and retrieval; anchor text"}, "Mengesan pautan nepotistik dengan perselisihan model bahasa Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] pemodelan bahasa; pautan spam; pencarian dan pengambilan maklumat; teks sauh"], [{"string": "Periodic transfers in mobile applications: network-wide origin, impact, and optimization Cellular networks employ a specific radio resource management policy distinguishing them from wired and Wi-Fi networks. A lack of awareness of this important mechanism potentially leads to resource-inefficient mobile applications. We perform the first network-wide, large-scale investigation of a particular type of application traffic pattern called periodic transfers where a handset periodically exchanges some data with a remote server every t seconds. Using packet traces containing 1.5 billion packets collected from a commercial cellular carrier, we found that periodic transfers are very prevalent in today's smartphone traffic. However, they are extremely resource-inefficient for both the network and end-user devices even though they predominantly generate very little traffic. This somewhat counter-intuitive behavior is a direct consequence of the adverse interaction between such periodic transfer patterns and the cellular network radio resource management policy. For example, for popular smartphone applications such as Facebook, periodic transfers account for only 1.7% of the overall traffic volume but contribute to 30% of the total handset radio energy consumption. We found periodic transfers are generated for various reasons such as keep-alive, polling, and user behavior measurements. We further investigate the potential of various traffic shaping and resource control algorithms. Depending on their traffic patterns, applications exhibit disparate responses to optimization strategies. Jointly using several strategies with moderate aggressiveness can eliminate almost all energy impact of periodic transfers for popular applications such as Facebook and Pandora.", "keywords": ["smartphone applications", "rrc state machine", "radio resource optimization", "3g networks", "periodicity detection", "periodic transfers"], "combined": "Periodic transfers in mobile applications: network-wide origin, impact, and optimization Cellular networks employ a specific radio resource management policy distinguishing them from wired and Wi-Fi networks. A lack of awareness of this important mechanism potentially leads to resource-inefficient mobile applications. We perform the first network-wide, large-scale investigation of a particular type of application traffic pattern called periodic transfers where a handset periodically exchanges some data with a remote server every t seconds. Using packet traces containing 1.5 billion packets collected from a commercial cellular carrier, we found that periodic transfers are very prevalent in today's smartphone traffic. However, they are extremely resource-inefficient for both the network and end-user devices even though they predominantly generate very little traffic. This somewhat counter-intuitive behavior is a direct consequence of the adverse interaction between such periodic transfer patterns and the cellular network radio resource management policy. For example, for popular smartphone applications such as Facebook, periodic transfers account for only 1.7% of the overall traffic volume but contribute to 30% of the total handset radio energy consumption. We found periodic transfers are generated for various reasons such as keep-alive, polling, and user behavior measurements. We further investigate the potential of various traffic shaping and resource control algorithms. Depending on their traffic patterns, applications exhibit disparate responses to optimization strategies. Jointly using several strategies with moderate aggressiveness can eliminate almost all energy impact of periodic transfers for popular applications such as Facebook and Pandora. [[EENNDD]] smartphone applications; rrc state machine; radio resource optimization; 3g networks; periodicity detection; periodic transfers"}, "Pemindahan berkala dalam aplikasi mudah alih: asal, kesan, dan pengoptimuman seluruh rangkaian Rangkaian selular menggunakan dasar pengurusan sumber radio tertentu yang membezakannya dari rangkaian berwayar dan Wi-Fi. Kurangnya kesedaran mengenai mekanisme penting ini berpotensi menyebabkan aplikasi mudah alih yang tidak cekap sumber. Kami melakukan penyiasatan berskala besar pertama di seluruh rangkaian mengenai jenis corak lalu lintas aplikasi tertentu yang disebut pemindahan berkala di mana telefon bimbit secara berkala menukar beberapa data dengan pelayan jarak jauh setiap t saat. Dengan menggunakan jejak paket yang mengandungi 1.5 bilion paket yang dikumpulkan dari pembawa selular komersial, kami mendapati bahawa pemindahan berkala sangat lazim dalam lalu lintas telefon pintar masa kini. Walau bagaimanapun, mereka sangat tidak cekap sumber untuk kedua-dua rangkaian dan peranti pengguna akhir walaupun kebanyakannya menghasilkan lalu lintas yang sangat sedikit. Tingkah laku yang agak berlawanan dengan intuisi ini adalah akibat langsung dari interaksi buruk antara corak pemindahan berkala seperti itu dan dasar pengurusan sumber radio rangkaian selular. Sebagai contoh, untuk aplikasi telefon pintar yang popular seperti Facebook, pemindahan berkala hanya menyumbang 1.7% daripada jumlah keseluruhan lalu lintas tetapi menyumbang kepada 30% daripada jumlah penggunaan tenaga radio telefon bimbit. Kami mendapati pemindahan berkala dihasilkan kerana pelbagai sebab seperti pengawalan hidup, pengundian, dan pengukuran tingkah laku pengguna. Kami seterusnya menyiasat potensi pelbagai algoritma pembentukan lalu lintas dan kawalan sumber. Bergantung pada corak lalu lintas mereka, aplikasi menunjukkan tindak balas yang berbeza terhadap strategi pengoptimuman. Bersama menggunakan beberapa strategi dengan tahap agresif yang sederhana dapat menghilangkan hampir semua kesan tenaga dari pemindahan berkala untuk aplikasi popular seperti Facebook dan Pandora. [[EENNDD]] aplikasi telefon pintar; mesin keadaan rrc; pengoptimuman sumber radio; Rangkaian 3g; pengesanan berkala; pemindahan berkala"], [{"string": "Automatically learning document taxonomies for hierarchical classification No contact information provided yet.", "keywords": ["automatic taxonomy learning", "information search and retrieval", "hierarchical classification", "content analysis and indexing"], "combined": "Automatically learning document taxonomies for hierarchical classification No contact information provided yet. [[EENNDD]] automatic taxonomy learning; information search and retrieval; hierarchical classification; content analysis and indexing"}, "Pembelajaran taksonomi dokumen secara automatik untuk klasifikasi hierarki Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pembelajaran taksonomi automatik; pencarian dan pengambilan maklumat; pengkelasan hierarki; analisis kandungan dan pengindeksan"], [{"string": "SemRank: ranking complex relationship search results on the semantic web No contact information provided yet.", "keywords": ["information search and retrieval", "semantic relationship search", "semantic similarity", "semantic summary", "semantic ranking", "semantic associations search", "path expression tree", "semantic web", "semantic match", "ranking complex relationships", "discovery query", "semrank"], "combined": "SemRank: ranking complex relationship search results on the semantic web No contact information provided yet. [[EENNDD]] information search and retrieval; semantic relationship search; semantic similarity; semantic summary; semantic ranking; semantic associations search; path expression tree; semantic web; semantic match; ranking complex relationships; discovery query; semrank"}, "SemRank: peringkat hasil carian hubungan kompleks di web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] carian dan pengambilan maklumat; carian hubungan semantik; kesamaan semantik; ringkasan semantik; peringkat semantik; carian persatuan semantik; pokok ungkapan jalan; web semantik; pertandingan semantik; peringkat hubungan kompleks; pertanyaan penemuan; semrank"], [{"string": "Searching with numbers No contact information provided yet.", "keywords": ["number-theoretic computations"], "combined": "Searching with numbers No contact information provided yet. [[EENNDD]] number-theoretic computations"}, "Mencari dengan nombor Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengiraan nombor teori"], [{"string": "A smart hill-climbing algorithm for application server configuration No contact information provided yet.", "keywords": ["system configuration", "gradient method", "importance sampling", "simulated annealing", "automatic tuning"], "combined": "A smart hill-climbing algorithm for application server configuration No contact information provided yet. [[EENNDD]] system configuration; gradient method; importance sampling; simulated annealing; automatic tuning"}, "Algoritma pendakian bukit pintar untuk konfigurasi pelayan aplikasi Belum ada maklumat hubungan. [[EENNDD]] konfigurasi sistem; kaedah kecerunan; persampelan kepentingan; penyepuhlindapan simulasi; penalaan automatik"], [{"string": "A convenient method for securely managing passwords No contact information provided yet.", "keywords": ["password security", "website user authentication"], "combined": "A convenient method for securely managing passwords No contact information provided yet. [[EENNDD]] password security; website user authentication"}, "Kaedah yang mudah untuk menguruskan kata laluan dengan selamat Belum ada maklumat hubungan yang diberikan. [[EENNDD]] keselamatan kata laluan; pengesahan pengguna laman web"], [{"string": "An ontology for internal and external business processes No contact information provided yet.", "keywords": ["ontology", "workflow modelling", "meta model integration", "choreography"], "combined": "An ontology for internal and external business processes No contact information provided yet. [[EENNDD]] ontology; workflow modelling; meta model integration; choreography"}, "Ontologi untuk proses perniagaan dalaman dan luaran Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ontologi; pemodelan aliran kerja; integrasi model meta; koreografi"], [{"string": "Bootstrapping semantics on the web: meaning elicitation from schemas No contact information provided yet.", "keywords": ["meaning elicitation", "semantic web", "schema matching"], "combined": "Bootstrapping semantics on the web: meaning elicitation from schemas No contact information provided yet. [[EENNDD]] meaning elicitation; semantic web; schema matching"}, "Semantik bootstrapping di web: bermaksud penggambaran dari skema Belum ada maklumat hubungan yang diberikan. [[EENNDD]] bermaksud elokasi; web semantik; pemadanan skema"], [{"string": "PRIVE: anonymous location-based queries in distributed mobile systems Nowadays, mobile users with global positioning devices canaccess Location Based Services (LBS) and query about pointsof interest in their proximity. For such applications to succeed,privacy and confidentiality are essential. Encryptionalone is not adequate; although it safeguards the systemagainst eavesdroppers, the queries themselves may disclosethe location and identity of the user. Recently, there havebeen proposed centralized architectures based on K-anonymity,which utilize an intermediate anonymizer between themobile users and the LBS. However, the anonymizer mustbe updated continuously with the current locations of allusers. Moreover, the complete knowledge of the entire systemposes a security threat, if the anonymizer is compromised.In this paper we address two issues: (i) We show thatexisting approaches may fail to provide spatial anonymityfor some distributions of user locations and describe a noveltechnique which solves this problem. (ii) We propose Prive,a decentralized architecture for preserving the anonymityof users issuing spatial queries to LBS. Mobile users self-organizeinto an overlay network with good fault toleranceand load balancing properties. Prive avoids the bottleneckcaused by centralized techniques both in terms of anonymizationand location updates. Moreover, the system state isdistributed in numerous users, rendering Prive resilient toattacks. Extensive experimental studies suggest that Priveis applicable to real-life scenarios with large populations ofmobile users.", "keywords": ["peer-to-peer", "anonymity", "privacy", "dpatial databases"], "combined": "PRIVE: anonymous location-based queries in distributed mobile systems Nowadays, mobile users with global positioning devices canaccess Location Based Services (LBS) and query about pointsof interest in their proximity. For such applications to succeed,privacy and confidentiality are essential. Encryptionalone is not adequate; although it safeguards the systemagainst eavesdroppers, the queries themselves may disclosethe location and identity of the user. Recently, there havebeen proposed centralized architectures based on K-anonymity,which utilize an intermediate anonymizer between themobile users and the LBS. However, the anonymizer mustbe updated continuously with the current locations of allusers. Moreover, the complete knowledge of the entire systemposes a security threat, if the anonymizer is compromised.In this paper we address two issues: (i) We show thatexisting approaches may fail to provide spatial anonymityfor some distributions of user locations and describe a noveltechnique which solves this problem. (ii) We propose Prive,a decentralized architecture for preserving the anonymityof users issuing spatial queries to LBS. Mobile users self-organizeinto an overlay network with good fault toleranceand load balancing properties. Prive avoids the bottleneckcaused by centralized techniques both in terms of anonymizationand location updates. Moreover, the system state isdistributed in numerous users, rendering Prive resilient toattacks. Extensive experimental studies suggest that Priveis applicable to real-life scenarios with large populations ofmobile users. [[EENNDD]] peer-to-peer; anonymity; privacy; dpatial databases"}, "PRIVE: pertanyaan berdasarkan lokasi tanpa nama dalam sistem mudah alih yang diedarkan Pada masa ini, pengguna mudah alih dengan peranti penentu kedudukan global dapat mengakses Perkhidmatan Berasaskan Lokasi (LBS) dan membuat pertanyaan mengenai tempat-tempat yang berminat dengan jarak mereka. Agar aplikasi seperti itu berjaya, privasi dan kerahsiaan sangat penting. Encryptionalone tidak mencukupi; walaupun ia melindungi sistem dari penyadap penyaring, pertanyaan itu sendiri mungkin mendedahkan lokasi dan identiti pengguna. Baru-baru ini, ada cadangan seni bina terpusat berdasarkan K-anonymity, yang menggunakan anonymizer perantara antara pengguna telefon bimbit dan LBS. Walau bagaimanapun, penyamar namanya mesti dikemas kini secara berterusan dengan lokasi semasa pengguna. Lebih-lebih lagi, pengetahuan lengkap mengenai keseluruhan sistem ini mengancam keselamatan, jika penyamaran dikompromikan. Dalam makalah ini kita menangani dua masalah: (i) Kami menunjukkan bahawa pendekatan yang ada mungkin gagal memberikan anonimiti ruang untuk beberapa pengedaran lokasi pengguna dan menerangkan teknik novel yang menyelesaikan masalah ini. (ii) Kami mencadangkan Prive, sebuah seni bina yang terdesentralisasi untuk menjaga kerahsiaan pengguna yang mengeluarkan pertanyaan spasial kepada LBS. Pengguna mudah alih mengatur sendiri ke rangkaian overlay dengan toleransi kesalahan yang baik dan sifat pengimbang beban. Prive mengelakkan masalah yang disebabkan oleh teknik terpusat baik dari segi anonimisasi dan kemas kini lokasi. Lebih-lebih lagi, keadaan sistem diagihkan dalam banyak pengguna, menjadikan Prive tahan terhadap serangan. Kajian eksperimen yang meluas menunjukkan bahawa Priveis berlaku untuk senario kehidupan sebenar dengan populasi pengguna bergerak yang besar. [[EENNDD]] rakan sebaya; tanpa nama; privasi; pangkalan data dpatial"], [{"string": "Online modeling of proactive moderation system for auction fraud detection We consider the problem of building online machine-learned models for detecting auction frauds in e-commence web sites. Since the emergence of the world wide web, online shopping and online auction have gained more and more popularity. While people are enjoying the benefits from online trading, criminals are also taking advantages to conduct fraudulent activities against honest parties to obtain illegal profit. Hence proactive fraud-detection moderation systems are commonly applied in practice to detect and prevent such illegal and fraud activities. Machine-learned models, especially those that are learned online, are able to catch frauds more efficiently and quickly than human-tuned rule-based systems. In this paper, we propose an online probit model framework which takes online feature selection, coefficient bounds from human knowledge and multiple instance learning into account simultaneously. By empirical experiments on a real-world online auction fraud detection data we show that this model can potentially detect more frauds and significantly reduce customer complaints compared to several baseline models and the human-tuned rule-based system.", "keywords": ["online feature selection", "online auction", "learning", "multiple instance learning", "fraudulence detection", "online modeling"], "combined": "Online modeling of proactive moderation system for auction fraud detection We consider the problem of building online machine-learned models for detecting auction frauds in e-commence web sites. Since the emergence of the world wide web, online shopping and online auction have gained more and more popularity. While people are enjoying the benefits from online trading, criminals are also taking advantages to conduct fraudulent activities against honest parties to obtain illegal profit. Hence proactive fraud-detection moderation systems are commonly applied in practice to detect and prevent such illegal and fraud activities. Machine-learned models, especially those that are learned online, are able to catch frauds more efficiently and quickly than human-tuned rule-based systems. In this paper, we propose an online probit model framework which takes online feature selection, coefficient bounds from human knowledge and multiple instance learning into account simultaneously. By empirical experiments on a real-world online auction fraud detection data we show that this model can potentially detect more frauds and significantly reduce customer complaints compared to several baseline models and the human-tuned rule-based system. [[EENNDD]] online feature selection; online auction; learning; multiple instance learning; fraudulence detection; online modeling"}, "Pemodelan dalam talian sistem penyederhanaan proaktif untuk pengesanan penipuan lelong Kami mempertimbangkan masalah membina model pembelajaran mesin dalam talian untuk mengesan penipuan lelong di laman web e-commence. Sejak kemunculan web di seluruh dunia, membeli-belah dalam talian dan lelong dalam talian telah mendapat semakin banyak populariti. Walaupun orang menikmati keuntungan dari perdagangan dalam talian, penjenayah juga mengambil kesempatan untuk melakukan aktiviti penipuan terhadap pihak yang jujur untuk mendapatkan keuntungan haram. Oleh itu, sistem penyederhanaan pengesanan penipuan proaktif biasanya digunakan dalam praktik untuk mengesan dan mencegah aktiviti haram dan penipuan tersebut. Model yang dipelajari oleh mesin, terutama yang dipelajari dalam talian, dapat menangkap penipuan dengan lebih berkesan dan cepat daripada sistem berdasarkan peraturan yang diselaraskan oleh manusia. Dalam makalah ini, kami mencadangkan kerangka model probit dalam talian yang mempertimbangkan pemilihan ciri dalam talian, batasan pekali dari pengetahuan manusia dan pembelajaran pelbagai contoh secara serentak. Dengan eksperimen empirikal pada data pengesanan penipuan lelongan dalam talian nyata, kami menunjukkan bahawa model ini berpotensi mengesan lebih banyak penipuan dan mengurangkan aduan pelanggan dengan ketara berbanding dengan beberapa model asas dan sistem berasaskan peraturan yang disesuaikan dengan manusia. [[EENNDD]] pemilihan ciri dalam talian; lelong dalam talian; belajar; pembelajaran pelbagai contoh; pengesanan penipuan; pemodelan dalam talian"], [{"string": "On deep annotation No contact information provided yet.", "keywords": ["annotation", "wrapping", "document and text editing", "mapping and merging", "semantic web", "metadata", "information integration"], "combined": "On deep annotation No contact information provided yet. [[EENNDD]] annotation; wrapping; document and text editing; mapping and merging; semantic web; metadata; information integration"}, "Mengenai penjelasan mendalam Belum ada maklumat hubungan yang diberikan. [[EENNDD]] anotasi; membungkus; penyuntingan dokumen dan teks; pemetaan dan penggabungan; web semantik; metadata; penyatuan maklumat"], [{"string": "Information transfer in social media Recent research has explored the increasingly important role of social media by examining the dynamics of individual and group behavior, characterizing patterns of information diffusion, and identifying influential individuals. In this paper we suggest a measure of causal relationships between nodes based on the information--theoretic notion of transfer entropy, or information transfer. This theoretically grounded measure is based on dynamic information, captures fine--grain notions of influence, and admits a natural, predictive interpretation. Networks inferred by transfer entropy can differ significantly from static friendship networks because most friendship links are not useful for predicting future dynamics. We demonstrate through analysis of synthetic and real-world data that transfer entropy reveals meaningful hidden network structures. In addition to altering our notion of who is influential, transfer entropy allows us to differentiate between weak influence over large groups and strong influence over small groups.", "keywords": ["spam", "prediction", "social networks", "causality", "point processes", "entropy"], "combined": "Information transfer in social media Recent research has explored the increasingly important role of social media by examining the dynamics of individual and group behavior, characterizing patterns of information diffusion, and identifying influential individuals. In this paper we suggest a measure of causal relationships between nodes based on the information--theoretic notion of transfer entropy, or information transfer. This theoretically grounded measure is based on dynamic information, captures fine--grain notions of influence, and admits a natural, predictive interpretation. Networks inferred by transfer entropy can differ significantly from static friendship networks because most friendship links are not useful for predicting future dynamics. We demonstrate through analysis of synthetic and real-world data that transfer entropy reveals meaningful hidden network structures. In addition to altering our notion of who is influential, transfer entropy allows us to differentiate between weak influence over large groups and strong influence over small groups. [[EENNDD]] spam; prediction; social networks; causality; point processes; entropy"}, "Pemindahan maklumat di media sosial Penyelidikan terbaru telah meneroka peranan media sosial yang semakin penting dengan mengkaji dinamika tingkah laku individu dan kumpulan, mencirikan corak penyebaran maklumat, dan mengenal pasti individu yang berpengaruh. Dalam makalah ini kami mencadangkan ukuran hubungan kausal antara node berdasarkan maklumat - tanggapan teori mengenai entropi pemindahan, atau pemindahan maklumat. Ukuran yang didasarkan secara teoritis ini berdasarkan maklumat dinamik, menangkap pengertian pengaruh halus, dan mengakui penafsiran ramalan semula jadi. Rangkaian yang disimpulkan oleh entropi pemindahan boleh berbeza secara signifikan dari rangkaian persahabatan statik kerana kebanyakan pautan persahabatan tidak berguna untuk meramalkan dinamika masa depan. Kami menunjukkan melalui analisis data sintetik dan dunia nyata bahawa pemindahan entropi menunjukkan struktur rangkaian tersembunyi yang bermakna. Selain mengubah tanggapan kita mengenai siapa yang berpengaruh, transfer entropi memungkinkan kita untuk membezakan antara pengaruh lemah terhadap kumpulan besar dan pengaruh kuat terhadap kumpulan kecil. [[EENNDD]] spam; ramalan; rangkaian sosial; sebab akibat; proses titik; entropi"], [{"string": "Mining photo-sharing websites to study ecological phenomena The popularity of social media websites like Flickr and Twitter has created enormous collections of user-generated content online. Latent in these content collections are observations of the world: each photo is a visual snapshot of what the world looked like at a particular point in time and space, for example, while each tweet is a textual expression of the state of a person and his or her environment. Aggregating these observations across millions of social sharing users could lead to new techniques for large-scale monitoring of the state of the world and how it is changing over time. In this paper we step towards that goal, showing that by analyzing the tags and image features of geo-tagged, time-stamped photos we can measure and quantify the occurrence of ecological phenomena including ground snow cover, snow fall and vegetation density. We compare several techniques for dealing with the large degree of noise in the dataset, and show how machine learning can be used to reduce errors caused by misleading tags and ambiguous visual content. We evaluate the accuracy of these techniques by comparing to ground truth data collected both by surface stations and by Earth-observing satellites. Besides the immediate application to ecology, our study gives insight into how to accurately crowd-source other types of information from large, noisy social sharing datasets.", "keywords": ["photo collections", "ecology", "social media", "crowd sourcing"], "combined": "Mining photo-sharing websites to study ecological phenomena The popularity of social media websites like Flickr and Twitter has created enormous collections of user-generated content online. Latent in these content collections are observations of the world: each photo is a visual snapshot of what the world looked like at a particular point in time and space, for example, while each tweet is a textual expression of the state of a person and his or her environment. Aggregating these observations across millions of social sharing users could lead to new techniques for large-scale monitoring of the state of the world and how it is changing over time. In this paper we step towards that goal, showing that by analyzing the tags and image features of geo-tagged, time-stamped photos we can measure and quantify the occurrence of ecological phenomena including ground snow cover, snow fall and vegetation density. We compare several techniques for dealing with the large degree of noise in the dataset, and show how machine learning can be used to reduce errors caused by misleading tags and ambiguous visual content. We evaluate the accuracy of these techniques by comparing to ground truth data collected both by surface stations and by Earth-observing satellites. Besides the immediate application to ecology, our study gives insight into how to accurately crowd-source other types of information from large, noisy social sharing datasets. [[EENNDD]] photo collections; ecology; social media; crowd sourcing"}, "Melombong laman web perkongsian foto untuk mengkaji fenomena ekologi. Populariti laman web media sosial seperti Flickr dan Twitter telah mencipta banyak koleksi kandungan yang dihasilkan pengguna dalam talian. Latent dalam koleksi kandungan ini adalah pemerhatian dunia: setiap foto adalah gambaran visual bagaimana dunia kelihatan pada titik waktu dan ruang tertentu, misalnya, sementara setiap tweet adalah ekspresi tekstual keadaan seseorang dan gambarnya atau persekitarannya. Menggabungkan pemerhatian ini di jutaan pengguna perkongsian sosial dapat menyebabkan teknik baru untuk pemantauan skala besar terhadap keadaan dunia dan bagaimana ia berubah dari masa ke masa. Dalam makalah ini kita melangkah ke arah tujuan itu, menunjukkan bahawa dengan menganalisis tanda dan ciri gambar foto berlabel geografis, kita dapat mengukur dan mengukur kejadian fenomena ekologi termasuk penutup salji tanah, turunnya salji dan kepadatan vegetasi. Kami membandingkan beberapa teknik untuk menangani tahap kebisingan yang besar dalam set data, dan menunjukkan bagaimana pembelajaran mesin dapat digunakan untuk mengurangkan kesalahan yang disebabkan oleh tag yang mengelirukan dan kandungan visual yang tidak jelas. Kami menilai ketepatan teknik ini dengan membandingkan dengan data kebenaran tanah yang dikumpulkan oleh stesen permukaan dan satelit pemerhatian Bumi. Selain penerapan langsung ke ekologi, kajian kami memberi gambaran tentang bagaimana mengumpulkan jenis maklumat lain dengan tepat dari kumpulan data perkongsian sosial yang besar dan bising. [[EENNDD]] koleksi gambar; ekologi; media sosial; sumber orang ramai"], [{"string": "Reliability analysis using weighted combinational models for web-based software In the past, some researches suggested that engineers can use combined software reliability growth models (SRGMs) to obtain more accurate reliability prediction during testing. In this paper, three weighted combinational models, namely, equal, linear, and nonlinear weight, are proposed for reliability estimation of web-based software. We further investigate the estimation accuracy of using genetic algorithm to determine the weight assignment for the proposed models. Preliminary result shows that the linearly and nonlinearly weighted combinational models have better prediction capability than single SRGM and equally weighted combinational model for web-based software.", "keywords": ["weighted combination", "genetic algorithm.", "software reliability growth model"], "combined": "Reliability analysis using weighted combinational models for web-based software In the past, some researches suggested that engineers can use combined software reliability growth models (SRGMs) to obtain more accurate reliability prediction during testing. In this paper, three weighted combinational models, namely, equal, linear, and nonlinear weight, are proposed for reliability estimation of web-based software. We further investigate the estimation accuracy of using genetic algorithm to determine the weight assignment for the proposed models. Preliminary result shows that the linearly and nonlinearly weighted combinational models have better prediction capability than single SRGM and equally weighted combinational model for web-based software. [[EENNDD]] weighted combination; genetic algorithm.; software reliability growth model"}, "Analisis kebolehpercayaan menggunakan model gabungan berwajaran untuk perisian berasaskan web Pada masa lalu, beberapa penyelidikan mencadangkan bahawa jurutera boleh menggunakan model pertumbuhan kebolehpercayaan gabungan (SRGM) untuk mendapatkan ramalan kebolehpercayaan yang lebih tepat semasa ujian. Dalam makalah ini, tiga model gabungan berwajaran, yaitu, berat sama, linier, dan tidak linier, diusulkan untuk anggaran kebolehpercayaan perisian berasaskan web. Kami seterusnya menyiasat ketepatan anggaran menggunakan algoritma genetik untuk menentukan penugasan berat bagi model yang dicadangkan. Hasil awal menunjukkan bahawa model gabungan berwajaran linear dan tidak linier mempunyai kemampuan ramalan yang lebih baik daripada SRGM tunggal dan model gabungan yang berwajaran sama untuk perisian berasaskan web. [[EENNDD]] gabungan berwajaran; algoritma genetik .; model pertumbuhan kebolehpercayaan perisian"], [{"string": "Evaluating the effectiveness of search task trails In this paper, we introduce \"task trail\" as a new concept to understand user search behaviors. We define task to be an atomic user information need. Web search logs have been studied mainly at session or query level where users may submit several queries within one task and handle several tasks within one session. Although previous studies have addressed the problem of task identification, little is known about the advantage of using task over session and query for search applications. In this paper, we conduct extensive analyses and comparisons to evaluate the effectiveness of task trails in three search applications: determining user satisfaction, predicting user search interests, and query suggestion. Experiments are conducted on large scale datasets from a commercial search engine. Experimental results show that: (1) Sessions and queries are not as precise as tasks in determining user satisfaction. (2) Task trails provide higher web page utilities to users than other sources. (3) Tasks represent atomic user information needs, and therefore can preserve topic similarity between query pairs. (4) Task-based query suggestion can provide complementary results to other models. The findings in this paper verify the need to extract task trails from web search logs and suggest potential applications in search and recommendation systems.", "keywords": ["task evaluation", "search log mining", "information search and retrieval", "log analysis", "task trail"], "combined": "Evaluating the effectiveness of search task trails In this paper, we introduce \"task trail\" as a new concept to understand user search behaviors. We define task to be an atomic user information need. Web search logs have been studied mainly at session or query level where users may submit several queries within one task and handle several tasks within one session. Although previous studies have addressed the problem of task identification, little is known about the advantage of using task over session and query for search applications. In this paper, we conduct extensive analyses and comparisons to evaluate the effectiveness of task trails in three search applications: determining user satisfaction, predicting user search interests, and query suggestion. Experiments are conducted on large scale datasets from a commercial search engine. Experimental results show that: (1) Sessions and queries are not as precise as tasks in determining user satisfaction. (2) Task trails provide higher web page utilities to users than other sources. (3) Tasks represent atomic user information needs, and therefore can preserve topic similarity between query pairs. (4) Task-based query suggestion can provide complementary results to other models. The findings in this paper verify the need to extract task trails from web search logs and suggest potential applications in search and recommendation systems. [[EENNDD]] task evaluation; search log mining; information search and retrieval; log analysis; task trail"}, "Menilai keberkesanan pencarian jalan pencarian Dalam makalah ini, kami memperkenalkan \"task trail\" sebagai konsep baru untuk memahami tingkah laku carian pengguna. Kami menentukan tugas sebagai keperluan maklumat pengguna atom. Log carian web telah dipelajari terutama pada sesi atau tahap pertanyaan di mana pengguna dapat mengirimkan beberapa pertanyaan dalam satu tugas dan menangani beberapa tugas dalam satu sesi. Walaupun kajian terdahulu telah menangani masalah pengenalpastian tugas, hanya sedikit yang diketahui mengenai kelebihan menggunakan tugas berbanding sesi dan pertanyaan untuk aplikasi carian. Dalam makalah ini, kami melakukan analisis dan perbandingan yang luas untuk menilai keberkesanan jejak tugas dalam tiga aplikasi carian: menentukan kepuasan pengguna, meramalkan minat carian pengguna, dan cadangan pertanyaan. Eksperimen dijalankan pada set data berskala besar dari mesin carian komersial. Hasil eksperimen menunjukkan bahawa: (1) Sesi dan pertanyaan tidak setepat tugas dalam menentukan kepuasan pengguna. (2) Jalur tugas menyediakan utiliti laman web yang lebih tinggi kepada pengguna daripada sumber lain. (3) Tugas mewakili keperluan maklumat pengguna atom, dan oleh itu dapat menjaga persamaan topik antara pasangan pertanyaan. (4) Cadangan pertanyaan berdasarkan tugas dapat memberikan hasil pelengkap kepada model lain. Penemuan dalam makalah ini mengesahkan keperluan untuk mengekstrak jejak tugas dari log carian web dan mencadangkan aplikasi yang berpotensi dalam sistem carian dan cadangan. [[EENNDD]] penilaian tugas; perlombongan log carian; carian dan pengambilan maklumat; analisis log; jejak tugas"], [{"string": "Spotting fake reviewer groups in consumer reviews Opinionated social media such as product reviews are now widely used by individuals and organizations for their decision making. However, due to the reason of profit or fame, people try to game the system by opinion spamming (e.g., writing fake reviews) to promote or demote some target products. For reviews to reflect genuine user experiences and opinions, such spam reviews should be detected. Prior works on opinion spam focused on detecting fake reviews and individual fake reviewers. However, a fake reviewer group (a group of reviewers who work collaboratively to write fake reviews) is even more damaging as they can take total control of the sentiment on the target product due to its size. This paper studies spam detection in the collaborative setting, i.e., to discover fake reviewer groups. The proposed method first uses a frequent itemset mining method to find a set of candidate groups. It then uses several behavioral models derived from the collusion phenomenon among fake reviewers and relation models based on the relationships among groups, individual reviewers, and products they reviewed to detect fake reviewer groups. Additionally, we also built a labeled dataset of fake reviewer groups. Although labeling individual fake reviews and reviewers is very hard, to our surprise labeling fake reviewer groups is much easier. We also note that the proposed technique departs from the traditional supervised learning approach for spam detection because of the inherent nature of our problem which makes the classic supervised learning approach less effective. Experimental results show that the proposed method outperforms multiple strong baselines including the state-of-the-art supervised classification, regression, and learning to rank algorithms.", "keywords": ["social and behavioral sciences", "group opinion spam", "fake review detection", "opinion spam"], "combined": "Spotting fake reviewer groups in consumer reviews Opinionated social media such as product reviews are now widely used by individuals and organizations for their decision making. However, due to the reason of profit or fame, people try to game the system by opinion spamming (e.g., writing fake reviews) to promote or demote some target products. For reviews to reflect genuine user experiences and opinions, such spam reviews should be detected. Prior works on opinion spam focused on detecting fake reviews and individual fake reviewers. However, a fake reviewer group (a group of reviewers who work collaboratively to write fake reviews) is even more damaging as they can take total control of the sentiment on the target product due to its size. This paper studies spam detection in the collaborative setting, i.e., to discover fake reviewer groups. The proposed method first uses a frequent itemset mining method to find a set of candidate groups. It then uses several behavioral models derived from the collusion phenomenon among fake reviewers and relation models based on the relationships among groups, individual reviewers, and products they reviewed to detect fake reviewer groups. Additionally, we also built a labeled dataset of fake reviewer groups. Although labeling individual fake reviews and reviewers is very hard, to our surprise labeling fake reviewer groups is much easier. We also note that the proposed technique departs from the traditional supervised learning approach for spam detection because of the inherent nature of our problem which makes the classic supervised learning approach less effective. Experimental results show that the proposed method outperforms multiple strong baselines including the state-of-the-art supervised classification, regression, and learning to rank algorithms. [[EENNDD]] social and behavioral sciences; group opinion spam; fake review detection; opinion spam"}, "Meninjau kumpulan pengulas palsu dalam ulasan pengguna Media sosial yang disukai seperti ulasan produk kini banyak digunakan oleh individu dan organisasi untuk membuat keputusan mereka. Namun, kerana alasan keuntungan atau kemasyhuran, orang cuba mempermainkan sistem dengan menghantar spam (misalnya, menulis ulasan palsu) untuk mempromosikan atau menurunkan beberapa produk sasaran. Agar ulasan dapat mencerminkan pengalaman dan pendapat pengguna yang tulen, ulasan spam tersebut harus dikesan. Kerja sebelumnya mengenai spam pendapat difokuskan pada mengesan ulasan palsu dan pengulas palsu individu. Walau bagaimanapun, kumpulan pengulas palsu (sekumpulan pengulas yang bekerjasama untuk menulis ulasan palsu) bahkan lebih merosakkan kerana mereka dapat mengawal sepenuhnya sentimen pada produk sasaran kerana ukurannya. Makalah ini mengkaji pengesanan spam dalam pengaturan kolaboratif, iaitu untuk menemui kumpulan penyemak palsu. Kaedah yang dicadangkan terlebih dahulu menggunakan kaedah perlombongan itemet yang kerap untuk mencari kumpulan kumpulan calon. Ia kemudian menggunakan beberapa model tingkah laku yang berasal dari fenomena kolusi di antara pengulas palsu dan model hubungan berdasarkan hubungan antara kumpulan, pengulas individu, dan produk yang mereka kaji untuk mengesan kumpulan pengulas palsu. Selain itu, kami juga membina kumpulan data berlabel kumpulan pengulas palsu. Walaupun melabelkan ulasan dan pengulas palsu secara individu adalah sangat sukar, yang mengejutkan kami ialah melabel kumpulan pengulas palsu adalah lebih mudah. Kami juga memperhatikan bahawa teknik yang dicadangkan bertolak dari pendekatan pembelajaran yang diawasi secara tradisional untuk pengesanan spam kerana sifat masalah kita yang wujud yang menjadikan pendekatan pembelajaran yang diselia klasik kurang berkesan. Hasil eksperimen menunjukkan bahawa kaedah yang dicadangkan mengatasi banyak garis dasar yang kuat termasuk klasifikasi, regresi, dan pembelajaran peringkat algoritma yang diawasi canggih. [[EENNDD]] sains sosial dan tingkah laku; spam pendapat kumpulan; pengesanan semakan palsu; spam pendapat"], [{"string": "AdHeat: an influence-based diffusion model for propagating hints to match ads In this paper, we present AdHeat, a social ad model considering user influence in addition to relevance for matching ads. Traditionally, ad placement employs the relevance model. Such a model matches ads with Web page content, user interests, or both. We have observed, however, on social networks that the relevance model suffers from two shortcomings. First, influential users (users who contribute opinions) seldom click ads that are highly relevant to their expertise. Second, because influential users' contents and activities are attractive to other users, hint words summarizing their expertise and activities may be widely preferred. Therefore, we propose AdHeat, which diffuses hint words of influential users to others and then matches ads for each user with aggregated hints. We performed experiments on a large online Q&amp;A community with half a million users. The experimental results show that AdHeat outperforms the relevance model on CTR (click through rate) by significant margins.", "keywords": ["behavior targeting", "influence propagation", "contextual advertising", "heat diffusion", "social network analysis", "online advertising"], "combined": "AdHeat: an influence-based diffusion model for propagating hints to match ads In this paper, we present AdHeat, a social ad model considering user influence in addition to relevance for matching ads. Traditionally, ad placement employs the relevance model. Such a model matches ads with Web page content, user interests, or both. We have observed, however, on social networks that the relevance model suffers from two shortcomings. First, influential users (users who contribute opinions) seldom click ads that are highly relevant to their expertise. Second, because influential users' contents and activities are attractive to other users, hint words summarizing their expertise and activities may be widely preferred. Therefore, we propose AdHeat, which diffuses hint words of influential users to others and then matches ads for each user with aggregated hints. We performed experiments on a large online Q&amp;A community with half a million users. The experimental results show that AdHeat outperforms the relevance model on CTR (click through rate) by significant margins. [[EENNDD]] behavior targeting; influence propagation; contextual advertising; heat diffusion; social network analysis; online advertising"}, ""], [{"string": "Not so creepy crawler: easy crawler generation with standard xml queries Web crawlers are increasingly used for focused tasks such as the extraction of data from Wikipedia or the analysis of social networks like last.fm. In these cases, pages are far more uniformly structured than in the general Web and thus crawlers can use the structure of Web pages for more precise data extraction and more expressive analysis.", "keywords": ["web crawler", "web query", "information search and retrieval", "data extraction", "xml"], "combined": "Not so creepy crawler: easy crawler generation with standard xml queries Web crawlers are increasingly used for focused tasks such as the extraction of data from Wikipedia or the analysis of social networks like last.fm. In these cases, pages are far more uniformly structured than in the general Web and thus crawlers can use the structure of Web pages for more precise data extraction and more expressive analysis. [[EENNDD]] web crawler; web query; information search and retrieval; data extraction; xml"}, "Crawler tidak begitu menyeramkan: penghasilan crawler yang mudah dengan pertanyaan xml standard Crawler web semakin banyak digunakan untuk tugas yang difokuskan seperti pengekstrakan data dari Wikipedia atau analisis rangkaian sosial seperti last.fm. Dalam kes ini, halaman lebih tersusun secara seragam daripada di laman web umum dan dengan itu perayap dapat menggunakan struktur halaman Web untuk pengekstrakan data yang lebih tepat dan analisis yang lebih ekspresif. [[EENNDD]] perayap web; pertanyaan web; pencarian dan pengambilan maklumat; pengekstrakan data; xml"], [{"string": "Modeling online reviews with multi-grain topic models In this paper we present a novel framework for extracting the ratable aspects of objects from online user reviews. Extracting such aspects is an important challenge in automatically mining product opinions from the web and in generating opinion-based summaries of user reviews [18, 19, 7, 12, 27, 36, 21]. Our models are based on extensions to standard topic modeling methods such as LDA and PLSA to induce multi-grain topics. We argue that multi-grain models are more appropriate for our task since standard models tend to produce topics that correspond to global properties of objects (e.g., the brand of a product type) rather than the aspects of an object that tend to be rated by a user. The models we present not only extract ratable aspects, but also cluster them into coherent topics, e.g., 'waitress' and 'bartender' are part of the same topic 'staff' for restaurants. This differentiates it from much of the previous work which extracts aspects through term frequency analysis with minimal clustering. We evaluate the multi-grain models both qualitatively and quantitatively to show that they improve significantly upon standard topic models.", "keywords": ["general", "content analysis and indexing", "opinion mining", "topic models", "clustering"], "combined": "Modeling online reviews with multi-grain topic models In this paper we present a novel framework for extracting the ratable aspects of objects from online user reviews. Extracting such aspects is an important challenge in automatically mining product opinions from the web and in generating opinion-based summaries of user reviews [18, 19, 7, 12, 27, 36, 21]. Our models are based on extensions to standard topic modeling methods such as LDA and PLSA to induce multi-grain topics. We argue that multi-grain models are more appropriate for our task since standard models tend to produce topics that correspond to global properties of objects (e.g., the brand of a product type) rather than the aspects of an object that tend to be rated by a user. The models we present not only extract ratable aspects, but also cluster them into coherent topics, e.g., 'waitress' and 'bartender' are part of the same topic 'staff' for restaurants. This differentiates it from much of the previous work which extracts aspects through term frequency analysis with minimal clustering. We evaluate the multi-grain models both qualitatively and quantitatively to show that they improve significantly upon standard topic models. [[EENNDD]] general; content analysis and indexing; opinion mining; topic models; clustering"}, "Memodelkan ulasan dalam talian dengan model topik berbilang butir Dalam makalah ini kami membentangkan kerangka novel untuk mengekstrak aspek objek yang dapat dinilai dari ulasan pengguna dalam talian. Mengekstrak aspek tersebut adalah cabaran penting dalam melombong pendapat produk secara automatik dari web dan dalam menghasilkan ringkasan ulasan pengguna berdasarkan pendapat [18, 19, 7, 12, 27, 36, 21]. Model kami didasarkan pada peluasan kaedah pemodelan topik standard seperti LDA dan PLSA untuk mendorong topik berbilang butir. Kami berpendapat bahawa model berbilang biji lebih sesuai untuk tugas kami kerana model standard cenderung menghasilkan topik yang sesuai dengan sifat global objek (contohnya, jenama jenis produk) daripada aspek objek yang cenderung dinilai oleh pengguna. Model-model yang kami sajikan tidak hanya mengekstrak aspek-aspek yang dapat dipercaya, tetapi juga menggabungkannya menjadi topik yang koheren, misalnya, 'pelayan' dan 'bartender' adalah bahagian dari topik yang sama 'kakitangan' untuk restoran. Ini membezakannya dari banyak karya sebelumnya yang mengekstrak aspek melalui analisis frekuensi istilah dengan pengumpulan minimum. Kami menilai model berbilang bijirin secara kualitatif dan kuantitatif untuk menunjukkan bahawa model ini bertambah baik berdasarkan model topik standard. [[EENNDD]] umum; analisis kandungan dan pengindeksan; perlombongan pendapat; model topik; pengelompokan"], [{"string": "Stop thinking, start tagging: tag semantics emerge from collaborative verbosity Recent research provides evidence for the presence of emergent semantics in collaborative tagging systems. While several methods have been proposed, little is known about the factors that influence the evolution of semantic structures in these systems. A natural hypothesis is that the quality of the emergent semantics depends on the pragmatics of tagging: Users with certain usage patterns might contribute more to the resulting semantics than others. In this work, we propose several measures which enable a pragmatic differentiation of taggers by their degree of contribution to emerging semantic structures. We distinguish between categorizers, who typically use a small set of tags as a replacement for hierarchical classification schemes, and describers, who are annotating resources with a wealth of freely associated, descriptive keywords. To study our hypothesis, we apply semantic similarity measures to 64 different partitions of a real-world and large-scale folksonomy containing different ratios of categorizers and describers. Our results not only show that \"verbose\" taggers are most useful for the emergence of tag semantics, but also that a subset containing only 40% of the most 'verbose' taggers can produce results that match and even outperform the semantic precision obtained from the whole dataset. Moreover, the results suggest that there exists a causal link between the pragmatics of tagging and resulting emergent semantics. This work is relevant for designers and analysts of tagging systems interested (i) in fostering the semantic development of their platforms, (ii) in identifying users introducing \"semantic noise\", and (iii) in learning ontologies.", "keywords": ["pragmatics", "folksonomies", "user characteristics", "semantics", "tagging"], "combined": "Stop thinking, start tagging: tag semantics emerge from collaborative verbosity Recent research provides evidence for the presence of emergent semantics in collaborative tagging systems. While several methods have been proposed, little is known about the factors that influence the evolution of semantic structures in these systems. A natural hypothesis is that the quality of the emergent semantics depends on the pragmatics of tagging: Users with certain usage patterns might contribute more to the resulting semantics than others. In this work, we propose several measures which enable a pragmatic differentiation of taggers by their degree of contribution to emerging semantic structures. We distinguish between categorizers, who typically use a small set of tags as a replacement for hierarchical classification schemes, and describers, who are annotating resources with a wealth of freely associated, descriptive keywords. To study our hypothesis, we apply semantic similarity measures to 64 different partitions of a real-world and large-scale folksonomy containing different ratios of categorizers and describers. Our results not only show that \"verbose\" taggers are most useful for the emergence of tag semantics, but also that a subset containing only 40% of the most 'verbose' taggers can produce results that match and even outperform the semantic precision obtained from the whole dataset. Moreover, the results suggest that there exists a causal link between the pragmatics of tagging and resulting emergent semantics. This work is relevant for designers and analysts of tagging systems interested (i) in fostering the semantic development of their platforms, (ii) in identifying users introducing \"semantic noise\", and (iii) in learning ontologies. [[EENNDD]] pragmatics; folksonomies; user characteristics; semantics; tagging"}, "Berhenti berfikir, mulakan penandaan: semantik tag muncul dari kata kerja kolaboratif Penyelidikan terbaru memberikan bukti untuk kehadiran semantik muncul dalam sistem penandaan kolaboratif. Walaupun beberapa kaedah telah dicadangkan, hanya sedikit yang diketahui mengenai faktor-faktor yang mempengaruhi evolusi struktur semantik dalam sistem ini. Hipotesis semula jadi adalah bahawa kualiti semantik yang muncul bergantung pada pragmatik penandaan: Pengguna dengan corak penggunaan tertentu mungkin menyumbang lebih banyak pada semantik yang dihasilkan daripada yang lain. Dalam karya ini, kami mencadangkan beberapa langkah yang memungkinkan pembezaan tagger secara pragmatik berdasarkan tahap sumbangan mereka terhadap struktur semantik yang muncul. Kami membezakan antara pengkategorian, yang biasanya menggunakan sekumpulan kecil tag sebagai pengganti skema klasifikasi hierarki, dan perihal, yang memberi penjelasan sumber dengan banyak kata kunci deskriptif yang bebas. Untuk mengkaji hipotesis kami, kami menerapkan ukuran kesamaan semantik pada 64 partisi yang berbeza dari folksonomi dunia nyata dan berskala besar yang mengandungi nisbah pengkategorator dan penerangan yang berbeza. Hasil kami tidak hanya menunjukkan bahawa penanda \"verbose\" sangat berguna untuk kemunculan semantik tag, tetapi juga subset yang mengandungi hanya 40% daripada tagger paling 'verbose' dapat menghasilkan hasil yang sesuai dan bahkan melebihi ketepatan semantik yang diperoleh dari keseluruhan dataset. Lebih-lebih lagi, hasilnya menunjukkan bahawa ada hubungan kausal antara pragmatik penandaan dan semantik yang muncul. Karya ini relevan untuk pereka dan penganalisis sistem penandaan yang berminat (i) dalam memupuk pengembangan semantik platform mereka, (ii) dalam mengenal pasti pengguna yang memperkenalkan \"semantik noise\", dan (iii) dalam mempelajari ontologi. [[EENNDD]] pragmatik; folksonomi; ciri pengguna; semantik; penandaan"], [{"string": "Using XForms to simplify Web programming No contact information provided yet.", "keywords": ["mvc", "j2ee", "web application", "xforms", "xmlbeans", "eclipse", "visual builder"], "combined": "Using XForms to simplify Web programming No contact information provided yet. [[EENNDD]] mvc; j2ee; web application; xforms; xmlbeans; eclipse; visual builder"}, "Menggunakan XForms untuk mempermudah pengaturcaraan Web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] mvc; j2ee; aplikasi sesawang; bentuk x; kacang xml; gerhana; pembina visual"], [{"string": "Dataplorer: a scalable search engine for the data web More and more structured information in the form of semantic data is nowadays available. It offers a wide range of new possibilities especially for semantic search and Web data integration. However, their effective exploitation still brings about a number of challenges, e.g. usability, scalability and uncertainty. In this paper, we present Dataplorer, a solution designed to address these challenges. We consider the usability through the use of hybrid queries and faceted search, while still preserving the scalability thanks to an extension of inverted index to support this type of query. Moreover, Dataplorer deals with uncertainty by means of a powerful ranking scheme to find relevant results. Our experimental results show that our proposed approach is promising and it makes us believe that it is possible to extend the current IR infrastructure to query and search the Web of data.", "keywords": ["faceted search", "ranking", "information search and retrieval", "hybrid query", "inverted index"], "combined": "Dataplorer: a scalable search engine for the data web More and more structured information in the form of semantic data is nowadays available. It offers a wide range of new possibilities especially for semantic search and Web data integration. However, their effective exploitation still brings about a number of challenges, e.g. usability, scalability and uncertainty. In this paper, we present Dataplorer, a solution designed to address these challenges. We consider the usability through the use of hybrid queries and faceted search, while still preserving the scalability thanks to an extension of inverted index to support this type of query. Moreover, Dataplorer deals with uncertainty by means of a powerful ranking scheme to find relevant results. Our experimental results show that our proposed approach is promising and it makes us believe that it is possible to extend the current IR infrastructure to query and search the Web of data. [[EENNDD]] faceted search; ranking; information search and retrieval; hybrid query; inverted index"}, "Dataplorer: mesin carian yang boleh diskalakan untuk web data Maklumat yang lebih tersusun dalam bentuk data semantik kini tersedia. Ini menawarkan pelbagai kemungkinan baru terutama untuk pencarian semantik dan integrasi data Web. Walau bagaimanapun, eksploitasi mereka yang berkesan masih membawa sejumlah cabaran, mis. kebolehgunaan, skala dan ketidakpastian. Dalam makalah ini, kami menyajikan Dataplorer, penyelesaian yang dirancang untuk mengatasi cabaran ini. Kami mempertimbangkan kebolehgunaan melalui penggunaan pertanyaan hibrid dan carian pelbagai, sementara masih mengekalkan skalabilitas berkat peluasan indeks terbalik untuk menyokong jenis pertanyaan ini. Lebih-lebih lagi, Dataplorer menangani ketidakpastian melalui skema peringkat yang kuat untuk mendapatkan hasil yang relevan. Hasil eksperimen kami menunjukkan bahawa pendekatan yang dicadangkan kami menjanjikan dan membuat kami percaya bahawa adalah mungkin untuk memperluas infrastruktur IR semasa untuk membuat pertanyaan dan mencari di Web data. [[EENNDD]] carian segi; peringkat; carian dan pengambilan maklumat; pertanyaan hibrid; indeks terbalik"], [{"string": "Web mashup scripting language The Web Mashup Scripting Language (WMSL) enables an end-user (you) working from his browser, e.g. not needing any other infrastructure, to quickly write mashups that integrate any two, or more, web services on the Web. The end-user accomplishes this by writing a web page that combines HTML, metadata in the form of mapping relations, and small piece of code, or script. The mapping relations enable not only the discovery and retrieval of the WMSL pages, but also affect a new programming paradigm that abstracts many programming complexities from the script writer. Furthermore, the WMSL Web pages or scripts that disparate end-users (you) write, can be harvested by Crawlers to automatically generate the concepts needed to build lightweight ontologies containing local semantics of a web service and its data model, to extend context ontologies or middle ontologies, and to develop links, or mappings, between these ontologies. This enables an open-source model of building ontologies based on the WMSL Web page or scripts that end users (you) write.", "keywords": ["ontologies", "html", "web services", "semantics", "scripts"], "combined": "Web mashup scripting language The Web Mashup Scripting Language (WMSL) enables an end-user (you) working from his browser, e.g. not needing any other infrastructure, to quickly write mashups that integrate any two, or more, web services on the Web. The end-user accomplishes this by writing a web page that combines HTML, metadata in the form of mapping relations, and small piece of code, or script. The mapping relations enable not only the discovery and retrieval of the WMSL pages, but also affect a new programming paradigm that abstracts many programming complexities from the script writer. Furthermore, the WMSL Web pages or scripts that disparate end-users (you) write, can be harvested by Crawlers to automatically generate the concepts needed to build lightweight ontologies containing local semantics of a web service and its data model, to extend context ontologies or middle ontologies, and to develop links, or mappings, between these ontologies. This enables an open-source model of building ontologies based on the WMSL Web page or scripts that end users (you) write. [[EENNDD]] ontologies; html; web services; semantics; scripts"}, "Bahasa skrip mashup web Bahasa Skrip Mashup Web (WMSL) membolehkan pengguna akhir (anda) bekerja dari penyemak imbasnya, mis. tidak memerlukan infrastruktur lain, untuk cepat menulis mashup yang menyatukan dua, atau lebih, perkhidmatan web di Web. Pengguna akhir mencapainya dengan menulis laman web yang menggabungkan HTML, metadata dalam bentuk hubungan pemetaan, dan sepotong kecil kod, atau skrip. Hubungan pemetaan memungkinkan bukan hanya penemuan dan pengambilan halaman WMSL, tetapi juga mempengaruhi paradigma pengaturcaraan baru yang mengaburkan banyak kerumitan pengaturcaraan dari penulis skrip. Selanjutnya, halaman atau skrip WMSL Web yang membezakan pengguna akhir (anda) menulis, dapat diambil oleh Crawlers untuk menghasilkan konsep yang diperlukan secara automatik untuk membina ontologi ringan yang mengandungi semantik tempatan dari perkhidmatan web dan model datanya, untuk memperluas ontologi konteks atau ontologi tengah, dan untuk membina hubungan, atau pemetaan, antara ontologi ini. Ini membolehkan model sumber terbuka membina ontologi berdasarkan halaman Web WMSL atau skrip yang ditulis oleh pengguna akhir (anda). [[EENNDD]] ontologi; html; perkhidmatan web; semantik; skrip"], [{"string": "Item-based collaborative filtering recommendation algorithms An abstract is not available.", "keywords": ["group and organization interfaces"], "combined": "Item-based collaborative filtering recommendation algorithms An abstract is not available. [[EENNDD]] group and organization interfaces"}, "Algoritma cadangan penapisan kolaboratif berdasarkan item Abstrak tidak tersedia. [[EENNDD]] antara muka kumpulan dan organisasi"], [{"string": "Web page ranking using link attributes No contact information provided yet.", "keywords": ["on-line information services", "web link ranking", "pagerank"], "combined": "Web page ranking using link attributes No contact information provided yet. [[EENNDD]] on-line information services; web link ranking; pagerank"}, "Peringkat laman web menggunakan atribut pautan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perkhidmatan maklumat dalam talian; kedudukan pautan web; pagerank"], [{"string": "Bayesian network based sentence retrieval model This paper makes an intensive investigation of the application of Bayesian network in sentence retrieval and introduces three Bayesian network based sentence retrieval models with or without consideration of term relationships. Term relationships in this paper are considered from two perspectives: relationships between pairs of terms and relationships between terms and term sets. Experiments have proven the efficiency of Bayesian network in the application of sentence retrieval. Particularly, retrieval result with consideration of the second kind of term relationship performs better in improving retrieval precision.", "keywords": ["term relationship", "sentence retrieval", "bayesian network"], "combined": "Bayesian network based sentence retrieval model This paper makes an intensive investigation of the application of Bayesian network in sentence retrieval and introduces three Bayesian network based sentence retrieval models with or without consideration of term relationships. Term relationships in this paper are considered from two perspectives: relationships between pairs of terms and relationships between terms and term sets. Experiments have proven the efficiency of Bayesian network in the application of sentence retrieval. Particularly, retrieval result with consideration of the second kind of term relationship performs better in improving retrieval precision. [[EENNDD]] term relationship; sentence retrieval; bayesian network"}, "Model pengambilan ayat berdasarkan rangkaian Bayesian Makalah ini membuat penyelidikan intensif terhadap aplikasi rangkaian Bayesian dalam pengambilan ayat dan memperkenalkan tiga model pengambilan ayat berdasarkan rangkaian Bayesian dengan atau tanpa pertimbangan hubungan istilah. Hubungan jangka dalam makalah ini dipertimbangkan dari dua perspektif: hubungan antara pasangan istilah dan hubungan antara istilah dan set istilah. Eksperimen telah membuktikan kecekapan rangkaian Bayesian dalam penerapan pengambilan ayat. Terutama, hasil pengambilan dengan pertimbangan jenis istilah hubungan kedua lebih baik dalam meningkatkan ketepatan pengambilan. [[EENNDD]] hubungan jangka; pengambilan ayat; rangkaian bayesian"], [{"string": "Temporal rules for mobile web personalization No contact information provided yet.", "keywords": ["user modeling", "hypertext/hypermedia", "mobile", "miscellaneous", "www", "wap", "user/machine systems", "temporal models"], "combined": "Temporal rules for mobile web personalization No contact information provided yet. [[EENNDD]] user modeling; hypertext/hypermedia; mobile; miscellaneous; www; wap; user/machine systems; temporal models"}, "Peraturan sementara untuk pemperibadian web mudah alih Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pemodelan pengguna; hiperteks / hipermedia; mudah alih; pelbagai; www; wap; sistem pengguna / mesin; model temporal"], [{"string": "Object-Z web environment and projections to UML An abstract is not available.", "keywords": ["object-z", "web", "xml/xsl/xmi", "xml", "uml"], "combined": "Object-Z web environment and projections to UML An abstract is not available. [[EENNDD]] object-z; web; xml/xsl/xmi; xml; uml"}, "Persekitaran web Object-Z dan unjuran ke UML Abstrak tidak tersedia. [[EENNDD]] objek-z; laman web; xml / xsl / xmi; xml; uml"], [{"string": "Brand awareness and the evaluation of search results We investigate the effect of search engine brand (i.e., the identifying name or logo that distinguishes a product from its competitors) on evaluation of system performance. This research is motivated by the large amount of search traffic directed to a handful of Web search engines, even though most are of equal technical quality with similar interfaces. We conducted a laboratory study with 32 participants to measure the effect of four search engine brands while controlling for the quality of search engine results. There was a 25% difference between the most highly rated search engine and the lowest using average relevance ratings, even though search engine results were identical in both content and presentation. Qualitative analysis suggests branding affects user views of popularity, trust and specialization. We discuss implications for search engine marketing and the design of search engine quality studies.", "keywords": ["search engines", "web searching", "brand"], "combined": "Brand awareness and the evaluation of search results We investigate the effect of search engine brand (i.e., the identifying name or logo that distinguishes a product from its competitors) on evaluation of system performance. This research is motivated by the large amount of search traffic directed to a handful of Web search engines, even though most are of equal technical quality with similar interfaces. We conducted a laboratory study with 32 participants to measure the effect of four search engine brands while controlling for the quality of search engine results. There was a 25% difference between the most highly rated search engine and the lowest using average relevance ratings, even though search engine results were identical in both content and presentation. Qualitative analysis suggests branding affects user views of popularity, trust and specialization. We discuss implications for search engine marketing and the design of search engine quality studies. [[EENNDD]] search engines; web searching; brand"}, "Kesedaran jenama dan penilaian hasil carian Kami menyiasat kesan jenama mesin pencari (iaitu, nama atau logo pengenal yang membezakan produk daripada pesaingnya) terhadap penilaian prestasi sistem. Penyelidikan ini didorong oleh sejumlah besar lalu lintas carian yang ditujukan kepada segelintir mesin carian Web, walaupun kebanyakannya mempunyai kualiti teknikal yang sama dengan antara muka yang serupa. Kami menjalankan kajian makmal dengan 32 peserta untuk mengukur kesan empat jenama mesin pencari sambil mengawal kualiti hasil mesin pencari. Terdapat perbezaan 25% antara mesin pencari yang diberi nilai paling tinggi dan terendah menggunakan penilaian relevansi rata-rata, walaupun hasil mesin pencari sama dalam kandungan dan persembahan. Analisis kualitatif menunjukkan bahawa penjenamaan mempengaruhi pandangan pengguna mengenai populariti, kepercayaan dan pengkhususan. Kami membincangkan implikasi untuk pemasaran mesin pencari dan reka bentuk kajian kualiti mesin pencari. [[EENNDD]] enjin carian; carian web; jenama"], [{"string": "Automatic identification of user goals in Web search No contact information provided yet.", "keywords": ["query classification", "user goals", "web search", "miscellaneous"], "combined": "Automatic identification of user goals in Web search No contact information provided yet. [[EENNDD]] query classification; user goals; web search; miscellaneous"}, "Pengenalpastian tujuan pengguna secara automatik dalam carian Web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] klasifikasi pertanyaan; tujuan pengguna; carian sesawang; pelbagai"], [{"string": "Exploring web scale language models for search query processing It has been widely observed that search queries are composed in a very different style from that of the body or the title of a document. Many techniques explicitly accounting for this language style discrepancy have shown promising results for information retrieval, yet a large scale analysis on the extent of the language differences has been lacking. In this paper, we present an extensive study on this issue by examining the language model properties of search queries and the three text streams associated with each web document: the body, the title, and the anchor text. Our information theoretical analysis shows that queries seem to be composed in a way most similar to how authors summarize documents in anchor texts or titles, offering a quantitative explanation to the observations in past work.", "keywords": ["information search and retrieval", "language models", "web n-gram", "very large-scale experiments", "search query processing"], "combined": "Exploring web scale language models for search query processing It has been widely observed that search queries are composed in a very different style from that of the body or the title of a document. Many techniques explicitly accounting for this language style discrepancy have shown promising results for information retrieval, yet a large scale analysis on the extent of the language differences has been lacking. In this paper, we present an extensive study on this issue by examining the language model properties of search queries and the three text streams associated with each web document: the body, the title, and the anchor text. Our information theoretical analysis shows that queries seem to be composed in a way most similar to how authors summarize documents in anchor texts or titles, offering a quantitative explanation to the observations in past work. [[EENNDD]] information search and retrieval; language models; web n-gram; very large-scale experiments; search query processing"}, "Meneroka model bahasa skala web untuk pemprosesan permintaan carian Telah banyak diperhatikan bahawa pertanyaan carian disusun dalam gaya yang sangat berbeza dari badan atau tajuk dokumen. Banyak teknik yang secara jelas menerangkan perbezaan gaya bahasa ini telah menunjukkan hasil yang menjanjikan untuk mendapatkan maklumat, namun analisis skala besar mengenai sejauh mana perbezaan bahasa kurang. Dalam makalah ini, kami menyajikan kajian luas mengenai masalah ini dengan meneliti sifat model bahasa dari pertanyaan pencarian dan tiga aliran teks yang terkait dengan setiap dokumen web: isi, judul, dan teks jangkar. Analisis teori maklumat kami menunjukkan bahawa pertanyaan sepertinya disusun dengan cara yang paling mirip dengan bagaimana pengarang meringkaskan dokumen dalam teks atau tajuk, yang memberikan penjelasan kuantitatif kepada pemerhatian dalam karya masa lalu. [[EENNDD]] carian dan pengambilan maklumat; model bahasa; laman web n-gram; eksperimen berskala sangat besar; pemprosesan pertanyaan carian"], [{"string": "Understanding the functions of business accounts on Twitter This paper performs an initial exploration of business Twitter accounts in order to start understanding how businesses interact with their users and viceversa. We provide an analysis of business tweet types and topics and show that specific business tweet classes such as deals and events can be reliably identified for customer use.", "keywords": ["business intelligence", "general", "twitter", "information extraction"], "combined": "Understanding the functions of business accounts on Twitter This paper performs an initial exploration of business Twitter accounts in order to start understanding how businesses interact with their users and viceversa. We provide an analysis of business tweet types and topics and show that specific business tweet classes such as deals and events can be reliably identified for customer use. [[EENNDD]] business intelligence; general; twitter; information extraction"}, "Memahami fungsi akaun perniagaan di Twitter Makalah ini melakukan penerokaan awal mengenai akaun Twitter perniagaan untuk mulai memahami bagaimana perniagaan berinteraksi dengan pengguna dan sebaliknya. Kami memberikan analisis mengenai jenis dan topik tweet perniagaan dan menunjukkan bahawa kelas tweet perniagaan tertentu seperti tawaran dan acara dapat dikenal pasti untuk penggunaan pelanggan. [[EENNDD]] kepintaran perniagaan; umum; twitter; pengekstrakan maklumat"], [{"string": "Facetedpedia: dynamic generation of query-dependent faceted interfaces for wikipedia This paper proposes Facetedpedia, a faceted retrieval system for information discovery and exploration in Wikipedia. Given the set of Wikipedia articles resulting from a keyword query, Facetedpedia generates a faceted interface for navigating the result articles. Compared with other faceted retrieval systems, Facetedpedia is fully automatic and dynamic in both facet generation and hierarchy construction, and the facets are based on the rich semantic information from Wikipedia. The essence of our approach is to build upon the collaborative vocabulary in Wikipedia, more specifically the intensive internal structures (hyperlinks) and folksonomy (category system). Given the sheer size and complexity of this corpus, the space of possible choices of faceted interfaces is prohibitively large. We propose metrics for ranking individual facet hierarchies by user's navigational cost, and metrics for ranking interfaces (each with k facets) by both their average pairwise similarities and average navigational costs. We thus develop faceted interface discovery algorithms that optimize the ranking metrics. Our experimental evaluation and user study verify the effectiveness of the system.", "keywords": ["data exploration", "wikipedia", "information search and retrieval", "faceted search"], "combined": "Facetedpedia: dynamic generation of query-dependent faceted interfaces for wikipedia This paper proposes Facetedpedia, a faceted retrieval system for information discovery and exploration in Wikipedia. Given the set of Wikipedia articles resulting from a keyword query, Facetedpedia generates a faceted interface for navigating the result articles. Compared with other faceted retrieval systems, Facetedpedia is fully automatic and dynamic in both facet generation and hierarchy construction, and the facets are based on the rich semantic information from Wikipedia. The essence of our approach is to build upon the collaborative vocabulary in Wikipedia, more specifically the intensive internal structures (hyperlinks) and folksonomy (category system). Given the sheer size and complexity of this corpus, the space of possible choices of faceted interfaces is prohibitively large. We propose metrics for ranking individual facet hierarchies by user's navigational cost, and metrics for ranking interfaces (each with k facets) by both their average pairwise similarities and average navigational costs. We thus develop faceted interface discovery algorithms that optimize the ranking metrics. Our experimental evaluation and user study verify the effectiveness of the system. [[EENNDD]] data exploration; wikipedia; information search and retrieval; faceted search"}, "Facetedpedia: generasi dinamik antara muka yang bergantung pada pertanyaan untuk wikipedia Makalah ini mencadangkan Facetedpedia, sistem pengambilan aspek untuk penemuan dan penerokaan maklumat di Wikipedia. Memandangkan kumpulan artikel Wikipedia yang dihasilkan dari pertanyaan kata kunci, Facetedpedia menghasilkan antara muka yang fasih untuk menavigasi artikel hasil. Berbanding dengan sistem pengambilan aspek lain, Facetedpedia sepenuhnya automatik dan dinamik dalam kedua-dua penjanaan faset dan pembinaan hierarki, dan aspek tersebut berdasarkan maklumat semantik yang kaya dari Wikipedia. Inti dari pendekatan kami adalah untuk membina perbendaharaan kata kolaboratif di Wikipedia, lebih khusus struktur dalaman yang intensif (hyperlink) dan folksonomi (sistem kategori). Memandangkan ukuran dan kerumitan korpus ini, ruang kemungkinan pilihan antara muka berukuran sangat besar. Kami mencadangkan metrik untuk menentukan hierarki aspek individu mengikut kos navigasi pengguna, dan metrik untuk peringkat antara muka (masing-masing dengan aspek k) dengan persamaan berpasangan rata-rata dan kos navigasi purata. Oleh itu, kami mengembangkan algoritma penemuan antara muka yang mengoptimumkan metrik kedudukan. Penilaian eksperimental dan kajian pengguna kami mengesahkan keberkesanan sistem. [[EENNDD]] penerokaan data; wikipedia; carian dan pengambilan maklumat; carian pelbagai segi"], [{"string": "Towards effective browsing of large scale social annotations This paper is concerned with the problem of browsing social annotations. Today, a lot of services (e.g., Del.icio.us, Filckr) have been provided for helping users to manage and share their favorite URLs and photos based on social annotations. Due to the exponential increasing of the social annotations, more and more users, however, are facing the problem how to effectively find desired resources from large annotation data. Existing methods such as tag cloud and annotation matching work well only on small annotation sets. Thus, an effective approach for browsing large scale annotation sets and the associated resources is in great demand by both ordinary users and service providers. In this paper, we propose a novel algorithm, namely Effective Large Scale Annotation Browser (ELSABer), to browse large-scale social annotation data. ELSABer helps the users browse huge number of annotations in a semantic, hierarchical and efficient way. More specifically, ELSABer has the following features: 1) the semantic relations between annotations are explored for browsing of similar resources; 2) the hierarchical relations between annotations are constructed for browsing in a top-down fashion; 3) the distribution of social annotations is studied for efficient browsing. By incorporating the personal and time information, ELSABer can be further extended for personalized and time-related browsing. A prototype system is implemented and shows promising results.", "keywords": ["social annotation", "annotation browsing", "evaluation", "clustering"], "combined": "Towards effective browsing of large scale social annotations This paper is concerned with the problem of browsing social annotations. Today, a lot of services (e.g., Del.icio.us, Filckr) have been provided for helping users to manage and share their favorite URLs and photos based on social annotations. Due to the exponential increasing of the social annotations, more and more users, however, are facing the problem how to effectively find desired resources from large annotation data. Existing methods such as tag cloud and annotation matching work well only on small annotation sets. Thus, an effective approach for browsing large scale annotation sets and the associated resources is in great demand by both ordinary users and service providers. In this paper, we propose a novel algorithm, namely Effective Large Scale Annotation Browser (ELSABer), to browse large-scale social annotation data. ELSABer helps the users browse huge number of annotations in a semantic, hierarchical and efficient way. More specifically, ELSABer has the following features: 1) the semantic relations between annotations are explored for browsing of similar resources; 2) the hierarchical relations between annotations are constructed for browsing in a top-down fashion; 3) the distribution of social annotations is studied for efficient browsing. By incorporating the personal and time information, ELSABer can be further extended for personalized and time-related browsing. A prototype system is implemented and shows promising results. [[EENNDD]] social annotation; annotation browsing; evaluation; clustering"}, "Menuju penjelajahan anotasi sosial berskala besar yang berkesan Makalah ini berkaitan dengan masalah melayari anotasi sosial. Hari ini, banyak perkhidmatan (mis., Del.icio.us, Filckr) telah disediakan untuk membantu pengguna mengurus dan berkongsi URL dan foto kegemaran mereka berdasarkan anotasi sosial. Oleh kerana peningkatan anotasi sosial secara eksponensial, semakin banyak pengguna, bagaimanapun, menghadapi masalah bagaimana mencari sumber yang diinginkan dengan berkesan dari data anotasi yang besar. Kaedah yang ada seperti cloud tag dan pencocokan anotasi hanya berfungsi dengan baik pada set anotasi kecil. Oleh itu, pendekatan yang berkesan untuk melayari set anotasi skala besar dan sumber yang berkaitan sangat diminati oleh pengguna biasa dan penyedia perkhidmatan. Dalam makalah ini, kami mencadangkan algoritma novel, iaitu Browser Anotasi Skala Besar Berkesan (ELSABer), untuk melihat data anotasi sosial berskala besar. ELSABer membantu pengguna melihat sebilangan besar anotasi dengan cara semantik, hierarki dan cekap. Lebih khusus lagi, ELSABer mempunyai ciri-ciri berikut: 1) hubungan semantik antara anotasi diterokai untuk melayari sumber yang serupa; 2) hubungan hierarki antara anotasi dibina untuk melayari secara top-down; 3) penyebaran anotasi sosial dikaji untuk melayari yang cekap. Dengan memasukkan maklumat peribadi dan masa, ELSABer dapat diperluas lagi untuk penyemakan imbas yang diperibadikan dan berkaitan dengan masa. Sistem prototaip dilaksanakan dan menunjukkan hasil yang menjanjikan. [[EENNDD]] anotasi sosial; melayari anotasi; penilaian; pengelompokan"], [{"string": "Hierarchical, perceptron-like learning for ontology-based information extraction Recent work on ontology-based Information Extraction (IE) has tried to make use of knowledge from the target ontology in order to improve semantic annotation results. However, very few approaches exploit the ontology structure itself, and those that do so, have some limitations. This paper introduces a hierarchical learning approach for IE, which uses the target ontology as an essential part of the extraction process, by taking into account the relations between concepts. The approach is evaluated on the largest available semantically annotated corpus. The results demonstrate clearly the benefits of using knowledge from the ontology as input to the information extraction process. We also demonstrate the advantages of our approach over other state-of-the-art learning systems on a commonly used benchmark dataset.", "keywords": ["semantic annotation", "hierarchical learning", "ontology-based information extraction"], "combined": "Hierarchical, perceptron-like learning for ontology-based information extraction Recent work on ontology-based Information Extraction (IE) has tried to make use of knowledge from the target ontology in order to improve semantic annotation results. However, very few approaches exploit the ontology structure itself, and those that do so, have some limitations. This paper introduces a hierarchical learning approach for IE, which uses the target ontology as an essential part of the extraction process, by taking into account the relations between concepts. The approach is evaluated on the largest available semantically annotated corpus. The results demonstrate clearly the benefits of using knowledge from the ontology as input to the information extraction process. We also demonstrate the advantages of our approach over other state-of-the-art learning systems on a commonly used benchmark dataset. [[EENNDD]] semantic annotation; hierarchical learning; ontology-based information extraction"}, "Pembelajaran hirarkis, seperti perceptron untuk pengekstrakan maklumat berasaskan ontologi Karya terbaru mengenai Pengekstrakan Maklumat berdasarkan ontologi (IE) telah berusaha memanfaatkan pengetahuan dari ontologi sasaran untuk meningkatkan hasil anotasi semantik. Walau bagaimanapun, sangat sedikit pendekatan yang memanfaatkan struktur ontologi itu sendiri, dan pendekatan yang melakukannya, mempunyai beberapa batasan. Makalah ini memperkenalkan pendekatan pembelajaran hierarki untuk IE, yang menggunakan ontologi sasaran sebagai bahagian penting dari proses pengekstrakan, dengan mempertimbangkan hubungan antara konsep. Pendekatan ini dinilai berdasarkan korpus anotasi semantik terbesar yang ada. Hasilnya menunjukkan dengan jelas faedah menggunakan pengetahuan dari ontologi sebagai input kepada proses pengekstrakan maklumat. Kami juga menunjukkan kelebihan pendekatan kami berbanding sistem pembelajaran canggih lain pada set data penanda aras yang biasa digunakan. [[EENNDD]] anotasi semantik; pembelajaran hierarki; pengekstrakan maklumat berasaskan ontologi"], [{"string": "Predicting click through rate for job listings Click Through Rate (CTR) is an important metric for ad systems, job portals, recommendation systems. CTR impacts publisher's revenue, advertiser's bid amounts in \"pay for performance\" business models. We learn regression models using features of the job, optional click history of job, features of \"related\" jobs. We show that our models predict CTR much better than predicting avg. CTR for all job listings, even in absence of the click history for the job listing.", "keywords": ["cpc", "linear regression", "gbdt", "click through rate", "information search and retrieval", "ctr", "learning", "prediction", "gradient boosted decision trees", "treenet", "jobs"], "combined": "Predicting click through rate for job listings Click Through Rate (CTR) is an important metric for ad systems, job portals, recommendation systems. CTR impacts publisher's revenue, advertiser's bid amounts in \"pay for performance\" business models. We learn regression models using features of the job, optional click history of job, features of \"related\" jobs. We show that our models predict CTR much better than predicting avg. CTR for all job listings, even in absence of the click history for the job listing. [[EENNDD]] cpc; linear regression; gbdt; click through rate; information search and retrieval; ctr; learning; prediction; gradient boosted decision trees; treenet; jobs"}, "Meramalkan kadar klik tayang untuk senarai pekerjaan Kadar Klik Melalui (RKT) adalah metrik penting untuk sistem iklan, portal pekerjaan, sistem cadangan. CTR mempengaruhi pendapatan penerbit, jumlah tawaran pengiklan dalam model perniagaan \"bayar untuk prestasi\". Kami mempelajari model regresi menggunakan ciri pekerjaan, sejarah klik pilihan, ciri pekerjaan \"berkaitan\". Kami menunjukkan bahawa model kami meramalkan CTR jauh lebih baik daripada meramalkan purata CTR untuk semua senarai pekerjaan, walaupun tidak ada sejarah klik untuk senarai pekerjaan. [[EENNDD]] cpc; regresi linear; gbdt; kadar klik melalui; pencarian dan pengambilan maklumat; ctr; belajar; ramalan; pokok keputusan yang dinaikkan secara berperingkat; treenet; pekerjaan"], [{"string": "SEMPL: a semantic portal No contact information provided yet.", "keywords": ["ontology", "semantic portal", "semantic web"], "combined": "SEMPL: a semantic portal No contact information provided yet. [[EENNDD]] ontology; semantic portal; semantic web"}, "SEMPL: portal semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ontologi; portal semantik; web semantik"], [{"string": "Enabling entity-based aggregators for web 2.0 data Selecting and presenting content culled from multiple heterogeneous and physically distributed sources is a challenging task. The exponential growth of the web data in modern times has brought new requirements to such integration systems. Data is not any more produced by content providers alone, but also from regular users through the highly popular Web 2.0 social and semantic web applications. The plethora of the available web content increased its demand by regular users who could not any more wait the development of advanced integration tools. They wanted to be able to build in a short time their own specialized integration applications. Aggregators came to the risk of these users. They allowed them not only to combine distributed content, but also to process it in ways that generate new services available for further consumption.", "keywords": ["design tools and techniques", "entity matching", "semantic web", "linked data"], "combined": "Enabling entity-based aggregators for web 2.0 data Selecting and presenting content culled from multiple heterogeneous and physically distributed sources is a challenging task. The exponential growth of the web data in modern times has brought new requirements to such integration systems. Data is not any more produced by content providers alone, but also from regular users through the highly popular Web 2.0 social and semantic web applications. The plethora of the available web content increased its demand by regular users who could not any more wait the development of advanced integration tools. They wanted to be able to build in a short time their own specialized integration applications. Aggregators came to the risk of these users. They allowed them not only to combine distributed content, but also to process it in ways that generate new services available for further consumption. [[EENNDD]] design tools and techniques; entity matching; semantic web; linked data"}, "Mengaktifkan agregator berasaskan entiti untuk data web 2.0 Memilih dan menyajikan kandungan yang dikeluarkan dari pelbagai sumber yang berbeza dan diedarkan secara fizikal adalah tugas yang mencabar. Pertumbuhan eksponensial data web pada zaman moden telah membawa keperluan baru untuk sistem integrasi tersebut. Data tidak lagi dihasilkan oleh penyedia kandungan sahaja, tetapi juga dari pengguna biasa melalui aplikasi web sosial dan semantik Web 2.0 yang sangat popular. Sebilangan besar kandungan web yang tersedia meningkatkan permintaannya oleh pengguna biasa yang tidak sabar lagi untuk mengembangkan alat integrasi lanjutan. Mereka mahu dapat membina aplikasi integrasi khusus mereka dalam masa yang singkat. Agregator menghadapi risiko pengguna ini. Mereka memungkinkan mereka tidak hanya untuk menggabungkan kandungan yang diedarkan, tetapi juga memprosesnya dengan cara yang menghasilkan perkhidmatan baru yang tersedia untuk penggunaan lebih lanjut. [[EENNDD]] alat dan teknik reka bentuk; pemadanan entiti; web semantik; data terpaut"], [{"string": "Truthy: mapping the spread of astroturf in microblog streams Online social media are complementing and in some cases replacing person-to-person social interaction and redefining the diffusion of information. In particular, microblogs have become crucial grounds on which public relations, marketing, and political battles are fought. We demonstrate a web service that tracks political memes in Twitter and helps detect astroturfing, smear campaigns, and other misinformation in the context of U.S. political elections. We also present some cases of abusive behaviors uncovered by our service. Our web service is based on an extensible framework that will enable the real-time analysis of meme diffusion in social media by mining, visualizing, mapping, classifying, and modeling massive streams of public microblogging events.", "keywords": ["truthy", "classification", "information diffusion", "politics", "microblogs", "twitter", "social media", "memes"], "combined": "Truthy: mapping the spread of astroturf in microblog streams Online social media are complementing and in some cases replacing person-to-person social interaction and redefining the diffusion of information. In particular, microblogs have become crucial grounds on which public relations, marketing, and political battles are fought. We demonstrate a web service that tracks political memes in Twitter and helps detect astroturfing, smear campaigns, and other misinformation in the context of U.S. political elections. We also present some cases of abusive behaviors uncovered by our service. Our web service is based on an extensible framework that will enable the real-time analysis of meme diffusion in social media by mining, visualizing, mapping, classifying, and modeling massive streams of public microblogging events. [[EENNDD]] truthy; classification; information diffusion; politics; microblogs; twitter; social media; memes"}, "Kebenaran: memetakan penyebaran astroturf dalam aliran mikroblog Media sosial dalam talian melengkapi dan dalam beberapa kes menggantikan interaksi sosial orang ke orang dan mentakrifkan semula penyebaran maklumat. Khususnya, mikroblog menjadi alasan penting di mana hubungan masyarakat, pemasaran, dan pertempuran politik dilancarkan. Kami menunjukkan perkhidmatan web yang melacak meme politik di Twitter dan membantu mengesan astroturfing, kempen smear, dan maklumat yang salah dalam konteks pilihan raya politik A.S. Kami juga menunjukkan beberapa kes tingkah laku kasar yang ditemui oleh perkhidmatan kami. Perkhidmatan web kami didasarkan pada kerangka yang dapat diperluas yang akan memungkinkan analisis masa nyata penyebaran meme di media sosial dengan melombong, memvisualisasikan, memetakan, mengklasifikasikan, dan memodelkan aliran besar peristiwa mikroblog awam. [[EENNDD]] benar; pengelasan; penyebaran maklumat; politik; blog mikro; twitter; media sosial; meme"], [{"string": "The role of standards in creating community No contact information provided yet.", "keywords": ["automatic programming", "museums online archive california", "thesaurus", "document preparation", "ontology", "design tools and techniques", "network architecture and design", "historic costume collection", "xml", "semantic web", "language classifications", "dublin core", "open archive initiative"], "combined": "The role of standards in creating community No contact information provided yet. [[EENNDD]] automatic programming; museums online archive california; thesaurus; document preparation; ontology; design tools and techniques; network architecture and design; historic costume collection; xml; semantic web; language classifications; dublin core; open archive initiative"}, "Peranan standard dalam mewujudkan komuniti Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengaturcaraan automatik; muzium dalam talian arkib california; tesaurus; penyediaan dokumen; ontologi; alat dan teknik reka bentuk; seni bina dan reka bentuk rangkaian; koleksi pakaian bersejarah; xml; web semantik; pengelasan bahasa; teras dublin; inisiatif terbuka arkib"], [{"string": "Causal relation of queries from temporal logs In this paper, we study a new problem of mining causal relation of queries in search engine query logs. Causal relation between two queries means event on one query is the causation of some event on the other. We first detect events in query logs by efficient statistical frequency threshold. Then the causal relation of queries is mined by the geometric features of the events. Finally the Granger Causality Test (GCT) is utilized to further re-rank the causal relation of queries according to their GCT coefficients. In addition, we develop a 2-dimensional visualization tool to display the detected relationship of events in a more intuitive way. The experimental results on the MSN search engine query logs demonstrate that our approach can accurately detect the events in temporal query logs and the causal relation of queries is detected effectively.", "keywords": ["time series", "search engine query log", "causal relation"], "combined": "Causal relation of queries from temporal logs In this paper, we study a new problem of mining causal relation of queries in search engine query logs. Causal relation between two queries means event on one query is the causation of some event on the other. We first detect events in query logs by efficient statistical frequency threshold. Then the causal relation of queries is mined by the geometric features of the events. Finally the Granger Causality Test (GCT) is utilized to further re-rank the causal relation of queries according to their GCT coefficients. In addition, we develop a 2-dimensional visualization tool to display the detected relationship of events in a more intuitive way. The experimental results on the MSN search engine query logs demonstrate that our approach can accurately detect the events in temporal query logs and the causal relation of queries is detected effectively. [[EENNDD]] time series; search engine query log; causal relation"}, "Hubungan sebab-sebab pertanyaan dari log temporal Dalam makalah ini, kami mengkaji masalah baru hubungan sebab-akibat perlombongan pertanyaan dalam log pertanyaan enjin carian. Hubungan kausal antara dua pertanyaan bermaksud peristiwa pada satu pertanyaan adalah penyebab beberapa peristiwa pada yang lain. Kami mula-mula mengesan peristiwa dalam log pertanyaan dengan ambang frekuensi statistik yang cekap. Kemudian hubungan sebab-akibat pertanyaan ditambang oleh ciri-ciri geometri peristiwa. Akhirnya Granger Causality Test (GCT) digunakan untuk menilai semula hubungan sebab-akibat pertanyaan mengikut pekali GCT mereka. Di samping itu, kami mengembangkan alat visualisasi 2 dimensi untuk memaparkan hubungan peristiwa yang dikesan dengan cara yang lebih intuitif. Hasil eksperimen pada log pertanyaan enjin carian MSN menunjukkan bahawa pendekatan kami dapat dengan tepat mengesan peristiwa dalam log pertanyaan temporal dan hubungan sebab-akibat pertanyaan dikesan dengan berkesan. [[EENNDD]] siri masa; log pertanyaan enjin carian; hubungan sebab-akibat"], [{"string": "VoiSTV: voice-enabled social TV Until recently, the TV viewing experience has not been a very social activity compared to activities on the World Wide Web. In this work, we will present a Voice-enabled Social TV system (VoiSTV) which allows users to interact, follow and monitor the online social media messages related to a TV show while watching it. Users can create, send, and reply to messages using spoken language. VoiSTV also provides metadata information about TV shows such as trends, hot topics, popularity as well as aggregated sentiment of show-related messages, all of which are valuable for TV program search and recommendation.", "keywords": ["iptv", "social data mining", "user interfaces", "speech interface", "twitter", "social tv"], "combined": "VoiSTV: voice-enabled social TV Until recently, the TV viewing experience has not been a very social activity compared to activities on the World Wide Web. In this work, we will present a Voice-enabled Social TV system (VoiSTV) which allows users to interact, follow and monitor the online social media messages related to a TV show while watching it. Users can create, send, and reply to messages using spoken language. VoiSTV also provides metadata information about TV shows such as trends, hot topics, popularity as well as aggregated sentiment of show-related messages, all of which are valuable for TV program search and recommendation. [[EENNDD]] iptv; social data mining; user interfaces; speech interface; twitter; social tv"}, "VoiSTV: TV sosial berkemampuan suara Sehingga baru-baru ini, pengalaman menonton TV tidak menjadi aktiviti sosial berbanding aktiviti di World Wide Web. Dalam karya ini, kami akan menyajikan sistem TV Sosial berkemampuan Suara (VoiSTV) yang membolehkan pengguna berinteraksi, mengikuti dan memantau mesej media sosial dalam talian yang berkaitan dengan rancangan TV semasa menontonnya. Pengguna dapat membuat, mengirim, dan membalas mesej menggunakan bahasa lisan. VoiSTV juga memberikan maklumat metadata mengenai rancangan TV seperti tren, topik hangat, populariti serta sentimen gabungan mesej yang berkaitan dengan rancangan, semuanya berharga untuk carian dan cadangan program TV. [[EENNDD]] iptv; perlombongan data sosial; antara muka pengguna; antara muka pertuturan; twitter; tv sosial"], [{"string": "Learning to rank with multiple objective functions We investigate the problem of learning to rank with document retrieval from the perspective of learning for multiple objective functions. We present solutions to two open problems in learning to rank: first, we show how multiple measures can be combined into a single graded measure that can be learned. This solves the problem of learning from a 'scorecard' of measures by making such scorecards comparable, and we show results where a standard web relevance measure (NDCG) is used for the top-tier measure, and a relevance measure derived from click data is used for the second-tier measure; the second-tier measure is shown to significantly improve while leaving the top-tier measure largely unchanged. Second, we note that the learning-to-rank problem can itself be viewed as changing as the ranking model learns: for example, early in learning, adjusting the rank of all documents can be advantageous, but later during training, it becomes more desirable to concentrate on correcting the top few documents for each query. We show how an analysis of these problems leads to an improved, iteration-dependent cost function that interpolates between a cost function that is more appropriate for early learning, with one that is more appropriate for late-stage learning. The approach results in a significant improvement in accuracy with the same size models. We investigate these ideas using LambdaMART, a state-of-the-art ranking algorithm.", "keywords": ["web search", "learning to rank", "learning", "relevance measures"], "combined": "Learning to rank with multiple objective functions We investigate the problem of learning to rank with document retrieval from the perspective of learning for multiple objective functions. We present solutions to two open problems in learning to rank: first, we show how multiple measures can be combined into a single graded measure that can be learned. This solves the problem of learning from a 'scorecard' of measures by making such scorecards comparable, and we show results where a standard web relevance measure (NDCG) is used for the top-tier measure, and a relevance measure derived from click data is used for the second-tier measure; the second-tier measure is shown to significantly improve while leaving the top-tier measure largely unchanged. Second, we note that the learning-to-rank problem can itself be viewed as changing as the ranking model learns: for example, early in learning, adjusting the rank of all documents can be advantageous, but later during training, it becomes more desirable to concentrate on correcting the top few documents for each query. We show how an analysis of these problems leads to an improved, iteration-dependent cost function that interpolates between a cost function that is more appropriate for early learning, with one that is more appropriate for late-stage learning. The approach results in a significant improvement in accuracy with the same size models. We investigate these ideas using LambdaMART, a state-of-the-art ranking algorithm. [[EENNDD]] web search; learning to rank; learning; relevance measures"}, "Belajar memberi peringkat dengan pelbagai fungsi objektif Kami menyiasat masalah pembelajaran untuk menentukan kedudukan dengan pengambilan dokumen dari perspektif pembelajaran untuk pelbagai fungsi objektif. Kami menyajikan penyelesaian untuk dua masalah terbuka dalam pembelajaran untuk peringkat: pertama, kami menunjukkan bagaimana pelbagai ukuran dapat digabungkan menjadi ukuran bergred tunggal yang dapat dipelajari. Ini menyelesaikan masalah belajar dari 'kad skor' langkah dengan membuat kad skor sebanding, dan kami menunjukkan hasil di mana ukuran relevansi web standard (NDCG) digunakan untuk ukuran tingkat atas, dan ukuran relevansi yang berasal dari data klik adalah digunakan untuk ukuran tahap kedua; ukuran tahap kedua terbukti meningkat dengan ketara sementara meninggalkan tahap atas sebahagian besarnya tidak berubah. Kedua, kita perhatikan bahawa masalah pembelajaran ke peringkat dapat dilihat sebagai perubahan ketika model peringkat belajar: sebagai contoh, pada awal pembelajaran, menyesuaikan kedudukan semua dokumen dapat bermanfaat, tetapi kemudian semasa latihan, menjadi lebih diinginkan untuk menumpukan perhatian untuk memperbaiki beberapa dokumen teratas untuk setiap pertanyaan. Kami menunjukkan bagaimana analisis masalah ini membawa kepada peningkatan fungsi kos yang bergantung pada iterasi yang berpadu antara fungsi kos yang lebih sesuai untuk pembelajaran awal, dengan analisis yang lebih sesuai untuk pembelajaran tahap akhir. Pendekatan ini menghasilkan peningkatan ketepatan yang ketara dengan model ukuran yang sama. Kami menyiasat idea-idea ini menggunakan LambdaMART, algoritma peringkat terkini. [[EENNDD]] carian web; belajar berpangkat; belajar; langkah-langkah kesesuaian"], [{"string": "Layered label propagation: a multiresolution coordinate-free ordering for compressing social networks We continue the line of research on graph compression started with WebGraph, but we move our focus to the compression of social networks in a proper sense (e.g., LiveJournal): the approaches that have been used for a long time to compress web graphs rely on a specific ordering of the nodes (lexicographical URL ordering) whose extension to general social networks is not trivial. In this paper, we propose a solution that mixes clusterings and orders, and devise a new algorithm, called Layered Label Propagation, that builds on previous work on scalable clustering and can be used to reorder very large graphs (billions of nodes). Our implementation uses task decomposition to perform aggressively on multi-core architecture, making it possible to reorder graphs of more than 600 millions nodes in a few hours.", "keywords": ["social networks", "graph clustering", "web graphs", "graph compression"], "combined": "Layered label propagation: a multiresolution coordinate-free ordering for compressing social networks We continue the line of research on graph compression started with WebGraph, but we move our focus to the compression of social networks in a proper sense (e.g., LiveJournal): the approaches that have been used for a long time to compress web graphs rely on a specific ordering of the nodes (lexicographical URL ordering) whose extension to general social networks is not trivial. In this paper, we propose a solution that mixes clusterings and orders, and devise a new algorithm, called Layered Label Propagation, that builds on previous work on scalable clustering and can be used to reorder very large graphs (billions of nodes). Our implementation uses task decomposition to perform aggressively on multi-core architecture, making it possible to reorder graphs of more than 600 millions nodes in a few hours. [[EENNDD]] social networks; graph clustering; web graphs; graph compression"}, "Penyebaran label berlapis: pesanan bebas koordinat multisolusi untuk memampatkan rangkaian sosial Kami meneruskan kajian mengenai pemampatan grafik yang dimulakan dengan WebGraph, tetapi kami mengalihkan fokus kami ke pemampatan rangkaian sosial dalam arti yang tepat (misalnya, LiveJournal): pendekatan yang telah lama digunakan untuk memampatkan grafik web bergantung pada susunan node tertentu (urutan URL leksikografi) yang peluasannya ke rangkaian sosial umum tidak sepele. Dalam makalah ini, kami mencadangkan penyelesaian yang mencampurkan kluster dan pesanan, dan membuat algoritma baru, yang disebut Layered Label Propagation, yang membangun karya sebelumnya pada pengelompokan berskala dan dapat digunakan untuk menyusun ulang grafik yang sangat besar (berbilion-bilion nod). Pelaksanaan kami menggunakan penguraian tugas untuk melakukan agresif pada seni bina pelbagai teras, memungkinkan untuk menyusun semula grafik lebih dari 600 juta nod dalam beberapa jam. [[EENNDD]] rangkaian sosial; pengelompokan grafik; grafik web; pemampatan grafik"], [{"string": "WS-replication: a framework for highly available web services No contact information provided yet.", "keywords": ["availability", "ws-caf", "web services", "transactions", "group communication"], "combined": "WS-replication: a framework for highly available web services No contact information provided yet. [[EENNDD]] availability; ws-caf; web services; transactions; group communication"}, "WS-replikasi: kerangka kerja untuk perkhidmatan web yang sangat tersedia Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ketersediaan; ws-caf; perkhidmatan web; urus niaga; komunikasi kumpulan"], [{"string": "Bucefalo: a tool for intelligent search and filtering for web-based personal health records In this poster, a tool named BUCEFALO is presented. This tool is specially designed to improve the information retrieval tasks in web-based Personal Health Records (PHR). This tool implements semantic and multilingual query expansion techniques and information filtering algorithms in order to help users find the most valuable information about a specific clinical case. The filtering model is based on fuzzy prototypes based filtering, data quality measures, user profiles and healthcare ontologies. The first experimental results illustrate the feasibility of this tool.", "keywords": ["web-based personal health record"], "combined": "Bucefalo: a tool for intelligent search and filtering for web-based personal health records In this poster, a tool named BUCEFALO is presented. This tool is specially designed to improve the information retrieval tasks in web-based Personal Health Records (PHR). This tool implements semantic and multilingual query expansion techniques and information filtering algorithms in order to help users find the most valuable information about a specific clinical case. The filtering model is based on fuzzy prototypes based filtering, data quality measures, user profiles and healthcare ontologies. The first experimental results illustrate the feasibility of this tool. [[EENNDD]] web-based personal health record"}, "Bucefalo: alat untuk mencari dan menyaring pintar untuk rekod kesihatan peribadi berasaskan web Dalam poster ini, alat bernama BUCEFALO disajikan. Alat ini direka khas untuk meningkatkan tugas pengambilan maklumat dalam Rekod Kesihatan Peribadi (PHR) berasaskan web. Alat ini menerapkan teknik pengembangan pertanyaan semantik dan multibahasa dan algoritma penyaringan maklumat untuk membantu pengguna mencari maklumat yang paling berharga mengenai kes klinikal tertentu. Model penapisan berdasarkan penapisan berdasarkan prototaip kabur, ukuran kualiti data, profil pengguna dan ontologi penjagaan kesihatan. Hasil eksperimen pertama menggambarkan kemungkinan alat ini. [[EENNDD]] rekod kesihatan peribadi berasaskan web"], [{"string": "Conversation specification: a new approach to design and analysis of e-service composition No contact information provided yet.", "keywords": ["communicating finite sate automata", "e-service composition", "conversation specification"], "combined": "Conversation specification: a new approach to design and analysis of e-service composition No contact information provided yet. [[EENNDD]] communicating finite sate automata; e-service composition; conversation specification"}, "Spesifikasi percakapan: pendekatan baru untuk reka bentuk dan analisis komposisi e-perkhidmatan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] berkomunikasi automata sate terhingga; komposisi e-perkhidmatan; spesifikasi perbualan"], [{"string": "Distributed community crawling No contact information provided yet.", "keywords": ["web communities", "distributed crawling", "web metrics"], "combined": "Distributed community crawling No contact information provided yet. [[EENNDD]] web communities; distributed crawling; web metrics"}, "Rangkak komuniti yang diedarkan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] komuniti web; merangkak diedarkan; sukatan web"], [{"string": "Knowledge sharing and yahoo answers: everyone knows something Yahoo Answers (YA) is a large and diverse question-answer forum, acting not only as a medium for sharing technical knowledge, but as a place where one can seek advice, gather opinions, and satisfy one's curiosity about a countless number of things. In this paper, we seek to understand YA's knowledge sharing and activity. We analyze the forum categories and cluster them according to content characteristics and patterns of interaction among the users. While interactions in some categories resemble expertise sharing forums, others incorporate discussion, everyday advice, and support. With such a diversity of categories in which one can participate, we find that some users focus narrowly on specific topics, while others participate across categories. This not only allows us to map related categories, but to characterize the entropy of the users' interests. We find that lower entropy correlates with receiving higher answer ratings, but only for categories where factual expertise is primarily sought after. We combine both user attributes and answer characteristics to predict, within a given category, whether a particular answer will be chosen as the best answer by the asker.", "keywords": ["expertise finding", "help seeking", "question answering", "knowledge sharing", "social network analysis", "online communities"], "combined": "Knowledge sharing and yahoo answers: everyone knows something Yahoo Answers (YA) is a large and diverse question-answer forum, acting not only as a medium for sharing technical knowledge, but as a place where one can seek advice, gather opinions, and satisfy one's curiosity about a countless number of things. In this paper, we seek to understand YA's knowledge sharing and activity. We analyze the forum categories and cluster them according to content characteristics and patterns of interaction among the users. While interactions in some categories resemble expertise sharing forums, others incorporate discussion, everyday advice, and support. With such a diversity of categories in which one can participate, we find that some users focus narrowly on specific topics, while others participate across categories. This not only allows us to map related categories, but to characterize the entropy of the users' interests. We find that lower entropy correlates with receiving higher answer ratings, but only for categories where factual expertise is primarily sought after. We combine both user attributes and answer characteristics to predict, within a given category, whether a particular answer will be chosen as the best answer by the asker. [[EENNDD]] expertise finding; help seeking; question answering; knowledge sharing; social network analysis; online communities"}, "Perkongsian pengetahuan dan jawapan yahoo: semua orang tahu sesuatu Yahoo Answers (YA) adalah forum soal jawab yang besar dan pelbagai, bertindak bukan hanya sebagai media untuk berkongsi pengetahuan teknikal, tetapi sebagai tempat di mana seseorang dapat meminta nasihat, mengumpulkan pendapat, dan memuaskan rasa ingin tahu seseorang tentang banyak perkara. Dalam makalah ini, kami berusaha memahami perkongsian dan aktiviti pengetahuan YA. Kami menganalisis kategori forum dan mengelompokkannya mengikut ciri kandungan dan corak interaksi antara pengguna. Walaupun interaksi dalam beberapa kategori menyerupai forum perkongsian kepakaran, yang lain merangkumi perbincangan, nasihat dan sokongan setiap hari. Dengan kepelbagaian kategori di mana seseorang dapat berpartisipasi, kita dapati bahawa sebilangan pengguna memfokuskan diri pada topik tertentu, sementara yang lain mengambil bahagian dalam pelbagai kategori. Ini bukan sahaja memungkinkan kita memetakan kategori yang berkaitan, tetapi untuk mencirikan entropi kepentingan pengguna. Kami mendapati bahawa entropi rendah berkorelasi dengan menerima penilaian jawapan yang lebih tinggi, tetapi hanya untuk kategori di mana kepakaran fakta dicari. Kami menggabungkan kedua-dua atribut pengguna dan ciri jawapan untuk meramalkan, dalam kategori tertentu, sama ada jawapan tertentu akan dipilih sebagai jawapan terbaik oleh penanya. [[EENNDD]] penemuan kepakaran; pertolongan mencari; menjawab soalan; perkongsian pengetahuan; analisis rangkaian sosial; komuniti dalam talian"], [{"string": "Unsupervised query categorization using automatically-built concept graphs Automatic categorization of user queries is an important component of general purpose (Web) search engines, particularly for triggering rich, query-specific content and sponsored links. We propose an unsupervised learning scheme that reduces dramatically the cost of setting up and maintaining such a categorizer, while retaining good categorization power. The model is stored as a graph of concepts where graph edges represent the cross-reference between the concepts. Concepts and relations are extracted from query logs by an offline Web mining process, which uses a search engine as a powerful summarizer for building a concept graph. Empirical evaluation indicates that the system compares favorably on publicly available data sets (such as KDD Cup 2005) as well as on portions of the current query stream of Yahoo! Search, where it is already changing the experience of millions of Web search users.", "keywords": ["concept networks", "information search and retrieval", "query categorization", "cross-reference", "knowledge based search", "unsupervised learning", "web mining"], "combined": "Unsupervised query categorization using automatically-built concept graphs Automatic categorization of user queries is an important component of general purpose (Web) search engines, particularly for triggering rich, query-specific content and sponsored links. We propose an unsupervised learning scheme that reduces dramatically the cost of setting up and maintaining such a categorizer, while retaining good categorization power. The model is stored as a graph of concepts where graph edges represent the cross-reference between the concepts. Concepts and relations are extracted from query logs by an offline Web mining process, which uses a search engine as a powerful summarizer for building a concept graph. Empirical evaluation indicates that the system compares favorably on publicly available data sets (such as KDD Cup 2005) as well as on portions of the current query stream of Yahoo! Search, where it is already changing the experience of millions of Web search users. [[EENNDD]] concept networks; information search and retrieval; query categorization; cross-reference; knowledge based search; unsupervised learning; web mining"}, "Pengkategorian pertanyaan yang tidak diawasi menggunakan grafik konsep yang dibina secara automatik Pengkategorian pertanyaan pengguna secara automatik adalah komponen penting dari enjin carian tujuan umum (Web), terutama untuk mencetuskan kandungan khusus, pertanyaan khusus dan pautan yang ditaja. Kami mencadangkan skema pembelajaran yang tidak diawasi yang secara dramatik mengurangkan biaya penyiapan dan penyelenggaraan pengkategorikan seperti itu, sambil mengekalkan kekuatan pengkategorian yang baik. Model disimpan sebagai grafik konsep di mana tepi grafik mewakili rujukan silang antara konsep. Konsep dan hubungan diekstrak dari log pertanyaan dengan proses perlombongan Web luar talian, yang menggunakan mesin carian sebagai ringkasan yang kuat untuk membina grafik konsep. Penilaian empirikal menunjukkan bahawa sistem membandingkan dengan set data yang tersedia untuk umum (seperti Piala KDD 2005) dan juga sebahagian aliran pertanyaan semasa Yahoo! Cari, di mana ia sudah mengubah pengalaman berjuta-juta pengguna carian Web. [[EENNDD]] rangkaian konsep; carian dan pengambilan maklumat; pengkategorian pertanyaan; rujukan silang; carian berasaskan pengetahuan; pembelajaran tanpa pengawasan; perlombongan web"], [{"string": "Topic initiator detection on the world wide web In this paper we introduce a new Web mining and search technique - Topic Initiator Detection (TID) on the Web. Given a topic query on the Internet and the resulting collection of time-stamped web documents which contain the query keywords, the task of TID is to automatically return which web document (or its author) initiated the topic or was the first to discuss about the topic.", "keywords": ["ranking", "topic initiator", "web mining", "information retrieval"], "combined": "Topic initiator detection on the world wide web In this paper we introduce a new Web mining and search technique - Topic Initiator Detection (TID) on the Web. Given a topic query on the Internet and the resulting collection of time-stamped web documents which contain the query keywords, the task of TID is to automatically return which web document (or its author) initiated the topic or was the first to discuss about the topic. [[EENNDD]] ranking; topic initiator; web mining; information retrieval"}, "Pengesanan pemula topik di web seluruh dunia Dalam makalah ini kami memperkenalkan teknik penambangan dan pencarian Web baru - Pengesanan Pemula Topik (TID) di Web. Memandangkan pertanyaan topik di Internet dan koleksi dokumen web yang dicap waktu yang dihasilkan yang mengandungi kata kunci pertanyaan, tugas TID adalah mengembalikan secara automatik dokumen web (atau pengarangnya) yang memulakan topik atau yang pertama membincangkan mengenai topik. [[EENNDD]] kedudukan; pemula topik; perlombongan web; pengambilan maklumat"], [{"string": "Facetnet: a framework for analyzing communities and their evolutions in dynamic networks We discover communities from social network data, and analyze the community evolution. These communities are inherent characteristics of human interaction in online social networks, as well as paper citation networks. Also, communities may evolve over time, due to changes to individuals' roles and social status in the network as well as changes to individuals' research interests. We present an innovative algorithm that deviates from the traditional two-step approach to analyze community evolutions. In the traditional approach, communities are first detected for each time slice, and then compared to determine correspondences. We argue that this approach is inappropriate in applications with noisy data. In this paper, we propose FacetNet for analyzing communities and their evolutions through a robust unified process. In this novel framework, communities not only generate evolutions, they also are regularized by the temporal smoothness of evolutions. As a result, this framework will discover communities that jointly maximize the fit to the observed data and the temporal evolution. Our approach relies on formulating the problem in terms of non-negative matrix factorization, where communities and their evolutions are factorized in a unified way. Then we develop an iterative algorithm, with proven low time complexity, which is guaranteed to converge to an optimal solution. We perform extensive experimental studies, on both synthetic datasets and real datasets, to demonstrate that our method discovers meaningful communities and provides additional insights not directly obtainable from traditional methods.", "keywords": ["evolution net", "soft membership", "non-negative matrix factorization", "community", "community net", "evolution"], "combined": "Facetnet: a framework for analyzing communities and their evolutions in dynamic networks We discover communities from social network data, and analyze the community evolution. These communities are inherent characteristics of human interaction in online social networks, as well as paper citation networks. Also, communities may evolve over time, due to changes to individuals' roles and social status in the network as well as changes to individuals' research interests. We present an innovative algorithm that deviates from the traditional two-step approach to analyze community evolutions. In the traditional approach, communities are first detected for each time slice, and then compared to determine correspondences. We argue that this approach is inappropriate in applications with noisy data. In this paper, we propose FacetNet for analyzing communities and their evolutions through a robust unified process. In this novel framework, communities not only generate evolutions, they also are regularized by the temporal smoothness of evolutions. As a result, this framework will discover communities that jointly maximize the fit to the observed data and the temporal evolution. Our approach relies on formulating the problem in terms of non-negative matrix factorization, where communities and their evolutions are factorized in a unified way. Then we develop an iterative algorithm, with proven low time complexity, which is guaranteed to converge to an optimal solution. We perform extensive experimental studies, on both synthetic datasets and real datasets, to demonstrate that our method discovers meaningful communities and provides additional insights not directly obtainable from traditional methods. [[EENNDD]] evolution net; soft membership; non-negative matrix factorization; community; community net; evolution"}, "Facetnet: kerangka kerja untuk menganalisis komuniti dan evolusi mereka dalam rangkaian dinamik. Kami menemui komuniti dari data rangkaian sosial, dan menganalisis evolusi komuniti. Komuniti-komuniti ini merupakan ciri interaksi manusia dalam rangkaian sosial dalam talian, dan juga rangkaian petikan kertas. Juga, masyarakat dapat berkembang dari masa ke masa, disebabkan oleh perubahan peranan individu dan status sosial dalam rangkaian serta perubahan terhadap kepentingan penyelidikan individu. Kami menyajikan algoritma inovatif yang menyimpang dari pendekatan dua langkah tradisional untuk menganalisis evolusi masyarakat. Dalam pendekatan tradisional, masyarakat pertama kali dikesan untuk setiap potongan masa, dan kemudian dibandingkan untuk menentukan korespondensi. Kami berpendapat bahawa pendekatan ini tidak sesuai dalam aplikasi dengan data yang bising. Dalam makalah ini, kami mencadangkan FacetNet untuk menganalisis komuniti dan evolusi mereka melalui proses penyatuan yang kuat. Dalam kerangka novel ini, masyarakat tidak hanya menghasilkan evolusi, mereka juga teratur oleh kelancaran evolusi temporal. Hasilnya, kerangka ini akan menemui masyarakat yang bersama-sama memaksimumkan kesesuaian dengan data yang diperhatikan dan evolusi temporal. Pendekatan kami bergantung pada merumuskan masalah dari segi faktorisasi matriks non-negatif, di mana komuniti dan evolusi mereka difaktorkan secara bersatu. Kemudian kami mengembangkan algoritma berulang, dengan kerumitan masa rendah yang terbukti, yang dijamin akan bergabung dengan penyelesaian yang optimum. Kami melakukan kajian eksperimental yang luas, baik pada kumpulan data sintetik dan set data sebenar, untuk menunjukkan bahawa kaedah kami menemui komuniti yang bermakna dan memberikan pandangan tambahan yang tidak dapat diperoleh secara langsung dari kaedah tradisional. [[EENNDD]] jaring evolusi; keahlian lembut; pemfaktoran matriks bukan negatif; komuniti; jaring komuniti; evolusi"], [{"string": "Towards robust service compositions in the context of functionally diverse services Web service composition provides a means of customized and flexible integration of service functionalities. Quality-of-Service (QoS) optimization algorithms select services in order to adapt workflows to the non-functional requirements of the user. With increasing number of services in a workflow, previous approaches fail to achieve a sufficient reliability. Moreover, expensive ad-hoc replanning is required to deal with service failures. The major problem with such sequential application of planning and replanning is that it ignores the potential costs during the initial planning and they consequently are hidden from the decision maker. Our basic idea to overcome this substantial problem is to compute a QoS optimized selection of service clusters that includes a sufficient number of backup services for each service employed. To support the human decision maker in the service selection task, our approach considers the possible repair costs directly in the initial composition. On the basis of a multi-objective approach and using a suitable service selection interface, the decision maker can select compositions in line with his/her personal risk preferences.", "keywords": ["service computing", "multi-objective optimization", "on-line information services", "robustness", "qos-aware service composition"], "combined": "Towards robust service compositions in the context of functionally diverse services Web service composition provides a means of customized and flexible integration of service functionalities. Quality-of-Service (QoS) optimization algorithms select services in order to adapt workflows to the non-functional requirements of the user. With increasing number of services in a workflow, previous approaches fail to achieve a sufficient reliability. Moreover, expensive ad-hoc replanning is required to deal with service failures. The major problem with such sequential application of planning and replanning is that it ignores the potential costs during the initial planning and they consequently are hidden from the decision maker. Our basic idea to overcome this substantial problem is to compute a QoS optimized selection of service clusters that includes a sufficient number of backup services for each service employed. To support the human decision maker in the service selection task, our approach considers the possible repair costs directly in the initial composition. On the basis of a multi-objective approach and using a suitable service selection interface, the decision maker can select compositions in line with his/her personal risk preferences. [[EENNDD]] service computing; multi-objective optimization; on-line information services; robustness; qos-aware service composition"}, "Ke arah komposisi perkhidmatan yang mantap dalam konteks perkhidmatan yang pelbagai fungsi Komposisi perkhidmatan web menyediakan kaedah penyatuan fungsi perkhidmatan yang disesuaikan dan fleksibel. Algoritma pengoptimuman Quality-of-Service (QoS) memilih perkhidmatan untuk menyesuaikan aliran kerja dengan keperluan pengguna yang tidak berfungsi. Dengan peningkatan jumlah perkhidmatan dalam aliran kerja, pendekatan sebelumnya gagal mencapai kebolehpercayaan yang mencukupi. Lebih-lebih lagi, penyusunan semula ad-hoc yang mahal diperlukan untuk menangani kegagalan perkhidmatan. Masalah utama dengan penerapan perencanaan dan penyusunan semula yang berurutan adalah bahawa ia mengabaikan kemungkinan kos semasa perancangan awal dan akibatnya ia tersembunyi dari pembuat keputusan. Idea asas kami untuk mengatasi masalah besar ini adalah mengira pemilihan kumpulan perkhidmatan yang dioptimumkan QoS yang merangkumi jumlah perkhidmatan sandaran yang mencukupi untuk setiap perkhidmatan yang digunakan. Untuk menyokong pembuat keputusan manusia dalam tugas pemilihan perkhidmatan, pendekatan kami mempertimbangkan kemungkinan kos pembaikan secara langsung dalam komposisi awal. Berdasarkan pendekatan multi-objektif dan menggunakan antara muka pemilihan perkhidmatan yang sesuai, pembuat keputusan dapat memilih komposisi sesuai dengan pilihan risiko peribadinya. [[EENNDD]] pengkomputeran perkhidmatan; pengoptimuman pelbagai objektif; perkhidmatan maklumat dalam talian; ketahanan; komposisi perkhidmatan qos-sedar"], [{"string": "Acquiring ontological knowledge from query logs We present a method for acquiring ontological knowledge using search query logs. We first use query logs to identify important contexts associated with terms belonging to a semantic category; we then use these contexts to harvest new words belonging to this category. Our evaluation on selected categories indicates that the method works very well to help harvesting terms, achieving 85% to 95% accuracy in categorizing newly acquired terms.", "keywords": ["ontology", "named entity", "query logs"], "combined": "Acquiring ontological knowledge from query logs We present a method for acquiring ontological knowledge using search query logs. We first use query logs to identify important contexts associated with terms belonging to a semantic category; we then use these contexts to harvest new words belonging to this category. Our evaluation on selected categories indicates that the method works very well to help harvesting terms, achieving 85% to 95% accuracy in categorizing newly acquired terms. [[EENNDD]] ontology; named entity; query logs"}, "Memperoleh pengetahuan ontologi dari log pertanyaan Kami menyajikan kaedah untuk memperoleh pengetahuan ontologi menggunakan log pertanyaan carian. Kami pertama kali menggunakan log pertanyaan untuk mengenal pasti konteks penting yang berkaitan dengan istilah yang tergolong dalam kategori semantik; kami kemudian menggunakan konteks ini untuk menuai kata-kata baru yang tergolong dalam kategori ini. Penilaian kami terhadap kategori terpilih menunjukkan bahawa kaedah ini berfungsi dengan baik untuk membantu penuaian istilah, mencapai ketepatan 85% hingga 95% dalam mengkategorikan istilah yang baru diperoleh. [[EENNDD]] ontologi; entiti bernama; log pertanyaan"], [{"string": "An abuse-free fair contract signing protocol based on the RSA signature No contact information provided yet.", "keywords": ["cryptographic protocols", "fair-exchange", "contract signing", "e-commerce", "network protocols", "rsa", "digital signatures"], "combined": "An abuse-free fair contract signing protocol based on the RSA signature No contact information provided yet. [[EENNDD]] cryptographic protocols; fair-exchange; contract signing; e-commerce; network protocols; rsa; digital signatures"}, "Protokol penandatanganan kontrak adil tanpa penyalahgunaan berdasarkan tandatangan RSA Belum ada maklumat hubungan yang diberikan. [[EENNDD]] protokol kriptografi; pertukaran wajar; menandatangani kontrak; e-dagang; protokol rangkaian; rsa; tandatangan digital"], [{"string": "Just the right amount: extracting modules from ontologies The ability to extract meaningful fragments from an ontology is key for ontology re-use. We propose a definition of a module that guarantees to completely capture the meaning of a given set of terms, i.e., to include all axioms relevant to the meaning of these terms, and study the problem of extracting minimal modules. We show that the problem of determining whether a subset of an ontology is a module for a given vocabulary is undecidable even for rather restricted sub-languages of OWL DL. Hence we propose two \"approximations\", i.e., alternative definitions of modules for a vocabulary that still provide the above guarantee, but that are possibly too strict, and that may thus result in larger modules: the first approximation is semantic and can be computed using existing DL reasoners; the second is syntactic, and can be computed in polynomial time. Finally, we report on an empirical evaluation of our syntactic approximation which demonstrates that the modules we extract are surprisingly small.", "keywords": ["ontologies", "description logics", "knowledge representation formalisms and methods", "semantic web", "owl"], "combined": "Just the right amount: extracting modules from ontologies The ability to extract meaningful fragments from an ontology is key for ontology re-use. We propose a definition of a module that guarantees to completely capture the meaning of a given set of terms, i.e., to include all axioms relevant to the meaning of these terms, and study the problem of extracting minimal modules. We show that the problem of determining whether a subset of an ontology is a module for a given vocabulary is undecidable even for rather restricted sub-languages of OWL DL. Hence we propose two \"approximations\", i.e., alternative definitions of modules for a vocabulary that still provide the above guarantee, but that are possibly too strict, and that may thus result in larger modules: the first approximation is semantic and can be computed using existing DL reasoners; the second is syntactic, and can be computed in polynomial time. Finally, we report on an empirical evaluation of our syntactic approximation which demonstrates that the modules we extract are surprisingly small. [[EENNDD]] ontologies; description logics; knowledge representation formalisms and methods; semantic web; owl"}, "Jumlah yang tepat: mengekstrak modul dari ontologi Keupayaan untuk mengekstrak serpihan yang bermakna dari ontologi adalah kunci untuk penggunaan semula ontologi. Kami mencadangkan definisi modul yang menjamin untuk benar-benar menangkap makna sekumpulan istilah yang diberikan, iaitu, merangkumi semua aksioma yang relevan dengan makna istilah ini, dan mengkaji masalah pengekstrakan modul minimum. Kami menunjukkan bahawa masalah untuk menentukan sama ada subset ontologi adalah modul untuk perbendaharaan kata tertentu tidak dapat ditentukan walaupun untuk sub-bahasa OWL DL yang agak terhad. Oleh itu, kami mencadangkan dua \"pendekatan\", iaitu, definisi alternatif modul untuk perbendaharaan kata yang masih memberikan jaminan di atas, tetapi mungkin terlalu ketat, dan dengan demikian dapat menghasilkan modul yang lebih besar: penghampiran pertama adalah semantik dan dapat dikira menggunakan penaakulan DL yang ada; yang kedua adalah sintaksis, dan dapat dikira dalam masa polinomial. Akhirnya, kami melaporkan penilaian empirik pendekatan sintaksis kami yang menunjukkan bahawa modul yang kami ekstrak sangat kecil. [[EENNDD]] ontologi; logik keterangan; formalisme dan kaedah perwakilan pengetahuan; web semantik; burung hantu"], [{"string": "A flexible learning system for wrapping tables and lists in HTML documents No contact information provided yet.", "keywords": ["reference matching", "canopy", "learning", "record linkage"], "combined": "A flexible learning system for wrapping tables and lists in HTML documents No contact information provided yet. [[EENNDD]] reference matching; canopy; learning; record linkage"}, "Sistem pembelajaran yang fleksibel untuk membungkus jadual dan senarai dalam dokumen HTML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] padanan rujukan; kanopi; belajar; pautan rakaman"], [{"string": "GalaTex: a conformant implementation of the XQuery full-text language No contact information provided yet.", "keywords": ["xquery", "full-text", "conformant prototype", "information search and retrieval"], "combined": "GalaTex: a conformant implementation of the XQuery full-text language No contact information provided yet. [[EENNDD]] xquery; full-text; conformant prototype; information search and retrieval"}, "GalaTex: pelaksanaan yang sesuai dari bahasa teks penuh XQuery Belum ada maklumat hubungan yang diberikan. [[EENNDD]] xquery; teks penuh; prototaip pemadu; pencarian dan pencarian maklumat"], [{"string": "Accelerating instant question search with database techniques Distributed question answering services, like Yahoo Answer and Aardvark, are known to be useful for end users and have also opened up numerous topics ranging in many research fields. In this paper, we propose a user-support tool for composing questions in such services. Our system incrementally recommends similar questions while users are typing their question in a sentence, which gives the users opportunities to know that there are similar questions that have already been solved. A question database is semantically analyzed and searched in the semantic space by boosting the performance of similarity searches with database techniques such as server/client caching and LSH (Locality Sensitive Hashing). The more text the user enters, the more similar the recommendations will become to the ultimately desired question. This unconscious editing-as-a-sequence-of-searches approach helps users to form their question incrementally through interactive supplementary information. Not only askers nor repliers, but also service providers have advantages such as that the knowledge of the service will be autonomously refined by avoiding for novice users to repeat questions which have been already solved.", "keywords": ["implementation", "question authoring", "lsh", "lsi"], "combined": "Accelerating instant question search with database techniques Distributed question answering services, like Yahoo Answer and Aardvark, are known to be useful for end users and have also opened up numerous topics ranging in many research fields. In this paper, we propose a user-support tool for composing questions in such services. Our system incrementally recommends similar questions while users are typing their question in a sentence, which gives the users opportunities to know that there are similar questions that have already been solved. A question database is semantically analyzed and searched in the semantic space by boosting the performance of similarity searches with database techniques such as server/client caching and LSH (Locality Sensitive Hashing). The more text the user enters, the more similar the recommendations will become to the ultimately desired question. This unconscious editing-as-a-sequence-of-searches approach helps users to form their question incrementally through interactive supplementary information. Not only askers nor repliers, but also service providers have advantages such as that the knowledge of the service will be autonomously refined by avoiding for novice users to repeat questions which have been already solved. [[EENNDD]] implementation; question authoring; lsh; lsi"}, "Mempercepat pencarian soalan segera dengan teknik pangkalan data Perkhidmatan menjawab soalan yang diedarkan, seperti Yahoo Answer dan Aardvark, diketahui berguna untuk pengguna akhir dan juga telah membuka banyak topik mulai dari banyak bidang penyelidikan. Dalam makalah ini, kami mencadangkan alat sokongan pengguna untuk menyusun soalan dalam perkhidmatan tersebut. Sistem kami secara bertahap mengesyorkan soalan serupa semasa pengguna mengetik soalan mereka dalam satu kalimat, yang memberi pengguna peluang untuk mengetahui bahawa ada pertanyaan serupa yang telah diselesaikan. Pangkalan data soalan dianalisis secara semantik dan dicari di ruang semantik dengan meningkatkan prestasi carian kesamaan dengan teknik pangkalan data seperti pelayan / caching pelanggan dan LSH (Lokal Sensitive Hashing). Semakin banyak teks yang dimasukkan oleh pengguna, saranan yang serupa akan menjadi persoalan yang diinginkan. Pendekatan penyuntingan-sebagai-urutan-carian yang tidak sedar ini membantu pengguna untuk membentuk soalan mereka secara bertahap melalui maklumat tambahan interaktif. Bukan hanya penanya atau pembalas, tetapi juga penyedia perkhidmatan mempunyai kelebihan seperti pengetahuan mengenai perkhidmatan tersebut akan diperbaiki secara autonomi dengan mengelakkan pengguna baru mengulangi soalan yang telah diselesaikan. [[EENNDD]] pelaksanaan; pengarang soalan; lsh; lsi"], [{"string": "To join or not to join: the illusion of privacy in social networks with mixed public and private user profiles In order to address privacy concerns, many social media websites allow users to hide their personal profiles from the public. In this work, we show how an adversary can exploit an online social network with a mixture of public and private user profiles to predict the private attributes of users. We map this problem to a relational classification problem and we propose practical models that use friendship and group membership information (which is often not hidden) to infer sensitive attributes. The key novel idea is that in addition to friendship links, groups can be carriers of significant information. We show that on several well-known social media sites, we can easily and accurately recover the information of private-profile users. To the best of our knowledge, this is the first work that uses link-based and group-based classification to study privacy implications in social networks with mixed public and private user profiles.", "keywords": ["social networks", "attribute inference", "privacy", "groups"], "combined": "To join or not to join: the illusion of privacy in social networks with mixed public and private user profiles In order to address privacy concerns, many social media websites allow users to hide their personal profiles from the public. In this work, we show how an adversary can exploit an online social network with a mixture of public and private user profiles to predict the private attributes of users. We map this problem to a relational classification problem and we propose practical models that use friendship and group membership information (which is often not hidden) to infer sensitive attributes. The key novel idea is that in addition to friendship links, groups can be carriers of significant information. We show that on several well-known social media sites, we can easily and accurately recover the information of private-profile users. To the best of our knowledge, this is the first work that uses link-based and group-based classification to study privacy implications in social networks with mixed public and private user profiles. [[EENNDD]] social networks; attribute inference; privacy; groups"}, "Untuk bergabung atau tidak bergabung: ilusi privasi di rangkaian sosial dengan profil pengguna awam dan swasta yang bercampur-campur Untuk mengatasi masalah privasi, banyak laman web media sosial membenarkan pengguna menyembunyikan profil peribadinya dari orang ramai. Dalam karya ini, kami menunjukkan bagaimana musuh dapat memanfaatkan jaringan sosial dalam talian dengan campuran profil pengguna awam dan swasta untuk meramalkan sifat peribadi pengguna. Kami memetakan masalah ini kepada masalah klasifikasi relasional dan kami mencadangkan model praktikal yang menggunakan maklumat persahabatan dan keahlian kumpulan (yang sering tidak disembunyikan) untuk menyimpulkan sifat sensitif. Idea novel utama adalah bahawa selain hubungan persahabatan, kumpulan dapat menjadi pembawa maklumat penting. Kami menunjukkan bahawa di beberapa laman media sosial yang terkenal, kami dapat dengan mudah dan tepat mendapatkan maklumat pengguna profil peribadi. Sepengetahuan kami, ini adalah karya pertama yang menggunakan klasifikasi berdasarkan pautan dan berdasarkan kumpulan untuk mengkaji implikasi privasi di rangkaian sosial dengan profil pengguna awam dan swasta yang bercampur. [[EENNDD]] rangkaian sosial; inferens atribut; privasi; kumpulan"], [{"string": "Combining RDF and XML schemas to enhance interoperability between metadata application profiles An abstract is not available.", "keywords": ["xslt", "interoperability", "document preparation", "schema", "xml", "rdf", "metadata"], "combined": "Combining RDF and XML schemas to enhance interoperability between metadata application profiles An abstract is not available. [[EENNDD]] xslt; interoperability; document preparation; schema; xml; rdf; metadata"}, "Menggabungkan skema RDF dan XML untuk meningkatkan interoperabiliti antara profil aplikasi metadata Abstrak tidak tersedia. [[EENNDD]] xslt; saling kendali; penyediaan dokumen; skema; xml; rdf; metadata"], [{"string": "Extracting semantic structure of web documents using content and visual information No contact information provided yet.", "keywords": ["dom", "hypertext/hypermedia", "naive bayes classifier", "page segmentation", "topic hierarchy", "vips"], "combined": "Extracting semantic structure of web documents using content and visual information No contact information provided yet. [[EENNDD]] dom; hypertext/hypermedia; naive bayes classifier; page segmentation; topic hierarchy; vips"}, "Mengambil struktur semantik dokumen web menggunakan kandungan dan maklumat visual Belum ada maklumat hubungan yang diberikan. [[EENNDD]] dom; hiperteks / hipermedia; pengkelasan bayes naif; pembahagian halaman; hierarki topik; vips"], [{"string": "Finding group shilling in recommendation system No contact information provided yet.", "keywords": ["collaborative filtering", "group shilling", "recommendation system"], "combined": "Finding group shilling in recommendation system No contact information provided yet. [[EENNDD]] collaborative filtering; group shilling; recommendation system"}, "Mencari shilling kumpulan dalam sistem cadangan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penapisan kolaboratif; shilling kumpulan; sistem cadangan"], [{"string": "On admission control for profit maximization of networked service providers No contact information provided yet.", "keywords": ["profit maximization", "service time estimation", "quality of service", "shortest remaining job first", "admission control", "short-term prediction", "web service", "performance of systems", "service level agreement"], "combined": "On admission control for profit maximization of networked service providers No contact information provided yet. [[EENNDD]] profit maximization; service time estimation; quality of service; shortest remaining job first; admission control; short-term prediction; web service; performance of systems; service level agreement"}, "Semasa kawalan kemasukan untuk memaksimumkan keuntungan penyedia perkhidmatan rangkaian Tidak ada maklumat hubungan yang disediakan. [[EENNDD]] memaksimumkan keuntungan; anggaran masa perkhidmatan; kualiti sesuatu servis; pekerjaan selebihnya terpendek terlebih dahulu; kawalan kemasukan; ramalan jangka pendek; perkhidmatan web; prestasi sistem; perjanjian tahap perkhidmatan"], [{"string": "Generating summaries for large collections of geo-referenced photographs No contact information provided yet.", "keywords": ["photo collections", "geo-referenced photographs", "semantic zoom", "information search and retrieval", "photo browsing", "collection summary", "geo-referenced information"], "combined": "Generating summaries for large collections of geo-referenced photographs No contact information provided yet. [[EENNDD]] photo collections; geo-referenced photographs; semantic zoom; information search and retrieval; photo browsing; collection summary; geo-referenced information"}, "Membuat ringkasan untuk koleksi besar gambar-gambar yang dirujuk secara geo. Belum ada maklumat hubungan yang diberikan. [[EENNDD]] koleksi gambar; gambar rujukan geo; zum semantik; pencarian dan pengambilan maklumat; melayari gambar; ringkasan koleksi; maklumat yang dirujuk secara geo"], [{"string": "Predicting outcomes of web navigation No contact information provided yet.", "keywords": ["web navigation", "lostness", "stratum", "path similarity", "compactness"], "combined": "Predicting outcomes of web navigation No contact information provided yet. [[EENNDD]] web navigation; lostness; stratum; path similarity; compactness"}, "Meramalkan hasil navigasi web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] navigasi web; kehilangan; stratum; persamaan jalan; kekompakan"], [{"string": "The case for multi-user design for computer aided learning in developing regions No contact information provided yet.", "keywords": ["developing regions", "computer uses in education"], "combined": "The case for multi-user design for computer aided learning in developing regions No contact information provided yet. [[EENNDD]] developing regions; computer uses in education"}, "Kes untuk reka bentuk pelbagai pengguna untuk pembelajaran berbantukan komputer di wilayah membangun Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] kawasan membangun; penggunaan komputer dalam pendidikan"], [{"string": "Mapping XML instances No contact information provided yet.", "keywords": ["miscellaneous"], "combined": "Mapping XML instances No contact information provided yet. [[EENNDD]] miscellaneous"}, "Memetakan contoh XML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pelbagai"], [{"string": "Enhancing the privacy of web-based communication No contact information provided yet.", "keywords": ["protection", "general", "profiling", "privacy"], "combined": "Enhancing the privacy of web-based communication No contact information provided yet. [[EENNDD]] protection; general; profiling; privacy"}, "Meningkatkan privasi komunikasi berasaskan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlindungan; umum; profil; privasi"], [{"string": "Economics of BitTorrent communities Over the years, private file-sharing communities built on the BitTorrent protocol have developed their own policies and mechanisms for motivating members to share content and contribute resources. By requiring members to maintain a minimum ratio between uploads and downloads, private communities effectively establish credit systems, and with them full-fledged economies. We report on a half-year-long measurement study of DIME -- a community for sharing live concert recordings -- that sheds light on the economic forces affecting users in such communities. A key observation is that while the download of files is priced only according to the size of the file, the rate of return for seeding new files is significantly greater than for seeding old files. We find via a natural experiment that users react to such differences in resale value by preferentially consuming older files during a 'free leech' period. We consider implications of these finding on a user's ability to earn credits and meet ratio enforcements, focusing in particular on the relationship between visitation frequency and wealth and on low bandwidth users. We then share details from an interview with DIME moderators, which highlights the goals of the community based on which we make suggestions for possible improvement.", "keywords": ["bittorrent", "resale value", "share ratio enforcement", "incentives", "private communities", "peer-to-peer", "distributed systems"], "combined": "Economics of BitTorrent communities Over the years, private file-sharing communities built on the BitTorrent protocol have developed their own policies and mechanisms for motivating members to share content and contribute resources. By requiring members to maintain a minimum ratio between uploads and downloads, private communities effectively establish credit systems, and with them full-fledged economies. We report on a half-year-long measurement study of DIME -- a community for sharing live concert recordings -- that sheds light on the economic forces affecting users in such communities. A key observation is that while the download of files is priced only according to the size of the file, the rate of return for seeding new files is significantly greater than for seeding old files. We find via a natural experiment that users react to such differences in resale value by preferentially consuming older files during a 'free leech' period. We consider implications of these finding on a user's ability to earn credits and meet ratio enforcements, focusing in particular on the relationship between visitation frequency and wealth and on low bandwidth users. We then share details from an interview with DIME moderators, which highlights the goals of the community based on which we make suggestions for possible improvement. [[EENNDD]] bittorrent; resale value; share ratio enforcement; incentives; private communities; peer-to-peer; distributed systems"}, "Ekonomi komuniti BitTorrent Selama bertahun-tahun, komuniti perkongsian fail peribadi yang dibina berdasarkan protokol BitTorrent telah mengembangkan dasar dan mekanisme mereka sendiri untuk memotivasi ahli untuk berkongsi kandungan dan menyumbang sumber. Dengan mewajibkan anggota untuk mengekalkan nisbah minimum antara muat naik dan muat turun, komuniti swasta secara efektif mewujudkan sistem kredit, dan dengan mereka ekonomi penuh. Kami melaporkan kajian pengukuran DIME selama setengah tahun - sebuah komuniti untuk berkongsi rakaman konsert secara langsung - yang menjelaskan kekuatan ekonomi yang mempengaruhi pengguna di komuniti tersebut. Pemerhatian utama adalah bahawa walaupun muat turun fail berharga hanya mengikut ukuran fail, kadar pengembalian untuk penyemaian fail baru jauh lebih besar daripada pembenihan fail lama. Kami dapati melalui eksperimen semula jadi bahawa pengguna bertindak balas terhadap perbezaan nilai penjualan semula dengan lebih baik menggunakan fail lama dalam tempoh 'lintah bebas'. Kami mempertimbangkan implikasi dari penemuan ini pada kemampuan pengguna untuk memperoleh kredit dan memenuhi penegakan nisbah, yang memfokuskan khususnya pada hubungan antara kekerapan lawatan dan kekayaan dan pada pengguna lebar jalur rendah. Kami kemudian berkongsi butiran dari temu bual dengan moderator DIME, yang menyoroti tujuan komuniti berdasarkan mana kami membuat cadangan untuk kemungkinan peningkatan. [[EENNDD]] bittorrent; nilai jualan semula; penguatkuasaan nisbah bahagian; insentif; komuniti swasta; rakan sebaya; sistem yang diedarkan"], [{"string": "Probabilistic models for discovering e-communities No contact information provided yet.", "keywords": ["statistical modeling", "data mining", "gibbs sampling", "learning", "email", "social network", "clustering"], "combined": "Probabilistic models for discovering e-communities No contact information provided yet. [[EENNDD]] statistical modeling; data mining; gibbs sampling; learning; email; social network; clustering"}, "Model probabilistik untuk menemui e-komuniti Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pemodelan statistik; perlombongan data; persampelan gibbs; belajar; e-mel; rangkaian sosial; pengelompokan"], [{"string": "The interoperability of learning object repositories and services: standards, implementations and lessons learned No contact information provided yet.", "keywords": ["learning object repositories", "interoperability"], "combined": "The interoperability of learning object repositories and services: standards, implementations and lessons learned No contact information provided yet. [[EENNDD]] learning object repositories; interoperability"}, "Kebolehoperasian repositori dan perkhidmatan objek pembelajaran: standard, pelaksanaan dan pelajaran yang dipelajari Belum ada maklumat hubungan yang diberikan. [[EENNDD]] repositori objek pembelajaran; saling kendali"], [{"string": "Characterizing search intent diversity into click models Modeling a user's click-through behavior in click logs is a challenging task due to the well-known position bias problem. Recent advances in click models have adopted the examination hypothesis which distinguishes document relevance from position bias. In this paper, we revisit the examination hypothesis and observe that user clicks cannot be completely explained by relevance and position bias. Specifically, users with different search intents may submit the same query to the search engine but expect different search results. Thus, there might be a bias between user search intent and the query formulated by the user, which can lead to the diversity in user clicks. This bias has not been considered in previous works such as UBM, DBN and CCM. In this paper, we propose a new intent hypothesis as a complement to the examination hypothesis. This hypothesis is used to characterize the bias between the user search intent and the query in each search session. This hypothesis is very general and can be applied to most of the existing click models to improve their capacities in learning unbiased relevance. Experimental results demonstrate that after adopting the intent hypothesis, click models can better interpret user clicks and achieve a significant NDCG improvement.", "keywords": ["search engine", "intent bias", "intent diversity", "user behavior", "click model"], "combined": "Characterizing search intent diversity into click models Modeling a user's click-through behavior in click logs is a challenging task due to the well-known position bias problem. Recent advances in click models have adopted the examination hypothesis which distinguishes document relevance from position bias. In this paper, we revisit the examination hypothesis and observe that user clicks cannot be completely explained by relevance and position bias. Specifically, users with different search intents may submit the same query to the search engine but expect different search results. Thus, there might be a bias between user search intent and the query formulated by the user, which can lead to the diversity in user clicks. This bias has not been considered in previous works such as UBM, DBN and CCM. In this paper, we propose a new intent hypothesis as a complement to the examination hypothesis. This hypothesis is used to characterize the bias between the user search intent and the query in each search session. This hypothesis is very general and can be applied to most of the existing click models to improve their capacities in learning unbiased relevance. Experimental results demonstrate that after adopting the intent hypothesis, click models can better interpret user clicks and achieve a significant NDCG improvement. [[EENNDD]] search engine; intent bias; intent diversity; user behavior; click model"}, "Mencirikan kepelbagaian maksud carian menjadi model klik Memodelkan tingkah laku klik-tayang pengguna dalam log klik adalah tugas yang mencabar kerana masalah bias kedudukan yang terkenal. Kemajuan terkini dalam model klik telah mengadopsi hipotesis pemeriksaan yang membezakan perkaitan dokumen dengan bias kedudukan. Dalam makalah ini, kami meninjau kembali hipotesis pemeriksaan dan melihat bahawa klik pengguna tidak dapat dijelaskan sepenuhnya oleh perkaitan dan bias kedudukan. Secara khusus, pengguna dengan maksud carian yang berbeza dapat mengirimkan pertanyaan yang sama ke mesin pencari tetapi mengharapkan hasil carian yang berbeza. Oleh itu, mungkin terdapat bias antara maksud carian pengguna dan pertanyaan yang dirumuskan oleh pengguna, yang dapat menyebabkan kepelbagaian dalam klik pengguna. Bias ini tidak dipertimbangkan dalam karya sebelumnya seperti UBM, DBN dan CCM. Dalam makalah ini, kami mencadangkan hipotesis niat baru sebagai pelengkap kepada hipotesis pemeriksaan. Hipotesis ini digunakan untuk mencirikan bias antara maksud carian pengguna dan pertanyaan dalam setiap sesi carian. Hipotesis ini sangat umum dan dapat diterapkan pada kebanyakan model klik yang ada untuk meningkatkan keupayaan mereka dalam mempelajari relevansi yang tidak berat sebelah. Hasil eksperimen menunjukkan bahawa setelah menggunakan hipotesis niat, model klik dapat menafsirkan klik pengguna dengan lebih baik dan mencapai peningkatan NDCG yang ketara. [[EENNDD]] enjin carian; berat sebelah niat; kepelbagaian niat; tingkah laku pengguna; model klik"], [{"string": "Mapping-driven XML transformation Clio is an existing schema-mapping tool that provides user-friendly means to manage and facilitate the complex task of transformation and integration of heterogeneous data such as XML over the Web or in XML databases. By means of mappings from source to target schemas, Clio can help users conveniently establish the precise semantics of data transformation and integration. In this paper we study the problem of how to efficiently implement such data transformation (i.e., generating target data from the source data based on schema mappings). We present a three-phase framework for high-performance XML-to-XML transformation based on schema mappings, and discuss methodologies and algorithms for implementing these phases. In particular, we elaborate on novel techniques such as streamed extraction of mapped source values and scalable disk-based merging of overlapping data (including duplicate elimination). We compare our transformation framework with alternative methods such as using XQuery or SQL/XML provided by current commercial databases. The results demonstrate that the three-phase framework (although as simple as it is) is highly scalable and outperforms the alternative methods by orders of magnitude.", "keywords": ["xml transformation", "schema mapping", "miscellaneous"], "combined": "Mapping-driven XML transformation Clio is an existing schema-mapping tool that provides user-friendly means to manage and facilitate the complex task of transformation and integration of heterogeneous data such as XML over the Web or in XML databases. By means of mappings from source to target schemas, Clio can help users conveniently establish the precise semantics of data transformation and integration. In this paper we study the problem of how to efficiently implement such data transformation (i.e., generating target data from the source data based on schema mappings). We present a three-phase framework for high-performance XML-to-XML transformation based on schema mappings, and discuss methodologies and algorithms for implementing these phases. In particular, we elaborate on novel techniques such as streamed extraction of mapped source values and scalable disk-based merging of overlapping data (including duplicate elimination). We compare our transformation framework with alternative methods such as using XQuery or SQL/XML provided by current commercial databases. The results demonstrate that the three-phase framework (although as simple as it is) is highly scalable and outperforms the alternative methods by orders of magnitude. [[EENNDD]] xml transformation; schema mapping; miscellaneous"}, "Transformasi XML yang didorong oleh pemetaan Clio adalah alat pemetaan skema yang ada yang menyediakan cara yang mesra pengguna untuk mengurus dan memfasilitasi tugas kompleks transformasi dan integrasi data heterogen seperti XML melalui Web atau dalam pangkalan data XML. Dengan pemetaan dari skema sumber ke sasaran, Clio dapat membantu pengguna dengan mudah membuat semantik tepat transformasi dan integrasi data. Dalam makalah ini kami mempelajari masalah bagaimana melaksanakan transformasi data seperti itu secara efisien (yaitu, menghasilkan data sasaran dari data sumber berdasarkan pemetaan skema). Kami menyajikan kerangka tiga fasa untuk transformasi XML-ke-XML berprestasi tinggi berdasarkan pemetaan skema, dan membincangkan metodologi dan algoritma untuk melaksanakan fasa-fasa ini. Khususnya, kami menghuraikan teknik baru seperti pengekstrakan aliran dari nilai sumber yang dipetakan dan penggabungan berdasarkan disk yang boleh diskalakan berdasarkan data bertindih (termasuk penghapusan pendua). Kami membandingkan kerangka transformasi kami dengan kaedah alternatif seperti menggunakan XQuery atau SQL / XML yang disediakan oleh pangkalan data komersial semasa. Hasilnya menunjukkan bahawa kerangka tiga fasa (walaupun sesederhana itu) sangat berskala dan mengungguli kaedah alternatif dengan susunan besar. [[EENNDD]] transformasi xml; pemetaan skema; pelbagai"], [{"string": "Smartback: supporting users in back navigation No contact information provided yet.", "keywords": ["usability study", "back navigation", "web trails", "browsing", "web usage", "revisitation"], "combined": "Smartback: supporting users in back navigation No contact information provided yet. [[EENNDD]] usability study; back navigation; web trails; browsing; web usage; revisitation"}, "Smartback: menyokong pengguna dalam navigasi belakang Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kajian kebolehgunaan; navigasi belakang; jejak web; melayari; penggunaan laman web; penyemakan semula"], [{"string": "Compressed web indexes Web search engines use indexes to efficiently retrieve pages containing specified query terms, as well as pages linking to specified pages. The problem of compressed indexes that permit such fast retrieval has a long history. We consider the problem: assuming that the terms in (or links to) a page are generated from a probability distribution, how well compactly can we build such indexes that allow fast retrieval? Of particular interest is the case when the probability distribution is Zipfian (or a similar power law), since these are the distributions that arise on the web. We obtain sharp bounds on the space requirement of Boolean indexes for text documents that follow Zipf's law. In the process we develop a general technique that applies to any probability distribution, not necessarily a power law; this is the first analysis of compression in indexes under arbitrary distributions. Our bounds lead to quantitative versions of rules of thumb that are folklore in indexing. Our experiments on several document collections show that the distribution of terms appears to follow a double-Pareto law rather than Zipf's law. Despite widely varying sets of documents, the index sizes observed in the experiments conform well to our theoretical predictions.", "keywords": ["double-pareto", "information search and retrieval", "compression", "power law", "index size"], "combined": "Compressed web indexes Web search engines use indexes to efficiently retrieve pages containing specified query terms, as well as pages linking to specified pages. The problem of compressed indexes that permit such fast retrieval has a long history. We consider the problem: assuming that the terms in (or links to) a page are generated from a probability distribution, how well compactly can we build such indexes that allow fast retrieval? Of particular interest is the case when the probability distribution is Zipfian (or a similar power law), since these are the distributions that arise on the web. We obtain sharp bounds on the space requirement of Boolean indexes for text documents that follow Zipf's law. In the process we develop a general technique that applies to any probability distribution, not necessarily a power law; this is the first analysis of compression in indexes under arbitrary distributions. Our bounds lead to quantitative versions of rules of thumb that are folklore in indexing. Our experiments on several document collections show that the distribution of terms appears to follow a double-Pareto law rather than Zipf's law. Despite widely varying sets of documents, the index sizes observed in the experiments conform well to our theoretical predictions. [[EENNDD]] double-pareto; information search and retrieval; compression; power law; index size"}, "Indeks web terkompresi Mesin pencari web menggunakan indeks untuk mengambil halaman yang mengandungi istilah pertanyaan yang ditentukan, serta halaman yang menghubungkan ke halaman tertentu dengan cekap. Masalah indeks termampat yang membolehkan pengambilan cepat seperti ini mempunyai sejarah yang panjang. Kami menganggap masalahnya: dengan mengandaikan bahawa istilah dalam (atau pautan ke) halaman dihasilkan dari taburan kebarangkalian, seberapa baik kita dapat membina indeks sedemikian yang memungkinkan pengambilan cepat? Yang menarik perhatian adalah kes apabila sebaran kebarangkalian adalah Zipfian (atau undang-undang kuasa yang serupa), kerana ini adalah pengedaran yang timbul di web. Kami memperoleh batasan tajam mengenai keperluan ruang indeks Boolean untuk dokumen teks yang mengikuti undang-undang Zipf. Dalam prosesnya kami mengembangkan teknik umum yang berlaku untuk sebaran kebarangkalian, tidak semestinya undang-undang kuasa; ini adalah analisis pertama pemampatan dalam indeks di bawah pengagihan sewenang-wenangnya. Batasan kami membawa kepada versi kuantitatif aturan praktis yang merupakan cerita rakyat dalam pengindeksan. Eksperimen kami pada beberapa koleksi dokumen menunjukkan bahawa pengedaran istilah nampaknya mengikuti undang-undang Pareto berganda dan bukannya undang-undang Zipf. Walaupun terdapat banyak dokumen yang berbeza-beza, ukuran indeks yang diperhatikan dalam eksperimen sesuai dengan ramalan teori kami. [[EENNDD]] double-pareto; carian dan pengambilan maklumat; pemampatan; kuasa undang-undang; ukuran indeks"], [{"string": "Measurement and analysis of cyberlocker services Cyberlocker Services (CLS) such as RapidShare and Megaupload have recently become popular. The decline of Peer-to-Peer (P2P) file sharing has prompted various services including CLS to replace it. We propose a comprehensive multi-level characterization of the CLS ecosystem. We answer three research questions: (a) what is a suitable measurement infrastructure for gathering CLS workloads; (b) what are the characteristics of the CLS ecosystem; and (c) what are the implications of CLS on Web 2.0 (and the Internet). To the best of our knowledge, this work is the first to characterize the CLS ecosystem. The work will highlight the content, usage, performance, infrastructure, quality of service, and evolution characteristics of CLS.", "keywords": ["rapidshare", "web 2.0", "cyberlockers", "file hosting services"], "combined": "Measurement and analysis of cyberlocker services Cyberlocker Services (CLS) such as RapidShare and Megaupload have recently become popular. The decline of Peer-to-Peer (P2P) file sharing has prompted various services including CLS to replace it. We propose a comprehensive multi-level characterization of the CLS ecosystem. We answer three research questions: (a) what is a suitable measurement infrastructure for gathering CLS workloads; (b) what are the characteristics of the CLS ecosystem; and (c) what are the implications of CLS on Web 2.0 (and the Internet). To the best of our knowledge, this work is the first to characterize the CLS ecosystem. The work will highlight the content, usage, performance, infrastructure, quality of service, and evolution characteristics of CLS. [[EENNDD]] rapidshare; web 2.0; cyberlockers; file hosting services"}, "Pengukuran dan analisis perkhidmatan cyberlocker Cyberlocker Services (CLS) seperti RapidShare dan Megaupload baru-baru ini menjadi popular. Penurunan perkongsian fail Peer-to-Peer (P2P) telah mendorong pelbagai perkhidmatan termasuk CLS untuk menggantinya. Kami mencadangkan pencirian pelbagai peringkat ekosistem CLS yang komprehensif. Kami menjawab tiga soalan penyelidikan: (a) apakah infrastruktur pengukuran yang sesuai untuk mengumpulkan beban kerja CLS; (b) apakah ciri-ciri ekosistem CLS; dan (c) apakah implikasi CLS di Web 2.0 (dan Internet). Sepengetahuan kami, karya ini adalah yang pertama mencirikan ekosistem CLS. Karya ini akan menyoroti kandungan, penggunaan, prestasi, infrastruktur, kualiti perkhidmatan, dan ciri evolusi CLS. [[EENNDD]] perkongsian cepat; laman web 2.0; penyekat siber; perkhidmatan hosting fail"], [{"string": "Optimizing scoring functions and indexes for proximity search in type-annotated corpora No contact information provided yet.", "keywords": ["indexing annotated text"], "combined": "Optimizing scoring functions and indexes for proximity search in type-annotated corpora No contact information provided yet. [[EENNDD]] indexing annotated text"}, "Mengoptimumkan fungsi pemarkahan dan indeks untuk carian jarak di korporat jenis-anotasi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] mengindeks teks beranotasi"], [{"string": "Constructing extensible XQuery mappings No contact information provided yet.", "keywords": ["xquery", "mapping", "automated support", "heterogeneous databases", "extensibility"], "combined": "Constructing extensible XQuery mappings No contact information provided yet. [[EENNDD]] xquery; mapping; automated support; heterogeneous databases; extensibility"}, "Membuat pemetaan XQuery yang boleh diperluas Belum ada maklumat hubungan yang diberikan. [[EENNDD]] xquery; pemetaan; sokongan automatik; pangkalan data heterogen; kepanjangan"], [{"string": "Long distance wireless mesh network planning: problem formulation and solution Several research efforts as well as deployments have chosen IEEE802.11 as a low-cost, long-distance access technology to bridge the digital divide. In this paper, we consider the important issue of planning such networks to the minimize system cost. This is a non-trivial task since it involves several sets of variables: the network topology, tower heights, antenna types to be used and the irorientations, and radio transmit powers. The task is further complicated due to the presence of network performance constraints, and the inter-dependence among the variables. Our first contribution in this paper is the formulation of this problem in terms of the variables, constraints and the optimization criterion. Our second contribution is in identifying the dependencies among the variables and breaking-down the problem into four tractable sub-parts. In this process, we extensively use domain knowledge to strike a balance between tractability and practicality.", "keywords": ["long-distance wifi", "802.11 mesh networks", "rural connectivity", "low cost networking"], "combined": "Long distance wireless mesh network planning: problem formulation and solution Several research efforts as well as deployments have chosen IEEE802.11 as a low-cost, long-distance access technology to bridge the digital divide. In this paper, we consider the important issue of planning such networks to the minimize system cost. This is a non-trivial task since it involves several sets of variables: the network topology, tower heights, antenna types to be used and the irorientations, and radio transmit powers. The task is further complicated due to the presence of network performance constraints, and the inter-dependence among the variables. Our first contribution in this paper is the formulation of this problem in terms of the variables, constraints and the optimization criterion. Our second contribution is in identifying the dependencies among the variables and breaking-down the problem into four tractable sub-parts. In this process, we extensively use domain knowledge to strike a balance between tractability and practicality. [[EENNDD]] long-distance wifi; 802.11 mesh networks; rural connectivity; low cost networking"}, "Perancangan rangkaian jarak jauh tanpa wayar: rumusan dan penyelesaian masalah Beberapa usaha penyelidikan dan penyebaran telah memilih IEEE802.11 sebagai teknologi akses jarak jauh yang murah untuk merapatkan jurang digital. Dalam makalah ini, kami mempertimbangkan masalah penting merancang rangkaian sedemikian untuk meminimumkan kos sistem. Ini adalah tugas yang tidak sepele kerana melibatkan beberapa set pemboleh ubah: topologi rangkaian, ketinggian menara, jenis antena yang akan digunakan dan irorientasi, dan daya penghantaran radio. Tugas ini semakin rumit kerana adanya kekangan prestasi rangkaian, dan saling bergantung antara pemboleh ubah. Sumbangan pertama kami dalam makalah ini adalah rumusan masalah ini dari segi pemboleh ubah, kekangan dan kriteria pengoptimuman. Sumbangan kedua kami adalah dalam mengenal pasti pergantungan antara pemboleh ubah dan memecah masalah menjadi empat sub-bahagian yang dapat disembuhkan. Dalam proses ini, kami menggunakan pengetahuan domain secara meluas untuk mencapai keseimbangan antara kebolehlenturan dan kepraktisan. [[EENNDD]] wifi jarak jauh; Rangkaian mesh 802.11; penyambungan luar bandar; rangkaian kos rendah"], [{"string": "Expertise networks in online communities: structure and algorithms Web-based communities have become important places for people to seek and share expertise. We find that networks in these communities typically differ in their topology from other online networks such as the World Wide Web. Systems targeted to augment web-based communities by automatically identifying users with expertise, for example, need to adapt to the underlying interaction dynamics. In this study, we analyze the Java Forum, a large online help-seeking community, using social network analysis methods. We test a set of network-based ranking algorithms, including PageRank and HITS, on this large size social network in order to identify users with high expertise. We then use simulations to identify a small number of simple simulation rules governing the question-answer dynamic in the network. These simple rules not only replicate the structural characteristics and algorithm performance on the empirically observed Java Forum, but also allow us to evaluate how other algorithms may perform in communities with different characteristics. We believe this approach will be fruitful for practical algorithm design and implementation for online expertise-sharing communities.", "keywords": ["expertise finding", "help seeking", "expertise locators", "simulation", "social network analysis", "online communities"], "combined": "Expertise networks in online communities: structure and algorithms Web-based communities have become important places for people to seek and share expertise. We find that networks in these communities typically differ in their topology from other online networks such as the World Wide Web. Systems targeted to augment web-based communities by automatically identifying users with expertise, for example, need to adapt to the underlying interaction dynamics. In this study, we analyze the Java Forum, a large online help-seeking community, using social network analysis methods. We test a set of network-based ranking algorithms, including PageRank and HITS, on this large size social network in order to identify users with high expertise. We then use simulations to identify a small number of simple simulation rules governing the question-answer dynamic in the network. These simple rules not only replicate the structural characteristics and algorithm performance on the empirically observed Java Forum, but also allow us to evaluate how other algorithms may perform in communities with different characteristics. We believe this approach will be fruitful for practical algorithm design and implementation for online expertise-sharing communities. [[EENNDD]] expertise finding; help seeking; expertise locators; simulation; social network analysis; online communities"}, "Jaringan kepakaran dalam komuniti dalam talian: struktur dan algoritma Komuniti berasaskan web telah menjadi tempat penting bagi orang untuk mencari dan berkongsi kepakaran. Kami mendapati bahawa rangkaian dalam komuniti ini biasanya berbeza dalam topologi mereka daripada rangkaian dalam talian lain seperti World Wide Web. Sistem yang disasarkan untuk meningkatkan komuniti berasaskan web dengan secara automatik mengenal pasti pengguna dengan kepakaran, misalnya, perlu menyesuaikan diri dengan dinamika interaksi yang mendasari. Dalam kajian ini, kami menganalisis Java Forum, sebuah komuniti mencari bantuan dalam talian yang besar, menggunakan kaedah analisis rangkaian sosial. Kami menguji satu set algoritma peringkat berdasarkan rangkaian, termasuk PageRank dan HITS, di rangkaian sosial bersaiz besar ini untuk mengenal pasti pengguna yang mempunyai kepakaran tinggi. Kami kemudian menggunakan simulasi untuk mengenal pasti sebilangan kecil peraturan simulasi sederhana yang mengatur dinamik soal jawab dalam rangkaian. Peraturan mudah ini bukan sahaja meniru ciri struktur dan prestasi algoritma pada Forum Java yang diamati secara empirik, tetapi juga membolehkan kita menilai bagaimana algoritma lain dapat berfungsi dalam komuniti dengan ciri yang berbeza. Kami yakin pendekatan ini akan bermanfaat untuk reka bentuk dan pelaksanaan algoritma praktikal untuk komuniti berkongsi kepakaran dalam talian. [[EENNDD]] penemuan kepakaran; pertolongan mencari; pencari kepakaran; simulasi; analisis rangkaian sosial; komuniti dalam talian"], [{"string": "Semantic web applications to e-science in silico experiments No contact information provided yet.", "keywords": ["annotation", "hypertext/hypermedia", "ontology", "provenance", "database applications", "e-science", "semantic web", "integration"], "combined": "Semantic web applications to e-science in silico experiments No contact information provided yet. [[EENNDD]] annotation; hypertext/hypermedia; ontology; provenance; database applications; e-science; semantic web; integration"}, "Aplikasi web semantik untuk e-sains dalam eksperimen silico Belum ada maklumat hubungan yang diberikan. [[EENNDD]] anotasi; hiperteks / hipermedia; ontologi; keturunan; aplikasi pangkalan data; e-sains; web semantik; penyatuan"], [{"string": "Generative models for name disambiguation Name ambiguity is a special case of identity uncertainty where one person can be referenced by multiple name variations in different situations or evenshare the same name with other people. In this paper, we present an efficient framework by using two novel topic-based models, extended from Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA). Our models explicitly introduce a new variable for persons and learn the distribution of topics with regard to persons and words. Experiments indicate that our approach consistently outperforms other unsupervised methods including spectral and DBSCAN clustering. Scalability is addressed by disambiguating authors in over 750,000 papers from the entire CiteSeer dataset.", "keywords": ["unsupervised machine learning", "information search and retrieval", "name disambiguation"], "combined": "Generative models for name disambiguation Name ambiguity is a special case of identity uncertainty where one person can be referenced by multiple name variations in different situations or evenshare the same name with other people. In this paper, we present an efficient framework by using two novel topic-based models, extended from Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA). Our models explicitly introduce a new variable for persons and learn the distribution of topics with regard to persons and words. Experiments indicate that our approach consistently outperforms other unsupervised methods including spectral and DBSCAN clustering. Scalability is addressed by disambiguating authors in over 750,000 papers from the entire CiteSeer dataset. [[EENNDD]] unsupervised machine learning; information search and retrieval; name disambiguation"}, "Model generatif untuk disambiguasi nama Kekaburan nama adalah kes khas ketidakpastian identiti di mana satu orang dapat dirujuk oleh pelbagai variasi nama dalam situasi yang berbeza atau menunjukkan nama yang sama dengan orang lain. Dalam makalah ini, kami memaparkan kerangka kerja yang efisien dengan menggunakan dua model berdasarkan topik baru, yang diperluas dari Probabilistic Latent Semantic Analysis (PLSA) dan Latent Dirichlet Allocation (LDA). Model kami secara eksplisit memperkenalkan pemboleh ubah baru untuk orang dan mempelajari pembahagian topik berkenaan dengan orang dan perkataan. Eksperimen menunjukkan bahawa pendekatan kami secara konsisten mengatasi kaedah lain yang tidak diawasi termasuk pengelompokan spektrum dan DBSCAN. Skalabiliti ditangani oleh pengurai penulis dalam lebih dari 750,000 makalah dari keseluruhan kumpulan data CiteSeer. [[EENNDD]] pembelajaran mesin tanpa pengawasan; pencarian dan pengambilan maklumat; disambiguasi nama"], [{"string": "Reactive rules inference from dynamic dependency models No contact information provided yet.", "keywords": ["reactive rules", "dependency models", "active systems", "rule engine", "relationships between entities", "active databases", "event correlation"], "combined": "Reactive rules inference from dynamic dependency models No contact information provided yet. [[EENNDD]] reactive rules; dependency models; active systems; rule engine; relationships between entities; active databases; event correlation"}, "Kesimpulan peraturan reaktif dari model pergantungan dinamik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] peraturan reaktif; model kebergantungan; sistem aktif; enjin peraturan; hubungan antara entiti; pangkalan data aktif; korelasi peristiwa"], [{"string": "Are web users really Markovian? User modeling on the Web has rested on the fundamental assumption of Markovian behavior --- a user's next action depends only on her current state, and not the history leading up to the current state. This forms the underpinning of PageRank web ranking, as well as a number of techniques for targeting advertising to users. In this work we examine the validity of this assumption, using data from a number of Web settings. Our main result invokes statistical order estimation tests for Markov chains to establish that Web users are not, in fact, Markovian. We study the extent to which the Markovian assumption is invalid, and derive a number of avenues for further research.", "keywords": ["markov chains", "browsing behavior", "general", "user models"], "combined": "Are web users really Markovian? User modeling on the Web has rested on the fundamental assumption of Markovian behavior --- a user's next action depends only on her current state, and not the history leading up to the current state. This forms the underpinning of PageRank web ranking, as well as a number of techniques for targeting advertising to users. In this work we examine the validity of this assumption, using data from a number of Web settings. Our main result invokes statistical order estimation tests for Markov chains to establish that Web users are not, in fact, Markovian. We study the extent to which the Markovian assumption is invalid, and derive a number of avenues for further research. [[EENNDD]] markov chains; browsing behavior; general; user models"}, "Adakah pengguna web benar-benar Markovian? Pemodelan pengguna di Web bergantung pada andaian asas tingkah laku Markovia --- tindakan selanjutnya pengguna hanya bergantung pada keadaannya sekarang, dan bukan sejarah yang menuju ke keadaan semasa. Ini membentuk pendukung peringkat laman web PageRank, serta sejumlah teknik untuk menargetkan iklan kepada pengguna. Dalam karya ini kami memeriksa kesahan anggapan ini, menggunakan data dari sejumlah tetapan Web. Hasil utama kami meminta ujian anggaran pesanan statistik untuk rantaian Markov untuk membuktikan bahawa pengguna Web sebenarnya bukan Markovian. Kami mengkaji sejauh mana anggapan Markovia tidak sah, dan memperoleh sejumlah jalan untuk penyelidikan lebih lanjut. [[EENNDD]] rantai markov; tingkah laku melayari; umum; model pengguna"], [{"string": "Addressing the testing challenge with a web-based e-assessment system that tutors as it assesses No contact information provided yet.", "keywords": ["intelligent tutoring system", "computer-assisted instruction", "learning", "assistment", "predict", "mcas"], "combined": "Addressing the testing challenge with a web-based e-assessment system that tutors as it assesses No contact information provided yet. [[EENNDD]] intelligent tutoring system; computer-assisted instruction; learning; assistment; predict; mcas"}, "Menangani cabaran ujian dengan sistem e-penilaian berasaskan web yang memberi tunjuk ajar semasa menilai Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] sistem bimbingan pintar; arahan berbantukan komputer; belajar; pertolongan; meramalkan; mcas"], [{"string": "Improved annotation of the blogosphere via autotagging and hierarchical clustering No contact information provided yet.", "keywords": ["automated annotation", "content analysis and indexing", "information search and retrieval", "hierarchical clustering", "blogs", "tagging"], "combined": "Improved annotation of the blogosphere via autotagging and hierarchical clustering No contact information provided yet. [[EENNDD]] automated annotation; content analysis and indexing; information search and retrieval; hierarchical clustering; blogs; tagging"}, "Anotasi blogosphere yang lebih baik melalui pemberian tag automatik dan pengelompokan hierarki Belum ada maklumat hubungan yang diberikan. [[EENNDD]] anotasi automatik; analisis kandungan dan pengindeksan; pencarian dan pengambilan maklumat; pengelompokan hierarki; blog; penandaan"], [{"string": "Mining RDF metadata for generalized association rules: knowledge discovery in the semantic web era No contact information provided yet.", "keywords": ["rdf mining", "association rule mining"], "combined": "Mining RDF metadata for generalized association rules: knowledge discovery in the semantic web era No contact information provided yet. [[EENNDD]] rdf mining; association rule mining"}, "Perlombongan metadata RDF untuk peraturan pergaulan umum: penemuan pengetahuan di era semantik web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlombongan rdf; perlombongan peraturan persatuan"], [{"string": "XQuery at your web service No contact information provided yet.", "keywords": ["xquery", "modules", "interface", "xml", "web services", "wsdl"], "combined": "XQuery at your web service No contact information provided yet. [[EENNDD]] xquery; modules; interface; xml; web services; wsdl"}, "XQuery di perkhidmatan web anda Belum ada maklumat hubungan yang diberikan. [[EENNDD]] xquery; modul; antara muka; xml; perkhidmatan web; wsdl"], [{"string": "From actors, politicians, to CEOs: domain adaptation of relational extractors using a latent relational mapping We propose a method to adapt an existing relation extraction system to extract new relation types with minimum supervision. Our proposed method comprises two stages: learning a lower-dimensional projection between different relations, and learning a relational classifier for the target relation type with instance sampling. We evaluate the proposed method using a dataset that contains 2000 instances for 20 different relation types. Our experimental results show that the proposed method achieves a statistically significant macro-average F-score of 62.77. Moreover, the proposed method outperforms numerous baselines and a previously proposed weakly-supervised relation extraction method.", "keywords": ["relation extraction", "domain adaptation", "web mining"], "combined": "From actors, politicians, to CEOs: domain adaptation of relational extractors using a latent relational mapping We propose a method to adapt an existing relation extraction system to extract new relation types with minimum supervision. Our proposed method comprises two stages: learning a lower-dimensional projection between different relations, and learning a relational classifier for the target relation type with instance sampling. We evaluate the proposed method using a dataset that contains 2000 instances for 20 different relation types. Our experimental results show that the proposed method achieves a statistically significant macro-average F-score of 62.77. Moreover, the proposed method outperforms numerous baselines and a previously proposed weakly-supervised relation extraction method. [[EENNDD]] relation extraction; domain adaptation; web mining"}, "Dari pelaku, ahli politik, hingga CEO: adaptasi domain pengekstrak relasional menggunakan pemetaan relasional laten Kami mencadangkan kaedah untuk menyesuaikan sistem pengekstrakan hubungan yang ada untuk mengekstrak jenis hubungan baru dengan pengawasan minimum. Kaedah yang dicadangkan kami merangkumi dua peringkat: belajar unjuran dimensi rendah antara hubungan yang berbeza, dan belajar pengkelasan relasional untuk jenis hubungan sasaran dengan contoh contoh. Kami menilai kaedah yang dicadangkan menggunakan set data yang mengandungi 2000 contoh untuk 20 jenis hubungan yang berbeza. Hasil eksperimen kami menunjukkan bahawa kaedah yang dicadangkan mencapai skor F-skor makro-rata-rata yang signifikan secara statistik sebanyak 62.77. Lebih-lebih lagi, kaedah yang dicadangkan mengatasi banyak garis dasar dan kaedah pengekstrakan hubungan yang diawasi dengan lemah sebelumnya. [[EENNDD]] pengekstrakan hubungan; penyesuaian domain; perlombongan web"], [{"string": "Design for verification for asynchronously communicating Web services No contact information provided yet.", "keywords": ["bpel", "composite web services", "design tools and techniques", "asynchronous communication", "design patterns"], "combined": "Design for verification for asynchronously communicating Web services No contact information provided yet. [[EENNDD]] bpel; composite web services; design tools and techniques; asynchronous communication; design patterns"}, "Reka bentuk untuk pengesahan untuk menyampaikan perkhidmatan Web secara tidak segerak Tidak ada maklumat hubungan yang disediakan. [[EENNDD]] bpel; perkhidmatan web komposit; alat dan teknik reka bentuk; komunikasi tak segerak; corak rekaan"], [{"string": "Securing web application code by static analysis and runtime protection No contact information provided yet.", "keywords": ["unauthorized access", "program security", "web application security", "invasive software", "type systems", "noninterference", "information flow", "security vulnerabilities"], "combined": "Securing web application code by static analysis and runtime protection No contact information provided yet. [[EENNDD]] unauthorized access; program security; web application security; invasive software; type systems; noninterference; information flow; security vulnerabilities"}, "Menjamin kod aplikasi web dengan analisis statik dan perlindungan masa berjalan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] akses tanpa kebenaran; keselamatan program; keselamatan aplikasi web; perisian invasif; sistem jenis; tanpa gangguan; aliran maklumat; kelemahan keselamatan"], [{"string": "Quicklink selection for navigational query results Quicklinks for a website are navigational shortcuts displayed below the website homepage on a search results page, and that let the users directly jump to selected points inside the website. Since the real-estate on a search results page is constrained and valuable, picking the best set of quicklinks to maximize the benefits for a majority of the users becomes an important problem for search engines. Using user browsing trails obtained from browser toolbars, and a simple probabilistic model, we formulate the quicklink selection problem as a combinatorial optimizaton problem. We first demonstrate the hardness of the objective, and then propose an algorithm that is provably within a factor of 1-1/e of the optimal. We also propose a different algorithm that works on trees and that can find the optimal solution; unlike the previous algorithm, this algorithm can incorporate natural constraints on the set of chosen quicklinks. The efficacy of our methods is demonstrated via empirical results on both a manually labeled set of websites and a set for which quicklink click-through rates for several webpages were obtained from a real-world search engine.", "keywords": ["toolbar data", "quicklinks", "trails", "navigational queries", "miscellaneous"], "combined": "Quicklink selection for navigational query results Quicklinks for a website are navigational shortcuts displayed below the website homepage on a search results page, and that let the users directly jump to selected points inside the website. Since the real-estate on a search results page is constrained and valuable, picking the best set of quicklinks to maximize the benefits for a majority of the users becomes an important problem for search engines. Using user browsing trails obtained from browser toolbars, and a simple probabilistic model, we formulate the quicklink selection problem as a combinatorial optimizaton problem. We first demonstrate the hardness of the objective, and then propose an algorithm that is provably within a factor of 1-1/e of the optimal. We also propose a different algorithm that works on trees and that can find the optimal solution; unlike the previous algorithm, this algorithm can incorporate natural constraints on the set of chosen quicklinks. The efficacy of our methods is demonstrated via empirical results on both a manually labeled set of websites and a set for which quicklink click-through rates for several webpages were obtained from a real-world search engine. [[EENNDD]] toolbar data; quicklinks; trails; navigational queries; miscellaneous"}, "Pemilihan pautan pantas untuk hasil pertanyaan navigasi Pautan pantas untuk laman web adalah jalan pintas navigasi yang dipaparkan di bawah laman utama laman web pada halaman hasil carian, dan yang memungkinkan pengguna langsung menuju ke titik terpilih di dalam laman web. Oleh kerana harta tanah di halaman hasil carian adalah terhad dan berharga, memilih rangkaian pautan terbaik untuk memaksimumkan faedah bagi sebahagian besar pengguna menjadi masalah penting bagi enjin carian. Dengan menggunakan jejak pelayaran pengguna yang diperoleh dari bar alat penyemak imbas, dan model probabilistik sederhana, kami merumuskan masalah pemilihan pautan cepat sebagai masalah pengoptimuman gabungan. Mula-mula kami menunjukkan kekerasan objektif, dan kemudian mencadangkan algoritma yang terbukti berada dalam faktor 1-1 / e yang optimum. Kami juga mencadangkan algoritma berbeza yang berfungsi pada pokok dan yang dapat mencari penyelesaian yang optimum; tidak seperti algoritma sebelumnya, algoritma ini dapat memasukkan batasan semula jadi pada set pautan cepat yang dipilih. Keberkesanan kaedah kami ditunjukkan melalui hasil empirik pada kedua-dua set laman web yang dilabel secara manual dan satu set yang mana kadar klik-pautan pautan cepat untuk beberapa halaman web diperoleh dari mesin carian dunia nyata. [[EENNDD]] data bar alat; pautan pantas; jejak; pertanyaan pelayaran; pelbagai"], [{"string": "Web customization using behavior-based remote executing agents No contact information provided yet.", "keywords": ["web customization", "remote agents", "software architectures", "dynamic deployment"], "combined": "Web customization using behavior-based remote executing agents No contact information provided yet. [[EENNDD]] web customization; remote agents; software architectures; dynamic deployment"}, "Penyesuaian web menggunakan ejen pelaksana jarak jauh berasaskan tingkah laku Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penyesuaian web; ejen terpencil; seni bina perisian; penyebaran dinamik"], [{"string": "Multiway SLCA-based keyword search in XML data Keyword search for smallest lowest common ancestors (SLCAs)in XML data has recently been proposed as a meaningful way to identify interesting data nodes inXML data where their subtrees contain an input set of keywords. In this paper, we generalize this useful search paradigm to support keyword search beyond the traditional AND semantics to include both AND and OR boolean operators as well. We first analyze properties of the LCA computation and propose improved algorithms to solve the traditional keyword search problem (with only AND semantics). We then extend our approach to handle general keyword search involving combinations of AND and OR boolean operators. The effectiveness of our new algorithms is demonstrated with a comprehensive experimental performance study.", "keywords": ["keyword search query", "lowest common ancestor", "xml"], "combined": "Multiway SLCA-based keyword search in XML data Keyword search for smallest lowest common ancestors (SLCAs)in XML data has recently been proposed as a meaningful way to identify interesting data nodes inXML data where their subtrees contain an input set of keywords. In this paper, we generalize this useful search paradigm to support keyword search beyond the traditional AND semantics to include both AND and OR boolean operators as well. We first analyze properties of the LCA computation and propose improved algorithms to solve the traditional keyword search problem (with only AND semantics). We then extend our approach to handle general keyword search involving combinations of AND and OR boolean operators. The effectiveness of our new algorithms is demonstrated with a comprehensive experimental performance study. [[EENNDD]] keyword search query; lowest common ancestor; xml"}, "Pencarian kata kunci berasaskan Multiway SLCA dalam data XML Pencarian kata kunci untuk nenek moyang biasa terkecil (SLCA) dalam data XML baru-baru ini telah dicadangkan sebagai kaedah yang bermakna untuk mengenal pasti node data yang menarik dalam data XML di mana subtrees mereka mengandungi set kata kunci. Dalam makalah ini, kami menggeneralisasikan paradigma carian berguna ini untuk menyokong pencarian kata kunci di luar semantik DAN tradisional untuk merangkumi operator AND dan OR boolean juga. Kami mula-mula menganalisis sifat pengiraan LCA dan mencadangkan algoritma yang lebih baik untuk menyelesaikan masalah carian kata kunci tradisional (hanya dengan semantik DAN). Kami kemudian memperluas pendekatan kami untuk menangani carian kata kunci umum yang melibatkan kombinasi operator AND dan OR boolean. Keberkesanan algoritma baru kami ditunjukkan dengan kajian prestasi eksperimental yang komprehensif. [[EENNDD]] pertanyaan carian kata kunci; nenek moyang paling rendah; xml"], [{"string": "Web page classification with heterogeneous data fusion Web pages are more than text and they contain much contextual and structural information, e.g., the title, the meta data, the anchor text,etc., each of which can be seen as a data source or are presentation. Due to the different dimensionality and different representing forms of these heterogeneous data sources, simply putting them together would not greatly enhance the classification performance. We observe that via a kernel function, different dimensions and types of data sources can be represented into acommon format of kernel matrix, which can be seen as a generalized similarity measure between a pair of web pages. In this sense, a kernel learning approach is employed to fuse these heterogeneous data sources. The experimental results on a collection of the ODP database validate the advantages of the proposed method over traditional methods based on any single data source and the uniformly weighted combination of them.", "keywords": ["kernel combination", "data fusion", "web page classification"], "combined": "Web page classification with heterogeneous data fusion Web pages are more than text and they contain much contextual and structural information, e.g., the title, the meta data, the anchor text,etc., each of which can be seen as a data source or are presentation. Due to the different dimensionality and different representing forms of these heterogeneous data sources, simply putting them together would not greatly enhance the classification performance. We observe that via a kernel function, different dimensions and types of data sources can be represented into acommon format of kernel matrix, which can be seen as a generalized similarity measure between a pair of web pages. In this sense, a kernel learning approach is employed to fuse these heterogeneous data sources. The experimental results on a collection of the ODP database validate the advantages of the proposed method over traditional methods based on any single data source and the uniformly weighted combination of them. [[EENNDD]] kernel combination; data fusion; web page classification"}, "Klasifikasi halaman web dengan gabungan data heterogen Halaman web lebih daripada teks dan mengandungi banyak maklumat kontekstual dan struktur, misalnya, judul, data meta, teks jangkar, dan lain-lain, yang masing-masing dapat dilihat sebagai sumber data atau pembentangan. Oleh kerana dimensi yang berbeza dan bentuk representasi yang berbeza dari sumber data yang heterogen ini, hanya menyatukannya tidak akan meningkatkan prestasi klasifikasi. Kami melihat bahawa melalui fungsi kernel, dimensi dan jenis sumber data yang berlainan dapat direpresentasikan ke dalam format biasa dari matriks kernel, yang dapat dilihat sebagai ukuran kesamaan umum antara sepasang halaman web. Dalam pengertian ini, pendekatan pembelajaran kernel digunakan untuk menyatukan sumber data yang heterogen ini. Hasil eksperimen pada pengumpulan pangkalan data ODP mengesahkan kelebihan kaedah yang dicadangkan berbanding kaedah tradisional berdasarkan sumber tunggal dan gabungan wajaran yang sama. [[EENNDD]] gabungan kernel; gabungan data; pengelasan laman web"], [{"string": "Towards identifying arguments in Wikipedia pages Wikipedia is one of the most widely used repositories of human knowledge today, contributed mostly by a few hundred thousand regular editors. In this open environment, inevitably, differences of opinion arise among editors of the same article. Especially for polemical topics such as religion and politics, difference of opinions among editors may lead to intense \"edit wars\" in which editors compete to have their opinions and points of view accepted. While such disputes can compromise the reliability of the article (or at least portions of it), they are recorded in the edit history of the articles. We posit that exposing such disputes to the reader, and pointing to the portions of the text where they manifest most prominently can be beneficial in helping concerned readers in understanding such topics. In this paper, we discuss our initial efforts towards the problem of automatic evaluation of extracting controversial points in Wikipedia pages.", "keywords": ["controversy", "argument", "wikipedia", "evaluation"], "combined": "Towards identifying arguments in Wikipedia pages Wikipedia is one of the most widely used repositories of human knowledge today, contributed mostly by a few hundred thousand regular editors. In this open environment, inevitably, differences of opinion arise among editors of the same article. Especially for polemical topics such as religion and politics, difference of opinions among editors may lead to intense \"edit wars\" in which editors compete to have their opinions and points of view accepted. While such disputes can compromise the reliability of the article (or at least portions of it), they are recorded in the edit history of the articles. We posit that exposing such disputes to the reader, and pointing to the portions of the text where they manifest most prominently can be beneficial in helping concerned readers in understanding such topics. In this paper, we discuss our initial efforts towards the problem of automatic evaluation of extracting controversial points in Wikipedia pages. [[EENNDD]] controversy; argument; wikipedia; evaluation"}, "Ke arah mengenal pasti hujah di halaman Wikipedia Wikipedia adalah salah satu repositori pengetahuan manusia yang paling banyak digunakan hari ini, disumbangkan oleh beberapa ratus ribu editor biasa. Dalam persekitaran terbuka ini, tidak dapat tidak, perbezaan pendapat timbul di antara penyunting artikel yang sama. Terutama untuk topik polemik seperti agama dan politik, perbezaan pendapat di antara penyunting boleh menyebabkan \"perang perang\" yang sengit di mana para editor bersaing agar pendapat dan pandangan mereka diterima. Walaupun perselisihan seperti itu dapat membahayakan kebolehpercayaan artikel (atau setidaknya sebagian daripadanya), pertikaian tersebut dicatat dalam sejarah pengeditan artikel. Kami berpendapat bahawa mendedahkan perselisihan seperti itu kepada pembaca, dan menunjukkan bahagian-bahagian teks yang paling jelas dapat bermanfaat dalam membantu pembaca yang bersangkutan dalam memahami topik-topik tersebut. Dalam makalah ini, kami membincangkan usaha awal kami terhadap masalah penilaian automatik mengekstrak poin kontroversi di halaman Wikipedia. [[EENNDD]] kontroversi; hujah; wikipedia; penilaian"], [{"string": "Spatial variation in search engine queries Local aspects of Web search - associating Web content and queries with geography - is a topic of growing interest. However, the underlying question of how spatial variation is manifested in search queries is still not well understood. Here we develop a probabilistic framework for quantifying such spatial variation; on complete Yahoo! query logs, we find that our model is able to localize large classes of queries to within a few miles of their natural centers based only on the distribution of activity for the query. Our model provides not only an estimate of a query's geographic center, but also a measure of its spatial dispersion, indicating whether it has highly local interest or broader regional or national appeal. We also show how variations on our model can track geographically shifting topics over time, annotate a map with each location's \"distinctive queries\", and delineate the \"spheres of influence\" for competing queries in the same general domain.", "keywords": ["web search", "geolocation"], "combined": "Spatial variation in search engine queries Local aspects of Web search - associating Web content and queries with geography - is a topic of growing interest. However, the underlying question of how spatial variation is manifested in search queries is still not well understood. Here we develop a probabilistic framework for quantifying such spatial variation; on complete Yahoo! query logs, we find that our model is able to localize large classes of queries to within a few miles of their natural centers based only on the distribution of activity for the query. Our model provides not only an estimate of a query's geographic center, but also a measure of its spatial dispersion, indicating whether it has highly local interest or broader regional or national appeal. We also show how variations on our model can track geographically shifting topics over time, annotate a map with each location's \"distinctive queries\", and delineate the \"spheres of influence\" for competing queries in the same general domain. [[EENNDD]] web search; geolocation"}, "Variasi spasial dalam pertanyaan enjin carian Aspek tempatan dalam carian Web - mengaitkan kandungan Web dan pertanyaan dengan geografi - adalah topik yang semakin diminati. Walau bagaimanapun, persoalan yang mendasari bagaimana variasi spasial ditunjukkan dalam pertanyaan carian masih belum difahami dengan baik. Di sini kita mengembangkan kerangka probabilistik untuk mengukur variasi spasial tersebut; pada Yahoo! log pertanyaan, kami dapati model kami dapat melokalisasikan kelas pertanyaan besar dalam jarak beberapa batu dari pusat semula jadi mereka berdasarkan hanya pengedaran aktiviti untuk pertanyaan. Model kami tidak hanya memberikan anggaran pusat geografi pertanyaan, tetapi juga ukuran penyebaran ruangnya, yang menunjukkan sama ada ia mempunyai kepentingan lokal atau daya tarikan wilayah atau nasional yang lebih luas. Kami juga menunjukkan bagaimana variasi pada model kami dapat melacak peralihan topik secara geografi dari masa ke masa, memberi penjelasan pada peta dengan \"pertanyaan khas\" setiap lokasi, dan menggambarkan \"ruang pengaruh\" untuk pertanyaan yang bersaing dalam domain umum yang sama. [[EENNDD]] carian web; geolokasi"], [{"string": "Prophiler: a fast filter for the large-scale detection of malicious web pages Malicious web pages that host drive-by-download exploits have become a popular means for compromising hosts on the Internet and, subsequently, for creating large-scale botnets. In a drive-by-download exploit, an attacker embeds a malicious script (typically written in JavaScript) into a web page. When a victim visits this page, the script is executed and attempts to compromise the browser or one of its plugins. To detect drive-by-download exploits, researchers have developed a number of systems that analyze web pages for the presence of malicious code. Most of these systems use dynamic analysis. That is, they run the scripts associated with a web page either directly in a real browser (running in a virtualized environment) or in an emulated browser, and they monitor the scripts' executions for malicious activity. While the tools are quite precise, the analysis process is costly, often requiring in the order of tens of seconds for a single page. Therefore, performing this analysis on a large set of web pages containing hundreds of millions of samples can be prohibitive.", "keywords": ["malicious web page analysis", "drive-by download exploits", "invasive software", "efficient web page filtering"], "combined": "Prophiler: a fast filter for the large-scale detection of malicious web pages Malicious web pages that host drive-by-download exploits have become a popular means for compromising hosts on the Internet and, subsequently, for creating large-scale botnets. In a drive-by-download exploit, an attacker embeds a malicious script (typically written in JavaScript) into a web page. When a victim visits this page, the script is executed and attempts to compromise the browser or one of its plugins. To detect drive-by-download exploits, researchers have developed a number of systems that analyze web pages for the presence of malicious code. Most of these systems use dynamic analysis. That is, they run the scripts associated with a web page either directly in a real browser (running in a virtualized environment) or in an emulated browser, and they monitor the scripts' executions for malicious activity. While the tools are quite precise, the analysis process is costly, often requiring in the order of tens of seconds for a single page. Therefore, performing this analysis on a large set of web pages containing hundreds of millions of samples can be prohibitive. [[EENNDD]] malicious web page analysis; drive-by download exploits; invasive software; efficient web page filtering"}, "Prophiler: penapis pantas untuk pengesanan skala besar laman web berbahaya Laman web berbahaya yang menjadi tuan rumah eksploitasi pemacu demi muat turun telah menjadi kaedah yang popular untuk menjejaskan hos di Internet dan, seterusnya, untuk membuat botnet berskala besar. Dalam eksploitasi drive-by-download, penyerang menyisipkan skrip jahat (biasanya ditulis dalam JavaScript) ke halaman web. Ketika mangsa mengunjungi halaman ini, skrip dijalankan dan cuba menjejaskan penyemak imbas atau salah satu pemalamnya. Untuk mengesan eksploitasi pemacu demi muat turun, para penyelidik telah mengembangkan sejumlah sistem yang menganalisis laman web untuk mengetahui adanya kod jahat. Sebilangan besar sistem ini menggunakan analisis dinamik. Artinya, mereka menjalankan skrip yang terkait dengan halaman web baik secara langsung dalam penyemak imbas nyata (berjalan dalam lingkungan virtual) atau dalam penyemak imbas yang ditiru, dan mereka memantau pelaksanaan skrip untuk kegiatan jahat. Walaupun alatnya cukup tepat, proses analisisnya mahal, seringkali memerlukan dalam urutan puluhan saat untuk satu halaman. Oleh itu, melakukan analisis ini pada sekumpulan besar laman web yang mengandungi beratus-ratus juta sampel boleh menjadi larangan. [[EENNDD]] analisis laman web berbahaya; eksploitasi muat turun drive-by; perisian invasif; penapisan laman web yang cekap"], [{"string": "Semantic web support for the business-to-business e-commerce lifecycle No contact information provided yet.", "keywords": ["service description", "electronic data interchange", "e-commerce", "automated negotiation", "semantic web", "daml", "matchmaking"], "combined": "Semantic web support for the business-to-business e-commerce lifecycle No contact information provided yet. [[EENNDD]] service description; electronic data interchange; e-commerce; automated negotiation; semantic web; daml; matchmaking"}, "Sokongan web semantik untuk kitaran hidup e-dagang perniagaan ke perniagaan Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] keterangan perkhidmatan; pertukaran data elektronik; e-dagang; rundingan automatik; web semantik; celaka; jodoh"], [{"string": "XVM: a bridge between xml data and its behavior No contact information provided yet.", "keywords": ["xvm", "components", "xml processing", "xml", "xml applications", "web applications"], "combined": "XVM: a bridge between xml data and its behavior No contact information provided yet. [[EENNDD]] xvm; components; xml processing; xml; xml applications; web applications"}, "XVM: jambatan antara data xml dan kelakuannya Belum ada maklumat hubungan yang diberikan. [[EENNDD]] xvm; komponen; pemprosesan xml; xml; aplikasi xml; aplikasi web"], [{"string": "Intelligent crawling of web applications for web archiving The steady growth of the World Wide Web raises challenges regarding the preservation of meaningful Web data. Tools used currently by Web archivists blindly crawl and store Web pages found while crawling, disregarding the kind of Web site currently accessed (which leads to suboptimal crawling strategies) and whatever structured content is contained in Web pages (which results in page-level archives whose content is hard to exploit). We focus in this PhD work on the crawling and archiving of publicly accessible Web applications, especially those of the social Web. A Web application is any application that uses Web standards such as HTML and HTTP to publish information on the Web, accessible by Web browsers. Examples include Web forums, social networks, geolocation services, etc. We claim that the best strategy to crawl these applications is to make the Web crawler aware of the kind of application currently processed, allowing it to refine the list of URLs to process, and to annotate the archive with information about the structure of crawled content. We add adaptive characteristics to an archival Web crawler: being able to identify when a Web page belongs to a given Web application and applying the appropriate crawling and content extraction methodology.", "keywords": ["hypertext/hypermedia", "archiving", "web application", "xpath", "extraction", "crawling"], "combined": "Intelligent crawling of web applications for web archiving The steady growth of the World Wide Web raises challenges regarding the preservation of meaningful Web data. Tools used currently by Web archivists blindly crawl and store Web pages found while crawling, disregarding the kind of Web site currently accessed (which leads to suboptimal crawling strategies) and whatever structured content is contained in Web pages (which results in page-level archives whose content is hard to exploit). We focus in this PhD work on the crawling and archiving of publicly accessible Web applications, especially those of the social Web. A Web application is any application that uses Web standards such as HTML and HTTP to publish information on the Web, accessible by Web browsers. Examples include Web forums, social networks, geolocation services, etc. We claim that the best strategy to crawl these applications is to make the Web crawler aware of the kind of application currently processed, allowing it to refine the list of URLs to process, and to annotate the archive with information about the structure of crawled content. We add adaptive characteristics to an archival Web crawler: being able to identify when a Web page belongs to a given Web application and applying the appropriate crawling and content extraction methodology. [[EENNDD]] hypertext/hypermedia; archiving; web application; xpath; extraction; crawling"}, "Penjelajahan pintar untuk aplikasi web untuk pengarkiban web Pertumbuhan World Wide Web yang mantap menimbulkan cabaran mengenai pemeliharaan data Web yang bermakna. Alat yang digunakan pada masa ini oleh arkib Web merangkak dan menyimpan halaman Web secara membuta tuli yang dijumpai semasa merangkak, mengabaikan jenis laman web yang saat ini diakses (yang membawa kepada strategi merangkak yang tidak optimum) dan apa sahaja kandungan berstruktur yang terkandung dalam halaman Web (yang menghasilkan arkib peringkat halaman yang kandungan sukar dieksploitasi). Kami memberi tumpuan dalam pekerjaan PhD ini pada merangkak dan mengarkibkan aplikasi Web yang dapat diakses oleh orang ramai, terutama yang terdapat di laman sosial. Aplikasi Web adalah aplikasi yang menggunakan standard Web seperti HTML dan HTTP untuk menerbitkan maklumat di Web, yang dapat diakses oleh penyemak imbas Web. Contohnya termasuk forum Web, rangkaian sosial, perkhidmatan geolokasi, dan lain-lain. Kami mendakwa bahawa strategi terbaik untuk merangkak aplikasi ini adalah dengan membuat perayap Web mengetahui jenis aplikasi yang sedang diproses, yang memungkinkan untuk menyempurnakan senarai URL untuk diproses, dan untuk memberi keterangan arkib dengan maklumat mengenai struktur kandungan yang dirangkak. Kami menambahkan ciri-ciri adaptif pada crawler Web arkib: dapat mengenal pasti bila halaman Web tergolong dalam aplikasi Web tertentu dan menerapkan metodologi perayapan dan pengekstrakan kandungan yang sesuai. [[EENNDD]] hiperteks / hipermedia; pengarkiban; aplikasi sesawang; xpath; pengekstrakan; merangkak"], [{"string": "SGPS: a semantic scheme for web service similarity Today's Web becomes a platform for services to be dynamically interconnected to produce a desired outcome. It is important to formalize the semantics of the contextual elements of web services. In this paper, we propose a novel technique called Semantic Genome Propagation Scheme (SGPS) for measuring similarity between semantic concepts. We show how SGPS is used to compute a multi-dimensional similarity between two services. We evaluate the SGPS similarity measurement in terms of the similarity performance and scalability.", "keywords": ["similarity", "web services", "context", "semantics"], "combined": "SGPS: a semantic scheme for web service similarity Today's Web becomes a platform for services to be dynamically interconnected to produce a desired outcome. It is important to formalize the semantics of the contextual elements of web services. In this paper, we propose a novel technique called Semantic Genome Propagation Scheme (SGPS) for measuring similarity between semantic concepts. We show how SGPS is used to compute a multi-dimensional similarity between two services. We evaluate the SGPS similarity measurement in terms of the similarity performance and scalability. [[EENNDD]] similarity; web services; context; semantics"}, "SGPS: skema semantik untuk kesamaan perkhidmatan web Web Hari Ini menjadi platform untuk perkhidmatan saling berkaitan secara dinamik untuk menghasilkan hasil yang diinginkan. Adalah penting untuk memformalkan semantik elemen kontekstual perkhidmatan web. Dalam makalah ini, kami mencadangkan teknik novel yang disebut Semantic Genome Propagation Scheme (SGPS) untuk mengukur kesamaan antara konsep semantik. Kami menunjukkan bagaimana SGPS digunakan untuk mengira persamaan pelbagai dimensi antara dua perkhidmatan. Kami menilai pengukuran kesamaan SGPS dari segi prestasi dan skalabiliti kesamaan. [[EENNDD]] persamaan; perkhidmatan web; konteks; semantik"], [{"string": "An integrated method for social network extraction No contact information provided yet.", "keywords": ["user interaction", "web mining", "social network", "miscellaneous"], "combined": "An integrated method for social network extraction No contact information provided yet. [[EENNDD]] user interaction; web mining; social network; miscellaneous"}, "Kaedah bersepadu untuk pengekstrakan rangkaian sosial Belum ada maklumat hubungan yang diberikan. [[EENNDD]] interaksi pengguna; perlombongan web; rangkaian sosial; pelbagai"], [{"string": "Foundations for service ontologies: aligning OWL-S to dolce No contact information provided yet.", "keywords": ["descriptions and situations", "core ontology of services", "daml-s", "web services", "owl-s", "semantic web", "dolce"], "combined": "Foundations for service ontologies: aligning OWL-S to dolce No contact information provided yet. [[EENNDD]] descriptions and situations; core ontology of services; daml-s; web services; owl-s; semantic web; dolce"}, "Asas untuk ontologi perkhidmatan: menyelaraskan OWL-S ke dolce Belum ada maklumat hubungan yang disediakan. [[EENNDD]] penerangan dan situasi; ontologi teras perkhidmatan; celaka-s; perkhidmatan web; burung hantu-s; web semantik; dolce"], [{"string": "On the high density of leadership nuclei in endorsement social networks In this paper we study the community structure of endorsement networks, i.e., social networks in which a directed edge u \u2192 v is asserting an action of support from user u to user v. Examples include scenarios in which a user u is favoring a photo, liking a post, or following the microblog of user v.", "keywords": ["communities", "communications applications", "endorsement social networks"], "combined": "On the high density of leadership nuclei in endorsement social networks In this paper we study the community structure of endorsement networks, i.e., social networks in which a directed edge u \u2192 v is asserting an action of support from user u to user v. Examples include scenarios in which a user u is favoring a photo, liking a post, or following the microblog of user v. [[EENNDD]] communities; communications applications; endorsement social networks"}, "Mengenai ketumpatan nukleus kepemimpinan yang tinggi dalam rangkaian sosial sokongan Dalam makalah ini kita mengkaji struktur komuniti rangkaian sokongan, iaitu, rangkaian sosial di mana tepi terarah u \u2192 v menegaskan tindakan sokongan dari pengguna u kepada pengguna v. Contohnya merangkumi senario di mana pengguna u menyukai foto, menyukai catatan, atau mengikuti mikroblog pengguna v. [[EENNDD]] komuniti; aplikasi komunikasi; sokongan rangkaian sosial"], [{"string": "A service creation environment based on end to end composition of Web services No contact information provided yet.", "keywords": ["planning", "web services composition", "semantic web"], "combined": "A service creation environment based on end to end composition of Web services No contact information provided yet. [[EENNDD]] planning; web services composition; semantic web"}, "Persekitaran pembuatan perkhidmatan berdasarkan komposisi perkhidmatan Web dari hujung ke hujung Belum ada maklumat hubungan yang diberikan. [[EENNDD]] merancang; komposisi perkhidmatan web; web semantik"], [{"string": "Diversified SCM standard for the Japanese retail industry No contact information provided yet.", "keywords": ["business process management", "b2b collaboration", "electronic data interchange", "soa", "ebxml", "web services", "retail industry", "supply chain management"], "combined": "Diversified SCM standard for the Japanese retail industry No contact information provided yet. [[EENNDD]] business process management; b2b collaboration; electronic data interchange; soa; ebxml; web services; retail industry; supply chain management"}, "Kepelbagaian standard SCM untuk industri runcit Jepun Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengurusan proses perniagaan; kerjasama b2b; pertukaran data elektronik; soa; ebxml; perkhidmatan web; industri runcit; pengurusan rantaian bekalan"], [{"string": "Constructing travel itineraries from tagged geo-temporal breadcrumbs Vacation planning is a frequent laborious task which requires skilled interaction with a multitude of resources. This paper develops an end-to-end approach for constructing intra-city travel itineraries automatically by tapping a latent source reflecting geo-temporal breadcrumbs left by millions of tourists. In particular, the popular rich media sharing site, Flickr, allows photos to be stamped by the date and time of when they were taken, and be mapped to Points Of Interest (POIs) by latitude-longitude information as well as semantic metadata (e.g., tags) that describe them.", "keywords": ["media applications", "rich media", "travel itinerary", "geo-tags", "mechanical turk", "orienteering problem", "flickr"], "combined": "Constructing travel itineraries from tagged geo-temporal breadcrumbs Vacation planning is a frequent laborious task which requires skilled interaction with a multitude of resources. This paper develops an end-to-end approach for constructing intra-city travel itineraries automatically by tapping a latent source reflecting geo-temporal breadcrumbs left by millions of tourists. In particular, the popular rich media sharing site, Flickr, allows photos to be stamped by the date and time of when they were taken, and be mapped to Points Of Interest (POIs) by latitude-longitude information as well as semantic metadata (e.g., tags) that describe them. [[EENNDD]] media applications; rich media; travel itinerary; geo-tags; mechanical turk; orienteering problem; flickr"}, "Membina jadual perjalanan dari serbuk roti geo-temporal yang ditandai Perancangan percutian adalah tugas yang sering dilakukan dan memerlukan interaksi yang mahir dengan banyak sumber. Makalah ini mengembangkan pendekatan end-to-end untuk membina jadual perjalanan intra-bandar secara automatik dengan mengetuk sumber pendam yang mencerminkan serbuk roti geo-temporal yang ditinggalkan oleh berjuta-juta pelancong. Khususnya, laman perkongsian media kaya yang popular, Flickr, membolehkan gambar dicap pada tarikh dan masa ketika ia diambil, dan dipetakan ke Tempat Menarik (POI) dengan maklumat garis bujur dan metadata semantik (mis. , tag) yang menerangkannya. [[EENNDD]] aplikasi media; media kaya; jadual perjalanan; teg geo; turk mekanikal; masalah orienteering; flickr"], [{"string": "Middleware services for web service compositions No contact information provided yet.", "keywords": ["bpel", "web service composition", "middleware"], "combined": "Middleware services for web service compositions No contact information provided yet. [[EENNDD]] bpel; web service composition; middleware"}, "Perkhidmatan middleware untuk komposisi perkhidmatan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] bpel; komposisi perkhidmatan web; alat tengah"], [{"string": "XJ: integration of XML processing into java No contact information provided yet.", "keywords": ["xml", "java", "language constructs and features", "data integration"], "combined": "XJ: integration of XML processing into java No contact information provided yet. [[EENNDD]] xml; java; language constructs and features; data integration"}, "XJ: penyatuan pemprosesan XML ke dalam java Belum ada maklumat hubungan yang diberikan. [[EENNDD]] xml; jawa; konstruk dan ciri bahasa; penyatuan data"], [{"string": "OntoTrix: a hybrid visualization for populated ontologies Most Semantic Web data visualization tools structure the representation according to the concept definitions and interrelations that constitute the ontology's vocabulary. Instances are often treated as somewhat peripheral information, when considered at all. These instances, that populate ontologies, represent an essential part of any knowledge base, and are often orders of magnitude more numerous than the concept definitions that give them machine-processable meaning. We present a visualization technique designed to enable users to visualize large instance sets and the relations that connect them. This hybrid visualization uses both node-link and adjacency matrix representations of graphs to visualize different parts of the data depending on their semantic and local structural properties, exploiting ontological knowledge to drive the graph layout. The representation is embedded in an environment that features advanced interaction techniques for easy navigation, including support for smooth continuous zooming and coordinated views.", "keywords": ["exploratory visualization", "graphs", "matrices", "user interfaces", "semantic web"], "combined": "OntoTrix: a hybrid visualization for populated ontologies Most Semantic Web data visualization tools structure the representation according to the concept definitions and interrelations that constitute the ontology's vocabulary. Instances are often treated as somewhat peripheral information, when considered at all. These instances, that populate ontologies, represent an essential part of any knowledge base, and are often orders of magnitude more numerous than the concept definitions that give them machine-processable meaning. We present a visualization technique designed to enable users to visualize large instance sets and the relations that connect them. This hybrid visualization uses both node-link and adjacency matrix representations of graphs to visualize different parts of the data depending on their semantic and local structural properties, exploiting ontological knowledge to drive the graph layout. The representation is embedded in an environment that features advanced interaction techniques for easy navigation, including support for smooth continuous zooming and coordinated views. [[EENNDD]] exploratory visualization; graphs; matrices; user interfaces; semantic web"}, "OntoTrix: visualisasi hibrid untuk ontologi berpenduduk Sebilangan besar alat visualisasi data Web Semantik menyusun perwakilan mengikut definisi konsep dan saling kaitan yang membentuk perbendaharaan kata ontologi. Contoh sering dianggap sebagai maklumat yang agak pinggiran, jika dipertimbangkan sama sekali. Contoh-contoh ini, yang merangkumi ontologi, merupakan bahagian penting dari mana-mana pangkalan pengetahuan, dan sering kali lebih besar daripada definisi konsep yang memberi mereka makna yang dapat diproses dengan mesin. Kami menyajikan teknik visualisasi yang dirancang untuk membolehkan pengguna memvisualisasikan kumpulan contoh besar dan hubungan yang menghubungkannya. Visualisasi hibrid ini menggunakan perwakilan matriks pautan node-link dan adjacency grafik untuk memvisualisasikan bahagian data yang berbeza bergantung pada sifat struktur semantik dan tempatan mereka, memanfaatkan pengetahuan ontologi untuk mendorong susun atur grafik. Perwakilan ini disertakan dalam lingkungan yang menampilkan teknik interaksi lanjutan untuk navigasi yang mudah, termasuk sokongan untuk zoom berterusan yang berterusan dan pandangan yang terkoordinasi. [[EENNDD]] visualisasi penerokaan; grafik; matrik; antara muka pengguna; web semantik"], [{"string": "Navigationaided retrieval Users searching for information in hypermedia environments often perform querying followed by manual navigation. Yet, the conventional text/hypertext retrieval paradigm does not explicity take post-query navigation into account. This paper proposes a new retrieval paradigm, called navigation-aided retrieval (NAR), which treats both querying and navigation as first-class activities. In the NAR paradigm, querying is seen as a means to identify starting points for navigation, and navigation is guided based on information supplied in the query. NAR is a generalization of the conventional probabilistic information retrieval paradigm, which implicitly assumes no navigation takes place. This paper presents a formal model for navigation-aided retrieval, and reports empirical results that point to the real-world applicability of the model. The experiments were performed over a large Web corpus provided by TREC, using human judgments on a new rating scale developed for navigation-aided retrieval. In the case of ambiguous queries, the new retrieval model identifies good starting points for post-query navigation. For less ambiguous queries that need not be paired with navigation, the output closely matches that of a conventional retrieval system.", "keywords": ["underspecified search tasks", "web search", "browsing", "information search and retrieval", "navigation", "link analysis"], "combined": "Navigationaided retrieval Users searching for information in hypermedia environments often perform querying followed by manual navigation. Yet, the conventional text/hypertext retrieval paradigm does not explicity take post-query navigation into account. This paper proposes a new retrieval paradigm, called navigation-aided retrieval (NAR), which treats both querying and navigation as first-class activities. In the NAR paradigm, querying is seen as a means to identify starting points for navigation, and navigation is guided based on information supplied in the query. NAR is a generalization of the conventional probabilistic information retrieval paradigm, which implicitly assumes no navigation takes place. This paper presents a formal model for navigation-aided retrieval, and reports empirical results that point to the real-world applicability of the model. The experiments were performed over a large Web corpus provided by TREC, using human judgments on a new rating scale developed for navigation-aided retrieval. In the case of ambiguous queries, the new retrieval model identifies good starting points for post-query navigation. For less ambiguous queries that need not be paired with navigation, the output closely matches that of a conventional retrieval system. [[EENNDD]] underspecified search tasks; web search; browsing; information search and retrieval; navigation; link analysis"}, "Pengambilan yang dilayari Navigasi Pengguna yang mencari maklumat dalam persekitaran hipermedia sering melakukan pertanyaan diikuti dengan navigasi manual. Namun, paradigma pengambilan teks / hiperteks konvensional tidak mengambil kira navigasi pasca-pertanyaan. Makalah ini mencadangkan paradigma pengambilan baru, yang disebut pengambilan bantuan navigasi (NAR), yang memperlakukan pertanyaan dan navigasi sebagai aktiviti kelas pertama. Dalam paradigma NAR, pertanyaan dilihat sebagai cara untuk mengenal pasti titik permulaan navigasi, dan navigasi dipandu berdasarkan maklumat yang diberikan dalam pertanyaan. NAR adalah generalisasi paradigma pengambilan maklumat probabilistik konvensional, yang secara implisit menganggap tidak ada navigasi yang berlaku. Makalah ini menyajikan model formal untuk pengambilan bantuan navigasi, dan melaporkan hasil empirikal yang menunjukkan kegunaan model dunia nyata. Eksperimen tersebut dilakukan melalui korpus Web besar yang disediakan oleh TREC, menggunakan penilaian manusia pada skala penilaian baru yang dikembangkan untuk pengambilan bantuan navigasi. Sekiranya terdapat pertanyaan samar-samar, model pengambilan baru mengenal pasti titik permulaan yang baik untuk navigasi pasca-pertanyaan. Untuk pertanyaan yang kurang samar-samar yang tidak perlu dipasangkan dengan navigasi, outputnya hampir sama dengan sistem pengambilan konvensional. [[EENNDD]] tugas carian yang tidak ditentukan; carian sesawang; melayari; carian dan pengambilan maklumat; pelayaran; analisis pautan"], [{"string": "Efficient URL caching for world wide web crawling No contact information provided yet.", "keywords": ["caching", "distributed crawlers", "web graph models", "url caching", "crawling", "web crawlers"], "combined": "Efficient URL caching for world wide web crawling No contact information provided yet. [[EENNDD]] caching; distributed crawlers; web graph models; url caching; crawling; web crawlers"}, "Cache URL yang cekap untuk merangkak web di seluruh dunia Belum ada maklumat hubungan yang diberikan. [[EENNDD]] caching; perayap diedarkan; model grafik web; url caching; merangkak; perayap web"], [{"string": "Matchbox: large scale online bayesian recommendations We present a probabilistic model for generating personalised recommendations of items to users of a web service. The Matchbox system makes use of content information in the form of user and item meta data in combination with collaborative filtering information from previous user behavior in order to predict the value of an item for a user. Users and items are represented by feature vectors which are mapped into a low-dimensional `trait space' in which similarity is measured in terms of inner products. The model can be trained from different types of feedback in order to learn user-item preferences. Here we present three alternatives: direct observation of an absolute rating each user gives to some items, observation of a binary preference (like/ don't like) and observation of a set of ordinal ratings on a user-specific scale. Efficient inference is achieved by approximate message passing involving a combination of Expectation Propagation (EP) and Variational Message Passing. We also include a dynamics model which allows an item's popularity, a user's taste or a user's personal rating scale to drift over time. By using Assumed-Density Filtering (ADF) for training, the model requires only a single pass through the training data. This is an on-line learning algorithm capable of incrementally taking account of new data so the system can immediately reflect the latest user preferences. We evaluate the performance of the algorithm on the MovieLens and Netflix data sets consisting of approximately 1,000,000 and 100,000,000 ratings respectively. This demonstrates that training the model using the on-line ADF approach yields state-of-the-art performance with the option of improving performance further if computational resources are available by performing multiple EP passes over the training data.", "keywords": ["recommender system", "collaborative filtering", "advertising", "online services", "machine learning", "bayesian inference"], "combined": "Matchbox: large scale online bayesian recommendations We present a probabilistic model for generating personalised recommendations of items to users of a web service. The Matchbox system makes use of content information in the form of user and item meta data in combination with collaborative filtering information from previous user behavior in order to predict the value of an item for a user. Users and items are represented by feature vectors which are mapped into a low-dimensional `trait space' in which similarity is measured in terms of inner products. The model can be trained from different types of feedback in order to learn user-item preferences. Here we present three alternatives: direct observation of an absolute rating each user gives to some items, observation of a binary preference (like/ don't like) and observation of a set of ordinal ratings on a user-specific scale. Efficient inference is achieved by approximate message passing involving a combination of Expectation Propagation (EP) and Variational Message Passing. We also include a dynamics model which allows an item's popularity, a user's taste or a user's personal rating scale to drift over time. By using Assumed-Density Filtering (ADF) for training, the model requires only a single pass through the training data. This is an on-line learning algorithm capable of incrementally taking account of new data so the system can immediately reflect the latest user preferences. We evaluate the performance of the algorithm on the MovieLens and Netflix data sets consisting of approximately 1,000,000 and 100,000,000 ratings respectively. This demonstrates that training the model using the on-line ADF approach yields state-of-the-art performance with the option of improving performance further if computational resources are available by performing multiple EP passes over the training data. [[EENNDD]] recommender system; collaborative filtering; advertising; online services; machine learning; bayesian inference"}, "Kotak Padan: cadangan bayesian dalam talian berskala besar Kami menyajikan model probabilistik untuk menghasilkan cadangan item yang diperibadikan kepada pengguna perkhidmatan web. Sistem Matchbox menggunakan maklumat kandungan dalam bentuk data meta pengguna dan item dalam kombinasi dengan penyaringan maklumat kolaboratif dari tingkah laku pengguna sebelumnya untuk memprediksi nilai item untuk pengguna. Pengguna dan item diwakili oleh vektor ciri yang dipetakan ke dalam \"ruang sifat\" dimensi rendah di mana persamaan diukur dari segi produk dalaman. Model ini dapat dilatih dari pelbagai jenis maklum balas untuk mempelajari pilihan item pengguna. Di sini kami menyajikan tiga alternatif: pemerhatian langsung terhadap penilaian mutlak yang diberikan setiap pengguna kepada beberapa item, pemerhatian keutamaan binari (suka / tidak suka) dan pemerhatian sekumpulan penilaian ordinal pada skala khusus pengguna. Inferens yang cekap dicapai dengan perkiraan penyebaran mesej yang melibatkan gabungan Expectation Propagation (EP) dan Variational Message Passing. Kami juga menyertakan model dinamika yang membolehkan populariti item, citarasa pengguna atau skala penilaian peribadi pengguna meningkat dari masa ke masa. Dengan menggunakan Assumed-Density Filtering (ADF) untuk latihan, model tersebut hanya memerlukan satu kali melalui data latihan. Ini adalah algoritma pembelajaran dalam talian yang dapat secara bertahap mengambil kira data baru sehingga sistem dapat segera menggambarkan pilihan pengguna terkini. Kami menilai prestasi algoritma pada set data MovieLens dan Netflix yang terdiri daripada kira-kira 1,000,000 dan 100,000,000 penilaian masing-masing. Ini menunjukkan bahawa melatih model menggunakan pendekatan ADF on-line menghasilkan prestasi canggih dengan pilihan untuk meningkatkan prestasi lebih jauh jika sumber komputasi tersedia dengan melakukan beberapa pas EP atas data latihan. [[EENNDD]] sistem cadangan; penapisan kolaboratif; mengiklankan; perkhidmatan dalam talian; pembelajaran mesin; inferens bayes"], [{"string": "On revenue in the generalized second price auction The Generalized Second Price (GSP) auction is the primary auction used for selling sponsored search advertisements. In this paper we consider the revenue of this auction at equilibrium. We prove that if agent values are drawn from identical regular distributions, then the GSP auction paired with an appropriate reserve price generates a constant fraction (1/6th) of the optimal revenue. In the full-information game, we show that at any Nash equilibrium of the GSP auction obtains at least half of the revenue of the VCG mechanism excluding the payment of a single participant. This bound holds also with any reserve price, and is tight.", "keywords": ["nonnumerical algorithms and problems", "sponsored search auctions", "revenue"], "combined": "On revenue in the generalized second price auction The Generalized Second Price (GSP) auction is the primary auction used for selling sponsored search advertisements. In this paper we consider the revenue of this auction at equilibrium. We prove that if agent values are drawn from identical regular distributions, then the GSP auction paired with an appropriate reserve price generates a constant fraction (1/6th) of the optimal revenue. In the full-information game, we show that at any Nash equilibrium of the GSP auction obtains at least half of the revenue of the VCG mechanism excluding the payment of a single participant. This bound holds also with any reserve price, and is tight. [[EENNDD]] nonnumerical algorithms and problems; sponsored search auctions; revenue"}, "Atas hasil dalam lelongan harga kedua secara umum Lelang Harga Kedua Umum (GSP) adalah lelongan utama yang digunakan untuk menjual iklan carian yang ditaja. Dalam makalah ini kami mempertimbangkan pendapatan lelong ini pada keseimbangan. Kami membuktikan bahawa jika nilai ejen diambil dari pengedaran tetap yang sama, maka lelongan GSP yang dipasangkan dengan harga rizab yang sesuai menghasilkan pecahan tetap (1/6) dari hasil yang optimum. Dalam permainan maklumat penuh, kami menunjukkan bahawa pada setiap keseimbangan Nash lelong GSP memperoleh sekurang-kurangnya separuh daripada pendapatan mekanisme VCG tidak termasuk pembayaran satu peserta. Ini terikat juga dengan sebarang harga rizab, dan ketat. [[EENNDD]] algoritma dan masalah bukan angka; lelongan carian yang ditaja; hasil"], [{"string": "A flight meta-search engine with metamorph We demonstrate a flight meta-search engine that is based on the Metamorph framework. Metamorph provides mechanisms to model web forms together with the interactions which are needed to fulfil a request, and can generate interaction sequences that pose queries using these web forms and collect the results. In this paper, we discuss an interesting new feature that makes use of the forms themselves as an information source. We show how data can be extracted from web forms (rather than the data behind web forms) to generate a graph of flight connections between cities.", "keywords": ["web form extraction", "systems and software", "web data extraction", "hidden web", "web form mapping"], "combined": "A flight meta-search engine with metamorph We demonstrate a flight meta-search engine that is based on the Metamorph framework. Metamorph provides mechanisms to model web forms together with the interactions which are needed to fulfil a request, and can generate interaction sequences that pose queries using these web forms and collect the results. In this paper, we discuss an interesting new feature that makes use of the forms themselves as an information source. We show how data can be extracted from web forms (rather than the data behind web forms) to generate a graph of flight connections between cities. [[EENNDD]] web form extraction; systems and software; web data extraction; hidden web; web form mapping"}, "Mesin carian meta penerbangan dengan metamorf Kami menunjukkan enjin carian meta penerbangan berdasarkan kerangka kerja Metamorph. Metamorph menyediakan mekanisme untuk memodelkan bentuk web bersama dengan interaksi yang diperlukan untuk memenuhi permintaan, dan dapat menghasilkan urutan interaksi yang menimbulkan pertanyaan menggunakan borang web ini dan mengumpulkan hasilnya. Dalam makalah ini, kita membincangkan ciri baru yang menarik yang menggunakan bentuk diri sebagai sumber maklumat. Kami menunjukkan bagaimana data dapat diekstrak dari borang web (dan bukannya data di belakang borang web) untuk menghasilkan grafik sambungan penerbangan antara bandar. [[EENNDD]] pengekstrakan borang web; sistem dan perisian; pengekstrakan data web; web tersembunyi; pemetaan borang web"], [{"string": "On incremental maintenance of 2-hop labeling of graphs Recent interests on XML, Semantic Web, and Web ontology, among other topics, have sparked a renewed interest on graph-structured databases. A fundamental query on graphs is the reachability test of nodes. Recently, 2-hop labeling has been proposed to index large collections of XML and/or graphs for efficient reachability tests. However, there has been few work on updates of 2-hop labeling. This is compounded by the fact that Web data changes over time. In response to these, this paper studies the incremental maintenance of 2-hop labeling. We identify the main reason for the inefficiency of updates of existing 2-hop labels. We propose two updatable 2-hop labelings, hybrids of 2-hop labeling, and their incremental maintenance algorithms. The proposed 2-hop labeling is derived from graph connectivities, as opposed to set cover which is used by all previous work. Our experimental evaluation illustrates the space efficiency and update performance of various kinds of 2-hop labeling. The main conclusion is that there is a natural way to spare some index size for update performance in 2-hop labeling.", "keywords": ["incremental maintenance", "2-hop", "graph indexing", "reachability test"], "combined": "On incremental maintenance of 2-hop labeling of graphs Recent interests on XML, Semantic Web, and Web ontology, among other topics, have sparked a renewed interest on graph-structured databases. A fundamental query on graphs is the reachability test of nodes. Recently, 2-hop labeling has been proposed to index large collections of XML and/or graphs for efficient reachability tests. However, there has been few work on updates of 2-hop labeling. This is compounded by the fact that Web data changes over time. In response to these, this paper studies the incremental maintenance of 2-hop labeling. We identify the main reason for the inefficiency of updates of existing 2-hop labels. We propose two updatable 2-hop labelings, hybrids of 2-hop labeling, and their incremental maintenance algorithms. The proposed 2-hop labeling is derived from graph connectivities, as opposed to set cover which is used by all previous work. Our experimental evaluation illustrates the space efficiency and update performance of various kinds of 2-hop labeling. The main conclusion is that there is a natural way to spare some index size for update performance in 2-hop labeling. [[EENNDD]] incremental maintenance; 2-hop; graph indexing; reachability test"}, "Mengenai penyelenggaraan pelabelan 2-hop secara bertahap Minat terkini pada XML, Semantic Web, dan ontologi Web, antara topik lain, telah memicu minat baru pada pangkalan data berstruktur grafik. Pertanyaan asas pada grafik adalah ujian kebolehcapaian nod. Baru-baru ini, pelabelan 2-hop telah dicadangkan untuk mengindeks koleksi besar XML dan / atau grafik untuk ujian jangkauan yang cekap. Walau bagaimanapun, terdapat sedikit kerja mengenai kemas kini pelabelan 2-hop. Ini ditambah dengan fakta bahawa data Web berubah dari masa ke masa. Sebagai tindak balas kepada ini, makalah ini mengkaji pemeliharaan tambahan pelabelan 2-hop. Kami mengenal pasti sebab utama ketidakcekapan kemas kini label 2-hop yang ada. Kami mencadangkan dua label 2-hop yang boleh dikemas kini, hibrida pelabelan 2-hop, dan algoritma pemeliharaan tambahan mereka. Pelabelan 2-hop yang dicadangkan berasal dari kesambungan grafik, berbanding penutup penutup yang digunakan oleh semua karya sebelumnya. Penilaian eksperimental kami menggambarkan kecekapan ruang dan mengemas kini prestasi pelbagai jenis pelabelan 2-hop. Kesimpulan utama adalah bahawa ada cara semula jadi untuk menyimpan beberapa ukuran indeks untuk prestasi kemas kini dalam pelabelan 2-hop. [[EENNDD]] penyelenggaraan tambahan; 2-hop; pengindeksan grafik; ujian kebolehcapaian"], [{"string": "Using d-gap patterns for index compression Sequential patterns of d-gaps exist pervasively in inverted lists of Web document collection indices due to the cluster property. In this paper the information of d-gap sequential patterns is used as a new dimension for improving inverted index compression. We first detect d-gap sequential patterns using a novel data structure, UpDown Tree. Based on the detected patterns, we further substitute each pattern with its pattern Id in the inverted lists that contain it. The resulted inverted lists are then coded with an existing coding scheme. Experiments show that this approach can effectively improve the compression ratio of existing codes.", "keywords": ["inverted file", "sequential pattern", "d-gap", "index compression"], "combined": "Using d-gap patterns for index compression Sequential patterns of d-gaps exist pervasively in inverted lists of Web document collection indices due to the cluster property. In this paper the information of d-gap sequential patterns is used as a new dimension for improving inverted index compression. We first detect d-gap sequential patterns using a novel data structure, UpDown Tree. Based on the detected patterns, we further substitute each pattern with its pattern Id in the inverted lists that contain it. The resulted inverted lists are then coded with an existing coding scheme. Experiments show that this approach can effectively improve the compression ratio of existing codes. [[EENNDD]] inverted file; sequential pattern; d-gap; index compression"}, "Menggunakan corak d-gap untuk pemampatan indeks Pola urutan d-jurang terdapat secara meluas dalam senarai indeks pengumpulan dokumen Web terbalik kerana sifat kluster. Dalam makalah ini, maklumat mengenai corak urutan d-gap digunakan sebagai dimensi baru untuk meningkatkan pemampatan indeks terbalik. Kami mula-mula mengesan corak urutan d-gap menggunakan struktur data baru, UpDown Tree. Berdasarkan corak yang dikesan, kami seterusnya mengganti setiap corak dengan Id coraknya dalam senarai terbalik yang mengandunginya. Senarai terbalik yang dihasilkan kemudian dikodkan dengan skema pengkodan yang ada. Eksperimen menunjukkan bahawa pendekatan ini dapat meningkatkan nisbah mampatan kod yang ada dengan berkesan. [[EENNDD]] fail terbalik; corak jujukan; d-jurang; pemampatan indeks"], [{"string": "DoNet: a semantic domotic framework No contact information provided yet.", "keywords": ["reusable software", "agents", "domotics", "software architectures", "semantic web"], "combined": "DoNet: a semantic domotic framework No contact information provided yet. [[EENNDD]] reusable software; agents; domotics; software architectures; semantic web"}, "DoNet: kerangka domantik semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perisian yang boleh digunakan semula; ejen; domotik; seni bina perisian; web semantik"], [{"string": "Collaborative location and activity recommendations with GPS history data With the increasing popularity of location-based services, such as tour guide and location-based social network, we now have accumulated many location data on the Web. In this paper, we show that, by using the location data based on GPS and users' comments at various locations, we can discover interesting locations and possible activities that can be performed there for recommendations. Our research is highlighted in the following location-related queries in our daily life: 1) if we want to do something such as sightseeing or food-hunting in a large city such as Beijing, where should we go? 2) If we have already visited some places such as the Bird's Nest building in Beijing's Olympic park, what else can we do there? By using our system, for the first question, we can recommend her to visit a list of interesting locations such as Tiananmen Square, Bird's Nest, etc. For the second question, if the user visits Bird's Nest, we can recommend her to not only do sightseeing but also to experience its outdoor exercise facilities or try some nice food nearby. To achieve this goal, we first model the users' location and activity histories that we take as input. We then mine knowledge, such as the location features and activity-activity correlations from the geographical databases and the Web, to gather additional inputs. Finally, we apply a collective matrix factorization method to mine interesting locations and activities, and use them to recommend to the users where they can visit if they want to perform some specific activities and what they can do if they visit some specific places. We empirically evaluated our system using a large GPS dataset collected by 162 users over a period of 2.5 years in the real-world. We extensively evaluated our system and showed that our system can outperform several state-of-the-art baselines.", "keywords": ["collaborative filtering", "location and activity recommendations"], "combined": "Collaborative location and activity recommendations with GPS history data With the increasing popularity of location-based services, such as tour guide and location-based social network, we now have accumulated many location data on the Web. In this paper, we show that, by using the location data based on GPS and users' comments at various locations, we can discover interesting locations and possible activities that can be performed there for recommendations. Our research is highlighted in the following location-related queries in our daily life: 1) if we want to do something such as sightseeing or food-hunting in a large city such as Beijing, where should we go? 2) If we have already visited some places such as the Bird's Nest building in Beijing's Olympic park, what else can we do there? By using our system, for the first question, we can recommend her to visit a list of interesting locations such as Tiananmen Square, Bird's Nest, etc. For the second question, if the user visits Bird's Nest, we can recommend her to not only do sightseeing but also to experience its outdoor exercise facilities or try some nice food nearby. To achieve this goal, we first model the users' location and activity histories that we take as input. We then mine knowledge, such as the location features and activity-activity correlations from the geographical databases and the Web, to gather additional inputs. Finally, we apply a collective matrix factorization method to mine interesting locations and activities, and use them to recommend to the users where they can visit if they want to perform some specific activities and what they can do if they visit some specific places. We empirically evaluated our system using a large GPS dataset collected by 162 users over a period of 2.5 years in the real-world. We extensively evaluated our system and showed that our system can outperform several state-of-the-art baselines. [[EENNDD]] collaborative filtering; location and activity recommendations"}, "Saranan lokasi dan aktiviti kolaboratif dengan data sejarah GPS Dengan semakin meningkatnya populariti perkhidmatan berdasarkan lokasi, seperti pemandu pelancong dan rangkaian sosial berasaskan lokasi, kami sekarang telah mengumpulkan banyak data lokasi di Web. Dalam makalah ini, kami menunjukkan bahawa, dengan menggunakan data lokasi berdasarkan GPS dan komentar pengguna di berbagai lokasi, kami dapat menemukan lokasi menarik dan kemungkinan kegiatan yang dapat dilakukan di sana untuk mendapatkan cadangan. Penyelidikan kami diketengahkan dalam pertanyaan berkaitan lokasi berikut dalam kehidupan seharian kami: 1) jika kita ingin melakukan sesuatu seperti bersiar-siar atau berburu makanan di sebuah bandar besar seperti Beijing, ke mana kita harus pergi? 2) Sekiranya kita telah mengunjungi beberapa tempat seperti bangunan Bird's Nest di taman Olimpik Beijing, apa lagi yang boleh kita lakukan di sana? Dengan menggunakan sistem kami, untuk pertanyaan pertama, kami dapat mengesyorkannya untuk mengunjungi senarai lokasi menarik seperti Tiananmen Square, Bird's Nest, dll. Untuk pertanyaan kedua, jika pengguna mengunjungi Bird's Nest, kami dapat mengesyorkannya agar tidak hanya bersiar-siar tetapi juga untuk menikmati kemudahan bersenam di luar atau mencuba makanan yang sedap di kawasan berhampiran. Untuk mencapai matlamat ini, pertama-tama kami memodelkan sejarah lokasi dan aktiviti pengguna yang kami ambil sebagai input. Kami kemudian menimba pengetahuan, seperti ciri lokasi dan korelasi aktiviti-aktiviti dari pangkalan data geografi dan Web, untuk mengumpulkan input tambahan. Akhirnya, kami menggunakan kaedah pemodelan matriks kolektif untuk melombong lokasi dan aktiviti yang menarik, dan menggunakannya untuk mengesyorkan kepada pengguna di mana mereka boleh mengunjungi jika mereka ingin melakukan beberapa aktiviti tertentu dan apa yang dapat mereka lakukan jika mereka mengunjungi beberapa tempat tertentu. Kami secara empirikal menilai sistem kami menggunakan set data GPS yang besar yang dikumpulkan oleh 162 pengguna dalam jangka masa 2.5 tahun di dunia nyata. Kami menilai sistem kami secara meluas dan menunjukkan bahawa sistem kami dapat mengatasi beberapa garis dasar canggih. [[EENNDD]] penapisan kolaboratif; cadangan lokasi dan aktiviti"], [{"string": "Mashroom: end-user mashup programming using nested tables This paper presents an end-user-oriented programming environment called Mashroom. Major contributions herein include an end-user programming model with an expressive data structure as well as a set of formally-defined mashup operators. The data structure takes advantage of nested table, and maintains the intuitiveness while allowing users to express complex data objects. The mashup operators are visualized with contextual menu and formula bar and can be directly applied on the data. Experiments and case studies reveal that end users have little difficulty in effectively and efficiently using Mashroom to build mashup applications.", "keywords": ["reusable software", "nested table", "spreadsheet", "end-user programming", "mashup"], "combined": "Mashroom: end-user mashup programming using nested tables This paper presents an end-user-oriented programming environment called Mashroom. Major contributions herein include an end-user programming model with an expressive data structure as well as a set of formally-defined mashup operators. The data structure takes advantage of nested table, and maintains the intuitiveness while allowing users to express complex data objects. The mashup operators are visualized with contextual menu and formula bar and can be directly applied on the data. Experiments and case studies reveal that end users have little difficulty in effectively and efficiently using Mashroom to build mashup applications. [[EENNDD]] reusable software; nested table; spreadsheet; end-user programming; mashup"}, "Mashroom: pengaturcaraan mashup pengguna akhir menggunakan jadual bersarang Kertas ini menyajikan persekitaran pengaturcaraan berorientasi pengguna akhir yang disebut Mashroom. Sumbangan utama di sini termasuk model pengaturcaraan pengguna akhir dengan struktur data ekspresif serta sekumpulan operator mashup yang ditentukan secara formal. Struktur data memanfaatkan jadual bersarang, dan mengekalkan intuitif sambil membolehkan pengguna mengekspresikan objek data yang kompleks. Operator mashup divisualisasikan dengan menu kontekstual dan bar formula dan boleh digunakan secara langsung pada data. Eksperimen dan kajian kes menunjukkan bahawa pengguna akhir tidak mempunyai kesukaran untuk menggunakan Mashroom dengan berkesan dan berkesan untuk membina aplikasi mashup. [[EENNDD]] perisian yang boleh digunakan semula; meja bersarang; hamparan; pengaturcaraan pengguna akhir; tumbuk"], [{"string": "Learning to recognize reliable users and content in social media with coupled mutual reinforcement Community Question Answering (CQA) has emerged as a popular forum for users to pose questions for other users to answer. Over the last few years, CQA portals such as Naver and Yahoo! Answers have exploded in popularity, and now provide a viable alternative to general purpose Web search. At the same time, the answers to past questions submitted in CQA sites comprise a valuable knowledge repository which could be a gold mine for information retrieval and automatic question answering. Unfortunately, the quality of the submitted questions and answers varies widely - increasingly so that a large fraction of the content is not usable for answering queries. Previous approaches for retrieving relevant and high quality content have been proposed, but they require large amounts of manually labeled data -- which limits the applicability of the supervised approaches to new sites and domains. In this paper we address this problem by developing a semi-supervised coupled mutual reinforcement framework for simultaneously calculating content quality and user reputation, that requires relatively few labeled examples to initialize the training process. Results of a large scale evaluation demonstrate that our methods are more effective than previous approaches for finding high-quality answers, questions, and users. More importantly, our quality estimation significantly improves the accuracy of search over CQA archives over the state-of-the-art methods.", "keywords": ["graph theory", "on-line information services", "information search and retrieval", "community question answering", "authority and expertise in online communities"], "combined": "Learning to recognize reliable users and content in social media with coupled mutual reinforcement Community Question Answering (CQA) has emerged as a popular forum for users to pose questions for other users to answer. Over the last few years, CQA portals such as Naver and Yahoo! Answers have exploded in popularity, and now provide a viable alternative to general purpose Web search. At the same time, the answers to past questions submitted in CQA sites comprise a valuable knowledge repository which could be a gold mine for information retrieval and automatic question answering. Unfortunately, the quality of the submitted questions and answers varies widely - increasingly so that a large fraction of the content is not usable for answering queries. Previous approaches for retrieving relevant and high quality content have been proposed, but they require large amounts of manually labeled data -- which limits the applicability of the supervised approaches to new sites and domains. In this paper we address this problem by developing a semi-supervised coupled mutual reinforcement framework for simultaneously calculating content quality and user reputation, that requires relatively few labeled examples to initialize the training process. Results of a large scale evaluation demonstrate that our methods are more effective than previous approaches for finding high-quality answers, questions, and users. More importantly, our quality estimation significantly improves the accuracy of search over CQA archives over the state-of-the-art methods. [[EENNDD]] graph theory; on-line information services; information search and retrieval; community question answering; authority and expertise in online communities"}, "Belajar mengenali pengguna dan kandungan yang boleh dipercayai di media sosial dengan saling memperkuat Komuniti Menjawab Soalan (CQA) telah muncul sebagai forum popular bagi pengguna untuk mengemukakan soalan untuk dijawab oleh pengguna lain. Sejak beberapa tahun kebelakangan ini, portal CQA seperti Naver dan Yahoo! Jawapan telah meletup populariti, dan sekarang memberikan alternatif yang sesuai untuk carian Web tujuan umum. Pada masa yang sama, jawapan untuk soalan masa lalu yang dikemukakan di laman CQA terdiri dari gudang pengetahuan yang berharga yang boleh menjadi lombong emas untuk pencarian maklumat dan menjawab soalan automatik. Malangnya, kualiti soalan dan jawapan yang dihantar berbeza-beza - sehingga sebahagian besar kandungan tidak dapat digunakan untuk menjawab pertanyaan. Pendekatan sebelumnya untuk mengambil kandungan yang relevan dan berkualiti tinggi telah diusulkan, tetapi mereka memerlukan sejumlah besar data berlabel secara manual - yang membatasi penerapan pendekatan yang diawasi ke laman web dan domain baru. Dalam makalah ini kita mengatasi masalah ini dengan mengembangkan kerangka penguat bersama berpasangan separa pengawasan untuk secara bersamaan mengira kualiti kandungan dan reputasi pengguna, yang memerlukan sedikit contoh berlabel untuk memulakan proses latihan. Hasil penilaian skala besar menunjukkan bahawa kaedah kami lebih efektif daripada pendekatan sebelumnya untuk mencari jawapan, pertanyaan, dan pengguna berkualiti tinggi. Lebih penting lagi, anggaran kualiti kami meningkatkan ketepatan carian ke atas arkib CQA dengan kaedah terkini. [[EENNDD]] teori grafik; perkhidmatan maklumat dalam talian; carian dan pengambilan maklumat; menjawab soalan masyarakat; autoriti dan kepakaran dalam komuniti dalam talian"], [{"string": "Parallel boosted regression trees for web search ranking Gradient Boosted Regression Trees (GBRT) are the current state-of-the-art learning paradigm for machine learned web-search ranking - a domain notorious for very large data sets. In this paper, we propose a novel method for parallelizing the training of GBRT. Our technique parallelizes the construction of the individual regression trees and operates using the master-worker paradigm as follows. The data are partitioned among the workers. At each iteration, the worker summarizes its data-partition using histograms. The master processor uses these to build one layer of a regression tree, and then sends this layer to the workers, allowing the workers to build histograms for the next layer. Our algorithm carefully orchestrates overlap between communication and computation to achieve good performance.", "keywords": ["general", "ranking", "distributed computing", "web search", "parallel computing", "boosting", "boosted regression trees", "machine learning"], "combined": "Parallel boosted regression trees for web search ranking Gradient Boosted Regression Trees (GBRT) are the current state-of-the-art learning paradigm for machine learned web-search ranking - a domain notorious for very large data sets. In this paper, we propose a novel method for parallelizing the training of GBRT. Our technique parallelizes the construction of the individual regression trees and operates using the master-worker paradigm as follows. The data are partitioned among the workers. At each iteration, the worker summarizes its data-partition using histograms. The master processor uses these to build one layer of a regression tree, and then sends this layer to the workers, allowing the workers to build histograms for the next layer. Our algorithm carefully orchestrates overlap between communication and computation to achieve good performance. [[EENNDD]] general; ranking; distributed computing; web search; parallel computing; boosting; boosted regression trees; machine learning"}, "Pohon regresi yang ditingkatkan selari untuk kedudukan carian web Gradient Boosted Regression Trees (GBRT) adalah paradigma pembelajaran terkini untuk peringkat carian web yang dipelajari oleh mesin - domain yang terkenal dengan set data yang sangat besar. Dalam makalah ini, kami mencadangkan kaedah baru untuk menyelaraskan latihan GBRT. Teknik kami selari dengan pembinaan pokok regresi individu dan beroperasi menggunakan paradigma master-pekerja seperti berikut. Data dibahagikan kepada pekerja. Pada setiap lelaran, pekerja meringkaskan pembahagian datanya menggunakan histogram. Pemproses induk menggunakan ini untuk membina satu lapisan pohon regresi, dan kemudian mengirimkan lapisan ini kepada pekerja, yang memungkinkan pekerja membangun histogram untuk lapisan berikutnya. Algoritma kami dengan teliti mengatur pertindihan antara komunikasi dan pengiraan untuk mencapai prestasi yang baik. [[EENNDD]] umum; peringkat; pengkomputeran diedarkan; carian sesawang; pengkomputeran selari; meningkatkan; pokok regresi yang ditingkatkan; pembelajaran mesin"], [{"string": "Object-level ranking: bringing order to Web objects No contact information provided yet.", "keywords": ["pagerank", "web objects", "information search and retrieval", "link analysis", "web information retrieval", "poprank"], "combined": "Object-level ranking: bringing order to Web objects No contact information provided yet. [[EENNDD]] pagerank; web objects; information search and retrieval; link analysis; web information retrieval; poprank"}, "Peringkat tahap objek: membawa pesanan ke objek Web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pagerank; objek web; pencarian dan pengambilan maklumat; analisis pautan; pengambilan maklumat web; poprank"], [{"string": "One document to bind them: combining XML, web services, and the semantic web No contact information provided yet.", "keywords": ["pipelining", "functional programming", "xml", "web services", "semantic web", "programming languages"], "combined": "One document to bind them: combining XML, web services, and the semantic web No contact information provided yet. [[EENNDD]] pipelining; functional programming; xml; web services; semantic web; programming languages"}, "Satu dokumen untuk mengikatnya: menggabungkan XML, perkhidmatan web, dan web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] saluran paip; pengaturcaraan berfungsi; xml; perkhidmatan web; web semantik; bahasa pengaturcaraan"], [{"string": "Improving text collection selection with coverage and overlap statistics No contact information provided yet.", "keywords": ["collection overlap", "statistics gathering", "collection selection"], "combined": "Improving text collection selection with coverage and overlap statistics No contact information provided yet. [[EENNDD]] collection overlap; statistics gathering; collection selection"}, "Memperbaiki pemilihan koleksi teks dengan statistik liputan dan pertindihan Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] pertindihan koleksi; pengumpulan statistik; pemilihan koleksi"], [{"string": "Personalizing web sites for mobile users An abstract is not available.", "keywords": ["world wide web", "cellular architecture"], "combined": "Personalizing web sites for mobile users An abstract is not available. [[EENNDD]] world wide web; cellular architecture"}, "Memperibadikan laman web untuk pengguna mudah alih Abstrak tidak tersedia. [[EENNDD]] web seluruh dunia; seni bina selular"], [{"string": "Measurement and analysis of an online content voting network: a case study of Digg In online content voting networks, aggregate user activities (e.g., submitting and rating content) make high-quality content thrive through the unprecedented scale, high dynamics and divergent quality of user generated content (UGC). To better understand the nature and impact of online content voting networks, we have analyzed Digg, a popular online social news aggregator and rating website. Based on a large amount of data collected, we provide an in-depth study of Digg. We study structural properties of Digg social network, revealing some strikingly distinct properties such as low link symmetry and the power-law distribution of node outdegree with truncated tails. We explore impact of the social network on user digging activities, and investigate the issues of content promotion, content filtering, vote spam and content censorship, which are inherent to content rating networks. We also provide insight into design of content promotion algorithms and recommendation-assisted content discovery. Overall, we believe that the results presented in this paper are crucial in understanding online content rating networks.", "keywords": ["content filtering", "social and behavioral sciences", "miscellaneous", "content promotion", "social networks"], "combined": "Measurement and analysis of an online content voting network: a case study of Digg In online content voting networks, aggregate user activities (e.g., submitting and rating content) make high-quality content thrive through the unprecedented scale, high dynamics and divergent quality of user generated content (UGC). To better understand the nature and impact of online content voting networks, we have analyzed Digg, a popular online social news aggregator and rating website. Based on a large amount of data collected, we provide an in-depth study of Digg. We study structural properties of Digg social network, revealing some strikingly distinct properties such as low link symmetry and the power-law distribution of node outdegree with truncated tails. We explore impact of the social network on user digging activities, and investigate the issues of content promotion, content filtering, vote spam and content censorship, which are inherent to content rating networks. We also provide insight into design of content promotion algorithms and recommendation-assisted content discovery. Overall, we believe that the results presented in this paper are crucial in understanding online content rating networks. [[EENNDD]] content filtering; social and behavioral sciences; miscellaneous; content promotion; social networks"}, "Pengukuran dan analisis rangkaian pengundian kandungan dalam talian: kajian kes mengenai rangkaian pengundian kandungan dalam talian Digg In, aktiviti pengguna agregat (misalnya, menghantar dan menilai kandungan) menjadikan kandungan berkualiti tinggi berkembang melalui skala yang belum pernah terjadi sebelumnya, dinamika tinggi dan kualiti pengguna yang berbeza. kandungan yang dihasilkan (UGC). Untuk lebih memahami sifat dan kesan rangkaian pengundian kandungan dalam talian, kami telah menganalisis Digg, laman web pengagregat berita sosial sosial yang popular. Berdasarkan sejumlah besar data yang dikumpulkan, kami memberikan kajian mendalam mengenai Digg. Kami mengkaji sifat struktur rangkaian sosial Digg, mendedahkan beberapa sifat yang sangat ketara seperti simetri pautan rendah dan pengedaran undang-undang kuasa node di luar tahap dengan ekor terpotong. Kami meneroka kesan rangkaian sosial pada aktiviti penggalian pengguna, dan menyelidiki masalah promosi kandungan, penapisan konten, spam suara dan penapisan kandungan, yang melekat pada rangkaian penilaian kandungan. Kami juga memberikan gambaran mengenai reka bentuk algoritma promosi kandungan dan penemuan kandungan yang dibantu oleh cadangan. Secara keseluruhan, kami percaya bahawa hasil yang dikemukakan dalam makalah ini sangat penting dalam memahami rangkaian penarafan kandungan dalam talian. [[EENNDD]] penapisan kandungan; sains sosial dan tingkah laku; pelbagai; promosi kandungan; rangkaian sosial"], [{"string": "Statically locating web application bugs caused by asynchronous calls Ajax becomes more and more important for web applications that care about client side user experience. It allows sending requests asynchronously, without blocking clients from continuing execution. Callback functions are only executed upon receiving the responses. While such mechanism makes browsing a smooth experience, it may cause severe problems in the presence of unexpected network latency, due to the non-determinism of asynchronism. In this paper, we demonstrate the possible problems caused by the asynchronism and propose a static program analysis to automatically detect such bugs in web applications. As client side Ajax code is often wrapped in server-side scripts, we also develop a technique that extracts client-side JavaScript code from server-side scripts. We evaluate our technique on a number of real-world web applications. Our results show that it can effectively identify real bugs. We also discuss possible ways to avoid such bugs.", "keywords": ["ajax", "static analysis", "javascript", "automatic debugging"], "combined": "Statically locating web application bugs caused by asynchronous calls Ajax becomes more and more important for web applications that care about client side user experience. It allows sending requests asynchronously, without blocking clients from continuing execution. Callback functions are only executed upon receiving the responses. While such mechanism makes browsing a smooth experience, it may cause severe problems in the presence of unexpected network latency, due to the non-determinism of asynchronism. In this paper, we demonstrate the possible problems caused by the asynchronism and propose a static program analysis to automatically detect such bugs in web applications. As client side Ajax code is often wrapped in server-side scripts, we also develop a technique that extracts client-side JavaScript code from server-side scripts. We evaluate our technique on a number of real-world web applications. Our results show that it can effectively identify real bugs. We also discuss possible ways to avoid such bugs. [[EENNDD]] ajax; static analysis; javascript; automatic debugging"}, "Mencari pepijat aplikasi web secara statik yang disebabkan oleh panggilan tak segerak Ajax menjadi semakin penting bagi aplikasi web yang mementingkan pengalaman pengguna di sisi pelanggan. Ini membenarkan pengiriman permintaan secara tidak serentak, tanpa menyekat klien daripada meneruskan pelaksanaan. Fungsi panggil balik hanya dilaksanakan setelah menerima respons. Walaupun mekanisme seperti itu menjadikan penyemakan imbas menjadi pengalaman yang lancar, ia mungkin menyebabkan masalah teruk dengan adanya latensi rangkaian yang tidak dijangka, disebabkan oleh ketidaktentuan fahaman asinkronisme. Dalam makalah ini, kami menunjukkan kemungkinan masalah yang disebabkan oleh asinkronisme dan mencadangkan analisis program statik untuk secara automatik mengesan bug tersebut dalam aplikasi web. Oleh kerana kod Ajax sisi pelanggan sering dibungkus dengan skrip sisi pelayan, kami juga mengembangkan teknik yang mengekstrak kod JavaScript sisi klien dari skrip sisi pelayan. Kami menilai teknik kami pada sebilangan aplikasi web dunia nyata. Hasil kajian kami menunjukkan bahawa ia dapat mengenal pasti pepijat sebenar dengan berkesan. Kami juga membincangkan cara-cara yang mungkin untuk mengelakkan pepijat tersebut. [[EENNDD]] ajax; analisis statik; javascript; penyahpepijatan automatik"], [{"string": "Counting beyond a Yottabyte, or how SPARQL 1.1 property paths will prevent adoption of the standard SPARQL -the standard query language for querying RDF- provides only limited navigational functionalities, although these features are of fundamental importance for graph data formats such as RDF. This has led the W3C to include the property path feature in the upcoming version of the standard, SPARQL 1.1.We tested several implementations of SPARQL 1.1 handling property path queries, and we observed that their evaluation methods for this class of queries have a poor performance even in some very simple scenarios. To formally explain this fact, we conduct a theoretical study of the computational complexity of property paths evaluation. Our results imply that the poor performance of the tested implementations is not a problem of these particular systems, but of the specification itself. In fact, we show that any implementation that adheres to the SPARQL 1.1 specification (as of November 2011) is doomed to show the same behavior, the key issue being the need for counting solutions imposed by the current specification. We provide several intractability results, that together with our empirical results, provide strong evidence against the current semantics of SPARQL 1.1 property paths. Finally, we put our results in perspective, and propose a natural alternative semantics with tractable evaluation, that we think may lead to a wide adoption of the language by practitioners, developers and theoreticians.", "keywords": ["bag semantics", "property paths", "sparql 1.1", "counting complexity"], "combined": "Counting beyond a Yottabyte, or how SPARQL 1.1 property paths will prevent adoption of the standard SPARQL -the standard query language for querying RDF- provides only limited navigational functionalities, although these features are of fundamental importance for graph data formats such as RDF. This has led the W3C to include the property path feature in the upcoming version of the standard, SPARQL 1.1.We tested several implementations of SPARQL 1.1 handling property path queries, and we observed that their evaluation methods for this class of queries have a poor performance even in some very simple scenarios. To formally explain this fact, we conduct a theoretical study of the computational complexity of property paths evaluation. Our results imply that the poor performance of the tested implementations is not a problem of these particular systems, but of the specification itself. In fact, we show that any implementation that adheres to the SPARQL 1.1 specification (as of November 2011) is doomed to show the same behavior, the key issue being the need for counting solutions imposed by the current specification. We provide several intractability results, that together with our empirical results, provide strong evidence against the current semantics of SPARQL 1.1 property paths. Finally, we put our results in perspective, and propose a natural alternative semantics with tractable evaluation, that we think may lead to a wide adoption of the language by practitioners, developers and theoreticians. [[EENNDD]] bag semantics; property paths; sparql 1.1; counting complexity"}, "Mengira di luar Yottabyte, atau bagaimana jalan harta SPARQL 1.1 akan menghalang penggunaan SPARQL standard - bahasa pertanyaan standard untuk membuat pertanyaan RDF - hanya menyediakan fungsi navigasi terhad, walaupun ciri-ciri ini sangat penting bagi format data grafik seperti RDF. Ini menyebabkan W3C memasukkan ciri laluan harta tanah dalam versi standard yang akan datang, SPARQL 1.1. Kami menguji beberapa pelaksanaan SPARQL 1.1 yang mengendalikan pertanyaan laluan harta tanah, dan kami melihat bahawa kaedah penilaian mereka untuk kelas pertanyaan ini mempunyai prestasi yang buruk walaupun dalam beberapa senario yang sangat sederhana. Untuk menjelaskan fakta ini secara rasmi, kami melakukan kajian teori mengenai kerumitan komputasi penilaian laluan harta tanah. Hasil kami menunjukkan bahawa prestasi pelaksanaan yang diuji tidak menjadi masalah sistem tertentu ini, tetapi dari spesifikasi itu sendiri. Sebenarnya, kami menunjukkan bahawa apa-apa pelaksanaan yang mematuhi spesifikasi SPARQL 1.1 (pada November 2011) ditakdirkan untuk menunjukkan tingkah laku yang sama, masalah utamanya adalah perlunya menghitung penyelesaian yang dikenakan oleh spesifikasi semasa. Kami memberikan beberapa hasil yang sukar dicapai, yang bersama-sama dengan hasil empirik kami, memberikan bukti yang kuat terhadap semantik saat ini jalan harta tanah SPARQL 1.1. Akhirnya, kami meletakkan hasil kami dalam perspektif, dan mengusulkan semantik alternatif semula jadi dengan penilaian yang dapat dirawat, yang kami fikir boleh menyebabkan penggunaan bahasa secara meluas oleh pengamal, pengembang dan ahli teori. [[EENNDD]] semantik beg; jalan harta tanah; sparql 1.1; mengira kerumitan"], [{"string": "Graph-based text database for knowledge discovery No contact information provided yet.", "keywords": ["knowledge discovery", "interactive search", "subject graphs"], "combined": "Graph-based text database for knowledge discovery No contact information provided yet. [[EENNDD]] knowledge discovery; interactive search; subject graphs"}, "Pangkalan data teks berasaskan grafik untuk penemuan pengetahuan Belum ada maklumat hubungan yang disediakan. [[EENNDD]] penemuan pengetahuan; carian interaktif; grafik subjek"], [{"string": "Large-scale text categorization by batch mode active learning No contact information provided yet.", "keywords": ["fisher information", "logistic regression", "information search and retrieval", "active learning", "text categorization", "convex optimization"], "combined": "Large-scale text categorization by batch mode active learning No contact information provided yet. [[EENNDD]] fisher information; logistic regression; information search and retrieval; active learning; text categorization; convex optimization"}, "Pengkategorian teks berskala besar mengikut pembelajaran aktif mod kumpulan Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] maklumat perikanan; regresi logistik; pencarian dan pengambilan maklumat; pembelajaran aktif; pengkategorian teks; pengoptimuman cembung"], [{"string": "An axiomatic approach for result diversification Understanding user intent is key to designing an effective ranking system in a search engine. In the absence of any explicit knowledge of user intent, search engines want to diversify results to improve user satisfaction. In such a setting, the probability ranking principle-based approach of presenting the most relevant results on top can be sub-optimal, and hence the search engine would like to trade-off relevance for diversity in the results.", "keywords": ["axiomatic framework", "search engine", "diversification", "information search and retrieval", "facility dispersion", "wikipedia", "approximation algorithms"], "combined": "An axiomatic approach for result diversification Understanding user intent is key to designing an effective ranking system in a search engine. In the absence of any explicit knowledge of user intent, search engines want to diversify results to improve user satisfaction. In such a setting, the probability ranking principle-based approach of presenting the most relevant results on top can be sub-optimal, and hence the search engine would like to trade-off relevance for diversity in the results. [[EENNDD]] axiomatic framework; search engine; diversification; information search and retrieval; facility dispersion; wikipedia; approximation algorithms"}, "Pendekatan aksiomatik untuk mempelbagaikan hasil Memahami maksud pengguna adalah kunci untuk merancang sistem pemeringkatan yang berkesan dalam mesin pencari. Sekiranya tidak ada pengetahuan eksplisit mengenai maksud pengguna, mesin carian ingin mempelbagaikan hasil untuk meningkatkan kepuasan pengguna. Dalam keadaan seperti itu, pendekatan berdasarkan prinsip kebarangkalian untuk menyampaikan hasil yang paling relevan di atas dapat menjadi kurang optimum, dan oleh itu mesin pencari ingin menukar relevan untuk kepelbagaian dalam hasilnya. [[EENNDD]] kerangka aksiomatik; enjin carian; kepelbagaian; pencarian dan pengambilan maklumat; penyebaran kemudahan; wikipedia; algoritma penghampiran"], [{"string": "A scalable application placement controller for enterprise data centers Given a set of machines and a set of Web applications with dynamically changing demands, an online application placement controller decides how many instances to run for each application and where to put them, while observing all kinds of resource constraints. This NP hard problem has real usage in commercial middleware products. Existing approximation algorithms for this problem can scale to at most a few hundred machines, and may produce placement solutions that are far from optimal when system resources are tight. In this paper, we propose a new algorithm that can produce within 30seconds high-quality solutions for hard placement problems with thousands of machines and thousands of applications. This scalability is crucial for dynamic resource provisioning in large-scale enterprise data centers. Our algorithm allows multiple applications to share a single machine, and strivesto maximize the total satisfied application demand, to minimize the number of application starts and stops, and to balance the load across machines. Compared with existing state-of-the-art algorithms, for systems with 100 machines or less, our algorithm is up to 134 times faster, reduces application starts and stops by up to 97%, and produces placement solutions that satisfy up to 25% more application demands. Our algorithm has been implemented and adopted in a leading commercial middleware product for managing the performance of Web applications.", "keywords": ["application placement", "performance management"], "combined": "A scalable application placement controller for enterprise data centers Given a set of machines and a set of Web applications with dynamically changing demands, an online application placement controller decides how many instances to run for each application and where to put them, while observing all kinds of resource constraints. This NP hard problem has real usage in commercial middleware products. Existing approximation algorithms for this problem can scale to at most a few hundred machines, and may produce placement solutions that are far from optimal when system resources are tight. In this paper, we propose a new algorithm that can produce within 30seconds high-quality solutions for hard placement problems with thousands of machines and thousands of applications. This scalability is crucial for dynamic resource provisioning in large-scale enterprise data centers. Our algorithm allows multiple applications to share a single machine, and strivesto maximize the total satisfied application demand, to minimize the number of application starts and stops, and to balance the load across machines. Compared with existing state-of-the-art algorithms, for systems with 100 machines or less, our algorithm is up to 134 times faster, reduces application starts and stops by up to 97%, and produces placement solutions that satisfy up to 25% more application demands. Our algorithm has been implemented and adopted in a leading commercial middleware product for managing the performance of Web applications. [[EENNDD]] application placement; performance management"}, "Pengawal penempatan aplikasi berskala untuk pusat data perusahaan Memandangkan satu set mesin dan satu set aplikasi Web dengan permintaan yang berubah secara dinamis, pengawal penempatan aplikasi dalam talian memutuskan berapa banyak contoh untuk dijalankan untuk setiap aplikasi dan di mana meletakkannya, sambil memerhatikan semua jenis kekangan sumber. Masalah sukar NP ini mempunyai penggunaan sebenar dalam produk middleware komersial. Algoritma penghampiran yang ada untuk masalah ini dapat mencapai paling banyak beberapa ratus mesin, dan mungkin menghasilkan penyelesaian penempatan yang jauh dari optimum ketika sumber daya sistem ketat. Dalam makalah ini, kami mencadangkan algoritma baru yang dapat menghasilkan penyelesaian berkualiti tinggi dalam 30 saat untuk masalah penempatan keras dengan ribuan mesin dan ribuan aplikasi. Skalabiliti ini sangat penting untuk penyediaan sumber yang dinamik di pusat data perusahaan berskala besar. Algoritma kami membolehkan berbilang aplikasi untuk berkongsi satu mesin, dan berusaha untuk memaksimumkan jumlah permintaan aplikasi yang memuaskan, untuk meminimumkan jumlah permulaan dan berhenti aplikasi, dan untuk menyeimbangkan beban di seluruh mesin. Berbanding dengan algoritma canggih yang ada, untuk sistem dengan 100 mesin atau kurang, algoritma kami meningkat sehingga 134 kali lebih cepat, mengurangkan permulaan dan berhenti aplikasi hingga 97%, dan menghasilkan penyelesaian penempatan yang memuaskan hingga 25% lebih banyak tuntutan permohonan. Algoritma kami telah dilaksanakan dan diadopsi dalam produk middleware komersial terkemuka untuk menguruskan prestasi aplikasi Web. [[EENNDD]] penempatan aplikasi; pengurusan Prestasi"], [{"string": "Online spelling correction for query completion In this paper, we study the problem of online spelling correction for query completions. Misspelling is a common phenomenon among search engines queries. In order to help users effectively express their information needs, mechanisms for automatically correcting misspelled queries are required. Online spelling correction aims to provide spell corrected completion suggestions as a query is incrementally entered. As latency is crucial to the utility of the suggestions, such an algorithm needs to be not only accurate, but also efficient.", "keywords": ["a* search", "transformation model", "spelling correction", "query completion"], "combined": "Online spelling correction for query completion In this paper, we study the problem of online spelling correction for query completions. Misspelling is a common phenomenon among search engines queries. In order to help users effectively express their information needs, mechanisms for automatically correcting misspelled queries are required. Online spelling correction aims to provide spell corrected completion suggestions as a query is incrementally entered. As latency is crucial to the utility of the suggestions, such an algorithm needs to be not only accurate, but also efficient. [[EENNDD]] a* search; transformation model; spelling correction; query completion"}, "Pembetulan ejaan dalam talian untuk penyelesaian pertanyaan Dalam makalah ini, kami mengkaji masalah pembetulan ejaan dalam talian untuk penyelesaian pertanyaan. Ejaan salah adalah fenomena yang biasa berlaku dalam pertanyaan enjin carian. Untuk membantu pengguna mengekspresikan keperluan maklumat dengan berkesan, diperlukan mekanisme untuk membetulkan pertanyaan yang salah eja secara automatik. Pembetulan ejaan dalam talian bertujuan untuk memberikan cadangan penyelesaian ejaan yang diperbaiki kerana pertanyaan dimasukkan secara bertahap. Oleh kerana latensi sangat penting untuk kegunaan cadangan, algoritma seperti itu bukan sahaja tepat, tetapi juga cekap. [[EENNDD]] carian *; model transformasi; pembetulan ejaan; penyelesaian pertanyaan"], [{"string": "Non-intrusive monitoring and service adaptation for WS-BPEL Web service processes currently lack monitoring and dynamic (runtime) adaptation mechanisms. In highly dynamic processes, services frequently need to be exchanged due to a variety of reasons. In this paper we present VieDAME, a system which allows monitoring of BPEL processes according to Quality of Service (QoS) attributes and replacement of existing partner services based on various (pluggable) replacement strategies. The chosen replacement services can be syntactically or semantically equivalent to the BPEL interface. Services can be automatically replaced during runtime without any downtime of the overall system. We implemented our solution with an aspect-oriented approach by intercepting SOAP messages and allow services to be exchanged during runtime with little performance penalty costs, as shown in our experiments, thereby making our approach suitable for high-availability BPEL environments.", "keywords": ["bpel", "service selection", "monitoring", "quality of service", "message mediation"], "combined": "Non-intrusive monitoring and service adaptation for WS-BPEL Web service processes currently lack monitoring and dynamic (runtime) adaptation mechanisms. In highly dynamic processes, services frequently need to be exchanged due to a variety of reasons. In this paper we present VieDAME, a system which allows monitoring of BPEL processes according to Quality of Service (QoS) attributes and replacement of existing partner services based on various (pluggable) replacement strategies. The chosen replacement services can be syntactically or semantically equivalent to the BPEL interface. Services can be automatically replaced during runtime without any downtime of the overall system. We implemented our solution with an aspect-oriented approach by intercepting SOAP messages and allow services to be exchanged during runtime with little performance penalty costs, as shown in our experiments, thereby making our approach suitable for high-availability BPEL environments. [[EENNDD]] bpel; service selection; monitoring; quality of service; message mediation"}, "Pemantauan dan penyesuaian perkhidmatan yang tidak mengganggu untuk proses perkhidmatan Web WS-BPEL pada masa ini kekurangan pemantauan dan mekanisme penyesuaian dinamik (runtime). Dalam proses yang sangat dinamik, perkhidmatan sering kali perlu ditukar kerana pelbagai alasan. Dalam makalah ini kami menyajikan VieDAME, sebuah sistem yang memungkinkan pemantauan proses BPEL sesuai dengan atribut Quality of Service (QoS) dan penggantian perkhidmatan rakan yang ada berdasarkan berbagai strategi penggantian (pluggable). Perkhidmatan penggantian yang dipilih boleh disamakan secara sintaksis atau semantik dengan antara muka BPEL. Perkhidmatan boleh diganti secara automatik semasa runtime tanpa downtime keseluruhan sistem. Kami melaksanakan penyelesaian kami dengan pendekatan berorientasikan aspek dengan memintas pesanan SOAP dan membolehkan perkhidmatan ditukar semasa waktu berjalan dengan sedikit kos penalti prestasi, seperti yang ditunjukkan dalam eksperimen kami, sehingga menjadikan pendekatan kami sesuai untuk persekitaran BPEL dengan ketersediaan tinggi. [[EENNDD]] bpel; pemilihan perkhidmatan; pemantauan; kualiti sesuatu servis; pengantaraan mesej"], [{"string": "Web scale NLP: a case study on url word breaking This paper uses the URL word breaking task as an example to elaborate what we identify as crucial in designing statistical natural language processing (NLP) algorithms for Web scale applications: (1) rudimentary multilingual capabilities to cope with the global nature of the Web, (2) multi-style modeling to handle diverse language styles seen in the Web contents, (3) fast adaptation to keep pace with the dynamic changes of the Web, (4) minimal heuristic assumptions for generalizability and robustness, and (5) possibilities of efficient implementations and minimal manual efforts for processing massive amount of data at a reasonable cost. We first show that the state-of-the-art word breaking techniques can be unified and generalized under the Bayesian minimum risk (BMR) framework that, using a Web scale N-gram, can meet the first three requirements. We discuss how the existing techniques can be viewed as introducing additional assumptions to the basic BMR framework, and describe a generic yet efficient implementation called word synchronous beam search. Testing the framework and its implementation on a series of large scale experiments reveals the following. First, the language style used to build the model plays a critical role in the word breaking task, and the most suitable for the URL word breaking task appears to be that of the document title where the best performance is obtained. Models created from other language styles, such as from document body, anchor text, and even queries, exhibit varying degrees of mismatch. Although all styles benefit from increasing modeling power which, in our experiments, corresponds to the use of a higher order N-gram, the gain is most recognizable for the title model. The heuristics proposed by the prior arts do contribute to the word breaking performance for mismatched or less powerful models, but are less effective and, in many cases, lead to poorer performance than the matched model with minimal assumptions. For the matched model based on document titles, an accuracy rate of 97.18% can already be achieved using simple trigram without any heuristics.", "keywords": ["url segmentation", "web scale word breaking", "compound splitting", "multi-style language model", "word segmentation"], "combined": "Web scale NLP: a case study on url word breaking This paper uses the URL word breaking task as an example to elaborate what we identify as crucial in designing statistical natural language processing (NLP) algorithms for Web scale applications: (1) rudimentary multilingual capabilities to cope with the global nature of the Web, (2) multi-style modeling to handle diverse language styles seen in the Web contents, (3) fast adaptation to keep pace with the dynamic changes of the Web, (4) minimal heuristic assumptions for generalizability and robustness, and (5) possibilities of efficient implementations and minimal manual efforts for processing massive amount of data at a reasonable cost. We first show that the state-of-the-art word breaking techniques can be unified and generalized under the Bayesian minimum risk (BMR) framework that, using a Web scale N-gram, can meet the first three requirements. We discuss how the existing techniques can be viewed as introducing additional assumptions to the basic BMR framework, and describe a generic yet efficient implementation called word synchronous beam search. Testing the framework and its implementation on a series of large scale experiments reveals the following. First, the language style used to build the model plays a critical role in the word breaking task, and the most suitable for the URL word breaking task appears to be that of the document title where the best performance is obtained. Models created from other language styles, such as from document body, anchor text, and even queries, exhibit varying degrees of mismatch. Although all styles benefit from increasing modeling power which, in our experiments, corresponds to the use of a higher order N-gram, the gain is most recognizable for the title model. The heuristics proposed by the prior arts do contribute to the word breaking performance for mismatched or less powerful models, but are less effective and, in many cases, lead to poorer performance than the matched model with minimal assumptions. For the matched model based on document titles, an accuracy rate of 97.18% can already be achieved using simple trigram without any heuristics. [[EENNDD]] url segmentation; web scale word breaking; compound splitting; multi-style language model; word segmentation"}, "NLP skala web: kajian kes mengenai pemecahan kata url Makalah ini menggunakan tugas pemecahan kata URL sebagai contoh untuk menghuraikan perkara yang kami kenal pasti penting dalam merancang algoritma pemprosesan bahasa semula jadi (NLP) statistik untuk aplikasi skala Web: (1) keupayaan multibahasa yang tidak betul untuk mengatasi sifat global Web, (2) pemodelan pelbagai gaya untuk menangani gaya bahasa yang beragam yang dilihat dalam isi Web, (3) penyesuaian cepat untuk mengikuti perubahan dinamis Web, (4) asumsi heuristik minimum untuk generalisasi dan ketahanan, dan (5) kemungkinan pelaksanaan yang efisien dan usaha manual minimum untuk memproses sejumlah besar data dengan kos yang berpatutan. Kami pertama kali menunjukkan bahawa teknik pemecahan kata canggih dapat disatukan dan digeneralisasikan di bawah kerangka risiko minimum Bayesian (BMR) yang, dengan menggunakan skala Web N-gram, dapat memenuhi tiga syarat pertama. Kami membincangkan bagaimana teknik yang ada dapat dilihat sebagai memperkenalkan andaian tambahan pada kerangka BMR asas, dan menerangkan pelaksanaan yang generik namun efisien yang disebut pencarian sinar sinkron kata. Menguji kerangka kerja dan pelaksanaannya pada serangkaian eksperimen berskala besar menunjukkan yang berikut. Pertama, gaya bahasa yang digunakan untuk membina model memainkan peranan penting dalam tugas pemecahan kata, dan yang paling sesuai untuk tugas pemecahan kata URL adalah seperti tajuk dokumen di mana prestasi terbaik diperoleh. Model yang dibuat dari gaya bahasa lain, seperti dari badan dokumen, teks jangkar, dan bahkan pertanyaan, menunjukkan tahap ketidakcocokan yang berbeza-beza. Walaupun semua gaya mendapat keuntungan dari peningkatan kekuatan pemodelan yang, dalam eksperimen kami, sesuai dengan penggunaan N-gram yang lebih tinggi, keuntungannya paling dikenali untuk model judul. Heuristik yang diusulkan oleh seni sebelumnya memang menyumbang kepada prestasi pemecahan kata untuk model yang tidak sesuai atau kurang kuat, tetapi kurang efektif dan, dalam banyak kes, menyebabkan prestasi yang lebih buruk daripada model yang dipadankan dengan andaian minimum. Untuk model yang dipadankan berdasarkan judul dokumen, kadar ketepatan 97.18% sudah dapat dicapai dengan menggunakan trigram sederhana tanpa heuristik. [[EENNDD]] segmentasi url; pemecahan perkataan skala web; pemecahan sebatian; model bahasa pelbagai gaya; pembahagian perkataan"], [{"string": "A novel clustering-based RSS aggregator In recent years, different commercial Weblog subscribing systems have been proposed to return stories from users. subscribed feeds. In this paper, we propose a novel clustering-based RSS aggregator called as RSS Clusgator System (RCS) for Weblog reading. Note that an RSS feed may have several different topics. A user may only be interested in a subset of these topics. In addition there could be many different stories from multiple RSS feeds, which discuss similar topic from different perspectives. A user may be interested in this topic but do not know how to collect all feeds related to this topic. In contrast to many previous works, we cluster all stories in RSS feeds into hierarchical structure to better serve the readers. Through this way, users can easily find all their interested stories. To make the system current, we propose a flexible time window for incremental clustering. RCS utilizes both link information and content information for efficient clustering. Experiments show the effectiveness of RCS.", "keywords": ["weblog", "story", "rss", "clustering"], "combined": "A novel clustering-based RSS aggregator In recent years, different commercial Weblog subscribing systems have been proposed to return stories from users. subscribed feeds. In this paper, we propose a novel clustering-based RSS aggregator called as RSS Clusgator System (RCS) for Weblog reading. Note that an RSS feed may have several different topics. A user may only be interested in a subset of these topics. In addition there could be many different stories from multiple RSS feeds, which discuss similar topic from different perspectives. A user may be interested in this topic but do not know how to collect all feeds related to this topic. In contrast to many previous works, we cluster all stories in RSS feeds into hierarchical structure to better serve the readers. Through this way, users can easily find all their interested stories. To make the system current, we propose a flexible time window for incremental clustering. RCS utilizes both link information and content information for efficient clustering. Experiments show the effectiveness of RCS. [[EENNDD]] weblog; story; rss; clustering"}, "Agregator RSS berasaskan pengelompokan novel Dalam beberapa tahun terakhir, sistem langganan Weblog komersial yang berbeza telah diusulkan untuk mengembalikan cerita dari pengguna. suapan yang dilanggan. Dalam makalah ini, kami mengusulkan agregator RSS berasaskan pengelompokan novel yang disebut sebagai Sistem RSS Clusgator (RCS) untuk pembacaan Weblog. Perhatikan bahawa suapan RSS mungkin mempunyai beberapa topik yang berbeza. Pengguna mungkin hanya berminat dengan subkumpulan topik ini. Di samping itu, mungkin terdapat banyak cerita yang berbeza dari pelbagai RSS feed, yang membincangkan topik serupa dari perspektif yang berbeza. Pengguna mungkin berminat dengan topik ini tetapi tidak tahu mengumpulkan semua suapan yang berkaitan dengan topik ini. Berbeza dengan banyak karya sebelumnya, kami mengumpulkan semua cerita dalam RSS feed ke dalam struktur hierarki untuk melayani pembaca dengan lebih baik. Dengan cara ini, pengguna dapat dengan mudah menemui semua kisah mereka yang berminat. Untuk menjadikan sistem terkini, kami mencadangkan tetingkap waktu yang fleksibel untuk penggabungan tambahan. RCS menggunakan maklumat pautan dan maklumat kandungan untuk pengelompokan yang cekap. Eksperimen menunjukkan keberkesanan RCS. [[EENNDD]] weblog; cerita; rss; pengelompokan"], [{"string": "A framework for XML data streams history checking and monitoring No contact information provided yet.", "keywords": ["xml", "semi-structured data"], "combined": "A framework for XML data streams history checking and monitoring No contact information provided yet. [[EENNDD]] xml; semi-structured data"}, "Kerangka kerja untuk data aliran data XML memeriksa dan memantau Belum ada maklumat hubungan yang diberikan. [[EENNDD]] xml; data separa berstruktur"], [{"string": "Adaptive web sites: user studies and simulation No contact information provided yet.", "keywords": ["adaptive web site", "learning", "ant colony optimization", "model validation and analysis"], "combined": "Adaptive web sites: user studies and simulation No contact information provided yet. [[EENNDD]] adaptive web site; learning; ant colony optimization; model validation and analysis"}, "Laman web adaptif: kajian dan simulasi pengguna Belum ada maklumat hubungan yang diberikan. [[EENNDD]] laman web adaptif; belajar; pengoptimuman koloni semut; pengesahan dan analisis model"], [{"string": "The complex dynamics of collaborative tagging The debate within the Web community over the optimal means by which to organize information often pits formalized classifications against distributed collaborative tagging systems. A number of questions remain unanswered, however, regarding the nature of collaborative tagging systems including whether coherent categorization schemes can emerge from unsupervised tagging by users. This paper uses data from the social bookmarking site delicio. us to examine the dynamics of collaborative tagging systems. In particular, we examine whether the distribution of the frequency of use of tags for \"popular\" sites with a long history (many tags and many users) can be described by a power law distribution, often characteristic of what are considered complex systems. We produce a generative model of collaborative tagging in order to understand the basic dynamics behind tagging, including how a power law distribution of tags could arise. We empirically examine the tagging history of sites in order to determine how this distribution arises over time and to determine the patterns prior to a stable distribution. Lastly, by focusing on the high-frequency tags of a site where the distribution of tags is a stabilized power law, we show how tag co-occurrence networks for a sample domain of tags can be used to analyze the meaning of particular tags given their relationship to other tags.", "keywords": ["emergent semantics", "power laws", "collaborative filtering", "knowledge representation formalisms and methods", "delicious", "tagging", "complex systems"], "combined": "The complex dynamics of collaborative tagging The debate within the Web community over the optimal means by which to organize information often pits formalized classifications against distributed collaborative tagging systems. A number of questions remain unanswered, however, regarding the nature of collaborative tagging systems including whether coherent categorization schemes can emerge from unsupervised tagging by users. This paper uses data from the social bookmarking site delicio. us to examine the dynamics of collaborative tagging systems. In particular, we examine whether the distribution of the frequency of use of tags for \"popular\" sites with a long history (many tags and many users) can be described by a power law distribution, often characteristic of what are considered complex systems. We produce a generative model of collaborative tagging in order to understand the basic dynamics behind tagging, including how a power law distribution of tags could arise. We empirically examine the tagging history of sites in order to determine how this distribution arises over time and to determine the patterns prior to a stable distribution. Lastly, by focusing on the high-frequency tags of a site where the distribution of tags is a stabilized power law, we show how tag co-occurrence networks for a sample domain of tags can be used to analyze the meaning of particular tags given their relationship to other tags. [[EENNDD]] emergent semantics; power laws; collaborative filtering; knowledge representation formalisms and methods; delicious; tagging; complex systems"}, "Dinamika penandaan kolaboratif yang kompleks Perbahasan dalam komuniti Web mengenai kaedah optimum untuk mengatur maklumat sering memberi klasifikasi formal terhadap sistem penandaan kolaboratif yang diedarkan. Sejumlah pertanyaan tetap tidak terjawab, bagaimanapun, mengenai sifat sistem penandaan kolaboratif termasuk apakah skema pengkategorian yang koheren dapat muncul dari penandaan yang tidak diawasi oleh pengguna. Makalah ini menggunakan data dari delicio laman penanda buku sosial. kami untuk mengkaji dinamika sistem penandaan kolaboratif. Secara khusus, kami mengkaji apakah penyebaran kekerapan penggunaan tag untuk laman web \"popular\" dengan sejarah yang panjang (banyak tag dan banyak pengguna) dapat dijelaskan oleh pengedaran undang-undang kuasa, yang sering menjadi ciri sistem yang dianggap kompleks. Kami menghasilkan model penandaan kolaboratif generatif untuk memahami dinamika asas di sebalik penandaan, termasuk bagaimana penyebaran tag kuasa boleh timbul. Kami secara empirik mengkaji sejarah penandaan laman web untuk menentukan bagaimana pengedaran ini timbul dari masa ke masa dan untuk menentukan corak sebelum pengedaran stabil. Terakhir, dengan memfokuskan pada tag frekuensi tinggi di laman web di mana pengedaran tag adalah undang-undang kuasa yang stabil, kami menunjukkan bagaimana rangkaian co-kejadian tag untuk domain sampel tag dapat digunakan untuk menganalisis makna tag tertentu yang diberikan hubungan dengan tag lain. [[EENNDD]] semantik baru muncul; undang-undang kuasa; penapisan kolaboratif; formalisme dan kaedah perwakilan pengetahuan; sedap; penandaan; sistem yang kompleks"], [{"string": "Characterizing web-based video sharing workloads An abstract is not available.", "keywords": ["general", "video sharing", "workload characterization", "ugc"], "combined": "Characterizing web-based video sharing workloads An abstract is not available. [[EENNDD]] general; video sharing; workload characterization; ugc"}, "Mencirikan beban kerja perkongsian video berasaskan web Abstrak tidak tersedia. [[EENNDD]] umum; perkongsian video; pencirian beban kerja; ugc"], [{"string": "Mobile search pattern evolution: the trend and the impact of voice queries In this paper we study the characteristics of search queries submitted from mobile devices using Yahoo! Search for Mobile during a 2 months period in early of 2010, and compare the results with a similar study conducted in late 2007. The major findings include 1) mobile search queries have become much more diverse, and 2) user interest and information needs have been substantially changed at least in some areas of search topics, including adult and local intent queries. In addition we investigate the impact of voice query search interface offered by Yahoo!'s mobile search service. We examine how unstructured spoken queries differ from conventional search queries.", "keywords": ["voice queries", "information search and retrieval", "mobile queries", "query categorization", "mobile search", "query log analysis", "mobile search query analysis"], "combined": "Mobile search pattern evolution: the trend and the impact of voice queries In this paper we study the characteristics of search queries submitted from mobile devices using Yahoo! Search for Mobile during a 2 months period in early of 2010, and compare the results with a similar study conducted in late 2007. The major findings include 1) mobile search queries have become much more diverse, and 2) user interest and information needs have been substantially changed at least in some areas of search topics, including adult and local intent queries. In addition we investigate the impact of voice query search interface offered by Yahoo!'s mobile search service. We examine how unstructured spoken queries differ from conventional search queries. [[EENNDD]] voice queries; information search and retrieval; mobile queries; query categorization; mobile search; query log analysis; mobile search query analysis"}, "Evolusi corak carian mudah alih: trend dan kesan pertanyaan suara Dalam makalah ini kami mengkaji ciri-ciri pertanyaan carian yang dihantar dari peranti mudah alih menggunakan Yahoo! Cari Mudah Alih selama 2 bulan pada awal tahun 2010, dan bandingkan hasilnya dengan kajian serupa yang dilakukan pada akhir tahun 2007. Penemuan utama merangkumi 1) pertanyaan carian mudah alih menjadi lebih pelbagai, dan 2) minat pengguna dan keperluan maklumat mempunyai telah banyak berubah sekurang-kurangnya di beberapa bidang topik carian, termasuk pertanyaan niat dewasa dan tempatan. Sebagai tambahan, kami menyiasat kesan antara muka carian pertanyaan suara yang ditawarkan oleh perkhidmatan carian mudah alih Yahoo !. Kami mengkaji bagaimana pertanyaan lisan yang tidak berstruktur berbeza dengan pertanyaan carian konvensional. [[EENNDD]] pertanyaan suara; pencarian dan pengambilan maklumat; pertanyaan mudah alih; pengkategorian pertanyaan; carian mudah alih; analisis log pertanyaan; analisis pertanyaan carian mudah alih"], [{"string": "Towards semantic knowledge propagation from text corpus to web images In this paper, we study the problem of transfer learning from text to images in the context of network data in which link based bridges are available to transfer the knowledge between the different domains. The problem of classification of image data is often much more challenging than text data because of the following two reasons: (a) Labeled text data is very widely available for classification purposes. On the other hand, this is often not the case for image data, in which a lot of images are available from many sources, but many of them are often not labeled. (b) The image features are not directly related to semantic concepts inherent in class labels. On the other hand, since text data tends to have natural semantic interpretability (because of their human origins), they are often more directly related to class labels. Therefore, the relationships between the images and text features also provide additional hints for the classification process in terms of the image feature transformations which provide the most effective results.", "keywords": ["text corpus and web images", "heterogeneous knowledge propagation", "translator function", "cross-domain label propagation"], "combined": "Towards semantic knowledge propagation from text corpus to web images In this paper, we study the problem of transfer learning from text to images in the context of network data in which link based bridges are available to transfer the knowledge between the different domains. The problem of classification of image data is often much more challenging than text data because of the following two reasons: (a) Labeled text data is very widely available for classification purposes. On the other hand, this is often not the case for image data, in which a lot of images are available from many sources, but many of them are often not labeled. (b) The image features are not directly related to semantic concepts inherent in class labels. On the other hand, since text data tends to have natural semantic interpretability (because of their human origins), they are often more directly related to class labels. Therefore, the relationships between the images and text features also provide additional hints for the classification process in terms of the image feature transformations which provide the most effective results. [[EENNDD]] text corpus and web images; heterogeneous knowledge propagation; translator function; cross-domain label propagation"}, "Menuju penyebaran pengetahuan semantik dari korpus teks ke imej web Dalam makalah ini, kami mengkaji masalah pemindahan pembelajaran dari teks ke gambar dalam konteks data rangkaian di mana jambatan berdasarkan pautan tersedia untuk memindahkan pengetahuan antara domain yang berbeza. Masalah pengkelasan data gambar selalunya jauh lebih mencabar daripada data teks kerana dua sebab berikut: (a) Data teks berlabel sangat banyak tersedia untuk tujuan klasifikasi. Sebaliknya, ini sering kali tidak berlaku untuk data gambar, di mana banyak gambar tersedia dari banyak sumber, tetapi banyak daripadanya sering tidak dilabel. (b) Ciri gambar tidak berkaitan langsung dengan konsep semantik yang terdapat pada label kelas. Sebaliknya, kerana data teks cenderung mempunyai penafsiran semantik semula jadi (kerana asal usul manusia), mereka sering kali lebih berkaitan langsung dengan label kelas. Oleh itu, hubungan antara gambar dan ciri teks juga memberikan petunjuk tambahan untuk proses klasifikasi dari segi transformasi ciri gambar yang memberikan hasil yang paling berkesan. [[EENNDD]] corpus teks dan gambar web; penyebaran pengetahuan heterogen; fungsi penterjemah; penyebaran label merentas domain"], [{"string": "Inferring query intent from reformulations and clicks Many researchers have noted that web search queries are often ambiguous or unclear. We present an approach for identifying the popular meanings of queries using web search logs and user click behavior. We show our approach to produce more complete and user-centric intents than expert judges by evaluating on TREC queries. This approach was also used by the TREC 2009 Web Track judges to obtain more representative topic descriptions from real queries.", "keywords": ["subtopics", "information search and retrieval", "diversity", "intents"], "combined": "Inferring query intent from reformulations and clicks Many researchers have noted that web search queries are often ambiguous or unclear. We present an approach for identifying the popular meanings of queries using web search logs and user click behavior. We show our approach to produce more complete and user-centric intents than expert judges by evaluating on TREC queries. This approach was also used by the TREC 2009 Web Track judges to obtain more representative topic descriptions from real queries. [[EENNDD]] subtopics; information search and retrieval; diversity; intents"}, "Menyimpulkan maksud pertanyaan dari penyusunan semula dan klik Banyak penyelidik menyatakan bahawa pertanyaan carian web sering samar-samar atau tidak jelas. Kami menyajikan pendekatan untuk mengenal pasti makna pertanyaan yang popular menggunakan log carian web dan tingkah laku klik pengguna. Kami menunjukkan pendekatan kami untuk menghasilkan niat yang lebih lengkap dan berpusatkan pengguna daripada hakim pakar dengan menilai pertanyaan TREC. Pendekatan ini juga digunakan oleh hakim Jejak Web TREC 2009 untuk mendapatkan deskripsi topik yang lebih representatif dari pertanyaan sebenar. [[EENNDD]] subtopik; pencarian dan pengambilan maklumat; kepelbagaian; niat"], [{"string": "Learning to map between ontologies on the semantic web No contact information provided yet.", "keywords": ["learning", "relaxation labeling", "heterogeneous databases", "semantic web", "ontology mapping", "machine learning"], "combined": "Learning to map between ontologies on the semantic web No contact information provided yet. [[EENNDD]] learning; relaxation labeling; heterogeneous databases; semantic web; ontology mapping; machine learning"}, "Belajar memetakan antara ontologi di web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pembelajaran; pelabelan kelonggaran; pangkalan data heterogen; web semantik; pemetaan ontologi; pembelajaran mesin"], [{"string": "SLL: running my web services on your WS platforms No contact information provided yet.", "keywords": ["decoupling", "service language layer", "transformation", "xml", "web services", "xml-based service language"], "combined": "SLL: running my web services on your WS platforms No contact information provided yet. [[EENNDD]] decoupling; service language layer; transformation; xml; web services; xml-based service language"}, "SLL: menjalankan perkhidmatan web saya di platform WS anda Belum ada maklumat hubungan yang diberikan. [[EENNDD]] memisahkan; lapisan bahasa perkhidmatan; transformasi; xml; perkhidmatan web; bahasa perkhidmatan berasaskan xml"], [{"string": "The million song dataset challenge We introduce the Million Song Dataset Challenge: a large-scale, personalized music recommendation challenge, where the goal is to predict the songs that a user will listen to, given both the user's listening history and full information (including meta-data and content analysis) for all songs. We explain the taste profile data, our goals and design choices in creating the challenge, and present baseline results using simple, off-the-shelf recommendation algorithms.", "keywords": ["music information retrieval", "miscellaneous", "recommender systems"], "combined": "The million song dataset challenge We introduce the Million Song Dataset Challenge: a large-scale, personalized music recommendation challenge, where the goal is to predict the songs that a user will listen to, given both the user's listening history and full information (including meta-data and content analysis) for all songs. We explain the taste profile data, our goals and design choices in creating the challenge, and present baseline results using simple, off-the-shelf recommendation algorithms. [[EENNDD]] music information retrieval; miscellaneous; recommender systems"}, "Tantangan set data lagu Kami memperkenalkan Million Song Dataset Challenge: cabaran cadangan muzik berskala besar dan berskala besar, di mana tujuannya adalah untuk meramalkan lagu yang akan didengarkan oleh pengguna, memandangkan sejarah pendengaran pengguna dan maklumat lengkap (termasuk meta -data dan analisis kandungan) untuk semua lagu. Kami menerangkan data profil rasa, tujuan dan pilihan reka bentuk kami dalam mewujudkan tantangan, dan menyajikan hasil awal menggunakan algoritma cadangan sederhana. [[EENNDD]] pengambilan maklumat muzik; pelbagai; sistem cadangan"], [{"string": "Dissemination of heterogeneous xml data A lot of recent research has focused on the content-based dissemination of XML data. However, due to the heterogeneous data schemas used by different data publishers even for data in the same domain, an important challenge is how to efficiently and effectively disseminate relevant data to subscribers whose subscriptions might be specified based on schemas that are different from those used by the data publishers. This paper examines the options to resolve this schema heterogeneity problem in XML data dissemination, and proposes a novel paradigm that is based on data rewriting. Our experimental results demonstrate the effectiveness of the data rewriting paradigm and identifies the tradeoffs of the various approaches", "keywords": ["heterogeneous", "dissemination", "miscellaneous", "xml", "data rewriting"], "combined": "Dissemination of heterogeneous xml data A lot of recent research has focused on the content-based dissemination of XML data. However, due to the heterogeneous data schemas used by different data publishers even for data in the same domain, an important challenge is how to efficiently and effectively disseminate relevant data to subscribers whose subscriptions might be specified based on schemas that are different from those used by the data publishers. This paper examines the options to resolve this schema heterogeneity problem in XML data dissemination, and proposes a novel paradigm that is based on data rewriting. Our experimental results demonstrate the effectiveness of the data rewriting paradigm and identifies the tradeoffs of the various approaches [[EENNDD]] heterogeneous; dissemination; miscellaneous; xml; data rewriting"}, "Penyebaran data xml heterogen Banyak penyelidikan baru-baru ini telah menumpukan pada penyebaran data XML berdasarkan kandungan. Walau bagaimanapun, kerana skema data yang heterogen yang digunakan oleh penerbit data yang berlainan walaupun untuk data dalam domain yang sama, satu cabaran penting adalah bagaimana menyebarkan data yang relevan dengan cekap dan berkesan kepada pelanggan yang langganannya mungkin ditentukan berdasarkan skema yang berbeda dari yang digunakan oleh penerbit data. Makalah ini mengkaji pilihan untuk menyelesaikan masalah heterogenitas skema ini dalam penyebaran data XML, dan mengusulkan paradigma novel yang berdasarkan penulisan semula data. Hasil eksperimen kami menunjukkan keberkesanan paradigma penulisan semula data dan mengenal pasti pertukaran pelbagai pendekatan [[EENNDD]] heterogen; penyebaran; pelbagai; xml; penulisan semula data"], [{"string": "Finding influential mediators in social networks Given a social network, who are the key players controlling the bottlenecks of influence propagation if some persons would like to activate specific individuals? In this paper, we tackle the problem of selecting a set of k mediator nodes as the influential gateways whose existence determines the activation probabilities of targeted nodes from some given seed nodes. We formally define the k-Mediators problem. To have an effective and efficient solution, we propose a three-step greedy method by considering the probabilistic influence and the structural connectivity on the pathways from sources to targets. To the best of our knowledge, this is the first work to consider the k-Mediators problem in networks. Experiments on the DBLP co-authorship graph show the effectiveness and efficiency of the proposed method.", "keywords": ["social networks", "viral marketing", "influential mediator"], "combined": "Finding influential mediators in social networks Given a social network, who are the key players controlling the bottlenecks of influence propagation if some persons would like to activate specific individuals? In this paper, we tackle the problem of selecting a set of k mediator nodes as the influential gateways whose existence determines the activation probabilities of targeted nodes from some given seed nodes. We formally define the k-Mediators problem. To have an effective and efficient solution, we propose a three-step greedy method by considering the probabilistic influence and the structural connectivity on the pathways from sources to targets. To the best of our knowledge, this is the first work to consider the k-Mediators problem in networks. Experiments on the DBLP co-authorship graph show the effectiveness and efficiency of the proposed method. [[EENNDD]] social networks; viral marketing; influential mediator"}, "Mencari orang tengah yang berpengaruh dalam rangkaian sosial Memandangkan rangkaian sosial, siapakah pemain utama yang mengawal hambatan penyebaran pengaruh jika ada orang yang ingin mengaktifkan individu tertentu? Dalam makalah ini, kami mengatasi masalah memilih sekumpulan n mediator k sebagai gerbang berpengaruh yang keberadaannya menentukan kebarangkalian pengaktifan node yang disasarkan dari beberapa simpul benih yang diberikan. Kami secara formal menentukan masalah k-Mediators. Untuk mempunyai penyelesaian yang berkesan dan cekap, kami mencadangkan kaedah tamak tiga langkah dengan mempertimbangkan pengaruh probabilistik dan hubungan struktur pada laluan dari sumber ke sasaran. Sepengetahuan kami, ini adalah kerja pertama yang mempertimbangkan masalah k-Mediators dalam rangkaian. Eksperimen pada grafik kepengarangan DBLP menunjukkan keberkesanan dan kecekapan kaedah yang dicadangkan. [[EENNDD]] rangkaian sosial; pemasaran viral; pengantara berpengaruh"], [{"string": "RankCompete: simultaneous ranking and clustering of web photos With the explosive growth of digital cameras and online media, it has become crucial to design efficient methods that help users browse and search large image collections. The recent VisualRank algorithm [4] employs visual similarity to represent the link structure in a graph so that the classic PageRank algorithm can be applied to select the most relevant images. However, measuring visual similarity is difficult when there exist diversified semantics in the image collection, and the results from VisualRank cannot supply good visual summarization with diversity. This paper proposes to rank the images in a structural fashion, which aims to discover the diverse structure embedded in photo collections, and rank the images according to their similarity among local neighborhoods instead of across the entire photo collection. We design a novel algorithm named RankCompete, which generalizes the PageRank algorithm for the task of simultaneous ranking and clustering. The experimental results show that RankCompete outperforms VisualRank and provides an efficient but effective tool for organizing web photos.", "keywords": ["pagerank", "image ranking", "information search and retrieval", "image summarization"], "combined": "RankCompete: simultaneous ranking and clustering of web photos With the explosive growth of digital cameras and online media, it has become crucial to design efficient methods that help users browse and search large image collections. The recent VisualRank algorithm [4] employs visual similarity to represent the link structure in a graph so that the classic PageRank algorithm can be applied to select the most relevant images. However, measuring visual similarity is difficult when there exist diversified semantics in the image collection, and the results from VisualRank cannot supply good visual summarization with diversity. This paper proposes to rank the images in a structural fashion, which aims to discover the diverse structure embedded in photo collections, and rank the images according to their similarity among local neighborhoods instead of across the entire photo collection. We design a novel algorithm named RankCompete, which generalizes the PageRank algorithm for the task of simultaneous ranking and clustering. The experimental results show that RankCompete outperforms VisualRank and provides an efficient but effective tool for organizing web photos. [[EENNDD]] pagerank; image ranking; information search and retrieval; image summarization"}, "RankCompete: peringkat dan pengelompokan foto web secara serentak Dengan pertumbuhan kamera digital dan media dalam talian yang meletup, menjadi penting untuk merancang kaedah yang cekap yang membantu pengguna melayari dan mencari koleksi gambar yang besar. Algoritma VisualRank baru-baru ini [4] menggunakan kesamaan visual untuk mewakili struktur pautan dalam grafik sehingga algoritma PageRank klasik dapat diterapkan untuk memilih gambar yang paling relevan. Namun, mengukur kesamaan visual sukar apabila terdapat semantik yang pelbagai dalam koleksi gambar, dan hasil dari VisualRank tidak dapat memberikan ringkasan visual yang baik dengan kepelbagaian. Makalah ini mencadangkan untuk menilai gambar secara struktural, yang bertujuan untuk mengetahui struktur yang pelbagai yang terdapat dalam koleksi foto, dan memberi peringkat gambar mengikut kesamaannya di antara kawasan setempat dan bukannya di seluruh koleksi foto. Kami merancang algoritma novel bernama RankCompete, yang menyamaratakan algoritma PageRank untuk tugas peringkat dan pengelompokan serentak. Hasil eksperimen menunjukkan bahawa RankCompete mengungguli VisualRank dan menyediakan alat yang cekap tetapi berkesan untuk mengatur foto web. [[EENNDD]] pagerank; kedudukan gambar; carian dan pengambilan maklumat; ringkasan gambar"], [{"string": "Improving recommendation for long-tail queries via templates The ability to aggregate huge volumes of queries over a large population of users allows search engines to build precise models for a variety of query-assistance features such as query recommendation, correction, etc. Yet, no matter how much data is aggregated, the long-tail distribution implies that a large fraction of queries are rare. As a result, most query assistance services perform poorly or are not even triggered on long-tail queries. We propose a method to extend the reach of query assistance techniques (and in particular query recommendation) to long-tail queries by reasoning about rules between query templates rather than individual query transitions, as currently done in query-flow graph models. As a simple example, if we recognize that 'Montezuma' is a city in the rare query \"Montezuma surf\" and if the rule 'city surf \u2192 beach has been observed, we are able to offer \"Montezuma beach\" as a recommendation, even if the two queries were never observed in a same session. We conducted experiments to validate our hypothesis, first via traditional small-scale editorial assessments but more interestingly via a novel automated large scale evaluation methodology. Our experiments show that general coverage can be relatively increased by 24% using templates without penalizing quality. Furthermore, for 36% of the 95M queries in our query flow graph, which have no out edges and thus could not be served recommendations, we can now offer at least one recommendation in 98% of the cases.", "keywords": ["query templates", "query recommendation", "query mining"], "combined": "Improving recommendation for long-tail queries via templates The ability to aggregate huge volumes of queries over a large population of users allows search engines to build precise models for a variety of query-assistance features such as query recommendation, correction, etc. Yet, no matter how much data is aggregated, the long-tail distribution implies that a large fraction of queries are rare. As a result, most query assistance services perform poorly or are not even triggered on long-tail queries. We propose a method to extend the reach of query assistance techniques (and in particular query recommendation) to long-tail queries by reasoning about rules between query templates rather than individual query transitions, as currently done in query-flow graph models. As a simple example, if we recognize that 'Montezuma' is a city in the rare query \"Montezuma surf\" and if the rule 'city surf \u2192 beach has been observed, we are able to offer \"Montezuma beach\" as a recommendation, even if the two queries were never observed in a same session. We conducted experiments to validate our hypothesis, first via traditional small-scale editorial assessments but more interestingly via a novel automated large scale evaluation methodology. Our experiments show that general coverage can be relatively increased by 24% using templates without penalizing quality. Furthermore, for 36% of the 95M queries in our query flow graph, which have no out edges and thus could not be served recommendations, we can now offer at least one recommendation in 98% of the cases. [[EENNDD]] query templates; query recommendation; query mining"}, "Meningkatkan cadangan untuk pertanyaan ekor panjang melalui templat Kemampuan untuk mengumpulkan jumlah pertanyaan yang banyak daripada populasi pengguna yang besar membolehkan enjin carian membuat model yang tepat untuk pelbagai ciri bantuan pertanyaan seperti cadangan pertanyaan, pembetulan, dll. Namun, tidak tidak kira seberapa banyak data digabungkan, pengedaran ekor panjang menunjukkan bahawa sebilangan besar pertanyaan jarang berlaku. Akibatnya, kebanyakan perkhidmatan bantuan pertanyaan berkinerja buruk atau bahkan tidak dicetuskan pada pertanyaan ekor panjang. Kami mencadangkan kaedah untuk memperluas jangkauan teknik bantuan pertanyaan (dan khususnya saranan permintaan) ke pertanyaan panjang-panjang dengan memberi alasan tentang peraturan antara templat pertanyaan dan bukannya peralihan pertanyaan individu, seperti yang saat ini dilakukan dalam model grafik aliran-pertanyaan. Sebagai contoh mudah, jika kita menyedari bahawa 'Montezuma' adalah sebuah bandar dalam pertanyaan \"Montezuma surfing\" yang jarang berlaku dan jika peraturan 'surfing kota \u2192 pantai telah diperhatikan, kita dapat menawarkan \"pantai Montezuma\" sebagai cadangan, bahkan sekiranya kedua-dua pertanyaan tidak pernah diperhatikan dalam sesi yang sama. Kami menjalankan eksperimen untuk mengesahkan hipotesis kami, pertama melalui penilaian editorial skala kecil tradisional tetapi lebih menarik lagi melalui metodologi penilaian skala besar automatik. Eksperimen kami menunjukkan bahawa liputan umum dapat meningkat secara relatif sebanyak 24% menggunakan templat tanpa menghukum kualiti. Tambahan lagi, untuk 36% daripada 95 juta pertanyaan dalam grafik aliran pertanyaan kami, yang tidak mempunyai kelebihan dan oleh itu tidak dapat disarankan, kami kini boleh menawarkan sekurang-kurangnya satu cadangan dalam 98% kes. [[EENNDD]] templat pertanyaan; cadangan pertanyaan; perlombongan pertanyaan"], [{"string": "Pay as you browse: microcomputations as micropayments in web-based services Currently, several online businesses deem that advertising revenues alone are not sufficient to generate profits and are therefore set to charge for online content. In this paper, we explore a complement to the current advertisement model; more specifically, we propose a micropayment model for non-specialized commodity web-services based on microcomputations. In our model, a user that wishes to access online content offered by a website does not need to register or pay to access the website; instead, he will accept to run microcomputations on behalf of the website in exchange for access to the content. These microcomputations can, for example, support ongoing computing projects that have clear social benefits (e.g., projects relating to HIV, dengue, cancer, etc.) or can contribute towards commercial computing projects. We argue that this micropayment model is economically and technically viable and that it can be integrated in existing distributed computing frameworks (e.g., the BOINC platform). We implement a preliminary prototype of a system based on our model through which we evaluate its performance and usability. Finally, we analyze the security and privacy of our proposal and we show that it ensures payment for the content while preserving the privacy of users.", "keywords": ["microcomputations", "micropayments", "monetization", "distributed computing"], "combined": "Pay as you browse: microcomputations as micropayments in web-based services Currently, several online businesses deem that advertising revenues alone are not sufficient to generate profits and are therefore set to charge for online content. In this paper, we explore a complement to the current advertisement model; more specifically, we propose a micropayment model for non-specialized commodity web-services based on microcomputations. In our model, a user that wishes to access online content offered by a website does not need to register or pay to access the website; instead, he will accept to run microcomputations on behalf of the website in exchange for access to the content. These microcomputations can, for example, support ongoing computing projects that have clear social benefits (e.g., projects relating to HIV, dengue, cancer, etc.) or can contribute towards commercial computing projects. We argue that this micropayment model is economically and technically viable and that it can be integrated in existing distributed computing frameworks (e.g., the BOINC platform). We implement a preliminary prototype of a system based on our model through which we evaluate its performance and usability. Finally, we analyze the security and privacy of our proposal and we show that it ensures payment for the content while preserving the privacy of users. [[EENNDD]] microcomputations; micropayments; monetization; distributed computing"}, "Bayar semasa anda melayari: pengiraan mikro sebagai pembayaran mikro dalam perkhidmatan berasaskan web Pada masa ini, beberapa perniagaan dalam talian menganggap bahawa pendapatan iklan sahaja tidak mencukupi untuk menjana keuntungan dan oleh itu ditetapkan untuk mengenakan bayaran untuk kandungan dalam talian. Dalam makalah ini, kami meneroka pelengkap model iklan semasa; lebih khusus lagi, kami mencadangkan model pembayaran mikro untuk perkhidmatan web komoditi bukan khusus berdasarkan pengiraan mikro. Dalam model kami, pengguna yang ingin mengakses kandungan dalam talian yang ditawarkan oleh laman web tidak perlu mendaftar atau membayar untuk mengakses laman web; sebaliknya, dia akan menerima untuk menjalankan pengiraan mikro bagi pihak laman web sebagai pertukaran untuk mengakses kandungan. Komputer mikro ini, misalnya, dapat menyokong projek pengkomputeran yang sedang berjalan yang mempunyai faedah sosial yang jelas (mis., Projek yang berkaitan dengan HIV, denggi, barah, dll.) Atau dapat menyumbang terhadap projek pengkomputeran komersial. Kami berpendapat bahawa model pembayaran mikro ini dapat dilaksanakan secara ekonomi dan teknikal dan dapat disatukan dalam kerangka pengkomputeran terdistribusi yang ada (mis., Platform BOINC). Kami melaksanakan prototaip awal sistem berdasarkan model kami di mana kami menilai prestasi dan kebolehgunaannya. Akhirnya, kami menganalisis keselamatan dan privasi cadangan kami dan kami menunjukkan bahawa ia memastikan pembayaran untuk kandungan sambil menjaga privasi pengguna. [[EENNDD]] pengiraan mikro; pembayaran mikro; pengewangan; pengkomputeran diedarkan"], [{"string": "Visualizing differences in web search algorithms using the expected weighted hoeffding distance We introduce a new dissimilarity function for ranked lists, the expected weighted Hoeffding distance, that has several advantages over current dissimilarity measures for ranked search results. First, it is easily customized for users who pay varying degrees of attention to websites at different ranks. Second, unlike existing measures such as generalized Kendall's tau, it is based on a true metric, preserving meaningful embeddings when visualization techniques like multi-dimensional scaling are applied. Third, our measure can effectively handle partial or missing rank information while retaining a probabilistic interpretation. Finally, the measure can be made computationally tractable and we give a highly efficient algorithm for computing it. We then apply our new metric with multi-dimensional scaling to visualize and explore relationships between the result sets from different search engines, showing how the weighted Hoeffding distance can distinguish important differences in search engine behavior that are not apparent with other rank-distance metrics. Such visualizations are highly effective at summarizing and analyzing insights on which search engines to use, what search strategies users can employ, and how search results evolve over time. We demonstrate our techniques using a collection of popular search engines, a representative set of queries, and frequently used query manipulation methods.", "keywords": ["ranking", "expected weighted hoeffding distance", "search algorithm dissimilarity"], "combined": "Visualizing differences in web search algorithms using the expected weighted hoeffding distance We introduce a new dissimilarity function for ranked lists, the expected weighted Hoeffding distance, that has several advantages over current dissimilarity measures for ranked search results. First, it is easily customized for users who pay varying degrees of attention to websites at different ranks. Second, unlike existing measures such as generalized Kendall's tau, it is based on a true metric, preserving meaningful embeddings when visualization techniques like multi-dimensional scaling are applied. Third, our measure can effectively handle partial or missing rank information while retaining a probabilistic interpretation. Finally, the measure can be made computationally tractable and we give a highly efficient algorithm for computing it. We then apply our new metric with multi-dimensional scaling to visualize and explore relationships between the result sets from different search engines, showing how the weighted Hoeffding distance can distinguish important differences in search engine behavior that are not apparent with other rank-distance metrics. Such visualizations are highly effective at summarizing and analyzing insights on which search engines to use, what search strategies users can employ, and how search results evolve over time. We demonstrate our techniques using a collection of popular search engines, a representative set of queries, and frequently used query manipulation methods. [[EENNDD]] ranking; expected weighted hoeffding distance; search algorithm dissimilarity"}, "Memvisualisasikan perbezaan dalam algoritma carian web menggunakan jarak hoeffding berwajaran yang diharapkan Kami memperkenalkan fungsi ketidaksamaan baru untuk senarai peringkat, jarak Hoeffding berwajaran yang diharapkan, yang mempunyai beberapa kelebihan berbanding ukuran ketidaksamaan semasa untuk hasil carian peringkat. Pertama, mudah disesuaikan untuk pengguna yang memberikan perhatian yang berbeza-beza ke laman web di peringkat yang berbeza. Kedua, tidak seperti langkah-langkah yang ada seperti tau Kendall yang digeneralisasikan, ini berdasarkan metrik yang benar, mengekalkan penyisipan yang bermakna ketika teknik visualisasi seperti penskalaan pelbagai dimensi diterapkan. Ketiga, ukuran kami dapat menangani maklumat peringkat separa atau hilang dengan berkesan sambil mengekalkan tafsiran probabilistik. Akhirnya, ukuran dapat dibuat secara komputasi dan kami memberikan algoritma yang sangat cekap untuk mengira. Kami kemudian menerapkan metrik baru kami dengan penskalaan pelbagai dimensi untuk memvisualisasikan dan meneroka hubungan antara kumpulan hasil dari mesin carian yang berbeza, menunjukkan bagaimana jarak Hoeffding berwajaran dapat membezakan perbezaan penting dalam tingkah laku mesin pencari yang tidak jelas dengan metrik jarak pangkat yang lain. Visualisasi semacam itu sangat berkesan untuk merangkum dan menganalisis wawasan tentang mesin pencari yang akan digunakan, strategi carian apa yang dapat digunakan oleh pengguna, dan bagaimana hasil carian berkembang dari masa ke masa. Kami menunjukkan teknik kami menggunakan koleksi mesin carian yang popular, satu set pertanyaan yang mewakili, dan kaedah manipulasi pertanyaan yang sering digunakan. [[EENNDD]] kedudukan; jarak kenaikan berat badan yang dijangkakan; carian ketidaksamaan algoritma"], [{"string": "SweetDeal: representing agent contracts with exceptions using XML rules, ontologies, and process descriptions No contact information provided yet.", "keywords": ["knowledge representation", "intelligent software agents", "xml", "owl", "ontologies", "electronic contracts", "rules", "process knowledge", "representations", "web services", "semantic web", "business process automation", "logic programs", "declarative", "semantic web services", "knowledge-based", "description logic", "process descriptions", "daml+oil", "rdf", "electronic commerce"], "combined": "SweetDeal: representing agent contracts with exceptions using XML rules, ontologies, and process descriptions No contact information provided yet. [[EENNDD]] knowledge representation; intelligent software agents; xml; owl; ontologies; electronic contracts; rules; process knowledge; representations; web services; semantic web; business process automation; logic programs; declarative; semantic web services; knowledge-based; description logic; process descriptions; daml+oil; rdf; electronic commerce"}, "SweetDeal: mewakili kontrak ejen dengan pengecualian menggunakan peraturan XML, ontologi, dan keterangan proses Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perwakilan pengetahuan; ejen perisian pintar; xml; burung hantu; ontologi; kontrak elektronik; peraturan; memproses pengetahuan; perwakilan; perkhidmatan web; web semantik; automasi proses perniagaan; program logik; deklaratif; perkhidmatan web semantik; berasaskan pengetahuan; logik keterangan; penerangan proses; daml + minyak; rdf; perdagangan elektronik"], [{"string": "Inverted index compression via online document routing Modern search engines are expected to make documents searchable shortly after they appear on the ever changing Web. To satisfy this requirement, the Web is frequently crawled. Due to the sheer size of their indexes, search engines distribute the crawled documents among thousands of servers in a scheme called local index-partitioning, such that each server indexes only several million pages. To ensure documents from the same host (e.g., www.nytimes.com) are distributed uniformly over the servers, for load balancing purposes, random routing of documents to servers is common. To expedite the time documents become searchable after being crawled, documents may be simply appended to the existing index partitions. However, indexing by merely appending documents, results in larger index sizes since document reordering for index compactness is no longer performed. This, in turn, degrades search query processing performance which depends heavily on index sizes.", "keywords": ["document routing", "index partitioning", "online algorithm", "inverted index", "miscellaneous", "index compression"], "combined": "Inverted index compression via online document routing Modern search engines are expected to make documents searchable shortly after they appear on the ever changing Web. To satisfy this requirement, the Web is frequently crawled. Due to the sheer size of their indexes, search engines distribute the crawled documents among thousands of servers in a scheme called local index-partitioning, such that each server indexes only several million pages. To ensure documents from the same host (e.g., www.nytimes.com) are distributed uniformly over the servers, for load balancing purposes, random routing of documents to servers is common. To expedite the time documents become searchable after being crawled, documents may be simply appended to the existing index partitions. However, indexing by merely appending documents, results in larger index sizes since document reordering for index compactness is no longer performed. This, in turn, degrades search query processing performance which depends heavily on index sizes. [[EENNDD]] document routing; index partitioning; online algorithm; inverted index; miscellaneous; index compression"}, "Pemampatan indeks terbalik melalui perutean dokumen dalam talian Mesin pencari moden diharapkan dapat membuat dokumen dicari tidak lama setelah muncul di Web yang selalu berubah. Untuk memenuhi syarat ini, Web sering di-crawl. Kerana ukuran indeksnya yang cukup besar, mesin pencari menyebarkan dokumen yang di-crawl di antara ribuan pelayan dalam skema yang disebut partisi indeks tempatan, sehingga setiap pelayan mengindeks hanya beberapa juta halaman. Untuk memastikan dokumen dari hos yang sama (mis., Www.nytimes.com) diedarkan secara seragam ke atas pelayan, untuk tujuan pengimbangan beban, perutean dokumen secara rawak ke pelayan adalah perkara biasa. Untuk mempercepat waktu dokumen menjadi dicari setelah dirayapi, dokumen mungkin hanya ditambahkan ke partisi indeks yang ada. Namun, pengindeksan dengan hanya menambahkan dokumen, menghasilkan ukuran indeks yang lebih besar kerana penyusunan semula dokumen untuk kekompakan indeks tidak lagi dilakukan. Ini seterusnya menurunkan prestasi pemprosesan pertanyaan carian yang sangat bergantung pada ukuran indeks. [[EENNDD]] penghalaan dokumen; pembahagian indeks; algoritma dalam talian; indeks terbalik; pelbagai; pemampatan indeks"], [{"string": "Community detection in incomplete information networks With the recent advances in information networks, the problem of community detection has attracted much attention in the last decade. While network community detection has been ubiquitous, the task of collecting complete network data remains challenging in many real-world applications. Usually the collected network is incomplete with most of the edges missing. Commonly, in such networks, all nodes with attributes are available while only the edges within a few local regions of the network can be observed. In this paper, we study the problem of detecting communities in incomplete information networks with missing edges. We first learn a distance metric to reproduce the link-based distance between nodes from the observed edges in the local information regions. We then use the learned distance metric to estimate the distance between any pair of nodes in the network. A hierarchical clustering approach is proposed to detect communities within the incomplete information networks. Empirical studies on real-world information networks demonstrate that our proposed method can effectively detect community structures within incomplete information networks.", "keywords": ["distance metric learning", "incomplete information networks", "community detection"], "combined": "Community detection in incomplete information networks With the recent advances in information networks, the problem of community detection has attracted much attention in the last decade. While network community detection has been ubiquitous, the task of collecting complete network data remains challenging in many real-world applications. Usually the collected network is incomplete with most of the edges missing. Commonly, in such networks, all nodes with attributes are available while only the edges within a few local regions of the network can be observed. In this paper, we study the problem of detecting communities in incomplete information networks with missing edges. We first learn a distance metric to reproduce the link-based distance between nodes from the observed edges in the local information regions. We then use the learned distance metric to estimate the distance between any pair of nodes in the network. A hierarchical clustering approach is proposed to detect communities within the incomplete information networks. Empirical studies on real-world information networks demonstrate that our proposed method can effectively detect community structures within incomplete information networks. [[EENNDD]] distance metric learning; incomplete information networks; community detection"}, "Pengesanan komuniti dalam rangkaian maklumat yang tidak lengkap Dengan kemajuan dalam rangkaian maklumat baru-baru ini, masalah pengesanan masyarakat telah menarik banyak perhatian dalam dekad terakhir. Walaupun pengesanan komuniti rangkaian ada di mana-mana, tugas mengumpulkan data rangkaian lengkap tetap mencabar di banyak aplikasi dunia nyata. Biasanya rangkaian yang dikumpulkan tidak lengkap dengan sebahagian besar pinggirnya hilang. Biasanya, dalam rangkaian seperti itu, semua node dengan atribut tersedia sementara hanya bahagian tepi dalam beberapa kawasan rangkaian yang dapat diperhatikan. Dalam makalah ini, kami mengkaji masalah mengesan komuniti dalam rangkaian maklumat yang tidak lengkap dengan kekurangan. Kami mula-mula belajar metrik jarak untuk menghasilkan semula jarak berdasarkan pautan antara nod dari tepi yang diperhatikan di kawasan maklumat tempatan. Kami kemudian menggunakan metrik jarak yang dipelajari untuk menganggarkan jarak antara sepasang nod dalam rangkaian. Pendekatan pengelompokan hierarki dicadangkan untuk mengesan komuniti dalam rangkaian maklumat yang tidak lengkap. Kajian empirikal di rangkaian maklumat dunia nyata menunjukkan bahawa kaedah yang dicadangkan kami dapat mengesan struktur komuniti dengan berkesan dalam rangkaian maklumat yang tidak lengkap. [[EENNDD]] pembelajaran metrik jarak jauh; rangkaian maklumat yang tidak lengkap; pengesanan masyarakat"], [{"string": "Topological spaces of the web An abstract is not available.", "keywords": ["general", "separation", "web metrics", "topology density"], "combined": "Topological spaces of the web An abstract is not available. [[EENNDD]] general; separation; web metrics; topology density"}, "Ruang topologi web Abstrak tidak tersedia. [[EENNDD]] umum; pemisahan; metrik web; ketumpatan topologi"], [{"string": "A geographical analysis of knowledge production in computer science We analyze knowledge production in Computer Science by means of coauthorship networks. For this, we consider 30 graduate programs of different regions of the world, being 8 programs in Brazil, 16 in North America (3 in Canada and 13 in the United States), and 6 in Europe (2 in France, 1 in Switzerland and 3 in the United Kingdom). We use a dataset that consists of 176,537 authors and 352,766 publication entries distributed among 2,176 publication venues. The results obtained for different metrics of collaboration social networks indicate the process of knowledge creation has \u00a0changed differently for each region. Research is increasingly done in teams across different fields of Computer Science. The size of the giant component indicates the existence of isolated collaboration groups in the European network, contrasting to the degree of connectivity found in the Brazilian and North-American counterparts. We also analyzed the temporal evolution of the social networks representing the three regions. The number of authors per paper experienced an increase in a time span of 12 years. We observe that the number of collaborations between authors grows faster than the number of authors, benefiting from the existing network structure. The temporal evolution shows differences between well-established fields, such as Databases and Computer Architecture, and emerging fields, like Bioinformatics and Geoinformatics. The patterns of collaboration\u00a0analyzed in this paper contribute to an overall understanding of Computer Science research in different geographical regions that could not be achieved without the use of complex networks and a large publication database.", "keywords": ["miscellaneous", "collaboration social networks", "computer science", "coauthorship networks"], "combined": "A geographical analysis of knowledge production in computer science We analyze knowledge production in Computer Science by means of coauthorship networks. For this, we consider 30 graduate programs of different regions of the world, being 8 programs in Brazil, 16 in North America (3 in Canada and 13 in the United States), and 6 in Europe (2 in France, 1 in Switzerland and 3 in the United Kingdom). We use a dataset that consists of 176,537 authors and 352,766 publication entries distributed among 2,176 publication venues. The results obtained for different metrics of collaboration social networks indicate the process of knowledge creation has \u00a0changed differently for each region. Research is increasingly done in teams across different fields of Computer Science. The size of the giant component indicates the existence of isolated collaboration groups in the European network, contrasting to the degree of connectivity found in the Brazilian and North-American counterparts. We also analyzed the temporal evolution of the social networks representing the three regions. The number of authors per paper experienced an increase in a time span of 12 years. We observe that the number of collaborations between authors grows faster than the number of authors, benefiting from the existing network structure. The temporal evolution shows differences between well-established fields, such as Databases and Computer Architecture, and emerging fields, like Bioinformatics and Geoinformatics. The patterns of collaboration\u00a0analyzed in this paper contribute to an overall understanding of Computer Science research in different geographical regions that could not be achieved without the use of complex networks and a large publication database. [[EENNDD]] miscellaneous; collaboration social networks; computer science; coauthorship networks"}, "Analisis geografi pengeluaran pengetahuan dalam sains komputer Kami menganalisis pengeluaran pengetahuan dalam Sains Komputer dengan menggunakan rangkaian coauthorship. Untuk ini, kami mempertimbangkan 30 program siswazah dari pelbagai wilayah di dunia, iaitu 8 program di Brazil, 16 di Amerika Utara (3 di Kanada dan 13 di Amerika Syarikat), dan 6 di Eropah (2 di Perancis, 1 di Switzerland dan 3 di United Kingdom). Kami menggunakan set data yang terdiri daripada 176,537 pengarang dan 352,766 entri penerbitan yang diedarkan di antara 2,176 tempat penerbitan. Hasil yang diperoleh untuk metrik kolaborasi rangkaian sosial yang berbeza menunjukkan proses penciptaan pengetahuan telah berubah secara berbeza untuk setiap wilayah. Penyelidikan semakin banyak dilakukan dalam pasukan di pelbagai bidang Sains Komputer. Ukuran komponen raksasa menunjukkan adanya kumpulan kolaborasi terpencil di rangkaian Eropah, berbeza dengan tahap kesambungan yang terdapat di rakan-rakan Brazil dan Amerika Utara. Kami juga menganalisis evolusi temporal rangkaian sosial yang mewakili ketiga wilayah tersebut. Bilangan pengarang setiap makalah mengalami peningkatan dalam jangka masa 12 tahun. Kami memerhatikan bahawa jumlah kolaborasi antara pengarang bertambah lebih cepat daripada jumlah pengarang, yang memanfaatkan struktur rangkaian yang ada. Evolusi temporal menunjukkan perbezaan antara bidang yang mapan, seperti Pangkalan Data dan Senibina Komputer, dan bidang yang muncul, seperti Bioinformatika dan Geoinformatika. Corak kolaborasi yang dianalisis dalam makalah ini menyumbang kepada pemahaman menyeluruh mengenai penyelidikan Sains Komputer di wilayah geografi yang berlainan yang tidak dapat dicapai tanpa menggunakan rangkaian yang kompleks dan pangkalan data penerbitan yang besar. [[EENNDD]] pelbagai; rangkaian sosial kolaborasi; Sains Komputer; rangkaian pengarang bersama"], [{"string": "Searching for events in the blogosphere Over the last few years, blogs (web logs) have gained massive popularity and have become one of the most influential web social media in our times. Every blog post in the Blogosphere has a well defined timestamp, which is not taken into account by search engines. By conducting research regarding this feature of the Blogosphere, we can attempt to discover bursty terms and correlations between them during a time interval. We apply Kleinberg's automaton on extracted titles of blog posts to discover bursty terms, we introduce a novel representation of a term's burstiness evolution called State Series and we employ a Euclidean-based distance metric to discover potential correlations between terms without taking into account their context. We evaluate the results trying to match them with real life events. Finally, we propose some ideas for further evaluation techniques and future research in the field.", "keywords": ["hot topics", "text mining", "information retrieval", "miscellaneous", "burst analysis", "blogs", "social media", "keyword correlation"], "combined": "Searching for events in the blogosphere Over the last few years, blogs (web logs) have gained massive popularity and have become one of the most influential web social media in our times. Every blog post in the Blogosphere has a well defined timestamp, which is not taken into account by search engines. By conducting research regarding this feature of the Blogosphere, we can attempt to discover bursty terms and correlations between them during a time interval. We apply Kleinberg's automaton on extracted titles of blog posts to discover bursty terms, we introduce a novel representation of a term's burstiness evolution called State Series and we employ a Euclidean-based distance metric to discover potential correlations between terms without taking into account their context. We evaluate the results trying to match them with real life events. Finally, we propose some ideas for further evaluation techniques and future research in the field. [[EENNDD]] hot topics; text mining; information retrieval; miscellaneous; burst analysis; blogs; social media; keyword correlation"}, "Mencari peristiwa di blogosphere Sejak beberapa tahun kebelakangan ini, blog (log web) telah mendapat populariti besar dan menjadi salah satu media sosial web yang paling berpengaruh pada zaman kita. Setiap catatan blog di Blogosphere mempunyai cap waktu yang ditentukan dengan baik, yang tidak diambil kira oleh mesin pencari. Dengan melakukan penelitian mengenai fitur Blogosphere ini, kita dapat berusaha menemukan istilah dan korelasi pecah di antara mereka selama selang waktu. Kami menggunakan automatik Kleinberg pada judul catatan blog yang diekstrak untuk mengetahui istilah pecah, kami memperkenalkan representasi novel evolusi burstness istilah yang disebut State Series dan kami menggunakan metrik jarak berdasarkan Euclidean untuk mencari kemungkinan hubungan antara istilah tanpa mengambil kira konteksnya. Kami menilai hasil yang cuba memadankannya dengan peristiwa kehidupan sebenar. Akhirnya, kami mencadangkan beberapa idea untuk teknik penilaian lebih lanjut dan penyelidikan masa depan di lapangan. [[EENNDD]] topik hangat; perlombongan teks; pengambilan maklumat; pelbagai; analisis pecah; blog; media sosial; korelasi kata kunci"], [{"string": "How to make a semantic web browser No contact information provided yet.", "keywords": ["bioinformatics", "design tools and techniques", "web services", "semantic web", "rdf", "user interface"], "combined": "How to make a semantic web browser No contact information provided yet. [[EENNDD]] bioinformatics; design tools and techniques; web services; semantic web; rdf; user interface"}, "Cara membuat penyemak imbas semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] bioinformatik; alat dan teknik reka bentuk; perkhidmatan web; web semantik; rdf; antaramuka pengguna"], [{"string": "Using Google distance to weight approximate ontology matches Discovering mappings between concept hierarchies is widely regarded as one of the hardest and most urgent problems facing the Semantic Web. The problem is even harder in domains where concepts are inherently vague and ill-defined, and cannot be given a crisp definition. A notion of approximate concept mapping is required in such domains, but until now, no such notion is vailable.", "keywords": ["approximation", "knowledge representation formalisms and methods", "google distance"], "combined": "Using Google distance to weight approximate ontology matches Discovering mappings between concept hierarchies is widely regarded as one of the hardest and most urgent problems facing the Semantic Web. The problem is even harder in domains where concepts are inherently vague and ill-defined, and cannot be given a crisp definition. A notion of approximate concept mapping is required in such domains, but until now, no such notion is vailable. [[EENNDD]] approximation; knowledge representation formalisms and methods; google distance"}, "Menggunakan jarak Google untuk menimbang perbandingan ontologi. Menemukan pemetaan antara hierarki konsep secara meluas dianggap sebagai salah satu masalah paling sukar dan mendesak yang dihadapi oleh Semantik Web. Masalahnya lebih sukar dalam domain di mana konsep pada dasarnya samar-samar dan tidak jelas, dan tidak dapat diberikan definisi yang jelas. Gagasan pemetaan konsep perkiraan diperlukan dalam domain seperti itu, tetapi sampai sekarang, tidak ada gagasan seperti itu yang tersedia. [[EENNDD]] penghampiran; formalisme dan kaedah perwakilan pengetahuan; jarak google"], [{"string": "SemTag and seeker: bootstrapping the semantic web via automated semantic annotation No contact information provided yet.", "keywords": ["data mining", "information retrieval", "automated semantic tagging", "large text datasets", "software architectures", "text analytics"], "combined": "SemTag and seeker: bootstrapping the semantic web via automated semantic annotation No contact information provided yet. [[EENNDD]] data mining; information retrieval; automated semantic tagging; large text datasets; software architectures; text analytics"}, "SemTag dan pencari: bootstrapping web semantik melalui anotasi semantik automatik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlombongan data; pengambilan maklumat; penandaan semantik automatik; set data teks besar; seni bina perisian; analisis teks"], [{"string": "A combined approach to checking web ontologies No contact information provided yet.", "keywords": ["ontologies", "racer", "daml+oil", "semantic web", "alloy", "z"], "combined": "A combined approach to checking web ontologies No contact information provided yet. [[EENNDD]] ontologies; racer; daml+oil; semantic web; alloy; z"}, "Pendekatan gabungan untuk memeriksa ontologi web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ontologi; pelumba; daml + minyak; web semantik; aloi; z"], [{"string": "Managing versions of web documents in a transaction-time web server No contact information provided yet.", "keywords": ["versioning", "observant system", "transaction time"], "combined": "Managing versions of web documents in a transaction-time web server No contact information provided yet. [[EENNDD]] versioning; observant system; transaction time"}, "Menguruskan versi dokumen web dalam pelayan web masa transaksi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] versi; sistem pemerhati; masa urus niaga"], [{"string": "Hybrid semantic tagging for information extraction No contact information provided yet.", "keywords": ["hmm", "text mining", "content analysis and indexing", "information extraction", "learning", "semantic web", "rules based systems"], "combined": "Hybrid semantic tagging for information extraction No contact information provided yet. [[EENNDD]] hmm; text mining; content analysis and indexing; information extraction; learning; semantic web; rules based systems"}, "Penandaan semantik hibrid untuk pengekstrakan maklumat Belum ada maklumat hubungan yang diberikan. [[EENNDD]] hmm; perlombongan teks; analisis kandungan dan pengindeksan; pengekstrakan maklumat; belajar; web semantik; sistem berasaskan peraturan"], [{"string": "Multimedia meets computer graphics in SMIL2.0: a time model for the web No contact information provided yet.", "keywords": ["animation", "video", "timing", "synchronization", "multimedia"], "combined": "Multimedia meets computer graphics in SMIL2.0: a time model for the web No contact information provided yet. [[EENNDD]] animation; video; timing; synchronization; multimedia"}, "Multimedia memenuhi grafik komputer dalam SMIL2.0: model masa untuk web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] animasi; video; masa; penyegerakan; multimedia"], [{"string": "Bridging the gap between OWL and relational databases Schema statements in OWL are interpreted quite differently from analogous statements in relational databases. If these statements are meant to be interpreted as integrity constraints (ICs), OWL's interpretation may seem confusing and/or inappropriate. Therefore, we propose an extension of OWL with ICs that captures the intuition behind ICs in relational databases. We discuss the algorithms for checking IC satisfaction for different types of knowledge bases, and show that, if the constraints are satisfied, we can disregard them while answering a broad range of positive queries.", "keywords": ["knowledge representation formalisms and methods", "owl", "semantic web", "relational databases"], "combined": "Bridging the gap between OWL and relational databases Schema statements in OWL are interpreted quite differently from analogous statements in relational databases. If these statements are meant to be interpreted as integrity constraints (ICs), OWL's interpretation may seem confusing and/or inappropriate. Therefore, we propose an extension of OWL with ICs that captures the intuition behind ICs in relational databases. We discuss the algorithms for checking IC satisfaction for different types of knowledge bases, and show that, if the constraints are satisfied, we can disregard them while answering a broad range of positive queries. [[EENNDD]] knowledge representation formalisms and methods; owl; semantic web; relational databases"}, "Merapatkan jurang antara OWL dan pangkalan data hubungan Pernyataan skema dalam OWL ditafsirkan dengan cara yang berbeza daripada pernyataan analog dalam pangkalan data hubungan. Sekiranya pernyataan ini dimaksudkan untuk ditafsirkan sebagai kekangan integriti (IC), tafsiran OWL mungkin kelihatan membingungkan dan / atau tidak sesuai. Oleh itu, kami mencadangkan perluasan OWL dengan IC yang menangkap intuisi di sebalik IC dalam pangkalan data hubungan. Kami membincangkan algoritma untuk memeriksa kepuasan IC untuk pelbagai jenis pangkalan pengetahuan, dan menunjukkan bahawa, jika kekangan itu dipenuhi, kami dapat mengabaikannya sambil menjawab pelbagai pertanyaan positif. [[EENNDD]] formalisme dan kaedah perwakilan pengetahuan; burung hantu; web semantik; pangkalan data hubungan"], [{"string": "Life is sharable: mechanisms to support and sustain blogging life experience Recent trend in the development of mobile devices, wireless communications, sensor technologies, weblogs, and peer-to-peer communications have prompted a new design opportunity for enhancing social interactions. This paper introduces our preliminary experiences in designing a prototype utilizing the aforementioned technologies to share life experience. Users equipped with camera phones coupled with short-range communication technology, such as RFID, can capture life experience and share it as weblogs to other people. However, in reality, this is easier said than done. The success of weblogs relies on the active participation and willingness of people to contribute. To encourage active participations, a ranking system, AgreeRank, is specifically developed to get them motivated.", "keywords": ["peer-to-peer communication", "mobile phone", "wireless networking", "collaborative system", "weblog", "rfid", "interaction styles"], "combined": "Life is sharable: mechanisms to support and sustain blogging life experience Recent trend in the development of mobile devices, wireless communications, sensor technologies, weblogs, and peer-to-peer communications have prompted a new design opportunity for enhancing social interactions. This paper introduces our preliminary experiences in designing a prototype utilizing the aforementioned technologies to share life experience. Users equipped with camera phones coupled with short-range communication technology, such as RFID, can capture life experience and share it as weblogs to other people. However, in reality, this is easier said than done. The success of weblogs relies on the active participation and willingness of people to contribute. To encourage active participations, a ranking system, AgreeRank, is specifically developed to get them motivated. [[EENNDD]] peer-to-peer communication; mobile phone; wireless networking; collaborative system; weblog; rfid; interaction styles"}, "Kehidupan dapat dilihat: mekanisme untuk menyokong dan mempertahankan pengalaman hidup blogging Trend terkini dalam pengembangan peranti mudah alih, komunikasi tanpa wayar, teknologi sensor, blog web, dan komunikasi peer-to-peer telah mendorong peluang reka bentuk baru untuk meningkatkan interaksi sosial. Makalah ini memperkenalkan pengalaman awal kami dalam merancang prototaip menggunakan teknologi yang disebutkan di atas untuk berkongsi pengalaman hidup. Pengguna yang dilengkapi dengan telefon bimbit yang digabungkan dengan teknologi komunikasi jarak dekat, seperti RFID, dapat menangkap pengalaman hidup dan membagikannya sebagai blog web kepada orang lain. Namun, pada hakikatnya, ini lebih senang dikatakan daripada dilakukan. Kejayaan weblog bergantung pada penyertaan aktif dan kesediaan orang untuk menyumbang. Untuk mendorong penyertaan aktif, sistem peringkat, AgreeRank, dikembangkan secara khusus untuk membuat mereka termotivasi. [[EENNDD]] komunikasi rakan sebaya; telefon bimbit; rangkaian tanpa wayar; sistem kolaboratif; blog web; rfid; gaya interaksi"], [{"string": "Clustering for probabilistic model estimation for CF No contact information provided yet.", "keywords": ["collaborative filtering", "probabilistic model"], "combined": "Clustering for probabilistic model estimation for CF No contact information provided yet. [[EENNDD]] collaborative filtering; probabilistic model"}, "Pengumpulan model probabilistik untuk CF Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] penapisan kolaboratif; model kebarangkalian"], [{"string": "Automated construction of web accessibility models from transaction click-streams Screen readers, the dominant assistive technology used by visually impaired people to access the Web, function by speaking out the content of the screen serially. Using screen readers for conducting online transactions can cause considerable information overload, because transactions, such as shopping and paying bills, typically involve a number of steps spanning several web pages. One can combat this overload by using a transaction model for web accessibility that presents only fragments of web pages that are needed for doing transactions. We can realize such a model by coupling a process automaton, encoding states of a transaction, with concept classifiers that identify page fragments \"relevant\" to a particular state of the transaction. In this paper we present a fully automated process that synergistically combines several techniques for transforming unlabeled click-stream data generated by transactions into a transactionmodel. These techniques include web content analysis to partition a web page into segments consisting of semantically related content, contextual analysis of data surrounding clickable objects in a page, and machine learning methods, such as clustering of page segments based on contextual analysis, statistical classification, and automata learning. The use of unlabeled click streams in building transaction models has important benefits: (i) visually impaired users do not have to depend on sighted users for creating manually labeled training data to construct the models; (ii) it is possible to mine personalized models from unlabeled transaction click-streams associated with sites that visually impaired users visit regularly; (iii) since unlabeled data is relatively easy to obtain, it is feasible to scale up the construction of domain-specific transaction models (e.g., separate models for shopping, airline reservations, bill payments, etc.); (iv) adjusting the performance of deployed models over timtime with new training data is also doable. We provide preliminary experimental evidence of the practical effectiveness of both domain-specific, as well as personalized accessibility transaction models built using our approach. Finally, this approach is applicable for building transaction models for mobile devices with limited-size displays, as well as for creating wrappers for information extraction from web sites.", "keywords": ["context", "process models", "user interface management systems", "web transaction", "machine learning"], "combined": "Automated construction of web accessibility models from transaction click-streams Screen readers, the dominant assistive technology used by visually impaired people to access the Web, function by speaking out the content of the screen serially. Using screen readers for conducting online transactions can cause considerable information overload, because transactions, such as shopping and paying bills, typically involve a number of steps spanning several web pages. One can combat this overload by using a transaction model for web accessibility that presents only fragments of web pages that are needed for doing transactions. We can realize such a model by coupling a process automaton, encoding states of a transaction, with concept classifiers that identify page fragments \"relevant\" to a particular state of the transaction. In this paper we present a fully automated process that synergistically combines several techniques for transforming unlabeled click-stream data generated by transactions into a transactionmodel. These techniques include web content analysis to partition a web page into segments consisting of semantically related content, contextual analysis of data surrounding clickable objects in a page, and machine learning methods, such as clustering of page segments based on contextual analysis, statistical classification, and automata learning. The use of unlabeled click streams in building transaction models has important benefits: (i) visually impaired users do not have to depend on sighted users for creating manually labeled training data to construct the models; (ii) it is possible to mine personalized models from unlabeled transaction click-streams associated with sites that visually impaired users visit regularly; (iii) since unlabeled data is relatively easy to obtain, it is feasible to scale up the construction of domain-specific transaction models (e.g., separate models for shopping, airline reservations, bill payments, etc.); (iv) adjusting the performance of deployed models over timtime with new training data is also doable. We provide preliminary experimental evidence of the practical effectiveness of both domain-specific, as well as personalized accessibility transaction models built using our approach. Finally, this approach is applicable for building transaction models for mobile devices with limited-size displays, as well as for creating wrappers for information extraction from web sites. [[EENNDD]] context; process models; user interface management systems; web transaction; machine learning"}, "Pembinaan automatik model kebolehaksesan web dari aliran klik transaksi Pembaca skrin, teknologi bantu dominan yang digunakan oleh orang cacat penglihatan untuk mengakses Web, berfungsi dengan menyampaikan kandungan skrin secara bersiri. Menggunakan pembaca skrin untuk melakukan transaksi dalam talian boleh menyebabkan banyak maklumat berlebihan, kerana transaksi, seperti membeli-belah dan membayar bil, biasanya melibatkan sejumlah langkah yang merangkumi beberapa halaman web. Seseorang dapat mengatasi kelebihan beban ini dengan menggunakan model transaksi untuk aksesibilitas web yang hanya menyajikan serpihan halaman web yang diperlukan untuk melakukan transaksi. Kita dapat merealisasikan model seperti itu dengan menggabungkan proses automaton, mengekod keadaan transaksi, dengan pengkelasan konsep yang mengenal pasti pecahan halaman \"relevan\" dengan keadaan transaksi tertentu. Dalam makalah ini kami menyajikan proses automatik sepenuhnya yang secara sinergis menggabungkan beberapa teknik untuk mengubah data aliran klik tanpa label yang dihasilkan oleh transaksi menjadi model transaksi. Teknik-teknik ini merangkumi analisis kandungan web untuk membagi halaman web ke dalam segmen-segmen yang terdiri dari isi yang berkaitan secara semantik, analisis kontekstual data di sekitar objek yang dapat diklik dalam halaman, dan metode pembelajaran mesin, seperti pengelompokan segmen halaman berdasarkan analisis kontekstual, klasifikasi statistik, dan pembelajaran automata. Penggunaan aliran klik tanpa label dalam membina model transaksi mempunyai faedah penting: (i) pengguna yang cacat penglihatan tidak perlu bergantung pada pengguna yang melihat kerana membuat data latihan berlabel secara manual untuk membina model; (ii) adalah mungkin untuk melombong model yang diperibadikan dari aliran klik transaksi yang tidak berlabel yang berkaitan dengan laman web yang kerap dikunjungi oleh pengguna yang cacat penglihatan; (iii) kerana data yang tidak dilabel relatif mudah diperoleh, layak untuk meningkatkan pembangunan model transaksi khusus domain (mis., model terpisah untuk membeli-belah, tempahan syarikat penerbangan, pembayaran bil, dll.); (iv) menyesuaikan prestasi model yang digunakan dari masa ke masa dengan data latihan baru juga dapat dilakukan. Kami memberikan bukti eksperimental awal mengenai keberkesanan praktikal kedua-dua domain khusus, dan juga model transaksi aksesibiliti yang diperibadikan yang dibina menggunakan pendekatan kami. Akhirnya, pendekatan ini berlaku untuk membangun model transaksi untuk peranti mudah alih dengan paparan ukuran terhad, dan juga untuk membuat pembungkus untuk pengekstrakan maklumat dari laman web. [[EENNDD]] konteks; model proses; sistem pengurusan antara muka pengguna; transaksi web; pembelajaran mesin"], [{"string": "Learning search engine specific query transformations for question answering An abstract is not available.", "keywords": ["query expansion", "information retrieval", "web search", "information search and retrieval", "question answering"], "combined": "Learning search engine specific query transformations for question answering An abstract is not available. [[EENNDD]] query expansion; information retrieval; web search; information search and retrieval; question answering"}, "Belajar transformasi pertanyaan khusus mesin carian untuk menjawab soalan Abstrak tidak tersedia. [[EENNDD]] pengembangan pertanyaan; pengambilan maklumat; carian sesawang; pencarian dan pengambilan maklumat; menjawab soalan"], [{"string": "Choosing reputable servents in a P2P network No contact information provided yet.", "keywords": ["credibility", "p2p network", "invasive software", "security and protection", "polling protocol", "reputation"], "combined": "Choosing reputable servents in a P2P network No contact information provided yet. [[EENNDD]] credibility; p2p network; invasive software; security and protection; polling protocol; reputation"}, "Memilih pegawai yang bereputasi dalam rangkaian P2P Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kredibiliti; rangkaian p2p; perisian invasif; keselamatan dan perlindungan; protokol pengundian; reputasi"], [{"string": "Smart Miner: a new framework for mining large scale web usage data In this paper, we propose a novel framework called Smart-Miner for web usage mining problem which uses link information for producing accurate user sessions and frequent navigation patterns. Unlike the simple session concepts in the time and navigation based approaches, where sessions are sequences of web pages requested from the server or viewed in the browser, Smart Miner sessions are set of paths traversed in the web graph that corresponds to users' navigations among web pages. We have modeled session construction as a new graph problem and utilized a new algorithm, Smart-SRA, to solve this problem efficiently. For the pattern discovery phase, we have developed an efficient version of the Apriori-All technique which uses the structure of web graph to increase the performance. From the experiments that we have performed on both real and simulated data, we have observed that Smart-Miner produces at least 30% more accurate web usage patterns than other approaches including previous session construction methods. We have also studied the effect of having the referrer information in the web server logs to show that different versions of Smart-SRA produce similar results. Our another contribution is that we have implemented distributed version of the Smart Miner framework by employing Map/Reduce Paradigm. We conclude that we can efficiently process terabytes of web server logs belonging to multiple web sites by our scalable framework.", "keywords": ["web user modeling", "map/reduce", "graph mining", "web usage mining", "parallel data mining"], "combined": "Smart Miner: a new framework for mining large scale web usage data In this paper, we propose a novel framework called Smart-Miner for web usage mining problem which uses link information for producing accurate user sessions and frequent navigation patterns. Unlike the simple session concepts in the time and navigation based approaches, where sessions are sequences of web pages requested from the server or viewed in the browser, Smart Miner sessions are set of paths traversed in the web graph that corresponds to users' navigations among web pages. We have modeled session construction as a new graph problem and utilized a new algorithm, Smart-SRA, to solve this problem efficiently. For the pattern discovery phase, we have developed an efficient version of the Apriori-All technique which uses the structure of web graph to increase the performance. From the experiments that we have performed on both real and simulated data, we have observed that Smart-Miner produces at least 30% more accurate web usage patterns than other approaches including previous session construction methods. We have also studied the effect of having the referrer information in the web server logs to show that different versions of Smart-SRA produce similar results. Our another contribution is that we have implemented distributed version of the Smart Miner framework by employing Map/Reduce Paradigm. We conclude that we can efficiently process terabytes of web server logs belonging to multiple web sites by our scalable framework. [[EENNDD]] web user modeling; map/reduce; graph mining; web usage mining; parallel data mining"}, "Smart Miner: kerangka baru untuk melombong data penggunaan web berskala besar Dalam makalah ini, kami mencadangkan kerangka baru yang disebut Smart-Miner untuk masalah perlombongan penggunaan web yang menggunakan maklumat pautan untuk menghasilkan sesi pengguna yang tepat dan corak navigasi yang kerap. Tidak seperti konsep sesi sederhana dalam pendekatan berdasarkan waktu dan navigasi, di mana sesi adalah urutan halaman web yang diminta dari pelayan atau dilihat di penyemak imbas, sesi Smart Miner adalah set jalan yang dilalui dalam grafik web yang sesuai dengan navigasi pengguna di antara web halaman. Kami telah memodelkan pembinaan sesi sebagai masalah grafik baru dan menggunakan algoritma baru, Smart-SRA, untuk menyelesaikan masalah ini dengan cekap. Untuk fasa penemuan corak, kami telah mengembangkan versi teknik Apriori-All yang cekap yang menggunakan struktur grafik web untuk meningkatkan prestasi. Dari eksperimen yang telah kami lakukan pada data nyata dan simulasi, kami telah memerhatikan bahawa Smart-Miner menghasilkan sekurang-kurangnya 30% corak penggunaan web yang lebih tepat daripada pendekatan lain termasuk kaedah pembinaan sesi sebelumnya. Kami juga telah mengkaji kesan mempunyai maklumat perujuk dalam log pelayan web untuk menunjukkan bahawa versi Smart-SRA yang berbeza menghasilkan hasil yang serupa. Sumbangan kami yang lain adalah kami telah melaksanakan versi Smart Miner framework yang diedarkan dengan menggunakan Map / Reduce Paradigm. Kami menyimpulkan bahawa kami dapat memproses terabyte log pelayan web dengan cekap yang dimiliki oleh beberapa laman web dengan rangka kerja kami yang boleh diskalakan. [[EENNDD]] pemodelan pengguna web; peta / mengurangkan; perlombongan grafik; perlombongan penggunaan web; perlombongan data selari"], [{"string": "Personalized web exploration with task models Personalized Web search has emerged as one of the hottest topics for both the Web industry and academic researchers. However, the majority of studies on personalized search focused on a rather simple type of search, which leaves an important research topic - the personalization in exploratory searches - as an under-studied area. In this paper, we present a study of personalization in task-based information exploration using a system called TaskSieve. TaskSieve is a Web search system that utilizes a relevance feedback based profile, called a \"task model\", for personalization. Its innovations include flexible and user controlled integration of queries and task models, task-infused text snippet generation, and on-screen visualization of task models. Through an empirical study using human subjects conducting task-based exploration searches, we demonstrate that TaskSieve pushes significantly more relevant documents to the top of search result lists as compared to a traditional search system. TaskSieve helps users select significantly more accurate information for their tasks, allows the users to do so with higher productivity, and is viewed more favorably by subjects under several usability related characteristics.", "keywords": ["personalization", "adaptive search", "empirical study", "user profile", "task model", "task-based information exploration"], "combined": "Personalized web exploration with task models Personalized Web search has emerged as one of the hottest topics for both the Web industry and academic researchers. However, the majority of studies on personalized search focused on a rather simple type of search, which leaves an important research topic - the personalization in exploratory searches - as an under-studied area. In this paper, we present a study of personalization in task-based information exploration using a system called TaskSieve. TaskSieve is a Web search system that utilizes a relevance feedback based profile, called a \"task model\", for personalization. Its innovations include flexible and user controlled integration of queries and task models, task-infused text snippet generation, and on-screen visualization of task models. Through an empirical study using human subjects conducting task-based exploration searches, we demonstrate that TaskSieve pushes significantly more relevant documents to the top of search result lists as compared to a traditional search system. TaskSieve helps users select significantly more accurate information for their tasks, allows the users to do so with higher productivity, and is viewed more favorably by subjects under several usability related characteristics. [[EENNDD]] personalization; adaptive search; empirical study; user profile; task model; task-based information exploration"}, "Penjelajahan web yang diperibadikan dengan model tugas Carian Web yang diperibadikan telah muncul sebagai salah satu topik terpanas bagi industri Web dan penyelidik akademik. Walau bagaimanapun, sebahagian besar kajian mengenai carian yang diperibadikan menumpukan pada jenis carian yang agak sederhana, yang menjadikan topik penyelidikan penting - pemperibadian dalam pencarian eksploratori - sebagai bidang yang tidak dikaji. Dalam makalah ini, kami menyajikan kajian pemperibadian dalam penerokaan maklumat berdasarkan tugas menggunakan sistem yang disebut TaskSieve. TaskSieve adalah sistem carian Web yang menggunakan profil berdasarkan maklum balas yang relevan, yang disebut \"model tugas\", untuk personalisasi. Inovasinya merangkumi integrasi pertanyaan dan model tugas yang fleksibel dan terkawal pengguna, penghasilan coretan teks yang disusun dengan tugas, dan visualisasi pada model tugas. Melalui kajian empirikal menggunakan subjek manusia yang melakukan pencarian penerokaan berdasarkan tugas, kami menunjukkan bahawa TaskSieve mendorong dokumen yang lebih relevan ke puncak senarai hasil carian berbanding sistem carian tradisional. TaskSieve membantu pengguna memilih maklumat yang lebih tepat untuk tugas mereka, membolehkan pengguna melakukannya dengan produktiviti yang lebih tinggi, dan dilihat lebih baik oleh subjek di bawah beberapa ciri berkaitan kegunaan. [[EENNDD]] pemperibadian; carian adaptif; Kajian empirikal; profil pengguna; model tugas; penerokaan maklumat berasaskan tugas"], [{"string": "Modeling the temporal dynamics of social rating networks using bidirectional effects of social relations and rating patterns A social rating network (SRN) is a social network in which edges represent social relationships and users (nodes) express ratings on some of the given items. Such networks play an increasingly important role in reviewing websites such as Epinions.com or online sharing websites like Flickr.com. In this paper, we first observe and analyze the temporal behavior of users in a social rating network, who express ratings and create social relations. Then, we model the temporal dynamics of an SRN based on our observations, using the bidirectional effects of ratings and social relations. While existing models for other types of social networks have captured some of the effects, our model is the first one to represent all four effects, i.e. social relations-on-ratings (social influence), social relations-on-social relations (transitivity), ratings-on-social relations (selection), and ratings-on-ratings (correlational influence). Existing works consider these effects as static and constant throughout the evolution of an SRN, however our observations reveal that these effects are actually dynamic. We propose a probabilistic generative model for SRNs, which models the strength and dynamics of each effect throughout the network evolution. This model can serve for the prediction of future links, ratings or community structures. Due to the sensitive nature of SRNs, another motivation for our work is the generation of synthetic SRN data sets for research purposes. Our experimental studies on two real life datasets (Epinions and Flickr) demonstrate that the proposed model produces social rating networks that agree with real world data on a comprehensive set of evaluation criteria.", "keywords": ["generative models", "temporal dynamics", "user behavioral modeling", "social rating networks"], "combined": "Modeling the temporal dynamics of social rating networks using bidirectional effects of social relations and rating patterns A social rating network (SRN) is a social network in which edges represent social relationships and users (nodes) express ratings on some of the given items. Such networks play an increasingly important role in reviewing websites such as Epinions.com or online sharing websites like Flickr.com. In this paper, we first observe and analyze the temporal behavior of users in a social rating network, who express ratings and create social relations. Then, we model the temporal dynamics of an SRN based on our observations, using the bidirectional effects of ratings and social relations. While existing models for other types of social networks have captured some of the effects, our model is the first one to represent all four effects, i.e. social relations-on-ratings (social influence), social relations-on-social relations (transitivity), ratings-on-social relations (selection), and ratings-on-ratings (correlational influence). Existing works consider these effects as static and constant throughout the evolution of an SRN, however our observations reveal that these effects are actually dynamic. We propose a probabilistic generative model for SRNs, which models the strength and dynamics of each effect throughout the network evolution. This model can serve for the prediction of future links, ratings or community structures. Due to the sensitive nature of SRNs, another motivation for our work is the generation of synthetic SRN data sets for research purposes. Our experimental studies on two real life datasets (Epinions and Flickr) demonstrate that the proposed model produces social rating networks that agree with real world data on a comprehensive set of evaluation criteria. [[EENNDD]] generative models; temporal dynamics; user behavioral modeling; social rating networks"}, "Memodelkan dinamika temporal rangkaian penarafan sosial menggunakan kesan dua arah hubungan sosial dan corak penilaian Rangkaian penarafan sosial (SRN) adalah rangkaian sosial di mana tepi mewakili hubungan sosial dan pengguna (node) menyatakan penilaian pada beberapa item yang diberikan. Rangkaian sedemikian memainkan peranan yang semakin penting dalam mengkaji laman web seperti Epinions.com atau laman web perkongsian dalam talian seperti Flickr.com. Dalam makalah ini, pertama-tama kita memerhatikan dan menganalisis tingkah laku temporal pengguna dalam rangkaian penarafan sosial, yang menyatakan penilaian dan membuat hubungan sosial. Kemudian, kami memodelkan dinamika temporal SRN berdasarkan pemerhatian kami, menggunakan kesan dua arah penilaian dan hubungan sosial. Walaupun model yang ada untuk jenis rangkaian sosial yang lain telah menangkap beberapa kesan, model kami adalah yang pertama untuk mewakili keempat-empat kesan, iaitu hubungan sosial-pada-peringkat (pengaruh sosial), hubungan sosial-hubungan-sosial (transitiviti) , hubungan pada hubungan sosial (pemilihan), dan peringkat pada peringkat (pengaruh korelasi). Karya yang ada menganggap kesan ini sebagai statik dan berterusan sepanjang evolusi SRN, namun pemerhatian kami menunjukkan bahawa kesan ini sebenarnya dinamik. Kami mencadangkan model generatif probabilistik untuk SRN, yang memodelkan kekuatan dan dinamika setiap kesan sepanjang evolusi rangkaian. Model ini dapat berfungsi untuk meramalkan pautan, penilaian atau struktur komuniti masa depan. Kerana sifat SRN yang sensitif, motivasi lain untuk kerja kami adalah penghasilan set data SRN sintetik untuk tujuan penyelidikan. Kajian eksperimental kami pada dua kumpulan data kehidupan nyata (Epinions dan Flickr) menunjukkan bahawa model yang dicadangkan menghasilkan rangkaian penilaian sosial yang bersetuju dengan data dunia nyata mengenai satu set kriteria penilaian yang komprehensif. [[EENNDD]] model generatif; dinamika temporal; pemodelan tingkah laku pengguna; rangkaian penilaian sosial"], [{"string": "Focused crawling by exploiting anchor text using decision tree No contact information provided yet.", "keywords": ["decision tree learning", "anchor text", "focused crawling", "shortest path"], "combined": "Focused crawling by exploiting anchor text using decision tree No contact information provided yet. [[EENNDD]] decision tree learning; anchor text; focused crawling; shortest path"}, "Fokus merangkak dengan memanfaatkan teks jangkar menggunakan pohon keputusan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pembelajaran pokok keputusan; teks sauh; merangkak fokus; jalan terpendek"], [{"string": "Topic segmentation of message hierarchies for indexing and navigation support No contact information provided yet.", "keywords": ["discussion boards", "navigational aid", "assistive technology for blind users", "segmentation"], "combined": "Topic segmentation of message hierarchies for indexing and navigation support No contact information provided yet. [[EENNDD]] discussion boards; navigational aid; assistive technology for blind users; segmentation"}, "Segmentasi topik hierarki mesej untuk pengindeksan dan sokongan navigasi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] papan perbincangan; bantuan pelayaran; teknologi bantu untuk pengguna buta; pembahagian"], [{"string": "A flexible generative model for preference aggregation Many areas of study, such as information retrieval, collaborative filtering, and social choice face the preference aggregation problem, in which multiple preferences over objects must be combined into a consensus ranking. Preferences over items can be expressed in a variety of forms, which makes the aggregation problem difficult. In this work we formulate a flexible probabilistic model over pairwise comparisons that can accommodate all these forms. Inference in the model is very fast, making it applicable to problems with hundreds of thousands of preferences. Experiments on benchmark datasets demonstrate superior performance to existing methods", "keywords": ["collaborative filtering", "preference aggregation", "learning", "meta search"], "combined": "A flexible generative model for preference aggregation Many areas of study, such as information retrieval, collaborative filtering, and social choice face the preference aggregation problem, in which multiple preferences over objects must be combined into a consensus ranking. Preferences over items can be expressed in a variety of forms, which makes the aggregation problem difficult. In this work we formulate a flexible probabilistic model over pairwise comparisons that can accommodate all these forms. Inference in the model is very fast, making it applicable to problems with hundreds of thousands of preferences. Experiments on benchmark datasets demonstrate superior performance to existing methods [[EENNDD]] collaborative filtering; preference aggregation; learning; meta search"}, "Model generatif yang fleksibel untuk penggabungan pilihan Banyak bidang kajian, seperti pengambilan maklumat, penapisan kolaboratif, dan pilihan sosial menghadapi masalah agregasi keutamaan, di mana pelbagai pilihan berbanding objek mesti digabungkan menjadi peringkat konsensus. Keutamaan terhadap item boleh dinyatakan dalam pelbagai bentuk, yang menyukarkan masalah pengagregatan. Dalam karya ini kami merumuskan model probabilistik fleksibel berbanding perbandingan berpasangan yang dapat mengakomodasi semua bentuk ini. Inferensi dalam model sangat cepat, menjadikannya dapat digunakan untuk masalah dengan ratusan ribu pilihan. Eksperimen pada set data penanda aras menunjukkan prestasi unggul daripada kaedah sedia ada [[EENNDD]] penapisan kolaboratif; penggabungan pilihan; belajar; carian meta"], [{"string": "Ensuring required failure atomicity of composite Web services No contact information provided yet.", "keywords": ["transactional models", "failure atomicity", "reliable web services compositions"], "combined": "Ensuring required failure atomicity of composite Web services No contact information provided yet. [[EENNDD]] transactional models; failure atomicity; reliable web services compositions"}, "Memastikan keberlangsungan kegagalan yang diperlukan dari perkhidmatan Web komposit Belum ada maklumat hubungan yang diberikan. [[EENNDD]] model transaksi; kegagalan atom; komposisi perkhidmatan web yang boleh dipercayai"], [{"string": "An information state-based dialogue manager for making voice web smarter In this paper we propose the integration of intelligent components technologies (natural language and discourse management) in voice web interfaces to make them smarter. We describe how we have integrated reusable components of dialogue management and language processing in a multilingual voice system to improve its friendliness and portability. The dialogue management component deals with complex dialogue phenomena, such as user-initiative dialogues, and follows the information state-based theory. The resulting dialogue system supports friendly communication (through the telephone and the web) in several languages: English, Spanish, Catalan and Italian. The dialogue system has been adapted to guide the users to access online public administration services.", "keywords": ["dialogue management", "user interface management systems", "voice web interfaces", "multilinguality"], "combined": "An information state-based dialogue manager for making voice web smarter In this paper we propose the integration of intelligent components technologies (natural language and discourse management) in voice web interfaces to make them smarter. We describe how we have integrated reusable components of dialogue management and language processing in a multilingual voice system to improve its friendliness and portability. The dialogue management component deals with complex dialogue phenomena, such as user-initiative dialogues, and follows the information state-based theory. The resulting dialogue system supports friendly communication (through the telephone and the web) in several languages: English, Spanish, Catalan and Italian. The dialogue system has been adapted to guide the users to access online public administration services. [[EENNDD]] dialogue management; user interface management systems; voice web interfaces; multilinguality"}, "Pengurus dialog berasaskan maklumat untuk menjadikan web suara lebih pintar Dalam makalah ini kami mencadangkan penggabungan teknologi komponen pintar (bahasa semula jadi dan pengurusan wacana) dalam antara muka web suara untuk menjadikannya lebih pintar. Kami menerangkan bagaimana kami telah menggabungkan komponen pengurusan dialog dan pemprosesan bahasa yang dapat digunakan kembali dalam sistem suara pelbagai bahasa untuk meningkatkan keramahan dan kemudahannya. Komponen pengurusan dialog berurusan dengan fenomena dialog yang kompleks, seperti dialog inisiatif pengguna, dan mengikuti teori berdasarkan maklumat negara. Sistem dialog yang dihasilkan menyokong komunikasi mesra (melalui telefon dan web) dalam beberapa bahasa: Inggeris, Sepanyol, Catalan dan Itali. Sistem dialog telah disesuaikan untuk membimbing pengguna mengakses perkhidmatan pentadbiran awam dalam talian. [[EENNDD]] pengurusan dialog; sistem pengurusan antara muka pengguna; antara muka web suara; berbilang bahasa"], [{"string": "Anycast-aware transport for content delivery networks Anycast-based content delivery networks (CDNs) have many properties that make them ideal for the large scale distribution of content on the Internet. However, because routing changes can result in a change of the endpoint that terminates the TCP session, TCP session disruption remains a concern for anycast CDNs, especially for large file downloads. In this paper we demonstrate that this problem does not require any complex solutions. In particular, we present the design of a simple, yet efficient, mechanism to handle session disruptions due to endpoint changes. With our mechanism, a client can continue the download of the content from the point at which it was before the endpoint change. Furthermore, CDN servers purge the TCP connection state quickly to handle frequent switching with low system overhead.", "keywords": ["internet", "connection disruption", "anycast", "content delivery networks"], "combined": "Anycast-aware transport for content delivery networks Anycast-based content delivery networks (CDNs) have many properties that make them ideal for the large scale distribution of content on the Internet. However, because routing changes can result in a change of the endpoint that terminates the TCP session, TCP session disruption remains a concern for anycast CDNs, especially for large file downloads. In this paper we demonstrate that this problem does not require any complex solutions. In particular, we present the design of a simple, yet efficient, mechanism to handle session disruptions due to endpoint changes. With our mechanism, a client can continue the download of the content from the point at which it was before the endpoint change. Furthermore, CDN servers purge the TCP connection state quickly to handle frequent switching with low system overhead. [[EENNDD]] internet; connection disruption; anycast; content delivery networks"}, "Pengangkutan yang menyedari Anycast untuk rangkaian penghantaran kandungan Rangkaian penghantaran kandungan berasaskan Anycast (CDN) mempunyai banyak sifat yang menjadikannya sesuai untuk pengedaran kandungan dalam skala besar di Internet. Namun, kerana perubahan routing dapat mengakibatkan perubahan titik akhir yang mengakhiri sesi TCP, gangguan sesi TCP tetap menjadi perhatian bagi CDN anycast, terutama untuk muat turun fail yang besar. Dalam makalah ini kami menunjukkan bahawa masalah ini tidak memerlukan penyelesaian yang kompleks. Secara khusus, kami menyajikan reka bentuk mekanisme yang sederhana namun efisien untuk menangani gangguan sesi kerana perubahan titik akhir. Dengan mekanisme kami, pelanggan dapat meneruskan muat turun kandungan dari titik sebelum perubahan titik akhir. Tambahan pula, pelayan CDN membersihkan keadaan sambungan TCP dengan cepat untuk menangani pertukaran yang kerap dengan overhead sistem yang rendah. [[EENNDD]] internet; gangguan sambungan; anycast; rangkaian penghantaran kandungan"], [{"string": "Tag clouds for summarizing web search results In this paper, we describe an application, PubCloud that uses tagclouds for the summarization of results from queries over thePubMed database of biomedical literature. PubCloud responds toqueries of this database with tag clouds generated from wordsextracted from the abstracts returned by the query. The results ofa user study comparing the PubCloud tag-cloud summarization ofquery results with the standard result list provided by PubMedindicated that the tag cloud interface is advantageous in presenting descriptive information and in reducing user frustrationbut that it is less effective at the task of enabling users to discoverrelations between concepts.", "keywords": ["text mining", "literature search", "natural language processing", "content summarization", "tagging", "pubmed", "visualization", "tag cloud"], "combined": "Tag clouds for summarizing web search results In this paper, we describe an application, PubCloud that uses tagclouds for the summarization of results from queries over thePubMed database of biomedical literature. PubCloud responds toqueries of this database with tag clouds generated from wordsextracted from the abstracts returned by the query. The results ofa user study comparing the PubCloud tag-cloud summarization ofquery results with the standard result list provided by PubMedindicated that the tag cloud interface is advantageous in presenting descriptive information and in reducing user frustrationbut that it is less effective at the task of enabling users to discoverrelations between concepts. [[EENNDD]] text mining; literature search; natural language processing; content summarization; tagging; pubmed; visualization; tag cloud"}, "Tag cloud untuk meringkaskan hasil carian web Dalam makalah ini, kami menerangkan sebuah aplikasi, PubCloud yang menggunakan tagclouds untuk meringkaskan hasil dari pertanyaan dari pangkalan data literatur bioperubatanPubMed. PubCloud bertindak balas terhadap pertanyaan pangkalan data ini dengan awan tag yang dihasilkan dari kata yang diekstrak dari abstrak yang dikembalikan oleh pertanyaan. Hasil kajian pengguna yang membandingkan hasil carian ringkasan tag-cloud PubCloud dengan senarai hasil standard yang disediakan oleh PubMedikasi bahawa antara muka awan tag bermanfaat dalam menyampaikan maklumat deskriptif dan dalam mengurangkan kekecewaan pengguna tetapi ia kurang berkesan untuk membolehkan pengguna penemuan hubungan antara konsep. [[EENNDD]] perlombongan teks; pencarian sastera; pemprosesan bahasa semula jadi; ringkasan kandungan; penandaan; ditayangkan; visualisasi; awan tag"], [{"string": "Learning to tag Social tagging provides valuable and crucial information for large-scale web image retrieval. It is ontology-free and easy to obtain; however, irrelevant tags frequently appear, and users typically will not tag all semantic objects in the image, which is also called semantic loss. To avoid noises and compensate for the semantic loss, tag recommendation is proposed in literature. However, current recommendation simply ranks the related tags based on the single modality of tag co-occurrence on the whole dataset, which ignores other modalities, such as visual correlation. This paper proposes a multi-modality recommendation based on both tag and visual correlation, and formulates the tag recommendation as a learning problem. Each modality is used to generate a ranking feature, and Rankboost algorithm is applied to learn an optimal combination of these ranking features from different modalities. Experiments on Flickr data demonstrate the effectiveness of this learning-based multi-modality recommendation strategy.", "keywords": ["multi-modality rankboost", "social tagging", "tag recommendation", "learning to tag"], "combined": "Learning to tag Social tagging provides valuable and crucial information for large-scale web image retrieval. It is ontology-free and easy to obtain; however, irrelevant tags frequently appear, and users typically will not tag all semantic objects in the image, which is also called semantic loss. To avoid noises and compensate for the semantic loss, tag recommendation is proposed in literature. However, current recommendation simply ranks the related tags based on the single modality of tag co-occurrence on the whole dataset, which ignores other modalities, such as visual correlation. This paper proposes a multi-modality recommendation based on both tag and visual correlation, and formulates the tag recommendation as a learning problem. Each modality is used to generate a ranking feature, and Rankboost algorithm is applied to learn an optimal combination of these ranking features from different modalities. Experiments on Flickr data demonstrate the effectiveness of this learning-based multi-modality recommendation strategy. [[EENNDD]] multi-modality rankboost; social tagging; tag recommendation; learning to tag"}, "Belajar memberi tag Penandaan sosial memberikan maklumat yang berharga dan penting untuk pengambilan gambar web berskala besar. Ia bebas ontologi dan senang didapati; namun, tag yang tidak relevan sering muncul, dan pengguna biasanya tidak akan menandai semua objek semantik dalam gambar, yang juga disebut kehilangan semantik. Untuk mengelakkan suara dan mengimbangi kehilangan semantik, cadangan tag dicadangkan dalam literatur. Walau bagaimanapun, cadangan semasa hanya menetapkan tag yang berkaitan berdasarkan satu-satu kesamaan tag pada keseluruhan dataset, yang mengabaikan kaedah lain, seperti korelasi visual. Makalah ini mengemukakan cadangan multi-modal berdasarkan korelasi tag dan visual, dan merumuskan cadangan tag sebagai masalah pembelajaran. Setiap modaliti digunakan untuk menghasilkan fitur peringkat, dan algoritma Rankboost diterapkan untuk mempelajari kombinasi optimum fitur peringkat ini dari berbagai cara. Eksperimen pada data Flickr menunjukkan keberkesanan strategi cadangan multi-modality berasaskan pembelajaran ini. [[EENNDD]] penunjuk peringkat pelbagai modal; penandaan sosial; cadangan teg; belajar memberi tag"], [{"string": "Soundness proof of Z semantics of OWL using institutions No contact information provided yet.", "keywords": ["owl", "comorphism of institutions", "institution", "z"], "combined": "Soundness proof of Z semantics of OWL using institutions No contact information provided yet. [[EENNDD]] owl; comorphism of institutions; institution; z"}, "Bukti kukuh semantik Z dari OWL menggunakan institusi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] burung hantu; persefahaman institusi; institusi; z"], [{"string": "Lessons from a Gnutella-web gateway No contact information provided yet.", "keywords": ["search engine", "gnutella", "peer-to-peer", "world wide web"], "combined": "Lessons from a Gnutella-web gateway No contact information provided yet. [[EENNDD]] search engine; gnutella; peer-to-peer; world wide web"}, "Pelajaran dari gerbang web Gnutella Belum ada maklumat hubungan yang diberikan. [[EENNDD]] enjin carian; gnutella; rakan sebaya; web seluruh dunia"], [{"string": "The web around the corner: augmenting the browser with gps No contact information provided yet.", "keywords": ["gps", "browser", "javascript", "lbs", "location-awareness"], "combined": "The web around the corner: augmenting the browser with gps No contact information provided yet. [[EENNDD]] gps; browser; javascript; lbs; location-awareness"}, "Web di sekitar sudut: menambah penyemak imbas dengan gps Belum ada maklumat hubungan yang disediakan. [[EENNDD]] gps; penyemak imbas; javascript; lbs; kesedaran lokasi"], [{"string": "Investigating web services on the world wide web Searching for Web service access points is no longer attached to service registries as Web search engines have become a new major source for discovering Web services. In this work, we conduct a thorough analytical investigation on the plurality of Web service interfaces that exist on the Web today. Using our Web Service Crawler Engine (WSCE), we collect metadata service information on retrieved interfaces through accessible UBRs, service portals and search engines. We use this data to determine Web service statistics and distribution based on object sizes, types of technologies employed, and the number of functioning services. This statistical data can be used to help determine the current status of Web services. We determine an intriguing result that 63% of the available Web services on the Web are considered to be active. We further use our findings to provide insights on improving the service retrieval process.", "keywords": ["service portals", "interface", "uddi", "wsce", "searching", "web services", "uddi business registries", "wsdl", "crawler", "crawling"], "combined": "Investigating web services on the world wide web Searching for Web service access points is no longer attached to service registries as Web search engines have become a new major source for discovering Web services. In this work, we conduct a thorough analytical investigation on the plurality of Web service interfaces that exist on the Web today. Using our Web Service Crawler Engine (WSCE), we collect metadata service information on retrieved interfaces through accessible UBRs, service portals and search engines. We use this data to determine Web service statistics and distribution based on object sizes, types of technologies employed, and the number of functioning services. This statistical data can be used to help determine the current status of Web services. We determine an intriguing result that 63% of the available Web services on the Web are considered to be active. We further use our findings to provide insights on improving the service retrieval process. [[EENNDD]] service portals; interface; uddi; wsce; searching; web services; uddi business registries; wsdl; crawler; crawling"}, "Menyelidiki perkhidmatan web di web seluruh dunia Mencari titik akses perkhidmatan Web tidak lagi melekat pada daftar perkhidmatan kerana mesin carian Web telah menjadi sumber utama baru untuk mencari perkhidmatan Web. Dalam karya ini, kami melakukan penyiasatan analitik menyeluruh mengenai pelbagai antara muka perkhidmatan Web yang ada di Web hari ini. Dengan menggunakan Web Service Crawler Engine (WSCE) kami, kami mengumpulkan maklumat perkhidmatan metadata pada antara muka yang diambil melalui UBR yang boleh diakses, portal perkhidmatan dan mesin carian. Kami menggunakan data ini untuk menentukan statistik dan penyebaran perkhidmatan Web berdasarkan ukuran objek, jenis teknologi yang digunakan, dan jumlah perkhidmatan yang berfungsi. Data statistik ini dapat digunakan untuk membantu menentukan status terkini perkhidmatan Web. Kami menentukan hasil yang menarik bahawa 63% perkhidmatan Web yang tersedia di Web dianggap aktif. Kami selanjutnya menggunakan penemuan kami untuk memberikan pandangan mengenai peningkatan proses pengambilan perkhidmatan. [[EENNDD]] portal perkhidmatan; antara muka; uddi; wsce; mencari; perkhidmatan web; daftar perniagaan uddi; wsdl; perangkak; merangkak"], [{"string": "Semantic search No contact information provided yet.", "keywords": ["information search and retrieval", "semantic web", "search"], "combined": "Semantic search No contact information provided yet. [[EENNDD]] information search and retrieval; semantic web; search"}, "Pencarian semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] carian dan pengambilan maklumat; web semantik; cari"], [{"string": "DIDO: a disease-determinants ontology from web sources This paper introduces DIDO, a system providing convenient access to knowledge about factors involved in human diseases, automatically extracted from textual Web sources. The knowledge base is bootstrapped by integrating entities from hand-crafted sources like MeSH and OMIM. As these are short on relationships between dierent types of biomedical entities, DIDO employs flexible and robust pattern learning and constraint-based reasoning methods to automatically extract new relational facts from textual sources. These facts can then be iteratively added to the knowledge base. The result is a semantic graph of typed entities and relations between diseases, their symptoms, and their factors, with emphasis on environmental factors but covering also molecular determinants. We demonstrate the value of DIDO for knowledge discovery about causal factors and properties of complex diseases, including factor-disease chains.", "keywords": ["disease factors", "general", "biomedical knowledge base", "ontology", "relation extraction"], "combined": "DIDO: a disease-determinants ontology from web sources This paper introduces DIDO, a system providing convenient access to knowledge about factors involved in human diseases, automatically extracted from textual Web sources. The knowledge base is bootstrapped by integrating entities from hand-crafted sources like MeSH and OMIM. As these are short on relationships between dierent types of biomedical entities, DIDO employs flexible and robust pattern learning and constraint-based reasoning methods to automatically extract new relational facts from textual sources. These facts can then be iteratively added to the knowledge base. The result is a semantic graph of typed entities and relations between diseases, their symptoms, and their factors, with emphasis on environmental factors but covering also molecular determinants. We demonstrate the value of DIDO for knowledge discovery about causal factors and properties of complex diseases, including factor-disease chains. [[EENNDD]] disease factors; general; biomedical knowledge base; ontology; relation extraction"}, "DIDO: ontologi penentu penyakit dari sumber web Makalah ini memperkenalkan DIDO, sistem yang menyediakan akses mudah ke pengetahuan mengenai faktor-faktor yang terlibat dalam penyakit manusia, secara automatik diekstrak dari sumber Web teks. Pangkalan pengetahuan diikat dengan mengintegrasikan entiti dari sumber buatan tangan seperti MeSH dan OMIM. Oleh kerana ini tidak berkaitan dengan hubungan antara jenis entiti bioperubatan yang berbeza, DIDO menggunakan kaedah pembelajaran fleksibel dan tegas dan kaedah penaakulan berdasarkan kekangan untuk secara automatik mengekstrak fakta hubungan baru dari sumber teks. Fakta-fakta ini kemudian dapat ditambahkan secara berulang ke pangkalan pengetahuan. Hasilnya adalah grafik semantik entiti yang ditaip dan hubungan antara penyakit, gejala mereka, dan faktornya, dengan penekanan pada faktor persekitaran tetapi meliputi juga penentu molekul. Kami menunjukkan nilai DIDO untuk penemuan pengetahuan mengenai faktor penyebab dan sifat penyakit kompleks, termasuk rantai faktor-penyakit. [[EENNDD]] faktor penyakit; umum; asas pengetahuan bioperubatan; ontologi; pengekstrakan hubungan"], [{"string": "A scalable machine-learning approach for semi-structured named entity recognition Named entity recognition studies the problem of locating and classifying parts of free text into a set of predefined categories. Although extensive research has focused on the detection of person, location and organization entities, there are many other entities of interest, including phone numbers, dates, times and currencies (to name a few examples). We refer to these types of entities as \"semi-structured named entities\", since they usually follow certain syntactic formats according to some conventions, although their structure is typically not well-defined. Regular expression solutions require significant amount of manual effort and supervised machine learning approaches rely on large sets of labeled training data. Therefore, these approaches do not scale when we need to support many semi-structured entity types in many languages and regions.", "keywords": ["boostrapping algorithm", "ner", "natural language processing", "weakly-supervised learning"], "combined": "A scalable machine-learning approach for semi-structured named entity recognition Named entity recognition studies the problem of locating and classifying parts of free text into a set of predefined categories. Although extensive research has focused on the detection of person, location and organization entities, there are many other entities of interest, including phone numbers, dates, times and currencies (to name a few examples). We refer to these types of entities as \"semi-structured named entities\", since they usually follow certain syntactic formats according to some conventions, although their structure is typically not well-defined. Regular expression solutions require significant amount of manual effort and supervised machine learning approaches rely on large sets of labeled training data. Therefore, these approaches do not scale when we need to support many semi-structured entity types in many languages and regions. [[EENNDD]] boostrapping algorithm; ner; natural language processing; weakly-supervised learning"}, "Pendekatan pembelajaran mesin yang boleh diskalakan untuk pengiktirafan entiti bernama semi-berstruktur Pengenalan entiti bernama mengkaji masalah mencari dan mengklasifikasikan bahagian teks bebas ke dalam sekumpulan kategori yang telah ditentukan. Walaupun penyelidikan yang luas telah menumpukan pada pengesanan entiti orang, lokasi dan organisasi, terdapat banyak entiti lain yang menarik, termasuk nombor telefon, tarikh, waktu dan mata wang (untuk menyebutkan beberapa contoh). Kami menyebut jenis entiti ini sebagai \"entiti bernama semi-berstruktur\", kerana mereka biasanya mengikuti format sintaksis tertentu menurut beberapa konvensyen, walaupun strukturnya biasanya tidak ditentukan dengan baik. Penyelesaian ekspresi biasa memerlukan sejumlah besar usaha manual dan pendekatan pembelajaran mesin yang diselia bergantung pada set besar data latihan berlabel. Oleh itu, pendekatan ini tidak mengikut skala apabila kita perlu menyokong banyak jenis entiti separa berstruktur dalam banyak bahasa dan wilayah. [[EENNDD]] meningkatkan algoritma ner; pemprosesan bahasa semula jadi; pembelajaran yang diselia dengan lemah"], [{"string": "OWL FA: a metamodeling extension of OWL D No contact information provided yet.", "keywords": ["ontology", "knowledge representation formalisms and methods", "reasoning", "metamodeling"], "combined": "OWL FA: a metamodeling extension of OWL D No contact information provided yet. [[EENNDD]] ontology; knowledge representation formalisms and methods; reasoning; metamodeling"}, "OWL FA: sambungan metamodelling OWL D Belum ada maklumat hubungan. [[EENNDD]] ontologi; formalisme dan kaedah perwakilan pengetahuan; penaakulan; metamodel"], [{"string": "Hearsay: enabling audio browsing on hypertext content No contact information provided yet.", "keywords": ["structural analysis", "semantic analysis", "html", "software architectures", "audio browser", "voicexml", "user interface", "world wide web"], "combined": "Hearsay: enabling audio browsing on hypertext content No contact information provided yet. [[EENNDD]] structural analysis; semantic analysis; html; software architectures; audio browser; voicexml; user interface; world wide web"}, "Hearsay: membolehkan penyemakan imbas audio pada kandungan hiperteks Belum ada maklumat hubungan yang diberikan. [[EENNDD]] analisis struktur; analisis semantik; html; seni bina perisian; penyemak imbas audio; suaraxml; antaramuka pengguna; web seluruh dunia"], [{"string": "Rapid prototyping of semantic mash-ups through semantic web pipes The use of RDF data published on the Web for applications is still a cumbersome and resource-intensive task due to the limited software support and the lack of standard programming paradigms to deal with everyday problems such as combination of RDF data from dierent sources, object identifier consolidation, ontology alignment and mediation, or plain querying and filtering tasks. In this paper we present a framework, Semantic Web Pipes, that supports fast implementation of Semantic data mash-ups while preserving desirable properties such as abstraction, encapsulation, component-orientation, code re-usability and maintainability which are common and well supported in other application areas.", "keywords": ["general", "pipes", "semantic web", "rdf", "mash-up"], "combined": "Rapid prototyping of semantic mash-ups through semantic web pipes The use of RDF data published on the Web for applications is still a cumbersome and resource-intensive task due to the limited software support and the lack of standard programming paradigms to deal with everyday problems such as combination of RDF data from dierent sources, object identifier consolidation, ontology alignment and mediation, or plain querying and filtering tasks. In this paper we present a framework, Semantic Web Pipes, that supports fast implementation of Semantic data mash-ups while preserving desirable properties such as abstraction, encapsulation, component-orientation, code re-usability and maintainability which are common and well supported in other application areas. [[EENNDD]] general; pipes; semantic web; rdf; mash-up"}, "Prototaip cepat penyusunan semantik melalui paip web semantik Penggunaan data RDF yang diterbitkan di Web untuk aplikasi masih merupakan tugas yang membebankan dan intensif sumber daya kerana sokongan perisian yang terhad dan kekurangan paradigma pengaturcaraan standard untuk menangani masalah sehari-hari seperti sebagai gabungan data RDF dari sumber yang berlainan, penyatuan pengenal objek, penyelarasan dan mediasi ontologi, atau tugas pertanyaan dan penyaringan biasa. Dalam makalah ini kami menyajikan kerangka kerja, Semantic Web Pipes, yang menyokong pelaksanaan cepat data Semantik mash-up sambil mengekalkan sifat-sifat yang diinginkan seperti abstraksi, enkapsulasi, komponen-orientasi, kegunaan semula kod dan pemeliharaan yang biasa dan disokong dengan baik di lain kawasan aplikasi. [[EENNDD]] umum; paip; web semantik; rdf; tumbuk"], [{"string": "ZenCrowd: leveraging probabilistic reasoning and crowdsourcing techniques for large-scale entity linking We tackle the problem of entity linking for large collections of online pages; Our system, ZenCrowd, identifies entities from natural language text using state of the art techniques and automatically connects them to the Linked Open Data cloud. We show how one can take advantage of human intelligence to improve the quality of the links by dynamically generating micro-tasks on an online crowdsourcing platform. We develop a probabilistic framework to make sensible decisions about candidate links and to identify unreliable human workers. We evaluate ZenCrowd in a real deployment and show how a combination of both probabilistic reasoning and crowdsourcing techniques can significantly improve the quality of the links, while limiting the amount of work performed by the crowd.", "keywords": ["probabilistic reasoning", "entity linking", "crowdsourcing", "linked data"], "combined": "ZenCrowd: leveraging probabilistic reasoning and crowdsourcing techniques for large-scale entity linking We tackle the problem of entity linking for large collections of online pages; Our system, ZenCrowd, identifies entities from natural language text using state of the art techniques and automatically connects them to the Linked Open Data cloud. We show how one can take advantage of human intelligence to improve the quality of the links by dynamically generating micro-tasks on an online crowdsourcing platform. We develop a probabilistic framework to make sensible decisions about candidate links and to identify unreliable human workers. We evaluate ZenCrowd in a real deployment and show how a combination of both probabilistic reasoning and crowdsourcing techniques can significantly improve the quality of the links, while limiting the amount of work performed by the crowd. [[EENNDD]] probabilistic reasoning; entity linking; crowdsourcing; linked data"}, "ZenCrowd: memanfaatkan penaakulan probabilistik dan teknik crowdsourcing untuk menghubungkan entiti berskala besar Kami menangani masalah pemautan entiti untuk koleksi laman dalam talian yang besar; Sistem kami, ZenCrowd, mengenal pasti entiti dari teks bahasa semula jadi menggunakan teknik canggih dan secara automatik menghubungkannya ke awan Data Terbuka Berkaitan. Kami menunjukkan bagaimana seseorang dapat memanfaatkan kecerdasan manusia untuk meningkatkan kualiti pautan dengan menghasilkan tugas mikro secara dinamik di platform sumber khalayak dalam talian. Kami mengembangkan kerangka probabilistik untuk membuat keputusan yang wajar mengenai hubungan calon dan untuk mengenal pasti pekerja manusia yang tidak boleh dipercayai. Kami menilai ZenCrowd secara nyata dan menunjukkan bagaimana gabungan kedua-dua teknik penaakulan probabilistik dan teknik crowdsourcing dapat meningkatkan kualiti pautan secara signifikan, sambil mengehadkan jumlah kerja yang dilakukan oleh orang ramai. [[EENNDD]] penaakulan probabilistik; menghubungkan entiti; sumber orang ramai; data terpaut"], [{"string": "OCTOPUS: aggressive search of multi-modality data using multifaceted knowledge base No contact information provided yet.", "keywords": ["multimedia retrieval", "layered graph model", "relevance feedback", "multi-modality data", "multifaceted knowledge base", "link analysis"], "combined": "OCTOPUS: aggressive search of multi-modality data using multifaceted knowledge base No contact information provided yet. [[EENNDD]] multimedia retrieval; layered graph model; relevance feedback; multi-modality data; multifaceted knowledge base; link analysis"}, "OCTOPUS: pencarian data multi-modality yang agresif menggunakan pangkalan pengetahuan pelbagai aspek Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengambilan multimedia; model grafik berlapis; maklum balas perkaitan; data pelbagai modal; pangkalan pengetahuan pelbagai aspek; analisis pautan"], [{"string": "Combining global optimization with local selection for efficient QoS-aware service composition The run-time binding of web services has been recently put forward in order to support rapid and dynamic web service compositions. With the growing number of alternative web services that provide the same functionality but differ in quality parameters, the service composition becomes a decision problem on which component services should be selected such that user's end-to-end QoS requirements (e.g. availability, response time) and preferences (e.g. price) are satisfied. Although very efficient, local selection strategy fails short in handling global QoS requirements. Solutions based on global optimization, on the other hand, can handle global constraints, but their poor performance renders them inappropriate for applications with dynamic and real-time requirements. In this paper we address this problem and propose a solution that combines global optimization with local selection techniques to benefit from the advantages of both worlds. The proposed solution consists of two steps: first, we use mixed integer programming (MIP) to find the optimal decomposition of global QoS constraints into local constraints. Second, we use distributed local selection to find the best web services that satisfy these local constraints. The results of experimental evaluation indicate that our approach significantly outperforms existing solutions in terms of computation time while achieving close-to-optimal results.", "keywords": ["optimization", "web services", "service composition", "qos"], "combined": "Combining global optimization with local selection for efficient QoS-aware service composition The run-time binding of web services has been recently put forward in order to support rapid and dynamic web service compositions. With the growing number of alternative web services that provide the same functionality but differ in quality parameters, the service composition becomes a decision problem on which component services should be selected such that user's end-to-end QoS requirements (e.g. availability, response time) and preferences (e.g. price) are satisfied. Although very efficient, local selection strategy fails short in handling global QoS requirements. Solutions based on global optimization, on the other hand, can handle global constraints, but their poor performance renders them inappropriate for applications with dynamic and real-time requirements. In this paper we address this problem and propose a solution that combines global optimization with local selection techniques to benefit from the advantages of both worlds. The proposed solution consists of two steps: first, we use mixed integer programming (MIP) to find the optimal decomposition of global QoS constraints into local constraints. Second, we use distributed local selection to find the best web services that satisfy these local constraints. The results of experimental evaluation indicate that our approach significantly outperforms existing solutions in terms of computation time while achieving close-to-optimal results. [[EENNDD]] optimization; web services; service composition; qos"}, "Menggabungkan pengoptimuman global dengan pilihan tempatan untuk komposisi perkhidmatan yang peka dengan QoS Pengikatan jangka masa perkhidmatan web baru-baru ini dikemukakan untuk menyokong komposisi perkhidmatan web yang cepat dan dinamik. Dengan bertambahnya bilangan perkhidmatan web alternatif yang menyediakan fungsi yang sama tetapi berbeza dalam parameter kualiti, komposisi perkhidmatan menjadi masalah keputusan perkhidmatan komponen mana yang harus dipilih sedemikian rupa sehingga keperluan QoS ujung ke ujung pengguna (misalnya ketersediaan, masa respons) dan pilihan (misalnya harga) berpuas hati. Walaupun sangat cekap, strategi pemilihan tempatan gagal menangani keperluan QoS global. Penyelesaian berdasarkan pengoptimuman global, sebaliknya, dapat mengatasi kekangan global, tetapi prestasi yang buruk menjadikannya tidak sesuai untuk aplikasi dengan keperluan dinamik dan masa nyata. Dalam makalah ini kami menangani masalah ini dan mencadangkan penyelesaian yang menggabungkan pengoptimuman global dengan teknik pemilihan tempatan untuk memanfaatkan kelebihan kedua dunia. Penyelesaian yang dicadangkan terdiri daripada dua langkah: pertama, kami menggunakan pengaturcaraan integer campuran (MIP) untuk mencari penguraian optimum kendala QoS global menjadi batasan tempatan. Kedua, kami menggunakan pilihan tempatan yang diedarkan untuk mencari perkhidmatan web terbaik yang memenuhi kekangan tempatan ini. Hasil penilaian eksperimental menunjukkan bahawa pendekatan kami jauh mengatasi penyelesaian yang ada dari segi masa pengiraan sambil mencapai hasil yang hampir dengan optimum. [[EENNDD]] pengoptimuman; perkhidmatan web; komposisi perkhidmatan; qos"], [{"string": "Geotracker: geospatial and temporal RSS navigation The Web is rapidly moving towards a platform for mass collaboration in content production and consumption. Fresh content on a variety of topics, people, and places is being created and made available on the Web at breathtaking speed. Navigating the content effectively not only requires techniques such as aggregating various RSS-enabled feeds, but it also demands a new browsing paradigm. In this paper, we present novel geospatial and temporal browsing techniques that provide users with the capability of aggregating and navigating RSS-enabled content in a timely, personalized and automatic manner. In particular, we describe a system called GeoTracker that utilizes both a geospatial representation and a temporal (chronological) presentation to help users spot the most relevant updates quickly. Within the context of this work, we provide a middleware engine that supports intelligent aggregation and dissemination of RSS feeds with personalization to desktops and mobile devices. We study the navigation capabilities of this system on two kinds of data sets, namely, 2006 World Cup soccer data collected over two months and breaking news items that occur every day. We also demonstrate that the application of such technologies to the video search results returned by YouTube and Google greatly enhances a user.s ability in locating and browsing videos based on his or her geographical interests. Finally, we demonstrate that the location inference performance of GeoTracker compares well against machine learning techniques used in the natural language processing/information retrieval community. Despite its algorithm simplicity, it preserves high recall percentages.", "keywords": ["rss", "blog", "geospatial tagging", "multimedia"], "combined": "Geotracker: geospatial and temporal RSS navigation The Web is rapidly moving towards a platform for mass collaboration in content production and consumption. Fresh content on a variety of topics, people, and places is being created and made available on the Web at breathtaking speed. Navigating the content effectively not only requires techniques such as aggregating various RSS-enabled feeds, but it also demands a new browsing paradigm. In this paper, we present novel geospatial and temporal browsing techniques that provide users with the capability of aggregating and navigating RSS-enabled content in a timely, personalized and automatic manner. In particular, we describe a system called GeoTracker that utilizes both a geospatial representation and a temporal (chronological) presentation to help users spot the most relevant updates quickly. Within the context of this work, we provide a middleware engine that supports intelligent aggregation and dissemination of RSS feeds with personalization to desktops and mobile devices. We study the navigation capabilities of this system on two kinds of data sets, namely, 2006 World Cup soccer data collected over two months and breaking news items that occur every day. We also demonstrate that the application of such technologies to the video search results returned by YouTube and Google greatly enhances a user.s ability in locating and browsing videos based on his or her geographical interests. Finally, we demonstrate that the location inference performance of GeoTracker compares well against machine learning techniques used in the natural language processing/information retrieval community. Despite its algorithm simplicity, it preserves high recall percentages. [[EENNDD]] rss; blog; geospatial tagging; multimedia"}, "Geotracker: navigasi RSS geospasial dan temporal Web dengan pantas bergerak menuju platform untuk kolaborasi besar-besaran dalam pengeluaran dan penggunaan kandungan. Isi segar mengenai berbagai topik, orang, dan tempat sedang dibuat dan tersedia di Web dengan kecepatan yang menakjubkan. Menavigasi kandungan dengan berkesan bukan sahaja memerlukan teknik seperti mengumpulkan pelbagai suapan berkemampuan RSS, tetapi juga menuntut paradigma penyemakan imbas baru. Dalam makalah ini, kami menyajikan teknik pelayaran geospasial dan temporal baru yang menyediakan kemampuan pengguna untuk mengagregasi dan menavigasi kandungan berkemampuan RSS secara tepat waktu, diperibadikan dan automatik. Khususnya, kami menerangkan sistem yang disebut GeoTracker yang menggunakan representasi geospasial dan persembahan temporal (kronologi) untuk membantu pengguna melihat kemas kini yang paling relevan dengan cepat. Dalam konteks kerja ini, kami menyediakan mesin middleware yang menyokong pengagregatan dan penyebaran feed RSS yang cerdas dengan pemperibadian ke desktop dan peranti mudah alih. Kami mengkaji keupayaan navigasi sistem ini pada dua jenis kumpulan data, iaitu data bola sepak Piala Dunia 2006 yang dikumpulkan selama dua bulan dan berita terkini yang berlaku setiap hari. Kami juga menunjukkan bahawa penerapan teknologi tersebut pada hasil carian video yang dikembalikan oleh YouTube dan Google sangat meningkatkan kemampuan pengguna dalam mencari dan melayari video berdasarkan minat geografinya. Akhirnya, kami menunjukkan bahawa prestasi inferensi lokasi GeoTracker dibandingkan dengan teknik pembelajaran mesin yang digunakan dalam komuniti pemprosesan bahasa / maklumat semula jadi. Walaupun kesederhanaan algoritma, ia mengekalkan peratusan penarikan yang tinggi. [[EENNDD]] rss; blog; penandaan geospatial; multimedia"], [{"string": "Automatic matchmaking of web services An abstract is not available.", "keywords": ["on-line information services", "matchmaking", "semantic web services"], "combined": "Automatic matchmaking of web services An abstract is not available. [[EENNDD]] on-line information services; matchmaking; semantic web services"}, "Pencocokan automatik perkhidmatan web Abstrak tidak tersedia. [[EENNDD]] perkhidmatan maklumat dalam talian; jodoh; perkhidmatan web semantik"], [{"string": "POLYPHONET: an advanced social network extraction system from the web No contact information provided yet.", "keywords": ["search engine", "information search and retrieval", "social network", "web mining"], "combined": "POLYPHONET: an advanced social network extraction system from the web No contact information provided yet. [[EENNDD]] search engine; information search and retrieval; social network; web mining"}, "POLYPHONET: sistem pengekstrakan rangkaian sosial yang maju dari web Belum ada maklumat hubungan. [[EENNDD]] enjin carian; pencarian dan pengambilan maklumat; rangkaian sosial; perlombongan web"], [{"string": "Personalizing E-commerce applications with on-line heuristic decision making An abstract is not available.", "keywords": ["pro-active intervention", "personalization", "b2c e-commerce", "decision support", "vortex rules system"], "combined": "Personalizing E-commerce applications with on-line heuristic decision making An abstract is not available. [[EENNDD]] pro-active intervention; personalization; b2c e-commerce; decision support; vortex rules system"}, "Memperibadikan aplikasi E-commerce dengan membuat keputusan heuristik dalam talian Abstrak tidak tersedia. [[EENNDD]] campur tangan pro-aktif; pemperibadian; e-dagang b2c; sokongan keputusan; sistem peraturan pusaran"], [{"string": "Serf and turf: crowdturfing for fun and profit Popular Internet services in recent years have shown that remarkable things can be achieved by harnessing the power of the masses using crowd-sourcing systems. However, crowd-sourcing systems can also pose a real challenge to existing security mechanisms deployed to protect Internet services. Many of these security techniques rely on the assumption that malicious activity is generated automatically by automated programs. Thus they would perform poorly or be easily bypassed when attacks are generated by real users working in a crowd-sourcing system. Through measurements, we have found surprising evidence showing that not only do malicious crowd-sourcing systems exist, but they are rapidly growing in both user base and total revenue. We describe in this paper a significant effort to study and understand these \"crowdturfing\" systems in today's Internet. We use detailed crawls to extract data about the size and operational structure of these crowdturfing systems. We analyze details of campaigns offered and performed in these sites, and evaluate their end-to-end effectiveness by running active, benign campaigns of our own. Finally, we study and compare the source of workers on crowdturfing sites in different countries. Our results suggest that campaigns on these systems are highly effective at reaching users, and their continuing growth poses a concrete threat to online communities both in the US and elsewhere.", "keywords": ["sybils", "spam", "crowdsourcing", "crowdturfing"], "combined": "Serf and turf: crowdturfing for fun and profit Popular Internet services in recent years have shown that remarkable things can be achieved by harnessing the power of the masses using crowd-sourcing systems. However, crowd-sourcing systems can also pose a real challenge to existing security mechanisms deployed to protect Internet services. Many of these security techniques rely on the assumption that malicious activity is generated automatically by automated programs. Thus they would perform poorly or be easily bypassed when attacks are generated by real users working in a crowd-sourcing system. Through measurements, we have found surprising evidence showing that not only do malicious crowd-sourcing systems exist, but they are rapidly growing in both user base and total revenue. We describe in this paper a significant effort to study and understand these \"crowdturfing\" systems in today's Internet. We use detailed crawls to extract data about the size and operational structure of these crowdturfing systems. We analyze details of campaigns offered and performed in these sites, and evaluate their end-to-end effectiveness by running active, benign campaigns of our own. Finally, we study and compare the source of workers on crowdturfing sites in different countries. Our results suggest that campaigns on these systems are highly effective at reaching users, and their continuing growth poses a concrete threat to online communities both in the US and elsewhere. [[EENNDD]] sybils; spam; crowdsourcing; crowdturfing"}, "Serf and turf: crowdturfing untuk keseronokan dan keuntungan Perkhidmatan Internet yang popular dalam beberapa tahun kebelakangan ini menunjukkan bahawa perkara yang luar biasa dapat dicapai dengan memanfaatkan kekuatan massa menggunakan sistem sumber orang ramai. Walau bagaimanapun, sistem sumber orang ramai juga dapat menimbulkan cabaran nyata bagi mekanisme keselamatan yang ada untuk melindungi perkhidmatan Internet. Sebilangan besar teknik keselamatan ini bergantung pada anggapan bahawa aktiviti jahat dihasilkan secara automatik oleh program automatik. Oleh itu, mereka akan berprestasi buruk atau mudah dilewati ketika serangan dihasilkan oleh pengguna sebenar yang bekerja dalam sistem sumber ramai. Melalui pengukuran, kami telah menemui bukti yang mengejutkan yang menunjukkan bahawa bukan sahaja terdapat sistem sumber orang ramai yang berniat jahat, tetapi juga berkembang dengan pesat di pangkalan pengguna dan jumlah pendapatan. Kami menerangkan dalam makalah ini suatu usaha yang signifikan untuk mengkaji dan memahami sistem \"crowdturfing\" ini di Internet masa kini. Kami menggunakan perayapan terperinci untuk mengekstrak data mengenai ukuran dan struktur operasi sistem crowdturfing ini. Kami menganalisis perincian kempen yang ditawarkan dan dilakukan di laman web ini, dan menilai keberkesanannya dari hujung ke hujung dengan menjalankan kempen aktif dan jinak kami sendiri. Akhirnya, kami mengkaji dan membandingkan sumber pekerja di laman crowdturfing di pelbagai negara. Hasil kami menunjukkan bahawa kempen pada sistem ini sangat berkesan untuk menjangkau pengguna, dan pertumbuhan berterusan mereka menimbulkan ancaman nyata bagi komuniti dalam talian di AS dan di tempat lain. [[EENNDD]] sybils; spam; sumber orang ramai; crowdturfing"], [{"string": "Towards autonomic web-sites based on learning automata No contact information provided yet.", "keywords": ["generalization", "autonomic website", "learning", "learning automata", "models"], "combined": "Towards autonomic web-sites based on learning automata No contact information provided yet. [[EENNDD]] generalization; autonomic website; learning; learning automata; models"}, "Ke arah laman web autonomi berdasarkan automata pembelajaran Belum ada maklumat hubungan yang diberikan. [[EENNDD]] generalisasi; laman web autonomi; belajar; belajar automata; model"], [{"string": "Mining directed social network from message board No contact information provided yet.", "keywords": ["internet message board", "directed social network", "computer-communication networks"], "combined": "Mining directed social network from message board No contact information provided yet. [[EENNDD]] internet message board; directed social network; computer-communication networks"}, "Perlombongan diarahkan rangkaian sosial dari papan mesej Belum ada maklumat hubungan yang diberikan. [[EENNDD]] papan pesanan internet; rangkaian sosial yang diarahkan; rangkaian komunikasi komputer"], [{"string": "Evaluating a new approach to strong web cache consistency with snapshots of collected content No contact information provided yet.", "keywords": ["applications", "cache consistency", "web caching", "change characteristics", "object composition", "server invalidation", "collected content", "object relationships"], "combined": "Evaluating a new approach to strong web cache consistency with snapshots of collected content No contact information provided yet. [[EENNDD]] applications; cache consistency; web caching; change characteristics; object composition; server invalidation; collected content; object relationships"}, "Menilai pendekatan baru untuk konsistensi cache web yang kuat dengan tangkapan kandungan yang dikumpulkan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] aplikasi; konsistensi cache; caching web; ubah ciri; komposisi objek; pembatalan pelayan; kandungan yang dikumpulkan; hubungan objek"], [{"string": "Towards a highly-scalable and effective metasearch engine An abstract is not available.", "keywords": ["distributed text database", "resource discovery", "database selection", "metasearch engine"], "combined": "Towards a highly-scalable and effective metasearch engine An abstract is not available. [[EENNDD]] distributed text database; resource discovery; database selection; metasearch engine"}, "Ke arah mesin carian metasearch yang sangat berskala dan berkesan Abstrak tidak tersedia. [[EENNDD]] pangkalan data teks diedarkan; penemuan sumber; pemilihan pangkalan data; enjin carian metas"], [{"string": "Hybrid multicasting in large-scale service networks No contact information provided yet.", "keywords": ["qos", "service composition", "multicast"], "combined": "Hybrid multicasting in large-scale service networks No contact information provided yet. [[EENNDD]] qos; service composition; multicast"}, "Multibahasa hibrid dalam rangkaian perkhidmatan berskala besar Belum ada maklumat hubungan yang diberikan. [[EENNDD]] qos; komposisi perkhidmatan; pelbagai siaran"], [{"string": "A large-scale study of web password habits We report the results of a large scale study of password use andpassword re-use habits. The study involved half a million users over athree month period. A client component on users' machines recorded a variety of password strength, usage and frequency metrics. This allows us to measure or estimate such quantities as the average number of passwords and average number of accounts each user has, how many passwords she types per day, how often passwords are shared among sites, and how often they are forgotten. We get extremely detailed data on password strength, the types and lengths of passwords chosen, and how they vary by site. The data is the first large scale study of its kind, and yields numerous other insights into the role the passwords play in users' online experience.", "keywords": ["measurements", "password"], "combined": "A large-scale study of web password habits We report the results of a large scale study of password use andpassword re-use habits. The study involved half a million users over athree month period. A client component on users' machines recorded a variety of password strength, usage and frequency metrics. This allows us to measure or estimate such quantities as the average number of passwords and average number of accounts each user has, how many passwords she types per day, how often passwords are shared among sites, and how often they are forgotten. We get extremely detailed data on password strength, the types and lengths of passwords chosen, and how they vary by site. The data is the first large scale study of its kind, and yields numerous other insights into the role the passwords play in users' online experience. [[EENNDD]] measurements; password"}, "Kajian skala besar tentang kebiasaan kata laluan web Kami melaporkan hasil kajian skala besar penggunaan kata laluan dan tabiat penggunaan semula kata laluan. Kajian ini melibatkan setengah juta pengguna dalam tempoh tiga bulan. Komponen pelanggan pada mesin pengguna mencatatkan pelbagai kekuatan kata laluan, penggunaan dan metrik frekuensi. Ini membolehkan kita mengukur atau menganggarkan kuantiti seperti jumlah kata laluan rata-rata dan jumlah akaun rata-rata setiap pengguna, berapa banyak kata laluan yang dia taip setiap hari, seberapa sering kata laluan dikongsi di antara laman web, dan seberapa sering mereka dilupakan. Kami mendapat data yang sangat terperinci mengenai kekuatan kata laluan, jenis dan panjang kata laluan yang dipilih, dan bagaimana ia berbeza mengikut laman web. Data ini merupakan kajian skala besar seumpamanya, dan menghasilkan banyak pandangan lain mengenai peranan kata laluan dalam pengalaman pengguna dalam talian. [[EENNDD]] pengukuran; kata laluan"], [{"string": "Supervised rank aggregation This paper is concerned with rank aggregation, the task of combining the ranking results of individual rankers at meta-search. Previously, rank aggregation was performed mainly by means of unsupervised learning. To further enhance ranking accuracies, we propose employing supervised learning to perform the task, using labeled data. We refer to the approach as Supervised Rank Aggregation. We set up a general framework for conducting Supervised Rank Aggregation, in which learning is formalized an optimization which minimizes disagreements between ranking results and the labeled data. As case study, we focus on Markov Chain based rank aggregation in this paper. The optimization for Markov Chain based methods is not a convex optimization problem, however, and thus is hard to solve. We prove that we can transform the optimization problem into that of Semidefinite Programming and solve it efficiently. Experimental results on meta-searches show that Supervised Rank Aggregation can significantly outperform existing unsupervised methods.", "keywords": ["semidefinite progromming", "supervised learning", "rank aggregation", "markov chain", "performance evaluation"], "combined": "Supervised rank aggregation This paper is concerned with rank aggregation, the task of combining the ranking results of individual rankers at meta-search. Previously, rank aggregation was performed mainly by means of unsupervised learning. To further enhance ranking accuracies, we propose employing supervised learning to perform the task, using labeled data. We refer to the approach as Supervised Rank Aggregation. We set up a general framework for conducting Supervised Rank Aggregation, in which learning is formalized an optimization which minimizes disagreements between ranking results and the labeled data. As case study, we focus on Markov Chain based rank aggregation in this paper. The optimization for Markov Chain based methods is not a convex optimization problem, however, and thus is hard to solve. We prove that we can transform the optimization problem into that of Semidefinite Programming and solve it efficiently. Experimental results on meta-searches show that Supervised Rank Aggregation can significantly outperform existing unsupervised methods. [[EENNDD]] semidefinite progromming; supervised learning; rank aggregation; markov chain; performance evaluation"}, "Pengagregatan peringkat yang diawasi Makalah ini berkaitan dengan pengagregatan peringkat, tugas menggabungkan hasil pemeringkatan peringkat individu pada pencarian meta. Sebelumnya, agregasi peringkat dilakukan terutamanya melalui pembelajaran tanpa pengawasan. Untuk meningkatkan lagi ketepatan peringkat, kami mencadangkan menggunakan pembelajaran yang diawasi untuk melaksanakan tugas, menggunakan data berlabel. Kami merujuk kepada pendekatan sebagai Agregasi Peringkat Terkawal. Kami menyusun kerangka umum untuk melakukan Pengumpulan Peringkat Terkawal, di mana pembelajaran diformalkan sebagai pengoptimuman yang meminimumkan ketidaksepakatan antara hasil peringkat dan data berlabel. Sebagai kajian kes, kami memfokuskan pada agregasi peringkat berdasarkan Markov Chain dalam makalah ini. Walau bagaimanapun, pengoptimuman untuk kaedah berdasarkan Markov Chain bukanlah masalah pengoptimuman cembung, dan sukar untuk diselesaikan. Kami membuktikan bahawa kami dapat mengubah masalah pengoptimuman menjadi Semidefinite Programming dan menyelesaikannya dengan cekap. Hasil eksperimen pada carian meta menunjukkan bahawa Gabungan Peringkat Terkawal secara signifikan dapat mengatasi kaedah yang tidak diselia yang ada. [[EENNDD]] progromming semidefinite; pembelajaran yang diselia; pengagregatan peringkat; rantaian markov; penilaian prestasi"], [{"string": "XML screamer: an integrated approach to high performance XML parsing, validation and deserialization No contact information provided yet.", "keywords": ["schema compilation", "sax", "xml schema", "xml", "validation", "jax-rpc"], "combined": "XML screamer: an integrated approach to high performance XML parsing, validation and deserialization No contact information provided yet. [[EENNDD]] schema compilation; sax; xml schema; xml; validation; jax-rpc"}, "Penjerit XML: pendekatan bersepadu untuk penghuraian, pengesahan dan deserialisasi XML berprestasi tinggi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penyusunan skema; saks; skema xml; xml; pengesahan; jax-rpc"], [{"string": "Web 2.0: blind to an accessible new world With the advent of Web 2.0 technologies, websites have evolved from static pages to dynamic, interactive Web-based applications with the ability to replicate common desktop functionality. However, for blind and visually impaired individuals who rely upon screen readers, Web 2.0 applications force them to adapt to an inaccessible use model. Many technologies, including WAI-ARIA, AJAX, and improved screen reader support, are rapidly evolving to improve this situation. However, simply combining them does not solve the problems of screen reader users. The main contributions of this paper are two models of interaction for screen reader users, for both traditional websites and Web 2.0 applications. Further contributions are a discussion of accessibility difficulties screen reader users encounter when interacting with Web 2.0 applications, a user workflow design model for improving Web 2.0 accessibility, and a set of design requirements for developers to ease the user's burden and increase accessibility. These models, accessibility difficulties, and design implications are based directly on responses and lessons learned from usability research focusing on Web 2.0 usage and screen reader users. Without the conscious effort of Web engineers and designers, most blind and visually impaired users will shy away from using new Web 2.0 technology in favor of desktop based applications.", "keywords": ["screen reader", "visually impaired", "web 2.0", "blind", "user models"], "combined": "Web 2.0: blind to an accessible new world With the advent of Web 2.0 technologies, websites have evolved from static pages to dynamic, interactive Web-based applications with the ability to replicate common desktop functionality. However, for blind and visually impaired individuals who rely upon screen readers, Web 2.0 applications force them to adapt to an inaccessible use model. Many technologies, including WAI-ARIA, AJAX, and improved screen reader support, are rapidly evolving to improve this situation. However, simply combining them does not solve the problems of screen reader users. The main contributions of this paper are two models of interaction for screen reader users, for both traditional websites and Web 2.0 applications. Further contributions are a discussion of accessibility difficulties screen reader users encounter when interacting with Web 2.0 applications, a user workflow design model for improving Web 2.0 accessibility, and a set of design requirements for developers to ease the user's burden and increase accessibility. These models, accessibility difficulties, and design implications are based directly on responses and lessons learned from usability research focusing on Web 2.0 usage and screen reader users. Without the conscious effort of Web engineers and designers, most blind and visually impaired users will shy away from using new Web 2.0 technology in favor of desktop based applications. [[EENNDD]] screen reader; visually impaired; web 2.0; blind; user models"}, "Web 2.0: buta dunia baru yang dapat diakses Dengan munculnya teknologi Web 2.0, laman web telah berkembang dari halaman statik menjadi aplikasi berasaskan Web yang dinamik dan interaktif dengan kemampuan untuk meniru fungsi desktop biasa. Namun, bagi individu buta dan cacat penglihatan yang bergantung pada pembaca skrin, aplikasi Web 2.0 memaksa mereka untuk menyesuaikan diri dengan model penggunaan yang tidak dapat diakses. Banyak teknologi, termasuk WAI-ARIA, AJAX, dan sokongan pembaca skrin yang lebih baik, berkembang dengan pesat untuk memperbaiki keadaan ini. Walau bagaimanapun, hanya menggabungkannya tidak menyelesaikan masalah pengguna pembaca skrin. Sumbangan utama makalah ini adalah dua model interaksi untuk pengguna pembaca skrin, untuk kedua-dua laman web tradisional dan aplikasi Web 2.0. Sumbangan selanjutnya adalah perbincangan mengenai kesukaran aksesibilitas yang dihadapi oleh pengguna pembaca layar ketika berinteraksi dengan aplikasi Web 2.0, model reka bentuk aliran kerja pengguna untuk meningkatkan aksesibilitas Web 2.0, dan sekumpulan keperluan reka bentuk untuk pembangun untuk meringankan beban pengguna dan meningkatkan aksesibilitas. Model-model ini, kesulitan aksesibilitas, dan implikasi reka bentuk didasarkan langsung pada respons dan pelajaran yang diperoleh dari penyelidikan kebolehgunaan yang memfokuskan pada penggunaan Web 2.0 dan pengguna pembaca skrin. Tanpa usaha sedar dari jurutera dan pereka Web, kebanyakan pengguna buta dan cacat penglihatan akan menjauhkan diri dari menggunakan teknologi Web 2.0 baru untuk aplikasi berasaskan desktop. [[EENNDD]] pembaca skrin; cacat penglihatan; laman web 2.0; buta; model pengguna"], [{"string": "Low-infrastructure methods to improve internet access for mobile users in emerging regions As information technology supports more aspects of modern life, digital access has become an important tool for developing regions to lift themselves from poverty. Though broadband internet connectivity will not be universally available in the short-term, widely-employed mobile devices coupled with novel delay-tolerant networking do allow limited forms of connectivity. This paper explores the design space for internet access systems operating with constrained connectivity. Our starting point is C-LINK, a collaborative caching system that enhances the performance of interactive web access over DTN and cellular connectivity. We discuss our experiences and results from deploying C-LINK in Nicaragua, before moving on to a broader design study of other issues that further influence operation. We consider the impact of (i) storing web content collaboratively cached across all user nodes, (ii) hybrid transport layers exploiting the best attributes of limited cellular and DTN-style connectivity. We also explore the behavior of future systems under a range of usage and mobility scenarios. Even under adverse conditions, our techniques can improve average service latency for page requests by a factor of 2X. Our results point to the considerable power of leveraging user mobility and collaboration in providing very-low-infrastructure internet access to developing regions.", "keywords": ["systems and software", "delay tolerant networking", "network architecture and design", "caching", "mobility", "simulation"], "combined": "Low-infrastructure methods to improve internet access for mobile users in emerging regions As information technology supports more aspects of modern life, digital access has become an important tool for developing regions to lift themselves from poverty. Though broadband internet connectivity will not be universally available in the short-term, widely-employed mobile devices coupled with novel delay-tolerant networking do allow limited forms of connectivity. This paper explores the design space for internet access systems operating with constrained connectivity. Our starting point is C-LINK, a collaborative caching system that enhances the performance of interactive web access over DTN and cellular connectivity. We discuss our experiences and results from deploying C-LINK in Nicaragua, before moving on to a broader design study of other issues that further influence operation. We consider the impact of (i) storing web content collaboratively cached across all user nodes, (ii) hybrid transport layers exploiting the best attributes of limited cellular and DTN-style connectivity. We also explore the behavior of future systems under a range of usage and mobility scenarios. Even under adverse conditions, our techniques can improve average service latency for page requests by a factor of 2X. Our results point to the considerable power of leveraging user mobility and collaboration in providing very-low-infrastructure internet access to developing regions. [[EENNDD]] systems and software; delay tolerant networking; network architecture and design; caching; mobility; simulation"}, "Kaedah infrastruktur rendah untuk meningkatkan akses internet untuk pengguna mudah alih di wilayah yang baru muncul Oleh kerana teknologi maklumat menyokong lebih banyak aspek kehidupan moden, akses digital telah menjadi alat penting untuk membangun wilayah untuk melepaskan diri dari kemiskinan. Walaupun penyambungan internet jalur lebar tidak akan tersedia secara universal dalam jangka pendek, peranti mudah alih yang digunakan secara meluas dengan rangkaian toleransi penangguhan baru membenarkan bentuk penyambungan yang terhad. Makalah ini meneroka ruang reka bentuk untuk sistem akses internet yang beroperasi dengan kesambungan yang terhad. Titik permulaan kami adalah C-LINK, sistem cache kolaboratif yang meningkatkan prestasi akses web interaktif melalui DTN dan sambungan selular. Kami membincangkan pengalaman dan hasil kami dari menyebarkan C-LINK di Nikaragua, sebelum beralih ke kajian reka bentuk yang lebih luas mengenai isu-isu lain yang lebih mempengaruhi operasi. Kami mempertimbangkan kesan (i) menyimpan kandungan web yang di-cache secara kolaboratif di semua nod pengguna, (ii) lapisan pengangkutan hibrid yang memanfaatkan atribut terbaik penyambungan gaya selular dan DTN yang terhad. Kami juga meneroka tingkah laku sistem masa depan di bawah pelbagai senario penggunaan dan mobiliti. Walaupun dalam keadaan buruk, teknik kami dapat meningkatkan latensi perkhidmatan rata-rata untuk permintaan halaman dengan faktor 2X. Hasil kami menunjukkan kekuatan yang cukup besar untuk meningkatkan mobilitas dan kolaborasi pengguna dalam menyediakan akses internet infrastruktur yang sangat rendah ke wilayah yang sedang membangun. [[EENNDD]] sistem dan perisian; melambatkan rangkaian toleransi; seni bina dan reka bentuk rangkaian; caching; mobiliti; simulasi"], [{"string": "Detecting image spam using local invariant features and pyramid match kernel Image spam is a new obfuscating method which spammers invented to more effectively bypass conventional text based spam filters. In this paper, we extract local invariant features of images and run a one-class SVM classifier which uses the pyramid match kernel as the kernel function to detect image spam. Experimental results demonstrate that our algorithm is effective for fighting image spam.", "keywords": ["image spam", "pyramid match kernel", "local invariant features"], "combined": "Detecting image spam using local invariant features and pyramid match kernel Image spam is a new obfuscating method which spammers invented to more effectively bypass conventional text based spam filters. In this paper, we extract local invariant features of images and run a one-class SVM classifier which uses the pyramid match kernel as the kernel function to detect image spam. Experimental results demonstrate that our algorithm is effective for fighting image spam. [[EENNDD]] image spam; pyramid match kernel; local invariant features"}, "Mengesan spam gambar menggunakan ciri invariant tempatan dan kernel pencocokan piramid Spam gambar adalah kaedah baru yang tidak menyenangkan yang dicipta oleh spammer untuk memintas penapis spam berasaskan teks konvensional dengan lebih berkesan. Dalam makalah ini, kami mengekstrak ciri gambar invarian tempatan dan menjalankan pengklasifikasi SVM satu kelas yang menggunakan kernel padam piramid sebagai fungsi kernel untuk mengesan spam gambar. Hasil eksperimen menunjukkan bahawa algoritma kami berkesan untuk memerangi spam gambar. [[EENNDD]] spam gambar; kernel padam piramid; ciri invarian tempatan"], [{"string": "Improving pseudo-relevance feedback in web information retrieval using web page segmentation No contact information provided yet.", "keywords": ["hypertext/hypermedia", "page segmentation", "query expansion", "web information retrieval"], "combined": "Improving pseudo-relevance feedback in web information retrieval using web page segmentation No contact information provided yet. [[EENNDD]] hypertext/hypermedia; page segmentation; query expansion; web information retrieval"}, "Meningkatkan maklum balas pseudo-relevan dalam pengambilan maklumat web menggunakan segmentasi halaman web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] hiperteks / hipermedia; pembahagian halaman; pengembangan pertanyaan; pengambilan maklumat web"], [{"string": "Behavioral classification on the click graph A bipartite query-URL graph, where an edge indicates that a document was clicked for a query, is a useful construct for finding groups of related queries and URLs. Here we use this behavior graph for classification. We choose a click graph sampled from two weeks of image search activity, and the task of \"adult\" filtering: identifying content in the graph that is inappropriate for minors. We show how to perform classification using random walks on this graph, and two methods for estimating classifier parameters.", "keywords": ["click data", "learning", "classification", "content analysis and indexing"], "combined": "Behavioral classification on the click graph A bipartite query-URL graph, where an edge indicates that a document was clicked for a query, is a useful construct for finding groups of related queries and URLs. Here we use this behavior graph for classification. We choose a click graph sampled from two weeks of image search activity, and the task of \"adult\" filtering: identifying content in the graph that is inappropriate for minors. We show how to perform classification using random walks on this graph, and two methods for estimating classifier parameters. [[EENNDD]] click data; learning; classification; content analysis and indexing"}, "Klasifikasi tingkah laku pada grafik klik Grafik URL-pertanyaan bipartit, di mana kelebihan menunjukkan bahawa dokumen diklik untuk pertanyaan, adalah konstruk yang berguna untuk mencari kumpulan pertanyaan dan URL yang berkaitan. Di sini kita menggunakan grafik tingkah laku ini untuk klasifikasi. Kami memilih grafik klik yang diambil dari aktiviti pencarian gambar selama dua minggu, dan tugas menyaring \"dewasa\": mengenal pasti kandungan dalam grafik yang tidak sesuai untuk anak di bawah umur. Kami menunjukkan cara melakukan klasifikasi menggunakan jalan rawak pada grafik ini, dan dua kaedah untuk menganggarkan parameter pengkelasan. [[EENNDD]] data klik; belajar; pengelasan; analisis kandungan dan pengindeksan"], [{"string": "Opinion integration through semi-supervised topic modeling Web 2.0 technology has enabled more and more people to freely express their opinions on the Web, making the Web an extremely valuable source for mining user opinions about all kinds of topics. In this paper we study how to automatically integrate opinions expressed in a well-written expert review with lots of opinions scattering in various sources such as blogspaces and forums. We formally define this new integration problem and propose to use semi-supervised topic models to solve the problem in a principled way. Experiments on integrating opinions about two quite different topics (a product and a political figure) show that the proposed method is effective for both topics and can generate useful aligned integrated opinion summaries. The proposed method is quite general. It can be used to integrate a well written review with opinions in an arbitrary text collection about any topic to potentially support many interesting applications in multiple domains.", "keywords": ["semi-supervised", "information search and retrieval", "expert review", "opinion integration", "probabilistic topic modeling"], "combined": "Opinion integration through semi-supervised topic modeling Web 2.0 technology has enabled more and more people to freely express their opinions on the Web, making the Web an extremely valuable source for mining user opinions about all kinds of topics. In this paper we study how to automatically integrate opinions expressed in a well-written expert review with lots of opinions scattering in various sources such as blogspaces and forums. We formally define this new integration problem and propose to use semi-supervised topic models to solve the problem in a principled way. Experiments on integrating opinions about two quite different topics (a product and a political figure) show that the proposed method is effective for both topics and can generate useful aligned integrated opinion summaries. The proposed method is quite general. It can be used to integrate a well written review with opinions in an arbitrary text collection about any topic to potentially support many interesting applications in multiple domains. [[EENNDD]] semi-supervised; information search and retrieval; expert review; opinion integration; probabilistic topic modeling"}, "Penyatuan pendapat melalui pemodelan topik separa diselia teknologi Web 2.0 telah membolehkan lebih banyak orang untuk bebas menyampaikan pendapat mereka di Web, menjadikan Web sebagai sumber yang sangat berharga untuk melombong pendapat pengguna tentang semua jenis topik. Dalam makalah ini kami mengkaji bagaimana mengintegrasikan pendapat secara automatik yang dinyatakan dalam tinjauan pakar yang ditulis dengan banyak pendapat yang tersebar di pelbagai sumber seperti ruang blog dan forum. Kami secara formal mendefinisikan masalah integrasi baru ini dan mencadangkan untuk menggunakan model topik separa penyeliaan untuk menyelesaikan masalah dengan prinsip. Eksperimen mengintegrasikan pendapat mengenai dua topik yang sangat berbeza (produk dan tokoh politik) menunjukkan bahawa kaedah yang dicadangkan berkesan untuk kedua-dua topik tersebut dan dapat menghasilkan ringkasan pendapat bersepadu yang berguna. Kaedah yang dicadangkan agak umum. Ini dapat digunakan untuk mengintegrasikan tinjauan yang ditulis dengan baik dengan pendapat dalam koleksi teks sewenang-wenangnya mengenai topik apa pun untuk berpotensi menyokong banyak aplikasi menarik dalam beberapa domain. [[EENNDD]] separa diselia; carian dan pengambilan maklumat; kajian pakar; penyatuan pendapat; pemodelan topik probabilistik"], [{"string": "A novel collaborative filtering-based framework for personalized services in m-commerce With the rapid growth of wireless technologies and handheld devices, m-commerce is becoming a promising research area. Personalization is especially important to the success of m-commerce. This paper proposes a novel collaborative filtering-based framework for personalized services in m-commerce. The framework extends our previous work by using Online Analytical Processing (OLAP) to represent the relations among user, content and context information, and adopting a multi-dimensional collaborative filtering model to perform inference. It provides a powerful and well-founded mechanism to personalization for m-commerce. We implemented it in an existing m-commerce platform, and experimental results demonstrate its feasibility and correctness.", "keywords": ["collaborative filtering", "personalization", "m-commerce"], "combined": "A novel collaborative filtering-based framework for personalized services in m-commerce With the rapid growth of wireless technologies and handheld devices, m-commerce is becoming a promising research area. Personalization is especially important to the success of m-commerce. This paper proposes a novel collaborative filtering-based framework for personalized services in m-commerce. The framework extends our previous work by using Online Analytical Processing (OLAP) to represent the relations among user, content and context information, and adopting a multi-dimensional collaborative filtering model to perform inference. It provides a powerful and well-founded mechanism to personalization for m-commerce. We implemented it in an existing m-commerce platform, and experimental results demonstrate its feasibility and correctness. [[EENNDD]] collaborative filtering; personalization; m-commerce"}, "Kerangka kerja penapisan kolaboratif baru untuk perkhidmatan yang diperibadikan dalam m-commerce Dengan pertumbuhan pesat teknologi tanpa wayar dan peranti genggam, m-commerce menjadi bidang penyelidikan yang menjanjikan. Pemperibadian sangat penting untuk kejayaan m-commerce. Makalah ini mencadangkan kerangka kerja penapisan kolaboratif baru untuk perkhidmatan yang diperibadikan dalam m-commerce. Kerangka kerja memperluas karya kami sebelumnya dengan menggunakan Pemprosesan Analisis Dalam Talian (OLAP) untuk mewakili hubungan antara pengguna, kandungan dan maklumat konteks, dan mengadopsi model penapisan kolaboratif multi-dimensi untuk melakukan inferensi. Ini menyediakan mekanisme yang kuat dan mantap untuk memperibadikan m-commerce. Kami menerapkannya dalam platform m-commerce yang ada, dan hasil eksperimen menunjukkan kelayakan dan kebenarannya. [[EENNDD]] penapisan kolaboratif; pemperibadian; m-dagang"], [{"string": "Generating diverse and representative image search results for landmarks Can we leverage the community-contributed collections of rich media on the web to automatically generate representative and diverse views of the world's landmarks? We use a combination of context- and content-based tools to generate representative sets of images for location-driven features and landmarks, a common search task. To do that, we using location and other metadata, as well as tags associated with images, and the images' visual features. We present an approach to extracting tags that represent landmarks. We show how to use unsupervised methods to extract representative views and images for each landmark. This approach can potentially scale to provide better search and representation for landmarks, worldwide. We evaluate the system in the context of image search using a real-life dataset of 110,000 images from the San Francisco area.", "keywords": ["photo collections", "social media", "geo-referenced photographs", "miscellaneous"], "combined": "Generating diverse and representative image search results for landmarks Can we leverage the community-contributed collections of rich media on the web to automatically generate representative and diverse views of the world's landmarks? We use a combination of context- and content-based tools to generate representative sets of images for location-driven features and landmarks, a common search task. To do that, we using location and other metadata, as well as tags associated with images, and the images' visual features. We present an approach to extracting tags that represent landmarks. We show how to use unsupervised methods to extract representative views and images for each landmark. This approach can potentially scale to provide better search and representation for landmarks, worldwide. We evaluate the system in the context of image search using a real-life dataset of 110,000 images from the San Francisco area. [[EENNDD]] photo collections; social media; geo-referenced photographs; miscellaneous"}, "Menjana hasil carian gambar yang pelbagai dan representatif untuk mercu tanda Bolehkah kita memanfaatkan koleksi media kaya yang disumbangkan oleh komuniti di web untuk menjana paparan mercu tanda dunia yang mewakili dan pelbagai secara automatik? Kami menggunakan gabungan alat berdasarkan konteks dan kandungan untuk menghasilkan set gambar yang representatif untuk ciri dan mercu tanda berdasarkan lokasi, tugas carian biasa. Untuk melakukan itu, kami menggunakan lokasi dan metadata lain, serta tag yang berkaitan dengan gambar, dan ciri visual gambar. Kami menyajikan pendekatan untuk mengekstrak tag yang mewakili mercu tanda. Kami menunjukkan cara menggunakan kaedah tanpa pengawasan untuk mengekstrak pandangan dan gambar perwakilan untuk setiap mercu tanda. Pendekatan ini berpotensi membuat skala untuk memberikan pencarian dan perwakilan yang lebih baik untuk mercu tanda, di seluruh dunia. Kami menilai sistem dalam konteks carian gambar menggunakan set data kehidupan sebenar 110,000 gambar dari kawasan San Francisco. [[EENNDD]] koleksi gambar; media sosial; gambar rujukan geo; pelbagai"], [{"string": "A pruning-based approach for supporting Top-K join queries No contact information provided yet.", "keywords": ["prune", "top-k", "join query"], "combined": "A pruning-based approach for supporting Top-K join queries No contact information provided yet. [[EENNDD]] prune; top-k; join query"}, "Pendekatan berasaskan pemangkasan untuk menyokong pertanyaan gabungan Top-K Belum ada maklumat hubungan yang diberikan. [[EENNDD]] prune; bahagian atas-k; sertai pertanyaan"], [{"string": "QUBE: a quick algorithm for updating betweenness centrality The betweenness centrality of a vertex in a graph is a measure for the participation of the vertex in the shortest paths in the graph. The Betweenness centrality is widely used in network analyses. Especially in a social network, the recursive computation of the betweenness centralities of vertices is performed for the community detection and finding the influential user in the network. Since a social network graph is frequently updated, it is necessary to update the betweenness centrality efficiently. When a graph is changed, the betweenness centralities of all the vertices should be recomputed from scratch using all the vertices in the graph. To the best of our knowledge, this is the first work that proposes an efficient algorithm which handles the update of the betweenness centralities of vertices in a graph. In this paper, we propose a method that efficiently reduces the search space by finding a candidate set of vertices whose betweenness centralities can be updated and computes their betweenness centeralities using candidate vertices only. As the cost of calculating the betweenness centrality mainly depends on the number of vertices to be considered, the proposed algorithm significantly reduces the cost of calculation. The proposed algorithm allows the transformation of an existing algorithm which does not consider the graph update. Experimental results on large real datasets show that the proposed algorithm speeds up the existing algorithm 2 to 2418 times depending on the dataset.", "keywords": ["update algorithm", "betweenness centrality"], "combined": "QUBE: a quick algorithm for updating betweenness centrality The betweenness centrality of a vertex in a graph is a measure for the participation of the vertex in the shortest paths in the graph. The Betweenness centrality is widely used in network analyses. Especially in a social network, the recursive computation of the betweenness centralities of vertices is performed for the community detection and finding the influential user in the network. Since a social network graph is frequently updated, it is necessary to update the betweenness centrality efficiently. When a graph is changed, the betweenness centralities of all the vertices should be recomputed from scratch using all the vertices in the graph. To the best of our knowledge, this is the first work that proposes an efficient algorithm which handles the update of the betweenness centralities of vertices in a graph. In this paper, we propose a method that efficiently reduces the search space by finding a candidate set of vertices whose betweenness centralities can be updated and computes their betweenness centeralities using candidate vertices only. As the cost of calculating the betweenness centrality mainly depends on the number of vertices to be considered, the proposed algorithm significantly reduces the cost of calculation. The proposed algorithm allows the transformation of an existing algorithm which does not consider the graph update. Experimental results on large real datasets show that the proposed algorithm speeds up the existing algorithm 2 to 2418 times depending on the dataset. [[EENNDD]] update algorithm; betweenness centrality"}, "QUBE: algoritma pantas untuk mengemas kini sentraliti jarak antara pusat Titik antara titik dalam graf adalah ukuran untuk penyertaan bucu dalam lintasan terpendek dalam grafik. Sentraliti Betweenness banyak digunakan dalam analisis rangkaian. Terutama dalam rangkaian sosial, pengiraan rekursif antara pusat antara simpul dilakukan untuk pengesanan masyarakat dan mencari pengguna yang berpengaruh dalam rangkaian. Oleh kerana grafik rangkaian sosial sering dikemas kini, adalah perlu untuk mengemas kini pusat antara dengan cekap. Apabila graf diubah, pusat antara jarak semua bucu harus dikira semula dari awal menggunakan semua bucu dalam grafik. Sepengetahuan kami, ini adalah karya pertama yang mencadangkan algoritma yang cekap yang menangani kemas kini pusat antara simpul dalam graf. Dalam makalah ini, kami mencadangkan kaedah yang dapat mengurangkan ruang pencarian dengan berkesan dengan mencari set simpul calon yang pusatnya antara dapat diperbaharui dan menghitung pusat antara mereka hanya menggunakan simpul calon. Oleh kerana kos pengiraan sentralitas antara bergantung terutamanya pada bilangan bucu yang akan dipertimbangkan, algoritma yang dicadangkan mengurangkan kos pengiraan dengan ketara. Algoritma yang dicadangkan membolehkan transformasi algoritma yang ada yang tidak mempertimbangkan kemas kini grafik. Hasil eksperimen pada set data nyata yang besar menunjukkan bahawa algoritma yang dicadangkan mempercepat algoritma yang ada 2 hingga 2418 kali bergantung pada set data. [[EENNDD]] algoritma kemas kini; sentraliti antara"], [{"string": "Providing ranked relevant results for web database queries No contact information provided yet.", "keywords": ["query suggestion", "web-enabled database", "content similarity"], "combined": "Providing ranked relevant results for web database queries No contact information provided yet. [[EENNDD]] query suggestion; web-enabled database; content similarity"}, "Menyediakan keputusan yang relevan untuk pertanyaan pangkalan data web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] cadangan pertanyaan; pangkalan data berkelayakan web; persamaan kandungan"], [{"string": "Web-scale entity-relation search architecture Enabling entity search and ranking at Web-scale is fraught with many challenges: annotating the corpus with entities and types, query language design, index design, query processing logic, and answer consolidation. We describe a Web-scale entity search engine we are building to handle over a billion Web pages, over 200,000 types, over 1,500,000 entities, and hundreds of entity annotations per page. We describe the design of compressed, token span oriented indices for entity and type annotations. Our prototype demonstrates the practicality of Web-scale entity-relation search.", "keywords": ["systems and software", "information search and retrieval", "entity-relation search", "index design", "web-scale"], "combined": "Web-scale entity-relation search architecture Enabling entity search and ranking at Web-scale is fraught with many challenges: annotating the corpus with entities and types, query language design, index design, query processing logic, and answer consolidation. We describe a Web-scale entity search engine we are building to handle over a billion Web pages, over 200,000 types, over 1,500,000 entities, and hundreds of entity annotations per page. We describe the design of compressed, token span oriented indices for entity and type annotations. Our prototype demonstrates the practicality of Web-scale entity-relation search. [[EENNDD]] systems and software; information search and retrieval; entity-relation search; index design; web-scale"}, "Senibina carian hubungan entiti berskala Web Mengaktifkan carian dan pemeringkatan entiti pada skala Web penuh dengan banyak cabaran: memberi penjelasan mengenai korpus dengan entiti dan jenis, reka bentuk bahasa pertanyaan, reka bentuk indeks, logik pemprosesan pertanyaan, dan penyatuan jawapan. Kami menerangkan mesin carian entiti berskala Web yang kami bangun untuk mengendalikan lebih dari satu bilion laman web, lebih dari 200,000 jenis, lebih dari 1,500,000 entiti, dan beratus-ratus anotasi entiti setiap halaman. Kami menerangkan reka bentuk indeks berorientasikan rentang token yang dimampatkan untuk anotasi entiti dan jenis. Prototaip kami menunjukkan kepraktisan carian hubungan entiti berskala Web. [[EENNDD]] sistem dan perisian; pencarian dan pengambilan maklumat; carian hubungan entiti; reka bentuk indeks; skala web"], [{"string": "Wireless SOAP: optimizations for mobile wireless web services No contact information provided yet.", "keywords": ["wireless", "applications", "protocol architecture", "standards", "services", "compression", "web services", "soap", "networks", "wsdl"], "combined": "Wireless SOAP: optimizations for mobile wireless web services No contact information provided yet. [[EENNDD]] wireless; applications; protocol architecture; standards; services; compression; web services; soap; networks; wsdl"}, "SOAP Tanpa Wayar: pengoptimuman untuk perkhidmatan web tanpa wayar mudah alih Belum ada maklumat hubungan yang diberikan. [[EENNDD]] tanpa wayar; permohonan; seni bina protokol; standard; perkhidmatan; pemampatan; perkhidmatan web; sabun; rangkaian; wsdl"], [{"string": "Integrating the document object model with hyperlinks for enhanced topic distillation and information extraction An abstract is not available.", "keywords": ["document object model", "topic distillation", "segmentation", "minimum description length principle"], "combined": "Integrating the document object model with hyperlinks for enhanced topic distillation and information extraction An abstract is not available. [[EENNDD]] document object model; topic distillation; segmentation; minimum description length principle"}, "Mengintegrasikan model objek dokumen dengan pautan hiper untuk penyulingan topik yang dipertingkatkan dan pengekstrakan maklumat Abstrak tidak tersedia. [[EENNDD]] model objek dokumen; penyulingan topik; segmentasi; prinsip panjang keterangan minimum"], [{"string": "Practical semantic analysis of web sites and documents No contact information provided yet.", "keywords": ["web engineering", "knowledge management", "web sites", "information system", "content management", "quality", "web site evolution", "formal semantics", "xml", "consistency", "logic programming"], "combined": "Practical semantic analysis of web sites and documents No contact information provided yet. [[EENNDD]] web engineering; knowledge management; web sites; information system; content management; quality; web site evolution; formal semantics; xml; consistency; logic programming"}, "Analisis semantik praktikal laman web dan dokumen Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kejuruteraan web; pengurusan pengetahuan; laman web; sistem informasi; pengurusan kandungan; kualiti; evolusi laman web; semantik formal; xml; ketekalan; pengaturcaraan logik"], [{"string": "Web taxonomy integration using support vector machines No contact information provided yet.", "keywords": ["classification", "transductive learning", "taxonomy integration", "semantic web", "ontology mapping", "support vector machines"], "combined": "Web taxonomy integration using support vector machines No contact information provided yet. [[EENNDD]] classification; transductive learning; taxonomy integration; semantic web; ontology mapping; support vector machines"}, "Penyatuan taksonomi web menggunakan mesin vektor sokongan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] klasifikasi; pembelajaran transduktif; penyatuan taksonomi; web semantik; pemetaan ontologi; mesin vektor sokongan"], [{"string": "The webgraph framework I: compression techniques No contact information provided yet.", "keywords": ["general", "compression", "web graph"], "combined": "The webgraph framework I: compression techniques No contact information provided yet. [[EENNDD]] general; compression; web graph"}, "Kerangka webgraf I: teknik pemampatan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] umum; pemampatan; grafik web"], [{"string": "Crawling multiple UDDI business registries As Web services proliferate, size and magnitude of UDDI Business Registries (UBRs) are likely to increase. The ability to discover Web services of interest then across multiple UBRs becomes a major challenge specially when using primitive search methods provided by existing UDDI APIs. Clients do not have the time to endlessly search accessible UBRs for finding appropriate services particularly when operating via mobile devices. Finding services of interest should be time effective and highly productive. This paper addresses issues relating to the efficient access and discovery of Web services across multiple UBRs and introduces a novel exploration engine, the Web Service Crawler Engine (WSCE). WSCE is capable of crawling multiple UBRs, and enables for the establishment of a centralized Web services repository that can be used for discovering Web services much more efficiently. The paper presents experimental validation, results, and analysis of the proposed ideas.", "keywords": ["uddi", "discovery", "uddi business registries", "web services", "crawler"], "combined": "Crawling multiple UDDI business registries As Web services proliferate, size and magnitude of UDDI Business Registries (UBRs) are likely to increase. The ability to discover Web services of interest then across multiple UBRs becomes a major challenge specially when using primitive search methods provided by existing UDDI APIs. Clients do not have the time to endlessly search accessible UBRs for finding appropriate services particularly when operating via mobile devices. Finding services of interest should be time effective and highly productive. This paper addresses issues relating to the efficient access and discovery of Web services across multiple UBRs and introduces a novel exploration engine, the Web Service Crawler Engine (WSCE). WSCE is capable of crawling multiple UBRs, and enables for the establishment of a centralized Web services repository that can be used for discovering Web services much more efficiently. The paper presents experimental validation, results, and analysis of the proposed ideas. [[EENNDD]] uddi; discovery; uddi business registries; web services; crawler"}, "Merangkak berbilang daftar perniagaan UDDI Oleh kerana perkhidmatan Web semakin bertambah, saiz dan besarnya Daftar Perniagaan UDDI (UBR) cenderung meningkat. Keupayaan untuk menemui perkhidmatan Web yang menarik di pelbagai UBR menjadi cabaran utama terutamanya ketika menggunakan kaedah carian primitif yang disediakan oleh API UDDI yang ada. Pelanggan tidak mempunyai masa untuk mencari UBR yang dapat diakses tanpa henti untuk mencari perkhidmatan yang sesuai terutama ketika beroperasi melalui peranti mudah alih. Mencari perkhidmatan yang menarik mestilah berkesan pada masa dan sangat produktif. Makalah ini membahas isu-isu yang berkaitan dengan akses dan penemuan perkhidmatan Web yang cekap di pelbagai UBR dan memperkenalkan mesin eksplorasi baru, Mesin Perkhidmatan Penjelajah Web (WSCE). WSCE mampu merangkak beberapa UBR, dan memungkinkan untuk mewujudkan repositori perkhidmatan Web terpusat yang dapat digunakan untuk menemui perkhidmatan Web dengan lebih berkesan. Makalah ini menunjukkan pengesahan eksperimen, hasil, dan analisis idea yang dicadangkan. [[EENNDD]] uddi; penemuan; daftar perniagaan uddi; perkhidmatan web; perangkak"], [{"string": "Pushing reactive services to XML repositories using active rules An abstract is not available.", "keywords": ["document management", "web", "active rules", "xml", "push technology", "soap", "query languages for xml"], "combined": "Pushing reactive services to XML repositories using active rules An abstract is not available. [[EENNDD]] document management; web; active rules; xml; push technology; soap; query languages for xml"}, "Mendorong perkhidmatan reaktif ke repositori XML menggunakan peraturan aktif Abstrak tidak tersedia. [[EENNDD]] pengurusan dokumen; laman web; peraturan aktif; xml; teknologi tolak; sabun; bahasa pertanyaan untuk xml"], [{"string": "Building a distributed full-text index for the Web An abstract is not available.", "keywords": ["inverted files", "text retrieval", "pipelining", "distributed indexing", "embedded databases"], "combined": "Building a distributed full-text index for the Web An abstract is not available. [[EENNDD]] inverted files; text retrieval; pipelining; distributed indexing; embedded databases"}, "Membina indeks teks penuh diedarkan untuk Web Abstrak tidak tersedia. [[EENNDD]] fail terbalik; pengambilan teks; pemasangan paip; pengindeksan diedarkan; pangkalan data terbenam"], [{"string": "Co-browsing dynamic web pages Collaborative browsing, or co-browsing, is the co-navigation of the web with other people at-a-distance, supported by software that takes care of synchronizing the browsers. Current state-of-the-art solutions are able to do co-browsing of \"static web pages\", and do not support the synchronization of JavaScript interactions. However, currently many web pages use JavaScript and Ajax techniques to create highly dynamic and interactive web applications. In this paper, we describe two approaches for co-browsing that both support the synchronization of the JavaScript and Ajax interactions of dynamic web pages. One approach is based on synchronizing the output of the JavaScript engine by sending over the changes made on the DOM tree. The other approach is based on synchronizing the input of the JavaScript engine by synchronizing UI events and incoming data. Since the latter solution offers a better user experience and is more scalable, it is elaborated in more detail. An important aspect of both approaches is that they operate at the DOM level. Therefore, the client-side can be implemented in JavaScript and no browser extensions are required. To the best of the authors' knowledge this is the first DOM-level co-browsing solution that also enables co-browsing of the dynamic interaction parts of web pages. The presented co-browsing solution has been implemented in a research demonstrator which allows users to do co-browsing of web-applications on browser-based networked televisions.", "keywords": ["collaborative computing", "shared browsing", "co-browsing", "collaboration", "web4ce"], "combined": "Co-browsing dynamic web pages Collaborative browsing, or co-browsing, is the co-navigation of the web with other people at-a-distance, supported by software that takes care of synchronizing the browsers. Current state-of-the-art solutions are able to do co-browsing of \"static web pages\", and do not support the synchronization of JavaScript interactions. However, currently many web pages use JavaScript and Ajax techniques to create highly dynamic and interactive web applications. In this paper, we describe two approaches for co-browsing that both support the synchronization of the JavaScript and Ajax interactions of dynamic web pages. One approach is based on synchronizing the output of the JavaScript engine by sending over the changes made on the DOM tree. The other approach is based on synchronizing the input of the JavaScript engine by synchronizing UI events and incoming data. Since the latter solution offers a better user experience and is more scalable, it is elaborated in more detail. An important aspect of both approaches is that they operate at the DOM level. Therefore, the client-side can be implemented in JavaScript and no browser extensions are required. To the best of the authors' knowledge this is the first DOM-level co-browsing solution that also enables co-browsing of the dynamic interaction parts of web pages. The presented co-browsing solution has been implemented in a research demonstrator which allows users to do co-browsing of web-applications on browser-based networked televisions. [[EENNDD]] collaborative computing; shared browsing; co-browsing; collaboration; web4ce"}, "Melayari laman web dinamik bersama Melayari kolaboratif, atau melayari bersama, adalah pelayaran bersama web dengan orang lain pada jarak yang jauh, disokong oleh perisian yang menjaga penyegerakan penyemak imbas. Penyelesaian canggih semasa dapat melakukan penyemakan imbas \"halaman web statik\", dan tidak menyokong penyegerakan interaksi JavaScript. Namun, pada masa ini banyak laman web menggunakan teknik JavaScript dan Ajax untuk membuat aplikasi web yang sangat dinamik dan interaktif. Dalam makalah ini, kami menerangkan dua pendekatan untuk melayari bersama yang kedua-duanya menyokong penyegerakan interaksi JavaScript dan Ajax dari laman web dinamik. Satu pendekatan didasarkan pada penyegerakan output mesin JavaScript dengan mengirimkan perubahan yang dibuat pada pohon DOM. Pendekatan lain didasarkan pada penyegerakan input mesin JavaScript dengan menyegerakkan peristiwa UI dan data masuk. Oleh kerana penyelesaian terakhir menawarkan pengalaman pengguna yang lebih baik dan lebih berskala, ia dihuraikan dengan lebih terperinci. Aspek penting dari kedua pendekatan tersebut ialah mereka beroperasi di peringkat DOM. Oleh itu, sisi klien dapat dilaksanakan dalam JavaScript dan tidak memerlukan pelanjutan penyemak imbas. Untuk pengetahuan penulis, ini adalah penyelesaian pelayaran bersama peringkat DOM pertama yang juga membolehkan penyemakan imbas bahagian interaksi dinamik dari laman web. Penyelesaian penjelajahan bersama yang disajikan telah diimplementasikan dalam demonstrasi penelitian yang memungkinkan pengguna melakukan penyemakan imbas aplikasi web di televisyen jaringan berasaskan penyemak imbas. [[EENNDD]] pengkomputeran kolaboratif; melayari bersama; melayari bersama; kerjasama; web4ce"], [{"string": "A generic uiml vocabulary for device- and modality independent user interfaces No contact information provided yet.", "keywords": ["multimodality", "graphical user interfaces", "generic user interface description", "mobile devices", "mobile networks", "device-independence", "interaction styles", "voice interfaces", "multimodal user interfaces", "uiml"], "combined": "A generic uiml vocabulary for device- and modality independent user interfaces No contact information provided yet. [[EENNDD]] multimodality; graphical user interfaces; generic user interface description; mobile devices; mobile networks; device-independence; interaction styles; voice interfaces; multimodal user interfaces; uiml"}, "Perbendaharaan kata uiml generik untuk antara muka pengguna bebas peranti dan modaliti Belum ada maklumat hubungan yang diberikan. [[EENNDD]] multimodaliti; antara muka pengguna grafik; penerangan antara muka pengguna generik; peranti mudah alih; rangkaian mudah alih; kebebasan peranti; gaya interaksi; antara muka suara; antara muka pengguna multimodal; uiml"], [{"string": "Data summaries for on-demand queries over linked data Typical approaches for querying structured Web Data collect (crawl) and pre-process (index) large amounts of data in a central data repository before allowing for query answering. However, this time-consuming pre-processing phase however leverages the benefits of Linked Data -- where structured data is accessible live and up-to-date at distributed Web resources that may change constantly -- only to a limited degree, as query results can never be current. An ideal query answering system for Linked Data should return current answers in a reasonable amount of time, even on corpora as large as the Web. Query processors evaluating queries directly on the live sources require knowledge of the contents of data sources. In this paper, we develop and evaluate an approximate index structure summarising graph-structured content of sources adhering to Linked Data principles, provide an algorithm for answering conjunctive queries over Linked Data on theWeb exploiting the source summary, and evaluate the system using synthetically generated queries. The experimental results show that our lightweight index structure enables complete and up-to-date query results over Linked Data, while keeping the overhead for querying low and providing a satisfying source ranking at no additional cost.", "keywords": ["rdf querying", "linked data", "index structures"], "combined": "Data summaries for on-demand queries over linked data Typical approaches for querying structured Web Data collect (crawl) and pre-process (index) large amounts of data in a central data repository before allowing for query answering. However, this time-consuming pre-processing phase however leverages the benefits of Linked Data -- where structured data is accessible live and up-to-date at distributed Web resources that may change constantly -- only to a limited degree, as query results can never be current. An ideal query answering system for Linked Data should return current answers in a reasonable amount of time, even on corpora as large as the Web. Query processors evaluating queries directly on the live sources require knowledge of the contents of data sources. In this paper, we develop and evaluate an approximate index structure summarising graph-structured content of sources adhering to Linked Data principles, provide an algorithm for answering conjunctive queries over Linked Data on theWeb exploiting the source summary, and evaluate the system using synthetically generated queries. The experimental results show that our lightweight index structure enables complete and up-to-date query results over Linked Data, while keeping the overhead for querying low and providing a satisfying source ranking at no additional cost. [[EENNDD]] rdf querying; linked data; index structures"}, "Ringkasan data untuk pertanyaan atas permintaan atas data yang dihubungkan Pendekatan khas untuk membuat pengumpulan data Web terstruktur (merangkak) dan pra-proses (indeks) sejumlah besar data di repositori pusat data sebelum memungkinkan untuk menjawab pertanyaan. Walau bagaimanapun, fasa pra-pemprosesan yang memakan masa ini memanfaatkan manfaat Data Terpaut - di mana data berstruktur dapat diakses secara langsung dan terkini di sumber Web yang diedarkan yang mungkin berubah secara berterusan - hanya pada tahap terhad, sebagai hasil pertanyaan tidak pernah dapat semasa. Sistem menjawab pertanyaan yang sesuai untuk Data Terhubung harus mengembalikan jawapan semasa dalam jangka masa yang munasabah, bahkan di korporat seluas Web. Pemproses pertanyaan yang menilai pertanyaan secara langsung pada sumber langsung memerlukan pengetahuan mengenai kandungan sumber data. Dalam makalah ini, kami mengembangkan dan menilai struktur indeks anggaran yang merangkum kandungan sumber berstruktur grafik yang berpegang pada prinsip-prinsip Data Tertaut, menyediakan algoritma untuk menjawab pertanyaan konjungtif daripada Data Terpaut di Web yang memanfaatkan ringkasan sumber, dan menilai sistem menggunakan pertanyaan yang dihasilkan secara sintetik. . Hasil eksperimen menunjukkan bahawa struktur indeks ringan kami membolehkan hasil pertanyaan lengkap dan terkini melalui Data Terhubung, sambil mengekalkan overhead untuk membuat pertanyaan rendah dan memberikan peringkat sumber yang memuaskan tanpa kos tambahan. [[EENNDD]] pertanyaan rdf; data yang dipautkan; struktur indeks"], [{"string": "Towards liquid service oriented architectures The advent of Cloud computing platforms, and the growing pervasiveness of Multicore processor architectures have revealed the inadequateness of traditional programming models based on sequential computations, opening up many challenges for research on parallel programming models for building distributed, service-oriented systems. More in detail, the dynamic nature of Cloud computing and its virtualized infrastructure pose new challenges in term of application design, deployment and dynamic reconfiguration. An application developed to be delivered as a service in the Cloud has to deal with poorly understood issues such as elasticity, infinite scalability and portability across heterogeneous virtualized environments. In this position paper we define the problem of providing a novel parallel programming model for building application services that can be transparently deployed on multicore and cloud execution environments. To this end, we introduce and motivate a research plan for the definition of a novel programming framework for Web service-based applications. Our vision called \"Liquid Architecture\" is based on a programming model inspired by core ideas tied to the REST architectural style coupled with a self-configuring runtime that allows transparent deployment of Web services on a broad range of heterogeneous platforms, from multicores to clouds.", "keywords": ["general", "liquid architectures", "web services", "rest", "programming models"], "combined": "Towards liquid service oriented architectures The advent of Cloud computing platforms, and the growing pervasiveness of Multicore processor architectures have revealed the inadequateness of traditional programming models based on sequential computations, opening up many challenges for research on parallel programming models for building distributed, service-oriented systems. More in detail, the dynamic nature of Cloud computing and its virtualized infrastructure pose new challenges in term of application design, deployment and dynamic reconfiguration. An application developed to be delivered as a service in the Cloud has to deal with poorly understood issues such as elasticity, infinite scalability and portability across heterogeneous virtualized environments. In this position paper we define the problem of providing a novel parallel programming model for building application services that can be transparently deployed on multicore and cloud execution environments. To this end, we introduce and motivate a research plan for the definition of a novel programming framework for Web service-based applications. Our vision called \"Liquid Architecture\" is based on a programming model inspired by core ideas tied to the REST architectural style coupled with a self-configuring runtime that allows transparent deployment of Web services on a broad range of heterogeneous platforms, from multicores to clouds. [[EENNDD]] general; liquid architectures; web services; rest; programming models"}, "Ke arah seni bina berorientasikan perkhidmatan cair Kemunculan platform pengkomputeran Cloud, dan penyebaran seni bina pemproses Multicore yang semakin meningkat telah menunjukkan ketidakcukupan model pengaturcaraan tradisional berdasarkan pengiraan berurutan, membuka banyak cabaran untuk penyelidikan mengenai model pengaturcaraan selari untuk bangunan yang diedarkan, berorientasikan perkhidmatan sistem. Lebih terperinci, sifat pengkomputeran Cloud yang dinamik dan infrastruktur maya menimbulkan cabaran baru dari segi reka bentuk aplikasi, penyebaran dan konfigurasi semula dinamik. Aplikasi yang dibangunkan untuk disampaikan sebagai perkhidmatan di Cloud harus menangani masalah yang kurang difahami seperti keanjalan, skalabilitas yang tidak terbatas dan mudah alih di persekitaran maya yang heterogen. Dalam makalah posisi ini kami mendefinisikan masalah penyediaan model pengaturcaraan selari baru untuk membina perkhidmatan aplikasi yang dapat diterapkan secara transparan pada lingkungan pelaksanaan multicore dan cloud. Untuk tujuan ini, kami memperkenalkan dan memotivasi rancangan penelitian untuk definisi kerangka pengaturcaraan baru untuk aplikasi berasaskan perkhidmatan Web. Visi kami yang disebut \"Liquid Architecture\" didasarkan pada model pengaturcaraan yang diilhami oleh idea teras yang terikat dengan gaya seni bina REST yang digabungkan dengan runtime yang dikonfigurasi sendiri yang membolehkan penyebaran perkhidmatan Web secara telus pada pelbagai platform heterogen, dari multicores hingga cloud. [[EENNDD]] umum; seni bina cecair; perkhidmatan web; berehat; model pengaturcaraan"], [{"string": "Best bets: thousands of queries in search of a client No contact information provided yet.", "keywords": ["proactive content delivery", "search", "information retrieval", "query"], "combined": "Best bets: thousands of queries in search of a client No contact information provided yet. [[EENNDD]] proactive content delivery; search; information retrieval; query"}, "Pertaruhan terbaik: beribu-ribu pertanyaan untuk mencari pelanggan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penyampaian kandungan yang proaktif; cari; pengambilan maklumat; pertanyaan"], [{"string": "Rapid development of spreadsheet-based web mashups The rapid growth of social networking sites and web communities have motivated web sites to expose their APIs to external developers who create mashups by assembling existing functionalities. Current APIs, however, aim toward developers with programming expertise; they are not directly usable by wider class of users who do not have programming background, but would nevertheless like to build their own mashups. To address this need, we propose a spreadsheet-based Web mashups development framework, which enables users to develop mashups in the popular spreadsheet environment. First, we provide a mechanism that makes structured data first class values of spreadsheet cells. Second, we propose a new component model that can be used to develop fairly sophisticated mashups, involving joining data sources and keeping spreadsheet data up to date. Third, to simplify mashup development, we provide a collection of spreadsheet-based mashup patterns that captures common Web data access and spreadsheet presentation functionalities. Users can reuse and customize these patterns to build spreadsheet-based Web mashups instead of developing them from scratch. Fourth, we enable users to manipulate structured data presented on spreadsheet in a drag-and-drop fashion. Finally, we have developed and tested a proof-of-concept prototype to demonstrate the utility of the proposed framework.", "keywords": ["graphical user interfaces", "spreadsheets", "component model", "spreadsheet-based mashup patterns", "web data mashups", "interaction styles"], "combined": "Rapid development of spreadsheet-based web mashups The rapid growth of social networking sites and web communities have motivated web sites to expose their APIs to external developers who create mashups by assembling existing functionalities. Current APIs, however, aim toward developers with programming expertise; they are not directly usable by wider class of users who do not have programming background, but would nevertheless like to build their own mashups. To address this need, we propose a spreadsheet-based Web mashups development framework, which enables users to develop mashups in the popular spreadsheet environment. First, we provide a mechanism that makes structured data first class values of spreadsheet cells. Second, we propose a new component model that can be used to develop fairly sophisticated mashups, involving joining data sources and keeping spreadsheet data up to date. Third, to simplify mashup development, we provide a collection of spreadsheet-based mashup patterns that captures common Web data access and spreadsheet presentation functionalities. Users can reuse and customize these patterns to build spreadsheet-based Web mashups instead of developing them from scratch. Fourth, we enable users to manipulate structured data presented on spreadsheet in a drag-and-drop fashion. Finally, we have developed and tested a proof-of-concept prototype to demonstrate the utility of the proposed framework. [[EENNDD]] graphical user interfaces; spreadsheets; component model; spreadsheet-based mashup patterns; web data mashups; interaction styles"}, "Perkembangan cepat mashup web berasaskan spreadsheet Pertumbuhan pesat laman rangkaian sosial dan komuniti web telah mendorong laman web untuk mendedahkan API mereka kepada pemaju luar yang membuat mashup dengan mengumpulkan fungsi yang ada. API semasa, bagaimanapun, bertujuan untuk pemaju dengan kepakaran pengaturcaraan; mereka tidak dapat digunakan secara langsung oleh kelas pengguna yang lebih luas yang tidak memiliki latar belakang pengaturcaraan, namun mereka ingin membina mashup mereka sendiri. Untuk mengatasi keperluan ini, kami mengusulkan kerangka kerja pengembangan mashup Web berasaskan spreadsheet, yang memungkinkan pengguna mengembangkan mashup di lingkungan spreadsheet yang popular. Pertama, kami menyediakan mekanisme yang menjadikan data berstruktur nilai kelas sel spreadsheet. Kedua, kami mencadangkan model komponen baru yang dapat digunakan untuk mengembangkan mashup yang cukup canggih, yang melibatkan penggabungan sumber data dan mengemas kini data spreadsheet. Ketiga, untuk mempermudah pengembangan mashup, kami menyediakan koleksi pola mashup berdasarkan spreadsheet yang menangkap fungsi akses data Web dan persembahan spreadsheet biasa. Pengguna dapat menggunakan semula dan menyesuaikan corak ini untuk membina mashup Web berasaskan spreadsheet dan bukannya mengembangkannya dari awal. Keempat, kami membolehkan pengguna memanipulasi data berstruktur yang disajikan pada spreadsheet secara drag-and-drop. Akhirnya, kami telah mengembangkan dan menguji prototaip bukti konsep untuk menunjukkan kegunaan kerangka kerja yang dicadangkan. [[EENNDD]] antara muka pengguna grafik; hamparan; model komponen; corak mashup berasaskan hamparan; mashup data web; gaya interaksi"], [{"string": "Equip tourists with knowledge mined from travelogues With the prosperity of tourism and Web 2.0 technologies, more and more people have willingness to share their travel experiences on the Web (e.g., weblogs, forums, or Web 2.0 communities). These so-called travelogues contain rich information, particularly including location-representative knowledge such as attractions (e.g., Golden Gate Bridge), styles (e.g., beach, history), and activities (e.g., diving, surfing). The location-representative information in travelogues can greatly facilitate other tourists' trip planning, if it can be correctly extracted and summarized. However, since most travelogues are unstructured and contain much noise, it is difficult for common users to utilize such knowledge effectively. In this paper, to mine location-representative knowledge from a large collection of travelogues, we propose a probabilistic topic model, named as Location-Topic model. This model has the advantages of (1) differentiability between two kinds of topics, i.e., local topics which characterize locations and global topics which represent other common themes shared by various locations, and (2) representation of locations in the local topic space to encode both location-representative knowledge and similarities between locations. Some novel applications are developed based on the proposed model, including (1) destination recommendation for on flexible queries, (2) characteristic summarization for a given destination with representative tags and snippets, and (3) identification of informative parts of a travelogue and enriching such highlights with related images. Based on a large collection of travelogues, the proposed framework is evaluated using both objective and subjective evaluation methods and shows promising results.", "keywords": ["travelogue mining", "recommendation", "probabilistic topic model"], "combined": "Equip tourists with knowledge mined from travelogues With the prosperity of tourism and Web 2.0 technologies, more and more people have willingness to share their travel experiences on the Web (e.g., weblogs, forums, or Web 2.0 communities). These so-called travelogues contain rich information, particularly including location-representative knowledge such as attractions (e.g., Golden Gate Bridge), styles (e.g., beach, history), and activities (e.g., diving, surfing). The location-representative information in travelogues can greatly facilitate other tourists' trip planning, if it can be correctly extracted and summarized. However, since most travelogues are unstructured and contain much noise, it is difficult for common users to utilize such knowledge effectively. In this paper, to mine location-representative knowledge from a large collection of travelogues, we propose a probabilistic topic model, named as Location-Topic model. This model has the advantages of (1) differentiability between two kinds of topics, i.e., local topics which characterize locations and global topics which represent other common themes shared by various locations, and (2) representation of locations in the local topic space to encode both location-representative knowledge and similarities between locations. Some novel applications are developed based on the proposed model, including (1) destination recommendation for on flexible queries, (2) characteristic summarization for a given destination with representative tags and snippets, and (3) identification of informative parts of a travelogue and enriching such highlights with related images. Based on a large collection of travelogues, the proposed framework is evaluated using both objective and subjective evaluation methods and shows promising results. [[EENNDD]] travelogue mining; recommendation; probabilistic topic model"}, "Melengkapkan pelancong dengan pengetahuan yang ditambang dari travelogues Dengan kemakmuran pelancongan dan teknologi Web 2.0, semakin banyak orang mempunyai kesediaan untuk berkongsi pengalaman perjalanan mereka di Web (mis. Blog web, forum, atau komuniti Web 2.0). Travelogues yang disebut ini mengandungi banyak maklumat, terutamanya termasuk pengetahuan perwakilan lokasi seperti tarikan (mis. Jambatan Golden Gate), gaya (mis., Pantai, sejarah), dan aktiviti (mis. Menyelam, melayari). Maklumat perwakilan lokasi dalam travelog dapat memudahkan perancangan perjalanan pelancong lain, jika dapat diekstrak dan diringkaskan dengan betul. Namun, oleh kerana kebanyakan travelog tidak berstruktur dan mengandungi banyak kebisingan, sukar bagi pengguna biasa untuk menggunakan pengetahuan tersebut dengan berkesan. Dalam makalah ini, untuk menimba pengetahuan perwakilan lokasi dari koleksi besar travelog, kami mencadangkan model topik probabilistik, dinamakan sebagai model Lokasi-Topik. Model ini mempunyai kelebihan (1) pembezaan antara dua jenis topik, yaitu, topik tempatan yang mencirikan lokasi dan topik global yang mewakili tema umum lain yang dikongsi oleh berbagai lokasi, dan (2) perwakilan lokasi di ruang topik tempatan untuk dikodkan pengetahuan perwakilan lokasi dan persamaan antara lokasi. Beberapa aplikasi novel dikembangkan berdasarkan model yang diusulkan, termasuk (1) saranan tujuan untuk pertanyaan fleksibel, (2) ringkasan karakteristik untuk tujuan tertentu dengan tag perwakilan dan potongan, dan (3) pengenalpastian bahagian informatif dari sebuah perjalanan dan memperkaya sorotan sedemikian dengan gambar yang berkaitan. Berdasarkan koleksi besar travelog, kerangka kerja yang dicadangkan dinilai menggunakan kaedah penilaian objektif dan subjektif dan menunjukkan hasil yang menjanjikan. [[EENNDD]] perlombongan travelog; cadangan; model topik probabilistik"], [{"string": "Factal: integrating deep web based on trust and relevance We demonstrate \"Factal\"--a system for integrating deep web sources. Factal is based on the recently introduced source selection method \"SourceRank\"; which is a measure of trust and relevance based on the agreement between the sources. SourceRank selects popular and trustworthy sources from autonomous and open collections like the deep web. This trust and popularity awareness distinguishes Factal from the existing systems like Google Product Search. Factal selects and searches active online databases on multiple domains. The demonstration scenarios include improved trustworthiness, relevance of results, and comparison shopping. We believe that by incorporating effective source selection based on the SourceRank, Factal demonstrates a significant step towards a deep-web-scale integration system.", "keywords": ["source selection", "web integration", "sourcerank", "deep web"], "combined": "Factal: integrating deep web based on trust and relevance We demonstrate \"Factal\"--a system for integrating deep web sources. Factal is based on the recently introduced source selection method \"SourceRank\"; which is a measure of trust and relevance based on the agreement between the sources. SourceRank selects popular and trustworthy sources from autonomous and open collections like the deep web. This trust and popularity awareness distinguishes Factal from the existing systems like Google Product Search. Factal selects and searches active online databases on multiple domains. The demonstration scenarios include improved trustworthiness, relevance of results, and comparison shopping. We believe that by incorporating effective source selection based on the SourceRank, Factal demonstrates a significant step towards a deep-web-scale integration system. [[EENNDD]] source selection; web integration; sourcerank; deep web"}, "Fakta: mengintegrasikan web dalam berdasarkan kepercayaan dan relevan Kami menunjukkan \"Fakta\" - sistem untuk mengintegrasikan sumber web dalam. Fakta berdasarkan kaedah pemilihan sumber \"SourceRank\" yang baru diperkenalkan; yang merupakan ukuran kepercayaan dan kaitan berdasarkan kesepakatan antara sumber. SourceRank memilih sumber yang popular dan boleh dipercayai dari koleksi autonomi dan terbuka seperti web dalam. Kepercayaan dan kesedaran populariti ini membezakan Factal dari sistem yang ada seperti Pencarian Produk Google. Factal memilih dan mencari pangkalan data dalam talian aktif di pelbagai domain. Senario demonstrasi merangkumi peningkatan kepercayaan, kesesuaian hasil, dan perbandingan perbandingan. Kami percaya bahawa dengan memasukkan pemilihan sumber yang berkesan berdasarkan SourceRank, Factal menunjukkan langkah penting menuju sistem integrasi skala web dalam. [[EENNDD]] pemilihan sumber; penyepaduan web; sourcerank; web dalam"], [{"string": "A unified approach to learning task-specific bit vector representations for fast nearest neighbor search Fast nearest neighbor search is necessary for a variety of large scale web applications such as information retrieval, nearest neighbor classification and nearest neighbor regression. Recently a number of machine learning algorithms have been proposed for representing the data to be searched as (short) bit vectors and then using hashing to do rapid search. These algorithms have been limited in their applicability in that they are suited for only one type of task -- e.g. Spectral Hashing learns bit vector representations for retrieval, but not say, classification. In this paper we present a unified approach to learning bit vector representations for many applications that use nearest neighbor search. The main contribution is a single learning algorithm that can be customized to learn a bit vector representation suited for the task at hand. This broadens the usefulness of bit vector representations to tasks beyond just conventional retrieval. We propose a learning-to-rank formulation to learn the bit vector representation of the data. LambdaRank algorithm is used for learning a function that computes a task-specific bit vector from an input data vector. Our approach outperforms state-of-the-art nearest neighbor methods on a number of real world text and image classification and retrieval datasets. It is scalable and learns a 32-bit representation on 1.46 million training cases in two days.", "keywords": ["hashing", "learning to rank", "information search and retrieval", "nearest neighbor search"], "combined": "A unified approach to learning task-specific bit vector representations for fast nearest neighbor search Fast nearest neighbor search is necessary for a variety of large scale web applications such as information retrieval, nearest neighbor classification and nearest neighbor regression. Recently a number of machine learning algorithms have been proposed for representing the data to be searched as (short) bit vectors and then using hashing to do rapid search. These algorithms have been limited in their applicability in that they are suited for only one type of task -- e.g. Spectral Hashing learns bit vector representations for retrieval, but not say, classification. In this paper we present a unified approach to learning bit vector representations for many applications that use nearest neighbor search. The main contribution is a single learning algorithm that can be customized to learn a bit vector representation suited for the task at hand. This broadens the usefulness of bit vector representations to tasks beyond just conventional retrieval. We propose a learning-to-rank formulation to learn the bit vector representation of the data. LambdaRank algorithm is used for learning a function that computes a task-specific bit vector from an input data vector. Our approach outperforms state-of-the-art nearest neighbor methods on a number of real world text and image classification and retrieval datasets. It is scalable and learns a 32-bit representation on 1.46 million training cases in two days. [[EENNDD]] hashing; learning to rank; information search and retrieval; nearest neighbor search"}, "Pendekatan terpadu untuk belajar perwakilan vektor bit khusus tugas untuk carian tetangga terdekat yang cepat Carian jiran terdekat yang cepat diperlukan untuk pelbagai aplikasi web berskala besar seperti pencarian maklumat, klasifikasi tetangga terdekat dan regresi tetangga terdekat. Baru-baru ini sejumlah algoritma pembelajaran mesin telah dicadangkan untuk mewakili data yang akan dicari sebagai vektor bit (pendek) dan kemudian menggunakan hashing untuk melakukan carian pantas. Algoritma ini terhad dalam kesesuaiannya kerana ia hanya sesuai untuk satu jenis tugas - mis. Spectral Hashing mempelajari perwakilan vektor bit untuk pengambilan, tetapi tidak mengatakan, klasifikasi. Dalam makalah ini kami menyajikan pendekatan terpadu untuk belajar representasi vektor bit untuk banyak aplikasi yang menggunakan carian tetangga terdekat. Sumbangan utama adalah algoritma pembelajaran tunggal yang dapat disesuaikan untuk mempelajari representasi vektor sedikit yang sesuai untuk tugas yang sedang dilakukan. Ini memperluas kegunaan representasi vektor bit untuk tugas-tugas di luar hanya pengambilan konvensional. Kami mencadangkan formulasi pembelajaran ke peringkat untuk mempelajari representasi bit vektor data. Algoritma LambdaRank digunakan untuk mempelajari fungsi yang mengira vektor bit khusus tugas dari vektor data input. Pendekatan kami mengungguli kaedah tetangga terdekat yang canggih pada sejumlah teks dan gambar klasifikasi dan pengambilan data dunia nyata. Ia boleh diskalakan dan mempelajari perwakilan 32-bit pada 1.46 juta kes latihan dalam dua hari. [[EENNDD]] hashing; belajar berpangkat; carian dan pengambilan maklumat; carian jiran terdekat"], [{"string": "Behavioral profiles for advanced email features We examine the behavioral patterns of email usage in a large-scale enterprise over a three-month period. In particular, we focus on two main questions: (Q1) what do replies depend on? and (Q2) what is the gain of augmenting contacts through the friends of friends from the email social graph? For Q1, we identify and evaluate the significance of several factors that affect the reply probability and the email response time. We find that all factors of our considered set are significant, provide their relative ordering, and identify the recipient list size, and the intensity of email communication between the correspondents as the dominant factors. We highlight various novel threshold behaviors and provide support for existing hypotheses such as that of the least-effort reply. For Q2, we find that the number of new contacts extracted from the friends-of-friends relationships amounts to a large number, but which is still a limited portion of the total enterprise size. We believe that our results provide significant insights towards informed design of advanced email features, including those of social-networking type.", "keywords": ["reply time", "email profiles", "reply probability"], "combined": "Behavioral profiles for advanced email features We examine the behavioral patterns of email usage in a large-scale enterprise over a three-month period. In particular, we focus on two main questions: (Q1) what do replies depend on? and (Q2) what is the gain of augmenting contacts through the friends of friends from the email social graph? For Q1, we identify and evaluate the significance of several factors that affect the reply probability and the email response time. We find that all factors of our considered set are significant, provide their relative ordering, and identify the recipient list size, and the intensity of email communication between the correspondents as the dominant factors. We highlight various novel threshold behaviors and provide support for existing hypotheses such as that of the least-effort reply. For Q2, we find that the number of new contacts extracted from the friends-of-friends relationships amounts to a large number, but which is still a limited portion of the total enterprise size. We believe that our results provide significant insights towards informed design of advanced email features, including those of social-networking type. [[EENNDD]] reply time; email profiles; reply probability"}, "Profil tingkah laku untuk ciri e-mel maju Kami meneliti corak tingkah laku penggunaan e-mel di perusahaan berskala besar dalam jangka masa tiga bulan. Secara khusus, kami memfokuskan pada dua soalan utama: (S1) apa yang bergantung kepada balasan? dan (Q2) apa keuntungan menambah kenalan melalui rakan rakan dari grafik sosial e-mel? Untuk Q1, kami mengenal pasti dan menilai kepentingan beberapa faktor yang mempengaruhi kebarangkalian balasan dan masa tindak balas e-mel. Kami mendapati bahawa semua faktor dari set yang kami anggap penting, memberikan pesanan relatifnya, dan mengenal pasti ukuran senarai penerima, dan intensiti komunikasi e-mel antara koresponden sebagai faktor dominan. Kami mengetengahkan pelbagai tingkah laku ambang novel dan memberikan sokongan untuk hipotesis yang ada seperti jawapan paling tidak berusaha. Untuk Q2, kami mendapati bahawa jumlah kenalan baru yang diekstrak dari hubungan rakan-rakan-rakan berjumlah besar, tetapi masih merupakan bahagian terhad dari keseluruhan ukuran perusahaan. Kami percaya bahawa hasil kami memberikan pandangan yang signifikan terhadap reka bentuk yang tepat mengenai ciri e-mel maju, termasuk jenis rangkaian sosial. [[EENNDD]] masa balasan; profil e-mel; kebarangkalian membalas"], [{"string": "An evaluation of TCP splice benefits in web proxy servers No contact information provided yet.", "keywords": ["web proxy", "tcp splice", "communications management"], "combined": "An evaluation of TCP splice benefits in web proxy servers No contact information provided yet. [[EENNDD]] web proxy; tcp splice; communications management"}, "Penilaian faedah penyambungan TCP dalam pelayan proksi web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] proksi web; sambung tcp; pengurusan komunikasi"], [{"string": "An application server for the semantic web No contact information provided yet.", "keywords": ["ontology", "software architectures", "application server", "semantic web"], "combined": "An application server for the semantic web No contact information provided yet. [[EENNDD]] ontology; software architectures; application server; semantic web"}, "Pelayan aplikasi untuk web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ontologi; seni bina perisian; pelayan aplikasi; web semantik"], [{"string": "Testbed for information extraction from deep web No contact information provided yet.", "keywords": ["meta search", "testbed", "performance evaluation", "wrapper", "deep web"], "combined": "Testbed for information extraction from deep web No contact information provided yet. [[EENNDD]] meta search; testbed; performance evaluation; wrapper; deep web"}, "Diuji untuk mendapatkan maklumat dari web dalam Belum ada maklumat hubungan yang diberikan. [[EENNDD]] carian meta; ujian; penilaian prestasi; pembungkus; web dalam"], [{"string": "GIO: a semantic web application using the information grid framework No contact information provided yet.", "keywords": ["databases", "information visualization", "browsing", "meta-data", "semantic web", "tools", "rdf", "user interface", "search"], "combined": "GIO: a semantic web application using the information grid framework No contact information provided yet. [[EENNDD]] databases; information visualization; browsing; meta-data; semantic web; tools; rdf; user interface; search"}, "GIO: aplikasi web semantik yang menggunakan kerangka grid maklumat Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pangkalan data; visualisasi maklumat; melayari; meta-data; web semantik; alat; rdf; antaramuka pengguna; cari"], [{"string": "METEOR: metadata and instance extraction from object referral lists on the web No contact information provided yet.", "keywords": ["instance", "semantic", "web", "object", "miscellaneous", "extraction", "metadata"], "combined": "METEOR: metadata and instance extraction from object referral lists on the web No contact information provided yet. [[EENNDD]] instance; semantic; web; object; miscellaneous; extraction; metadata"}, "METEOR: metadata dan pengambilan contoh dari senarai rujukan objek di web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] contoh; semantik; laman web; objek; pelbagai; pengekstrakan; metadata"], [{"string": "Constraint SVG No contact information provided yet.", "keywords": ["constraints", "differential scaling", "adaptivity", "svg", "constraint-based graphics", "miscellaneous", "document formats", "scalable vector graphics", "user interfaces", "semantic zooming", "csvg", "interaction"], "combined": "Constraint SVG No contact information provided yet. [[EENNDD]] constraints; differential scaling; adaptivity; svg; constraint-based graphics; miscellaneous; document formats; scalable vector graphics; user interfaces; semantic zooming; csvg; interaction"}, "Kekangan SVG Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kekangan; penskalaan pembezaan; penyesuaian; svg; grafik berasaskan kekangan; pelbagai; format dokumen; grafik vektor berskala; antara muka pengguna; zum semantik; csvg; interaksi"], [{"string": "Personal DJ, an architecture for personalised content delivery An abstract is not available.", "keywords": ["internet", "user evaluation", "radio", "personalisation", "moods"], "combined": "Personal DJ, an architecture for personalised content delivery An abstract is not available. [[EENNDD]] internet; user evaluation; radio; personalisation; moods"}, "DJ peribadi, seni bina untuk penyampaian kandungan yang diperibadikan Abstrak tidak tersedia. [[EENNDD]] internet; penilaian pengguna; radio; pemperibadian; suasana hati"], [{"string": "Webanywhere: enabling a screen reading interface for the web on any computer People often use computers other than their own to access web content, but blind users are restricted to using computers equipped with expensive, special-purpose screen reading programs that they use to access the web. WebAnywhere is a web-based, self-voicing web application that enables blind web users to access the web from almost any computer that can produce sound without installing new software. WebAnywhere could serve as a convenient, low-cost solution for blind users on-the-go, for blind users unable to afford another screen reader and for web developers targeting accessible design. This paper describes the implementation of WebAnywhere, overviews an evaluation of it by blind web users, and summarizes a survey of public terminals that shows it can run on most public computers.", "keywords": ["blind users", "screen reader", "web accessibility", "user interfaces"], "combined": "Webanywhere: enabling a screen reading interface for the web on any computer People often use computers other than their own to access web content, but blind users are restricted to using computers equipped with expensive, special-purpose screen reading programs that they use to access the web. WebAnywhere is a web-based, self-voicing web application that enables blind web users to access the web from almost any computer that can produce sound without installing new software. WebAnywhere could serve as a convenient, low-cost solution for blind users on-the-go, for blind users unable to afford another screen reader and for web developers targeting accessible design. This paper describes the implementation of WebAnywhere, overviews an evaluation of it by blind web users, and summarizes a survey of public terminals that shows it can run on most public computers. [[EENNDD]] blind users; screen reader; web accessibility; user interfaces"}, "Webanywhere: mengaktifkan antara muka pembacaan skrin untuk web di mana-mana komputer Orang sering menggunakan komputer selain daripada mereka sendiri untuk mengakses kandungan web, tetapi pengguna buta dilarang menggunakan komputer yang dilengkapi dengan program membaca skrin khas yang mahal yang mereka gunakan untuk mengakses laman web. WebAnywhere adalah aplikasi web yang bersuara sendiri yang membolehkan pengguna web buta mengakses web dari hampir semua komputer yang dapat menghasilkan suara tanpa memasang perisian baru. WebAnywhere boleh berfungsi sebagai penyelesaian kos rendah yang mudah untuk pengguna buta dalam perjalanan, untuk pengguna buta yang tidak mampu membeli pembaca skrin lain dan untuk pembangun web yang menyasarkan reka bentuk yang dapat diakses. Makalah ini menjelaskan pelaksanaan WebAnywhere, meninjau penilaiannya oleh pengguna web buta, dan meringkaskan tinjauan terminal awam yang menunjukkan ia dapat dijalankan di kebanyakan komputer awam. [[EENNDD]] pengguna buta; pembaca skrin; kebolehcapaian laman web; antara muka pengguna"], [{"string": "Identifying and discriminating between web and peer-to-peer traffic in the network core Traffic classification is the ability to identify and categorize network traffic by application type. In this paper, we consider the problem of traffic classification in the network core.Classification at the core is challenging because only partial information about the flows and their contributors is available. We address this problem by developing a framework that can classify a flow using only unidirectional flow information. We evaluated this approach using recent packet traces that we collected and pre-classified to establish a \"base truth\". From our evaluation, we find that flow statistics for the server-to-client direction of a TCP connection provide greater classification accuracy than the flow statistics for the client-to-server direction. Because collection of the server-to-client flow statistics may not always be feasible, we developed and validated an algorithm that can estimate the missing statistics froma unidirectional packet trace.", "keywords": ["machine learning", "applications", "clustering", "traffic classification"], "combined": "Identifying and discriminating between web and peer-to-peer traffic in the network core Traffic classification is the ability to identify and categorize network traffic by application type. In this paper, we consider the problem of traffic classification in the network core.Classification at the core is challenging because only partial information about the flows and their contributors is available. We address this problem by developing a framework that can classify a flow using only unidirectional flow information. We evaluated this approach using recent packet traces that we collected and pre-classified to establish a \"base truth\". From our evaluation, we find that flow statistics for the server-to-client direction of a TCP connection provide greater classification accuracy than the flow statistics for the client-to-server direction. Because collection of the server-to-client flow statistics may not always be feasible, we developed and validated an algorithm that can estimate the missing statistics froma unidirectional packet trace. [[EENNDD]] machine learning; applications; clustering; traffic classification"}, "Mengenal dan membezakan antara trafik web dan peer-to-peer dalam teras rangkaian Klasifikasi lalu lintas adalah keupayaan untuk mengenal pasti dan mengkategorikan trafik rangkaian mengikut jenis aplikasi. Dalam makalah ini, kami mempertimbangkan masalah klasifikasi lalu lintas di inti rangkaian. Pengkelasan di inti adalah mencabar kerana hanya terdapat sebahagian maklumat mengenai aliran dan penyumbang mereka. Kami mengatasi masalah ini dengan mengembangkan kerangka kerja yang dapat mengklasifikasikan aliran menggunakan hanya maklumat aliran searah. Kami menilai pendekatan ini dengan menggunakan jejak paket baru yang kami kumpulkan dan pra-diklasifikasikan untuk menetapkan \"kebenaran asas\". Dari penilaian kami, kami mendapati bahawa statistik aliran untuk arah pelayan-ke-pelanggan sambungan TCP memberikan ketepatan klasifikasi yang lebih besar daripada statistik aliran untuk arah klien-ke-pelayan. Oleh kerana pengumpulan statistik aliran pelayan ke klien tidak selalu dapat dilaksanakan, kami mengembangkan dan mengesahkan algoritma yang dapat menganggarkan statistik yang hilang dari jejak paket sehala. [[EENNDD]] pembelajaran mesin; permohonan; pengelompokan; pengelasan lalu lintas"], [{"string": "Context-aware citation recommendation When you write papers, how many times do you want to make some citations at a place but you are not sure which papers to cite? Do you wish to have a recommendation system which can recommend a small number of good candidates for every place that you want to make some citations? In this paper, we present our initiative of building a context-aware citation recommendation system. High quality citation recommendation is challenging: not only should the citations recommended be relevant to the paper under composition, but also should match the local contexts of the places citations are made. Moreover, it is far from trivial to model how the topic of the whole paper and the contexts of the citation places should affect the selection and ranking of citations. To tackle the problem, we develop a context-aware approach. The core idea is to design a novel non-parametric probabilistic model which can measure the context-based relevance between a citation context and a document. Our approach can recommend citations for a context effectively. Moreover, it can recommend a set of citations for a paper with high quality. We implement a prototype system in CiteSeerX. An extensive empirical evaluation in the CiteSeerX digital library against many baselines demonstrates the effectiveness and the scalability of our approach.", "keywords": ["recommender systems", "bibliometrics", "gleason's theorem", "context"], "combined": "Context-aware citation recommendation When you write papers, how many times do you want to make some citations at a place but you are not sure which papers to cite? Do you wish to have a recommendation system which can recommend a small number of good candidates for every place that you want to make some citations? In this paper, we present our initiative of building a context-aware citation recommendation system. High quality citation recommendation is challenging: not only should the citations recommended be relevant to the paper under composition, but also should match the local contexts of the places citations are made. Moreover, it is far from trivial to model how the topic of the whole paper and the contexts of the citation places should affect the selection and ranking of citations. To tackle the problem, we develop a context-aware approach. The core idea is to design a novel non-parametric probabilistic model which can measure the context-based relevance between a citation context and a document. Our approach can recommend citations for a context effectively. Moreover, it can recommend a set of citations for a paper with high quality. We implement a prototype system in CiteSeerX. An extensive empirical evaluation in the CiteSeerX digital library against many baselines demonstrates the effectiveness and the scalability of our approach. [[EENNDD]] recommender systems; bibliometrics; gleason's theorem; context"}, "Cadangan petikan yang peka konteks Semasa anda menulis makalah, berapa kali anda ingin membuat beberapa petikan di suatu tempat tetapi anda tidak pasti kertas yang mana yang hendak dikutip? Adakah anda ingin mempunyai sistem cadangan yang dapat mengesyorkan sebilangan kecil calon yang baik untuk setiap tempat yang anda ingin membuat beberapa petikan? Dalam makalah ini, kami memaparkan inisiatif kami untuk membina sistem cadangan kutipan yang peka konteks. Cadangan kutipan berkualiti tinggi adalah mencabar: bukan sahaja petikan yang disarankan harus relevan dengan makalah yang dalam komposisi, tetapi juga harus sesuai dengan konteks lokal tempat-tempat kutipan dibuat. Lebih jauh lagi, ini adalah hal yang remeh untuk memodelkan bagaimana topik keseluruhan makalah dan konteks tempat kutipan harus mempengaruhi pemilihan dan peringkat petikan. Untuk mengatasi masalah tersebut, kami mengembangkan pendekatan yang peka dengan konteks. Idea utamanya adalah merancang model probabilistik bukan parametrik novel yang dapat mengukur perkaitan berdasarkan konteks antara konteks petikan dan dokumen. Pendekatan kami dapat mengesyorkan petikan untuk konteks dengan berkesan. Lebih-lebih lagi, ia dapat mengesyorkan satu set petikan untuk kertas dengan kualiti tinggi. Kami menerapkan sistem prototaip di CiteSeerX. Penilaian empirikal yang luas di perpustakaan digital CiteSeerX terhadap banyak garis panduan menunjukkan keberkesanan dan skalabilitas pendekatan kami. [[EENNDD]] sistem cadangan; bibliometrik; teorema gleason; konteks"], [{"string": "Time-dependent semantic similarity measure of queries using historical click-through data No contact information provided yet.", "keywords": ["event detection", "marginalized kernel", "semantic similarity measure", "click-through data", "evolution pattern"], "combined": "Time-dependent semantic similarity measure of queries using historical click-through data No contact information provided yet. [[EENNDD]] event detection; marginalized kernel; semantic similarity measure; click-through data; evolution pattern"}, "Ukuran kesamaan semantik yang bergantung pada masa bagi pertanyaan menggunakan data klik-tayang sejarah Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengesanan acara; kernel terpinggir; ukuran kesamaan semantik; data klik-tayang; corak evolusi"], [{"string": "Bilingual web page and site readability assessment No contact information provided yet.", "keywords": ["english", "web sites", "readability", "miscellaneous", "chinese", "web pages"], "combined": "Bilingual web page and site readability assessment No contact information provided yet. [[EENNDD]] english; web sites; readability; miscellaneous; chinese; web pages"}, "Penilaian kebolehbacaan laman web dan laman web dwibahasa Belum ada maklumat hubungan yang diberikan. [[EENNDD]] bahasa Inggeris; laman web; kebolehbacaan; pelbagai; cina; laman sesawang"], [{"string": "Where to adapt dynamic service compositions Peer services depend on one another to accomplish their tasks, and their structures may evolve. A service composition may be designed to replace its member services whenever the quality of the composite service fails to meet certain quality-of-service (QoS) requirements. Finding services and service invocation endpoints having the greatest impact on the quality are important to guide subsequent service adaptations. This paper proposes a technique that samples the QoS of composite services and continually analyzes them to identify artifacts for service adaptation. The preliminary results show that our technique has the potential to effectively find such artifacts in services.", "keywords": ["service adaptation", "service composition"], "combined": "Where to adapt dynamic service compositions Peer services depend on one another to accomplish their tasks, and their structures may evolve. A service composition may be designed to replace its member services whenever the quality of the composite service fails to meet certain quality-of-service (QoS) requirements. Finding services and service invocation endpoints having the greatest impact on the quality are important to guide subsequent service adaptations. This paper proposes a technique that samples the QoS of composite services and continually analyzes them to identify artifacts for service adaptation. The preliminary results show that our technique has the potential to effectively find such artifacts in services. [[EENNDD]] service adaptation; service composition"}, "Di mana untuk menyesuaikan komposisi perkhidmatan dinamik Perkhidmatan rakan sebaya bergantung kepada satu sama lain untuk menyelesaikan tugas mereka, dan strukturnya mungkin berkembang. Komposisi perkhidmatan mungkin dirancang untuk menggantikan perkhidmatan anggotanya setiap kali kualiti perkhidmatan komposit gagal memenuhi syarat kualiti perkhidmatan (QoS) tertentu. Mencari titik akhir perkhidmatan dan perkhidmatan yang memberi kesan terbesar pada kualiti adalah penting untuk memandu penyesuaian perkhidmatan seterusnya. Makalah ini mencadangkan teknik yang menguji QoS perkhidmatan komposit dan terus menerus menganalisisnya untuk mengenal pasti artifak untuk penyesuaian perkhidmatan. Hasil awal menunjukkan bahawa teknik kami berpotensi mencari artifak sedemikian dalam perkhidmatan. [[EENNDD]] penyesuaian perkhidmatan; komposisi perkhidmatan"], [{"string": "The value of socially tagged urls for a search engine Social bookmarking has emerged as a growing source of human generated content on the web. In essence, bookmarking involves URLs and tags on them. In this paper, we perform a large scale study of the usefulness of bookmarked URLs from the top social bookmarking site Delicious. Instead of focusing on the dimension of tags, which has been covered in the previous work, we explore social bookmarking from the dimension of URLs. More specifically, we investigate the Delicious URLs and their content to quantify their value to a search engine. For their value in leading to good content, we show that the Delicious URLs have higher quality content and more external outlinks. For their value in satisfying users, we show that the Delicious URLs have more clicked URLs as well as get more clicks. We suggest that based on their value, the Delicious URLs should be used as another source of seed URLs for crawlers.", "keywords": ["social bookmarking", "delicious", "content quality"], "combined": "The value of socially tagged urls for a search engine Social bookmarking has emerged as a growing source of human generated content on the web. In essence, bookmarking involves URLs and tags on them. In this paper, we perform a large scale study of the usefulness of bookmarked URLs from the top social bookmarking site Delicious. Instead of focusing on the dimension of tags, which has been covered in the previous work, we explore social bookmarking from the dimension of URLs. More specifically, we investigate the Delicious URLs and their content to quantify their value to a search engine. For their value in leading to good content, we show that the Delicious URLs have higher quality content and more external outlinks. For their value in satisfying users, we show that the Delicious URLs have more clicked URLs as well as get more clicks. We suggest that based on their value, the Delicious URLs should be used as another source of seed URLs for crawlers. [[EENNDD]] social bookmarking; delicious; content quality"}, "Nilai url yang diberi tag sosial untuk mesin carian Penanda buku sosial telah muncul sebagai sumber kandungan manusia yang dihasilkan di web yang semakin meningkat. Pada hakikatnya, penanda buku melibatkan URL dan tag di atasnya. Dalam makalah ini, kami melakukan kajian skala besar mengenai kegunaan URL yang ditandai dari laman penanda buku sosial teratas Delicious. Daripada memfokuskan pada dimensi tag, yang telah diliputi dalam karya sebelumnya, kami meneroka penanda halaman sosial dari dimensi URL. Lebih khusus lagi, kami menyiasat URL Delicious dan kandungannya untuk mengukur nilainya ke mesin pencari. Oleh kerana nilainya menghasilkan kandungan yang baik, kami menunjukkan bahawa URL Delicious mempunyai kandungan berkualiti tinggi dan lebih banyak pautan luar. Untuk nilai mereka dalam memuaskan pengguna, kami menunjukkan bahawa URL Delicious mempunyai lebih banyak URL yang diklik dan juga mendapat lebih banyak klik. Kami mencadangkan bahawa berdasarkan nilai mereka, URL Delicious harus digunakan sebagai sumber URL unggulan lain untuk perayap. [[EENNDD]] penanda buku sosial; sedap; kualiti kandungan"], [{"string": "Using landing pages for sponsored search ad selection We explore the use of the landing page content in sponsored search ad selection. Specifically, we compare the use of the ad's intrinsic content to augmenting the ad with the whole, or parts, of the landing page. We explore two types of extractive summarization techniques to select useful regions from the landing pages: out-of-context and in-context methods. Out-of-context methods select salient regions from the landing page by analyzing the content alone, without taking into account the ad associated with the landing page. In-context methods use the ad context (including its title, creative, and bid phrases) to help identify regions of the landing page that should be used by the ad selection engine. In addition, we introduce a simple yet effective unsupervised algorithm to enrich the ad context to further improve the ad selection. Experimental evaluation confirms that the use of landing pages can significantly improve the quality of ad selection. We also find that our extractive summarization techniques reduce the size of landing pages substantially, while retaining or even improving the performance of ad retrieval over the method that utilize the entire landing page.", "keywords": ["extractive summarization", "sponsored search", "landing pages", "compositional semantics"], "combined": "Using landing pages for sponsored search ad selection We explore the use of the landing page content in sponsored search ad selection. Specifically, we compare the use of the ad's intrinsic content to augmenting the ad with the whole, or parts, of the landing page. We explore two types of extractive summarization techniques to select useful regions from the landing pages: out-of-context and in-context methods. Out-of-context methods select salient regions from the landing page by analyzing the content alone, without taking into account the ad associated with the landing page. In-context methods use the ad context (including its title, creative, and bid phrases) to help identify regions of the landing page that should be used by the ad selection engine. In addition, we introduce a simple yet effective unsupervised algorithm to enrich the ad context to further improve the ad selection. Experimental evaluation confirms that the use of landing pages can significantly improve the quality of ad selection. We also find that our extractive summarization techniques reduce the size of landing pages substantially, while retaining or even improving the performance of ad retrieval over the method that utilize the entire landing page. [[EENNDD]] extractive summarization; sponsored search; landing pages; compositional semantics"}, "Menggunakan halaman arahan untuk pilihan iklan carian yang ditaja Kami meneroka penggunaan kandungan halaman arahan dalam pilihan iklan carian yang ditaja. Secara khusus, kami membandingkan penggunaan konten intrinsik iklan dengan menambah iklan dengan keseluruhan, atau sebagian, dari halaman arahan. Kami meneroka dua jenis teknik ringkasan ekstraktif untuk memilih kawasan yang berguna dari halaman arahan: kaedah di luar konteks dan dalam konteks. Kaedah di luar konteks memilih kawasan yang menonjol dari halaman arahan dengan menganalisis kandungan sahaja, tanpa mengambil kira iklan yang dikaitkan dengan halaman arahan. Kaedah dalam konteks menggunakan konteks iklan (termasuk tajuknya, kreatif, dan frasa bida) untuk membantu mengenal pasti kawasan halaman arahan yang harus digunakan oleh mesin pemilihan iklan. Di samping itu, kami memperkenalkan algoritma tanpa pengawasan yang sederhana namun berkesan untuk memperkayakan konteks iklan untuk meningkatkan lagi pilihan iklan. Penilaian eksperimental mengesahkan bahawa penggunaan halaman arahan dapat meningkatkan kualiti pemilihan iklan dengan ketara. Kami juga mendapati bahawa teknik ringkasan ekstraktif kami mengurangkan ukuran halaman pendaratan dengan banyak, sambil mengekalkan atau bahkan meningkatkan prestasi pengambilan iklan berbanding kaedah yang menggunakan keseluruhan halaman pendaratan. [[EENNDD]] ringkasan ekstraktif; carian tajaan; halaman arahan; semantik penggubalan"], [{"string": "Trust-based recommendation systems: an axiomatic approach High-quality, personalized recommendations are a key feature in many online systems. Since these systems often have explicit knowledge of social network structures, the recommendations may incorporate this information. This paper focuses on networks that represent trust and recommendation systems that incorporate these trust relationships. The goal of a trust-based recommendation system is to generate personalized recommendations by aggregating the opinions of other users in the trust network.", "keywords": ["general", "axiomatic approach", "trust networks", "reputation systems", "recommendation systems"], "combined": "Trust-based recommendation systems: an axiomatic approach High-quality, personalized recommendations are a key feature in many online systems. Since these systems often have explicit knowledge of social network structures, the recommendations may incorporate this information. This paper focuses on networks that represent trust and recommendation systems that incorporate these trust relationships. The goal of a trust-based recommendation system is to generate personalized recommendations by aggregating the opinions of other users in the trust network. [[EENNDD]] general; axiomatic approach; trust networks; reputation systems; recommendation systems"}, "Sistem cadangan berasaskan kepercayaan: pendekatan aksiomatik Saranan berkualiti tinggi yang diperibadikan adalah ciri utama dalam banyak sistem dalam talian. Oleh kerana sistem ini sering mempunyai pengetahuan yang jelas mengenai struktur rangkaian sosial, cadangan dapat memasukkan maklumat ini. Makalah ini memfokuskan pada rangkaian yang mewakili sistem kepercayaan dan cadangan yang menggabungkan hubungan kepercayaan ini. Matlamat sistem cadangan berasaskan kepercayaan adalah untuk menghasilkan cadangan yang diperibadikan dengan mengumpulkan pendapat pengguna lain dalam rangkaian kepercayaan. [[EENNDD]] umum; pendekatan aksiomatik; rangkaian kepercayaan; sistem reputasi; sistem cadangan"], [{"string": "iRIN: image retrieval in image-rich information networks In this demo, we present a system called iRIN designed for performing image retrieval in image-rich information networks. We first introduce MoK-SimRank to significantly improve the speed of SimRank, one of the most popular algorithms for computing node similarity in information networks. Next, we propose an algorithm called SimLearn to (1) extend MoK-SimRank to heterogeneous image-rich information network, and (2) account for both link-based and content-based similarities by seamlessly integrating reinforcement learning with feature learning.", "keywords": ["ranking", "image retrieval", "content analysis and indexing", "information search and retrieval", "information network"], "combined": "iRIN: image retrieval in image-rich information networks In this demo, we present a system called iRIN designed for performing image retrieval in image-rich information networks. We first introduce MoK-SimRank to significantly improve the speed of SimRank, one of the most popular algorithms for computing node similarity in information networks. Next, we propose an algorithm called SimLearn to (1) extend MoK-SimRank to heterogeneous image-rich information network, and (2) account for both link-based and content-based similarities by seamlessly integrating reinforcement learning with feature learning. [[EENNDD]] ranking; image retrieval; content analysis and indexing; information search and retrieval; information network"}, "iRIN: pengambilan gambar dalam rangkaian maklumat yang kaya dengan gambar Dalam demo ini, kami menyajikan sistem yang disebut iRIN yang dirancang untuk melakukan pengambilan gambar dalam rangkaian maklumat yang kaya dengan gambar. Mula-mula kami memperkenalkan MoK-SimRank untuk meningkatkan kelajuan SimRank, salah satu algoritma yang paling popular untuk kesamaan nod pengkomputeran dalam rangkaian maklumat. Seterusnya, kami mencadangkan algoritma yang disebut SimLearn untuk (1) memperluas MoK-SimRank ke rangkaian maklumat kaya gambar yang heterogen, dan (2) memperhitungkan kesamaan berdasarkan pautan dan berdasarkan kandungan dengan menggabungkan pembelajaran pengukuhan dengan pembelajaran ciri dengan lancar. [[EENNDD]] kedudukan; pengambilan gambar; analisis kandungan dan pengindeksan; pencarian dan pengambilan maklumat; rangkaian maklumat"], [{"string": "Effort estimation: how valuable is it for a web company to use a cross-company data set, compared to using its own single-company data set? Previous studies comparing the prediction accuracy of effort models built using Web cross- and single-company data sets have been inconclusive, and as such replicated studies are necessary to determine under what circumstances a company can place reliance on a cross-company effort model.", "keywords": ["effort estimation", "web projects", "single-company effort model", "case-based reasoning", "cross-company effort model", "stepwise regression", "web applications"], "combined": "Effort estimation: how valuable is it for a web company to use a cross-company data set, compared to using its own single-company data set? Previous studies comparing the prediction accuracy of effort models built using Web cross- and single-company data sets have been inconclusive, and as such replicated studies are necessary to determine under what circumstances a company can place reliance on a cross-company effort model. [[EENNDD]] effort estimation; web projects; single-company effort model; case-based reasoning; cross-company effort model; stepwise regression; web applications"}, "Anggaran usaha: betapa berharganya syarikat web menggunakan set data lintas syarikat, berbanding dengan menggunakan set data syarikat tunggal sendiri? Kajian sebelumnya yang membandingkan ketepatan ramalan model usaha yang dibuat menggunakan set data lintas syarikat dan syarikat tunggal tidak dapat disimpulkan, dan oleh itu kajian ulangan diperlukan untuk menentukan dalam keadaan apa syarikat dapat bergantung pada model usaha lintas syarikat. [[EENNDD]] anggaran usaha; projek web; model usaha syarikat tunggal; penaakulan berdasarkan kes; model usaha merentas syarikat; regresi bertahap; aplikasi web"], [{"string": "Networked graphs: a declarative mechanism for SPARQL rules, SPARQL views and RDF data integration on the web Easy reuse and integration of declaratively described information in a distributed setting is one of the main motivations for building the Semantic Web. Despite of this claim, reuse and recombination of RDF data today is mostly done using data replication and procedural code. A simple declarative mechanism for reusing and combining RDF data would help users to generate content for the semantic web. Having such a mechanism, the Semantic Web could better benefit from user generated content, as it is broadly present in the so called Web 2.0, but also from better linkage of existing content.", "keywords": ["views", "rules", "sparql", "semantic web", "distributed rules", "information storage and retrieval", "well founded semantics"], "combined": "Networked graphs: a declarative mechanism for SPARQL rules, SPARQL views and RDF data integration on the web Easy reuse and integration of declaratively described information in a distributed setting is one of the main motivations for building the Semantic Web. Despite of this claim, reuse and recombination of RDF data today is mostly done using data replication and procedural code. A simple declarative mechanism for reusing and combining RDF data would help users to generate content for the semantic web. Having such a mechanism, the Semantic Web could better benefit from user generated content, as it is broadly present in the so called Web 2.0, but also from better linkage of existing content. [[EENNDD]] views; rules; sparql; semantic web; distributed rules; information storage and retrieval; well founded semantics"}, "Grafik rangkaian: mekanisme deklarasi untuk peraturan SPARQL, pandangan SPARQL dan integrasi data RDF di web Penggunaan semula dan penyatuan maklumat yang dijelaskan secara deklaratif dalam persekitaran yang diedarkan adalah salah satu motivasi utama untuk membina Web Semantik. Walaupun terdapat tuntutan ini, penggunaan semula dan penggabungan data RDF hari ini kebanyakannya dilakukan dengan menggunakan replikasi data dan kod prosedur. Mekanisme deklaratif yang mudah untuk menggunakan kembali dan menggabungkan data RDF akan membantu pengguna menghasilkan kandungan untuk web semantik. Dengan adanya mekanisme seperti itu, Web Semantik dapat memperoleh manfaat yang lebih baik dari konten yang dihasilkan pengguna, karena terdapat secara luas dalam apa yang disebut Web 2.0, tetapi juga dari hubungan yang lebih baik dari kandungan yang ada. [[EENNDD]] paparan; peraturan; sparql; web semantik; peraturan yang diedarkan; penyimpanan dan pengambilan maklumat; semantik yang betul"], [{"string": "Survivability-oriented self-tuning of web systems Running in a highly uncertain and changing environment, Web systems cannot always provide full set of services with optim\u00acal quality, especially when the workload is high or failures in subsys-tems occur frequently. It is thus desirable to continuously maintain a high satisfaction level of the system value proposition, hereafter survivability assurance, while relaxing/sacrificing certain quality/functional requirements that are not crucial to the survival of the Web systems. In this paper, we propose a requirements-driven self-tuning method for survivability assurance of Web systems. Using a value-based feedback controller plus a requirements-oriented reasoner, our method makes both quality and functional requirements tradeoffs decisions at runtime.", "keywords": ["survivability", "methodologies", "requirements", "reasoning", "value", "self-tuning"], "combined": "Survivability-oriented self-tuning of web systems Running in a highly uncertain and changing environment, Web systems cannot always provide full set of services with optim\u00acal quality, especially when the workload is high or failures in subsys-tems occur frequently. It is thus desirable to continuously maintain a high satisfaction level of the system value proposition, hereafter survivability assurance, while relaxing/sacrificing certain quality/functional requirements that are not crucial to the survival of the Web systems. In this paper, we propose a requirements-driven self-tuning method for survivability assurance of Web systems. Using a value-based feedback controller plus a requirements-oriented reasoner, our method makes both quality and functional requirements tradeoffs decisions at runtime. [[EENNDD]] survivability; methodologies; requirements; reasoning; value; self-tuning"}, "Penyesuaian diri sistem berorientasikan kelangsungan hidup Berjalan dalam persekitaran yang sangat tidak menentu dan berubah, sistem Web tidak selalu dapat menyediakan set perkhidmatan penuh dengan kualiti yang optimum, terutama ketika beban kerja tinggi atau kegagalan dalam subsistem sering terjadi. Oleh itu, adalah wajar untuk terus mengekalkan tahap kepuasan tinggi dari cadangan nilai sistem, selepas ini jaminan kelangsungan hidup, sambil bersantai / mengorbankan keperluan kualiti / fungsi tertentu yang tidak penting untuk kelangsungan sistem Web. Dalam makalah ini, kami mencadangkan kaedah penalaan kendiri berdasarkan keperluan untuk jaminan keselamatan sistem Web. Dengan menggunakan pengawal maklum balas berasaskan nilai dan penaakulan berorientasikan keperluan, kaedah kami membuat kedua-dua keperluan kualiti dan fungsional menukar keputusan pada waktu berjalan. [[EENNDD]] kelangsungan hidup; metodologi; keperluan; penaakulan; nilai; penyesuaian diri"], [{"string": "Fine grained access control for SOAP E-services An abstract is not available.", "keywords": ["internet", "standards", "access control", "security and protection", "xml", "soap", "certificates", "roles"], "combined": "Fine grained access control for SOAP E-services An abstract is not available. [[EENNDD]] internet; standards; access control; security and protection; xml; soap; certificates; roles"}, "Kawalan akses halus untuk E-perkhidmatan SOAP Abstrak tidak tersedia. [[EENNDD]] internet; standard; kawalan akses; keselamatan dan perlindungan; xml; sabun; sijil; peranan"], [{"string": "Browsing on small screens: recasting web-page segmentation into an efficient machine learning framework No contact information provided yet.", "keywords": ["information interfaces and presentation", "web page segmentation", "thumbnail browsing", "mobile devices", "browser", "mobile browsing", "small screen", "machine learning"], "combined": "Browsing on small screens: recasting web-page segmentation into an efficient machine learning framework No contact information provided yet. [[EENNDD]] information interfaces and presentation; web page segmentation; thumbnail browsing; mobile devices; browser; mobile browsing; small screen; machine learning"}, "Melayari di skrin kecil: menyusun semula segmentasi halaman web menjadi kerangka pembelajaran mesin yang cekap Belum ada maklumat hubungan yang diberikan. [[EENNDD]] antara muka dan persembahan maklumat; segmentasi halaman web; melayari gambar kecil; peranti mudah alih; penyemak imbas; penyemakan imbas mudah alih; skrin kecil; pembelajaran mesin"], [{"string": "Web browsing performance of wireless thin-client computing No contact information provided yet.", "keywords": ["wireless and mobility", "web performance", "thin-client computing"], "combined": "Web browsing performance of wireless thin-client computing No contact information provided yet. [[EENNDD]] wireless and mobility; web performance; thin-client computing"}, "Prestasi melayari laman web pengkomputeran pelanggan tipis tanpa wayar Belum ada maklumat hubungan yang diberikan. [[EENNDD]] tanpa wayar dan mobiliti; prestasi web; pengkomputeran pelanggan tipis"], [{"string": "Opinion observer: analyzing and comparing opinions on the Web No contact information provided yet.", "keywords": ["visualization", "information extraction", "opinion analysis", "sentiment analysis"], "combined": "Opinion observer: analyzing and comparing opinions on the Web No contact information provided yet. [[EENNDD]] visualization; information extraction; opinion analysis; sentiment analysis"}, "Pemerhati pendapat: menganalisis dan membandingkan pendapat di Web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] visualisasi; pengekstrakan maklumat; analisis pendapat; analisis sentimen"], [{"string": "Type based service composition No contact information provided yet.", "keywords": ["runtime non-determinism", "web services", "type based composition", "partial matches", "large scale discovery"], "combined": "Type based service composition No contact information provided yet. [[EENNDD]] runtime non-determinism; web services; type based composition; partial matches; large scale discovery"}, "Komposisi perkhidmatan berdasarkan jenis Belum ada maklumat hubungan yang diberikan. [[EENNDD]] jangka masa non-determinisme; perkhidmatan web; komposisi berdasarkan jenis; pertandingan separa; penemuan skala besar"], [{"string": "Associative sources and agents for zero-input publishing No contact information provided yet.", "keywords": ["context", "polymorphism", "agents", "associativity", "communications applications", "reconnaissance", "web services", "aggregation"], "combined": "Associative sources and agents for zero-input publishing No contact information provided yet. [[EENNDD]] context; polymorphism; agents; associativity; communications applications; reconnaissance; web services; aggregation"}, "Sumber dan ejen bersekutu untuk penerbitan input sifar Belum ada maklumat hubungan yang diberikan. [[EENNDD]] konteks; polimorfisme; ejen; persatuan; aplikasi komunikasi; pengintaian; perkhidmatan web; pengagregatan"], [{"string": "Agent-based semantic web services No contact information provided yet.", "keywords": ["ontologies", "daml-s", "agent communication languages", "web services", "semantic web"], "combined": "Agent-based semantic web services No contact information provided yet. [[EENNDD]] ontologies; daml-s; agent communication languages; web services; semantic web"}, "Perkhidmatan web semantik berasaskan ejen Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ontologi; celaka-s; bahasa komunikasi ejen; perkhidmatan web; web semantik"], [{"string": "Compositional knowledge management for medical services on semantic web No contact information provided yet.", "keywords": ["pragmatic knowledge", "service composition"], "combined": "Compositional knowledge management for medical services on semantic web No contact information provided yet. [[EENNDD]] pragmatic knowledge; service composition"}, "Pengurusan pengetahuan komposisi untuk perkhidmatan perubatan di laman web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengetahuan pragmatik; komposisi perkhidmatan"], [{"string": "Yago: a core of semantic knowledge We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as HASONEPRIZE). The facts have been automatically extracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques.", "keywords": ["general", "wikipedia", "wordnet"], "combined": "Yago: a core of semantic knowledge We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as HASONEPRIZE). The facts have been automatically extracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques. [[EENNDD]] general; wikipedia; wordnet"}, "Yago: teras pengetahuan semantik Kami menghadirkan YAGO, ontologi ringan dan luas dengan liputan dan kualiti yang tinggi. YAGO membina entiti dan hubungan dan pada masa ini mengandungi lebih daripada 1 juta entiti dan 5 juta fakta. Ini merangkumi hierarki Is-A serta hubungan bukan taksonomi antara entiti (seperti HASONEPRIZE). Fakta telah diekstrak secara automatik dari Wikipedia dan disatukan dengan WordNet, menggunakan gabungan kaedah berdasarkan peraturan dan heuristik yang dirancang dengan teliti yang dijelaskan dalam makalah ini. Pangkalan pengetahuan yang dihasilkan adalah langkah utama melampaui WordNet: dalam kualiti dengan menambahkan pengetahuan tentang individu seperti orang, organisasi, produk, dan lain-lain dengan hubungan semantik mereka - dan dalam kuantiti dengan meningkatkan jumlah fakta lebih daripada satu urutan besarnya. Penilaian empirikal kebenaran fakta menunjukkan ketepatan sekitar 95%. YAGO didasarkan pada model yang bersih secara logik, yang dapat diputuskan, dapat diperluas, dan serasi dengan RDFS. Akhirnya, kami menunjukkan bagaimana YAGO dapat diperkembangkan lagi dengan teknik pengekstrakan maklumat terkini. [[EENNDD]] umum; wikipedia; jaring perkataan"], [{"string": "Practical end-to-end web content integrity Widespread growth of open wireless hotspots has made it easy to carry out man-in-the-middle attacks and impersonate web sites. Although HTTPS can be used to prevent such attacks, its universal adoption is hindered by its performance cost and its inability to leverage caching at intermediate servers (such as CDN servers and caching proxies) while maintaining end-to-end security. To complement HTTPS, we revive an old idea from SHTTP, a protocol that offers end-to-end web integrity without confidentiality. We name the protocol HTTPi and give it an efficient design that is easy to deploy for today's web. In particular, we tackle several previously-unidentified challenges, such as supporting progressive page loading on the client's browser, handling mixed content, and defining access control policies among HTTP, HTTPi, and HTTPS content from the same domain. Our prototyping and evaluation experience show that HTTPi incurs negligible performance overhead over HTTP, can leverage existing web infrastructure such as CDNs or caching proxies without any modifications to them, and can make many of the mixed-content problems in existing HTTPS web sites easily go away. Based on this experience, we advocate browser and web server vendors to adopt HTTPi.", "keywords": ["content integrity", "web security", "caching", "security and protection"], "combined": "Practical end-to-end web content integrity Widespread growth of open wireless hotspots has made it easy to carry out man-in-the-middle attacks and impersonate web sites. Although HTTPS can be used to prevent such attacks, its universal adoption is hindered by its performance cost and its inability to leverage caching at intermediate servers (such as CDN servers and caching proxies) while maintaining end-to-end security. To complement HTTPS, we revive an old idea from SHTTP, a protocol that offers end-to-end web integrity without confidentiality. We name the protocol HTTPi and give it an efficient design that is easy to deploy for today's web. In particular, we tackle several previously-unidentified challenges, such as supporting progressive page loading on the client's browser, handling mixed content, and defining access control policies among HTTP, HTTPi, and HTTPS content from the same domain. Our prototyping and evaluation experience show that HTTPi incurs negligible performance overhead over HTTP, can leverage existing web infrastructure such as CDNs or caching proxies without any modifications to them, and can make many of the mixed-content problems in existing HTTPS web sites easily go away. Based on this experience, we advocate browser and web server vendors to adopt HTTPi. [[EENNDD]] content integrity; web security; caching; security and protection"}, "Integriti kandungan web hujung-ke-ujung yang praktikal Pertumbuhan hotspot tanpa wayar terbuka yang meluas menjadikannya mudah untuk melakukan serangan man-in-the-middle dan menyamar sebagai laman web. Walaupun HTTPS dapat digunakan untuk mencegah serangan seperti itu, penerapannya secara universal terhalang oleh biaya kinerjanya dan ketidakmampuannya memanfaatkan cache di pelayan perantaraan (seperti pelayan CDN dan proksi cache) sambil menjaga keamanan dari ujung ke ujung. Untuk melengkapkan HTTPS, kami menghidupkan semula idea lama dari SHTTP, protokol yang menawarkan integriti web end-to-end tanpa kerahsiaan. Kami memberi nama protokol HTTPi dan memberikannya reka bentuk yang cekap yang mudah digunakan untuk web masa kini. Khususnya, kami menangani beberapa cabaran yang sebelumnya tidak dikenali, seperti menyokong pemuatan halaman progresif pada penyemak imbas pelanggan, menangani kandungan campuran, dan menentukan dasar kawalan akses di antara kandungan HTTP, HTTPi, dan HTTPS dari domain yang sama. Pengalaman membuat prototaip dan penilaian kami menunjukkan bahawa HTTPi mengalami overhead prestasi yang tidak dapat diabaikan melalui HTTP, dapat memanfaatkan infrastruktur web yang ada seperti CDN atau proksi cache tanpa pengubahsuaian pada mereka, dan dapat membuat banyak masalah kandungan campuran di laman web HTTPS yang ada dengan mudah hilang . Berdasarkan pengalaman ini, kami menganjurkan vendor pelayan dan pelayan web untuk menggunakan HTTPi. [[EENNDD]] integriti kandungan; keselamatan web; caching; keselamatan dan perlindungan"], [{"string": "Ontalk: ontology-based personal document management system No contact information provided yet.", "keywords": ["ontology", "knowledge management", "inference etc."], "combined": "Ontalk: ontology-based personal document management system No contact information provided yet. [[EENNDD]] ontology; knowledge management; inference etc."}, "Ontalk: sistem pengurusan dokumen peribadi berasaskan ontologi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ontologi; pengurusan pengetahuan; inferens dll."], [{"string": "PAKE-based mutual HTTP authentication for preventing phishing attacks We developed a new Web authentication protocol with password-based mutual authentication which prevents various kinds of phishing attacks. This protocol provides a protection of user's passwords against any phishers even if a dictionary attack is employed, and prevents phishers from imitating a false sense of successful authentication to users. The protocol is designed considering interoperability with many recent Web applications which requires many features which current HTTP authentication does not provide. The protocol is proposed as an Internet Draft submitted to IETF, and implemented in both server side (as an Apache extension) and client side (as a Mozilla-based browser and an IE-based one).", "keywords": ["web systems", "http", "applications", "mutual authentication"], "combined": "PAKE-based mutual HTTP authentication for preventing phishing attacks We developed a new Web authentication protocol with password-based mutual authentication which prevents various kinds of phishing attacks. This protocol provides a protection of user's passwords against any phishers even if a dictionary attack is employed, and prevents phishers from imitating a false sense of successful authentication to users. The protocol is designed considering interoperability with many recent Web applications which requires many features which current HTTP authentication does not provide. The protocol is proposed as an Internet Draft submitted to IETF, and implemented in both server side (as an Apache extension) and client side (as a Mozilla-based browser and an IE-based one). [[EENNDD]] web systems; http; applications; mutual authentication"}, "Pengesahan HTTP saling berasaskan PAKE untuk mencegah serangan pancingan data Kami mengembangkan protokol pengesahan Web baru dengan pengesahan bersama berasaskan kata laluan yang menghalang pelbagai jenis serangan pancingan data. Protokol ini memberikan perlindungan kata laluan pengguna terhadap sebarang phisher walaupun serangan kamus digunakan, dan mencegah phisher meniru rasa palsu pengesahan yang berjaya kepada pengguna. Protokol ini dirancang dengan mempertimbangkan interoperabilitas dengan banyak aplikasi Web terkini yang memerlukan banyak ciri yang tidak disediakan oleh pengesahan HTTP semasa. Protokol ini diusulkan sebagai Draf Internet yang diserahkan kepada IETF, dan dilaksanakan di kedua sisi pelayan (sebagai pelanjutan Apache) dan di sisi klien (sebagai penyemak imbas berasaskan Mozilla dan yang berasaskan IE). [[EENNDD]] sistem web; http; permohonan; pengesahan bersama"], [{"string": "Modeling and predicting behavioral dynamics on the web User behavior on the Web changes over time. For example, the queries that people issue to search engines, and the underlying informational goals behind the queries vary over time. In this paper, we examine how to model and predict this temporal user behavior. We develop a temporal modeling framework adapted from physics and signal processing that can be used to predict time-varying user behavior using smoothing and trends. We also explore other dynamics of Web behaviors, such as the detection of periodicities and surprises. We develop a learning procedure that can be used to construct models of users' activities based on features of current and historical behaviors. The results of experiments indicate that by using our framework to predict user behavior, we can achieve significant improvements in prediction compared to baseline models that weight historical evidence the same for all queries. We also develop a novel learning algorithm that explicitly learns when to apply a given prediction model among a set of such models. Our improved temporal modeling of user behavior can be used to enhance query suggestions, crawling policies, and result ranking.", "keywords": ["predictive behavioral models", "applications", "behavioral analysis"], "combined": "Modeling and predicting behavioral dynamics on the web User behavior on the Web changes over time. For example, the queries that people issue to search engines, and the underlying informational goals behind the queries vary over time. In this paper, we examine how to model and predict this temporal user behavior. We develop a temporal modeling framework adapted from physics and signal processing that can be used to predict time-varying user behavior using smoothing and trends. We also explore other dynamics of Web behaviors, such as the detection of periodicities and surprises. We develop a learning procedure that can be used to construct models of users' activities based on features of current and historical behaviors. The results of experiments indicate that by using our framework to predict user behavior, we can achieve significant improvements in prediction compared to baseline models that weight historical evidence the same for all queries. We also develop a novel learning algorithm that explicitly learns when to apply a given prediction model among a set of such models. Our improved temporal modeling of user behavior can be used to enhance query suggestions, crawling policies, and result ranking. [[EENNDD]] predictive behavioral models; applications; behavioral analysis"}, "Memodelkan dan meramalkan dinamika tingkah laku di web Tingkah laku pengguna di Web berubah dari masa ke masa. Contohnya, pertanyaan yang dikeluarkan oleh pengguna ke enjin carian, dan tujuan maklumat yang mendasari pertanyaan berbeza dari masa ke masa. Dalam makalah ini, kami mengkaji bagaimana memodelkan dan meramalkan tingkah laku pengguna sementara ini. Kami mengembangkan kerangka pemodelan temporal yang diadaptasi dari fizik dan pemprosesan isyarat yang dapat digunakan untuk memprediksi tingkah laku pengguna yang berubah-ubah mengikut waktu menggunakan kelancaran dan tren. Kami juga meneroka dinamika tingkah laku Web lain, seperti pengesanan berkala dan kejutan. Kami mengembangkan prosedur pembelajaran yang dapat digunakan untuk membina model aktiviti pengguna berdasarkan ciri-ciri tingkah laku semasa dan sejarah. Hasil eksperimen menunjukkan bahawa dengan menggunakan kerangka kerja kami untuk meramalkan tingkah laku pengguna, kami dapat mencapai peningkatan yang signifikan dalam ramalan dibandingkan dengan model garis dasar yang memberikan bukti sejarah yang sama untuk semua pertanyaan. Kami juga mengembangkan algoritma pembelajaran novel yang secara eksplisit belajar kapan menerapkan model ramalan yang diberikan di antara satu set model tersebut. Pemodelan temporal tingkah laku pengguna kami yang lebih baik dapat digunakan untuk meningkatkan cadangan pertanyaan, dasar merangkak, dan peringkat hasil. [[EENNDD]] model tingkah laku ramalan; permohonan; analisis tingkah laku"], [{"string": "Off the beaten tracks: exploring three aspects of web navigation No contact information provided yet.", "keywords": ["user modeling", "clickstream study", "navigation", "browser interfaces", "hypertext"], "combined": "Off the beaten tracks: exploring three aspects of web navigation No contact information provided yet. [[EENNDD]] user modeling; clickstream study; navigation; browser interfaces; hypertext"}, "Di luar jalan: meneroka tiga aspek navigasi web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pemodelan pengguna; kajian aliran klik; pelayaran; antara muka penyemak imbas; hiperteks"], [{"string": "The social honeypot project: protecting online communities from spammers We present the conceptual framework of the Social Honeypot Project for uncovering social spammers who target online communities and initial empirical results from Twitter and MySpace. Two of the key components of the Social Honeypot Project are: (1) The deployment of social honeypots for harvesting deceptive spam profiles from social networking communities; and (2) Statistical analysis of the properties of these spam profiles for creating spam classifiers to actively filter out existing and new spammers.", "keywords": ["social media", "social honeypots", "spam"], "combined": "The social honeypot project: protecting online communities from spammers We present the conceptual framework of the Social Honeypot Project for uncovering social spammers who target online communities and initial empirical results from Twitter and MySpace. Two of the key components of the Social Honeypot Project are: (1) The deployment of social honeypots for harvesting deceptive spam profiles from social networking communities; and (2) Statistical analysis of the properties of these spam profiles for creating spam classifiers to actively filter out existing and new spammers. [[EENNDD]] social media; social honeypots; spam"}, "Projek honeypot sosial: melindungi komuniti dalam talian dari spammer Kami membentangkan kerangka konsep Projek Honeypot Sosial untuk mendedahkan spammer sosial yang mensasarkan komuniti dalam talian dan hasil empirikal awal dari Twitter dan MySpace. Dua komponen penting dalam Projek Honeypot Sosial adalah: (1) Penyebaran honeypots sosial untuk mengambil profil spam yang menipu dari komuniti rangkaian sosial; dan (2) Analisis statistik sifat profil spam ini untuk membuat pengklasifikasi spam untuk secara aktif menyaring spammer yang ada dan baru. [[EENNDD]] media sosial; honeypots sosial; spam"], [{"string": "Crawling English-Japanese person-name transliterations from the web Automatic compilation of lexicon is a dream of lexicon compilers as well as lexicon users. This paper proposes a system that crawls English-Japanese person-name transliterations from the Web, which works a back-end collector for automatic compilation of bilingual person-name lexicon. Our crawler collected 561K transliterations in five months. From them, an English-Japanese person-name lexicon with 406K entries has been compiled by an automatic post processing. This lexicon is much larger than other similar resources including English-Japanese lexicon of HeiNER obtained from Wikipedia.", "keywords": ["mining transliteration pairs", "natural language processing", "information search and retrieval", "automatic lexicon compilation", "person name"], "combined": "Crawling English-Japanese person-name transliterations from the web Automatic compilation of lexicon is a dream of lexicon compilers as well as lexicon users. This paper proposes a system that crawls English-Japanese person-name transliterations from the Web, which works a back-end collector for automatic compilation of bilingual person-name lexicon. Our crawler collected 561K transliterations in five months. From them, an English-Japanese person-name lexicon with 406K entries has been compiled by an automatic post processing. This lexicon is much larger than other similar resources including English-Japanese lexicon of HeiNER obtained from Wikipedia. [[EENNDD]] mining transliteration pairs; natural language processing; information search and retrieval; automatic lexicon compilation; person name"}, "Merangkak transliterasi nama orang Inggeris-Jepun dari web Penyusunan leksikon automatik adalah impian penyusun leksikon dan juga pengguna leksikon. Makalah ini mencadangkan sistem yang merangkumi transliterasi nama orang Inggeris-Jepun dari Web, yang berfungsi sebagai pengumpul belakang untuk penyusunan automatik kamus dwibahasa nama orang. Perayap kami mengumpulkan 561K transliterasi dalam lima bulan. Dari mereka, leksikon nama orang Inggeris-Jepun dengan 406K penyertaan telah disusun oleh pemprosesan pos automatik. Leksikon ini jauh lebih besar daripada sumber lain yang serupa termasuk leksikon Inggeris-Jepun HeiNER yang diperoleh dari Wikipedia. [[EENNDD]] pasangan transliterasi perlombongan; pemprosesan bahasa semula jadi; pencarian dan pengambilan maklumat; penyusunan leksikon automatik; nama orang"], [{"string": "Malicious interface design: exploiting the user In an ideal world, interface design is the art and science of helping users accomplish tasks in a timely, efficient, and pleasurable manner. This paper studies the inverse situation, the vast emergence of deliberately constructed malicious interfaces that violate design best practices in order to accomplish goals counter to those of the user. This has become a commonplace occurrence both on and off the desktop, particularly on the web. A primary objective of this paper is to formally define this problem, including construction of a taxonomy of malicious interface techniques and a preliminary analysis of their impact on users. Findings are presented that gauge the self-reported tolerance and expectation levels of users with regard to malicious interfaces as well as the effectiveness and ease of use of existing countermeasures. A second objective of this paper is to increase awareness, dialogue, and research in a domain that we consider largely unexplored but critical to future usability of the WWW. Our results were accomplished through significant compilation of malicious interface techniques based on review of thousands of web sites and by conducting three surveys. Ultimately, this paper concludes that malicious interfaces are a ubiquitous problem that demands intervention by the security and human computer interaction communities in order to reduce the negative impact on the global user population.", "keywords": ["web usability guidelines", "miscellaneous", "evil interfaces", "adversarial interface design", "design principles", "malicious interfaces"], "combined": "Malicious interface design: exploiting the user In an ideal world, interface design is the art and science of helping users accomplish tasks in a timely, efficient, and pleasurable manner. This paper studies the inverse situation, the vast emergence of deliberately constructed malicious interfaces that violate design best practices in order to accomplish goals counter to those of the user. This has become a commonplace occurrence both on and off the desktop, particularly on the web. A primary objective of this paper is to formally define this problem, including construction of a taxonomy of malicious interface techniques and a preliminary analysis of their impact on users. Findings are presented that gauge the self-reported tolerance and expectation levels of users with regard to malicious interfaces as well as the effectiveness and ease of use of existing countermeasures. A second objective of this paper is to increase awareness, dialogue, and research in a domain that we consider largely unexplored but critical to future usability of the WWW. Our results were accomplished through significant compilation of malicious interface techniques based on review of thousands of web sites and by conducting three surveys. Ultimately, this paper concludes that malicious interfaces are a ubiquitous problem that demands intervention by the security and human computer interaction communities in order to reduce the negative impact on the global user population. [[EENNDD]] web usability guidelines; miscellaneous; evil interfaces; adversarial interface design; design principles; malicious interfaces"}, "Reka bentuk antara muka berniat jahat: mengeksploitasi pengguna Dalam dunia yang ideal, reka bentuk antara muka adalah seni dan sains untuk membantu pengguna menyelesaikan tugas dengan tepat pada masanya, cekap, dan menyenangkan. Makalah ini mengkaji keadaan terbalik, kemunculan banyak antara muka berniat jahat yang sengaja dibuat yang melanggar amalan terbaik reka bentuk untuk mencapai tujuan yang bertentangan dengan pengguna. Ini telah menjadi kejadian biasa di dalam dan di luar desktop, terutama di web. Objektif utama makalah ini adalah untuk secara formal menentukan masalah ini, termasuk pembinaan taksonomi teknik antara muka yang berniat jahat dan analisis awal kesannya terhadap pengguna. Penemuan disajikan yang mengukur tahap toleransi dan jangkaan pengguna yang dilaporkan sendiri berkenaan dengan antara muka yang berniat jahat serta keberkesanan dan kemudahan penggunaan tindakan balas yang ada. Objektif kedua makalah ini adalah untuk meningkatkan kesedaran, dialog, dan penyelidikan dalam domain yang kami anggap sebahagian besarnya belum diterokai tetapi penting untuk kegunaan WWW di masa depan. Hasil kami dicapai melalui penyusunan teknik antara muka berniat jahat yang signifikan berdasarkan tinjauan ribuan laman web dan dengan melakukan tiga tinjauan. Akhirnya, makalah ini menyimpulkan bahawa antara muka berniat jahat adalah masalah di mana-mana yang menuntut campur tangan oleh komuniti keselamatan dan interaksi komputer manusia untuk mengurangkan kesan negatif terhadap populasi pengguna global. [[EENNDD]] garis panduan kebolehgunaan web; pelbagai; antara muka jahat; reka bentuk antara muka lawan; prinsip reka bentuk; antara muka berniat jahat"], [{"string": "LINDEN: linking named entities with knowledge base via semantic knowledge Integrating the extracted facts with an existing knowledge base has raised an urgent need to address the problem of entity linking. Specifically, entity linking is the task to link the entity mention in text with the corresponding real world entity in the existing knowledge base. However, this task is challenging due to name ambiguity, textual inconsistency, and lack of world knowledge in the knowledge base. Several methods have been proposed to tackle this problem, but they are largely based on the co-occurrence statistics of terms between the text around the entity mention and the document associated with the entity. In this paper, we propose LINDEN, a novel framework to link named entities in text with a knowledge base unifying Wikipedia and WordNet, by leveraging the rich semantic knowledge embedded in the Wikipedia and the taxonomy of the knowledge base. We extensively evaluate the performance of our proposed LINDEN over two public data sets and empirical results show that LINDEN significantly outperforms the state-of-the-art methods in terms of accuracy.", "keywords": ["knowledge base", "semantic knowledge", "information search and retrieval", "entity linking", "wikipedia", "fact integration"], "combined": "LINDEN: linking named entities with knowledge base via semantic knowledge Integrating the extracted facts with an existing knowledge base has raised an urgent need to address the problem of entity linking. Specifically, entity linking is the task to link the entity mention in text with the corresponding real world entity in the existing knowledge base. However, this task is challenging due to name ambiguity, textual inconsistency, and lack of world knowledge in the knowledge base. Several methods have been proposed to tackle this problem, but they are largely based on the co-occurrence statistics of terms between the text around the entity mention and the document associated with the entity. In this paper, we propose LINDEN, a novel framework to link named entities in text with a knowledge base unifying Wikipedia and WordNet, by leveraging the rich semantic knowledge embedded in the Wikipedia and the taxonomy of the knowledge base. We extensively evaluate the performance of our proposed LINDEN over two public data sets and empirical results show that LINDEN significantly outperforms the state-of-the-art methods in terms of accuracy. [[EENNDD]] knowledge base; semantic knowledge; information search and retrieval; entity linking; wikipedia; fact integration"}, "LINDEN: menghubungkan entiti bernama dengan pangkalan pengetahuan melalui pengetahuan semantik Menggabungkan fakta yang diekstrak dengan pangkalan pengetahuan yang ada telah menimbulkan keperluan mendesak untuk mengatasi masalah penghubung entiti. Secara khusus, penghubung entiti adalah tugas untuk menghubungkan entiti dalam teks dengan entiti dunia nyata yang sesuai di pangkalan pengetahuan yang ada. Walau bagaimanapun, tugas ini mencabar kerana kesamaran nama, ketidakkonsistenan teks, dan kurangnya pengetahuan dunia di pangkalan pengetahuan. Beberapa kaedah telah diusulkan untuk mengatasi masalah ini, tetapi sebagian besar berdasarkan statistik kejadian bersama antara teks di sekitar entiti yang disebutkan dan dokumen yang terkait dengan entiti. Dalam makalah ini, kami mengusulkan LINDEN, kerangka novel untuk menghubungkan entiti bernama dalam teks dengan pangkalan pengetahuan yang menyatukan Wikipedia dan WordNet, dengan memanfaatkan pengetahuan semantik kaya yang terdapat dalam Wikipedia dan taksonomi pangkalan pengetahuan. Kami menilai secara menyeluruh prestasi LINDEN yang kami cadangkan di atas dua set data awam dan hasil empirik menunjukkan bahawa LINDEN secara signifikan mengatasi kaedah terkini dari segi ketepatan. [[EENNDD]] pangkalan pengetahuan; pengetahuan semantik; carian dan pengambilan maklumat; menghubungkan entiti; wikipedia; penyatuan fakta"], [{"string": "Efficient overlap and content reuse detection in blogs and online news articles The use of blogs to track and comment on real world (political, news, entertainment) events is growing. Similarly, as more individuals start relying on the Web as their primary information source and as more traditional media outlets try reaching consumers through alternative venues, the number of news sites on the Web is also continuously increasing. Content-reuse, whether in the form of extensive quotations or content borrowing across media outlets, is very common in blogs and news entries outlets tracking the same real-world event. Knowledge about which web entries re-use content from which others can be an effective asset when organizing these entries for presentation. On the other hand, this knowledge is not cheap to acquire: considering the size of the related space web entries, it is essential that the techniques developed for identifying re-use are fast and scalable. Furthermore, the dynamic nature of blog and news entries necessitates incremental processing for reuse detection. In this paper, we develop a novel qSign algorithm that efficiently and effectively analyze the blogosphere for quotation and reuse identification. Experiment results show that with qSign processing time gains from 10X to 100X are possible while maintaining reuse detection rates of upto 90%. Furthermore, processing time gains can be pushed multiple orders of magnitude (from 100X to 1000X) for 70% recall.", "keywords": ["weblogs", "information search and retrieval", "reuse detection"], "combined": "Efficient overlap and content reuse detection in blogs and online news articles The use of blogs to track and comment on real world (political, news, entertainment) events is growing. Similarly, as more individuals start relying on the Web as their primary information source and as more traditional media outlets try reaching consumers through alternative venues, the number of news sites on the Web is also continuously increasing. Content-reuse, whether in the form of extensive quotations or content borrowing across media outlets, is very common in blogs and news entries outlets tracking the same real-world event. Knowledge about which web entries re-use content from which others can be an effective asset when organizing these entries for presentation. On the other hand, this knowledge is not cheap to acquire: considering the size of the related space web entries, it is essential that the techniques developed for identifying re-use are fast and scalable. Furthermore, the dynamic nature of blog and news entries necessitates incremental processing for reuse detection. In this paper, we develop a novel qSign algorithm that efficiently and effectively analyze the blogosphere for quotation and reuse identification. Experiment results show that with qSign processing time gains from 10X to 100X are possible while maintaining reuse detection rates of upto 90%. Furthermore, processing time gains can be pushed multiple orders of magnitude (from 100X to 1000X) for 70% recall. [[EENNDD]] weblogs; information search and retrieval; reuse detection"}, "Pengesanan pertindihan dan penggunaan semula kandungan yang cekap dalam blog dan artikel berita dalam talian Penggunaan blog untuk mengesan dan memberi komen mengenai peristiwa dunia nyata (politik, berita, hiburan) semakin meningkat. Begitu juga, apabila semakin banyak individu mulai bergantung pada Web sebagai sumber maklumat utama mereka dan ketika lebih banyak media tradisional berusaha menjangkau pengguna melalui tempat alternatif, jumlah laman berita di Web juga terus meningkat. Penggunaan semula kandungan, sama ada dalam bentuk sebut harga yang luas atau peminjaman konten di seluruh media, sangat biasa di blog dan kedai entri berita yang mengesan peristiwa dunia nyata yang sama. Pengetahuan mengenai entri web mana yang menggunakan semula kandungan dari mana yang lain dapat menjadi aset yang berkesan ketika mengatur entri ini untuk persembahan. Sebaliknya, pengetahuan ini tidak murah untuk diperoleh: memandangkan ukuran entri web ruang yang berkaitan, adalah mustahak bahawa teknik yang dikembangkan untuk mengenal pasti penggunaan semula cepat dan dapat ditingkatkan. Tambahan pula, sifat blog dan entri berita yang dinamik memerlukan pemprosesan tambahan untuk pengesanan penggunaan semula. Dalam makalah ini, kami mengembangkan algoritma qSign novel yang menganalisis blogosfera dengan cekap dan berkesan untuk sebut harga dan pengenalan semula. Hasil eksperimen menunjukkan bahawa dengan proses pemprosesan qSign, keuntungan dari 10X hingga 100X adalah mungkin sambil mengekalkan kadar pengesanan penggunaan semula sehingga 90%. Tambahan pula, keuntungan masa pemprosesan dapat diturunkan dengan banyak pesanan (100X hingga 1000X) untuk penarikan 70%. [[EENNDD]] blog web; carian dan pengambilan maklumat; pengesanan semula"], [{"string": "Incentivizing high-quality user-generated content We model the economics of incentivizing high-quality user generated content (UGC), motivated by settings such as online review forums, question-answer sites, and comments on news articles and blogs. We provide a game-theoretic model within which to study the problem of incentivizing high quality UGC, in which contributors are strategic and motivated by exposure. Our model has the feature that both the quality of contributions as well as the extent of participation is determined endogenously in a free-entry Nash equilibrium.", "keywords": ["attention economics", "quality of online content", "user generated content", "game theory"], "combined": "Incentivizing high-quality user-generated content We model the economics of incentivizing high-quality user generated content (UGC), motivated by settings such as online review forums, question-answer sites, and comments on news articles and blogs. We provide a game-theoretic model within which to study the problem of incentivizing high quality UGC, in which contributors are strategic and motivated by exposure. Our model has the feature that both the quality of contributions as well as the extent of participation is determined endogenously in a free-entry Nash equilibrium. [[EENNDD]] attention economics; quality of online content; user generated content; game theory"}, "Memasukkan kandungan yang dihasilkan pengguna berkualiti tinggi Kami memodelkan ekonomi memberi insentif kepada kandungan yang dihasilkan pengguna berkualiti tinggi (UGC), yang dimotivasi oleh tetapan seperti forum ulasan dalam talian, laman web soal jawab, dan komen pada artikel berita dan blog. Kami menyediakan model teori permainan untuk mengkaji masalah insentif UGC berkualiti tinggi, di mana penyumbang bersifat strategik dan termotivasi oleh pendedahan. Model kami mempunyai ciri bahawa kualiti sumbangan dan tahap penyertaan ditentukan secara endogen dalam keseimbangan Nash kemasukan percuma. [[EENNDD]] ekonomi perhatian; kualiti kandungan dalam talian; kandungan yang dihasilkan pengguna; teori permainan"], [{"string": "Testing google interfaces modified for the blind No contact information provided yet.", "keywords": ["search engine", "user interface design", "accessibility", "usability", "blind"], "combined": "Testing google interfaces modified for the blind No contact information provided yet. [[EENNDD]] search engine; user interface design; accessibility; usability; blind"}, "Menguji antara muka google yang diubah suai untuk orang buta Belum ada maklumat hubungan yang diberikan. [[EENNDD]] enjin carian; reka bentuk antara muka pengguna; kebolehcapaian; kebolehgunaan; buta"], [{"string": "we.b: the web of short urls Short URLs have become ubiquitous. Especially popular within social networking services, short URLs have seen a significant increase in their usage over the past years, mostly due to Twitter's restriction of message length to 140 characters. In this paper, we provide a first characterization on the usage of short URLs. Specifically, our goal is to examine the content short URLs point to, how they are published, their popularity and activity over time, as well as their potential impact on the performance of the web.", "keywords": ["general", "twitter", "online social networks", "short urls"], "combined": "we.b: the web of short urls Short URLs have become ubiquitous. Especially popular within social networking services, short URLs have seen a significant increase in their usage over the past years, mostly due to Twitter's restriction of message length to 140 characters. In this paper, we provide a first characterization on the usage of short URLs. Specifically, our goal is to examine the content short URLs point to, how they are published, their popularity and activity over time, as well as their potential impact on the performance of the web. [[EENNDD]] general; twitter; online social networks; short urls"}, "we.b: web url pendek URL pendek telah terdapat di mana-mana. Terutama popular dalam perkhidmatan rangkaian sosial, URL pendek telah menyaksikan peningkatan penggunaannya yang ketara sejak beberapa tahun kebelakangan ini, kebanyakannya disebabkan oleh sekatan Twitter sehingga panjang mesej hingga 140 aksara. Dalam makalah ini, kami memberikan pencirian pertama mengenai penggunaan URL pendek. Secara khusus, tujuan kami adalah untuk meneliti isi URL pendek yang ditunjuk, bagaimana ia diterbitkan, populariti dan aktiviti mereka dari masa ke masa, serta potensi kesannya terhadap prestasi web. [[EENNDD]] umum; twitter; rangkaian sosial dalam talian; url pendek"], [{"string": "Mining clickthrough data for collaborative web search No contact information provided yet.", "keywords": ["cube-clustering", "collaborative web search", "clickthrough data"], "combined": "Mining clickthrough data for collaborative web search No contact information provided yet. [[EENNDD]] cube-clustering; collaborative web search; clickthrough data"}, "Menambang data klik-tayang untuk carian web kolaboratif Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengelompokan kubus; carian web kolaboratif; data klik lalu"], [{"string": "Named graphs, provenance and trust No contact information provided yet.", "keywords": ["knowledge representation formalisms and methods", "provenance", "trust", "semantic web", "rdf"], "combined": "Named graphs, provenance and trust No contact information provided yet. [[EENNDD]] knowledge representation formalisms and methods; provenance; trust; semantic web; rdf"}, "Grafik, asalnya dan kepercayaan yang dinamakan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] formalisme dan kaedah perwakilan pengetahuan; keturunan; kepercayaan; web semantik; rdf"], [{"string": "An admission control scheme for predictable server response time for web accesses An abstract is not available.", "keywords": ["internet", "admission control", "bounded response time", "pacers", "qos", "service differentiating internet servers"], "combined": "An admission control scheme for predictable server response time for web accesses An abstract is not available. [[EENNDD]] internet; admission control; bounded response time; pacers; qos; service differentiating internet servers"}, "Skema kawalan kemasukan untuk masa tindak balas pelayan yang dapat diramalkan untuk akses web Abstrak tidak tersedia. [[EENNDD]] internet; kawalan kemasukan; masa tindak balas terikat; perentas; qos; perkhidmatan membezakan pelayan internet"], [{"string": "Boosting SVM classifiers by ensemble No contact information provided yet.", "keywords": ["neural nets", "information search and retrieval", "design methodology", "information filtering", "text processing", "classifier design and evaluation", "machine learning"], "combined": "Boosting SVM classifiers by ensemble No contact information provided yet. [[EENNDD]] neural nets; information search and retrieval; design methodology; information filtering; text processing; classifier design and evaluation; machine learning"}, "Meningkatkan pengkelasan SVM dengan ensemble Belum ada maklumat hubungan yang diberikan. [[EENNDD]] jaring saraf; pencarian dan pengambilan maklumat; metodologi reka bentuk; tapisan maklumat; pemprosesan teks; reka bentuk dan penilaian pengkelasan; pembelajaran mesin"], [{"string": "PageRank as a function of the damping factor No contact information provided yet.", "keywords": ["approximation", "graph theory", "pagerank", "web graph"], "combined": "PageRank as a function of the damping factor No contact information provided yet. [[EENNDD]] approximation; graph theory; pagerank; web graph"}, "PageRank sebagai fungsi faktor redaman Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] penghampiran; teori grafik; pagerank; grafik web"], [{"string": "Video summarization via transferrable structured learning It is well-known that textual information such as video transcripts and video reviews can significantly enhance the performance of video summarization algorithms. Unfortunately, many videos on the Web such as those from the popular video sharing site YouTube do not have useful textual information. The goal of this paper is to propose a transfer learning framework for video summarization: in the training process both the video features and textual features are exploited to train a summarization algorithm while for summarizing a new video only its video features are utilized. The basic idea is to explore the transferability between videos and their corresponding textual information. Based on the assumption that video features and textual features are highly correlated with each other, we can transfer textual information into knowledge on summarization using video information only. In particular, we formulate the video summarization problem as that of learning a mapping from a set of shots of a video to a subset of the shots using the general framework of SVM-based structured learning. Textual information is transferred by encoding them into a set of constraints used in the structured learning process which tend to provide a more detailed and accurate characterization of the different subsets of shots. Experimental results show significant performance improvement of our approach and demonstrate the utility of textual information for enhancing video summarization.", "keywords": ["transfer learning", "video summarization", "structural svm"], "combined": "Video summarization via transferrable structured learning It is well-known that textual information such as video transcripts and video reviews can significantly enhance the performance of video summarization algorithms. Unfortunately, many videos on the Web such as those from the popular video sharing site YouTube do not have useful textual information. The goal of this paper is to propose a transfer learning framework for video summarization: in the training process both the video features and textual features are exploited to train a summarization algorithm while for summarizing a new video only its video features are utilized. The basic idea is to explore the transferability between videos and their corresponding textual information. Based on the assumption that video features and textual features are highly correlated with each other, we can transfer textual information into knowledge on summarization using video information only. In particular, we formulate the video summarization problem as that of learning a mapping from a set of shots of a video to a subset of the shots using the general framework of SVM-based structured learning. Textual information is transferred by encoding them into a set of constraints used in the structured learning process which tend to provide a more detailed and accurate characterization of the different subsets of shots. Experimental results show significant performance improvement of our approach and demonstrate the utility of textual information for enhancing video summarization. [[EENNDD]] transfer learning; video summarization; structural svm"}, "Ringkasan video melalui pembelajaran terstruktur yang dapat dipindahkan Sudah diketahui bahawa maklumat teks seperti transkrip video dan ulasan video dapat meningkatkan prestasi algoritma ringkasan video dengan ketara. Malangnya, banyak video di Web seperti video dari laman perkongsian video yang popular di YouTube tidak mempunyai maklumat teks yang berguna. Tujuan makalah ini adalah untuk mengusulkan kerangka pembelajaran pemindahan untuk ringkasan video: dalam proses latihan, kedua-dua ciri video dan ciri teks dieksploitasi untuk melatih algoritma ringkasan sementara untuk meringkaskan video baru, hanya ciri-ciri videonya yang digunakan. Idea asasnya adalah untuk meneroka kebolehpindahan antara video dan maklumat teks yang sesuai. Berdasarkan andaian bahawa ciri-ciri video dan ciri teks sangat berkorelasi antara satu sama lain, kita dapat memindahkan maklumat teks ke pengetahuan mengenai ringkasan menggunakan maklumat video sahaja. Secara khusus, kami merumuskan masalah ringkasan video seperti belajar pemetaan dari sekumpulan gambar video ke subset gambar menggunakan kerangka umum pembelajaran berstruktur berbasis SVM. Maklumat teks ditransfer dengan mengekodkannya ke dalam sekumpulan kekangan yang digunakan dalam proses pembelajaran berstruktur yang cenderung memberikan pencirian yang lebih terperinci dan tepat dari pelbagai kumpulan gambar. Hasil eksperimen menunjukkan peningkatan prestasi yang signifikan dari pendekatan kami dan menunjukkan kegunaan maklumat teks untuk meningkatkan ringkasan video. [[EENNDD]] memindahkan pembelajaran; ringkasan video; struktur svm"], [{"string": "IEPAD: information extraction based on pattern discovery An abstract is not available.", "keywords": ["extraction rule", "information extraction", "web", "multiple string alignment", "pat tree", "segmentation"], "combined": "IEPAD: information extraction based on pattern discovery An abstract is not available. [[EENNDD]] extraction rule; information extraction; web; multiple string alignment; pat tree; segmentation"}, "IEPAD: pengekstrakan maklumat berdasarkan penemuan corak Abstrak tidak tersedia. [[EENNDD]] peraturan pengekstrakan; pengekstrakan maklumat; laman web; penjajaran rentetan pelbagai; pokok tepuk; pembahagian"], [{"string": "A framework for evaluating network measures for functional importance Many metrics such as degree, closeness, and PageRank have been introduced to determine the relative importance of a node within a network. The desired function of a network, however, is domain-specific. For example, the robustness can be crucial for a communication network, while efficiency is more preferred for fast spreading of advertisements in viral marketing. The information provided by some widely used measures are often conflicting under such varying demands. In this paper, we present a novel framework for evaluating network metrics regarding typical functional requirements. We also propose an analysis of five well established measures to compare their performance of ranking nodes on functional importance in a real-life network.", "keywords": ["network metrics", "functional importance", "database applications"], "combined": "A framework for evaluating network measures for functional importance Many metrics such as degree, closeness, and PageRank have been introduced to determine the relative importance of a node within a network. The desired function of a network, however, is domain-specific. For example, the robustness can be crucial for a communication network, while efficiency is more preferred for fast spreading of advertisements in viral marketing. The information provided by some widely used measures are often conflicting under such varying demands. In this paper, we present a novel framework for evaluating network metrics regarding typical functional requirements. We also propose an analysis of five well established measures to compare their performance of ranking nodes on functional importance in a real-life network. [[EENNDD]] network metrics; functional importance; database applications"}, "Kerangka kerja untuk menilai langkah-langkah rangkaian untuk kepentingan fungsional Banyak metrik seperti darjah, kedekatan, dan PageRank telah diperkenalkan untuk menentukan kepentingan relatif suatu simpul dalam jaringan. Fungsi rangkaian yang diinginkan, bagaimanapun, adalah khusus domain. Sebagai contoh, kekukuhan boleh menjadi penting untuk rangkaian komunikasi, sementara kecekapan lebih disukai untuk penyebaran iklan dalam pemasaran viral dengan cepat. Maklumat yang diberikan oleh beberapa langkah yang digunakan secara meluas sering bertentangan dengan tuntutan yang berbeza-beza. Dalam makalah ini, kami memaparkan kerangka baru untuk menilai metrik rangkaian mengenai keperluan fungsional khas. Kami juga mencadangkan analisis lima langkah yang mapan untuk membandingkan prestasi mereka dari peringkat node pada kepentingan fungsional dalam rangkaian kehidupan nyata. [[EENNDD]] metrik rangkaian; kepentingan fungsi; aplikasi pangkalan data"], [{"string": "Anonymizing user profiles for personalized web search We study the problem of anonymizing user profiles so that user privacy is sufficiently protected while the anonymized profiles are still effective in enabling personalized web search. We propose a Bayes-optimal privacy notion to bound the prior and posterior probability of associating a user with an individual term in the anonymized user profile set. We also propose a novel bundling technique that clusters user profiles into groups by taking into account the semantic relationships between the terms while satisfying the privacy constraint. We evaluate our approach through a set of preliminary experiments using real data demonstrating its feasibility and effectiveness.", "keywords": ["personalized search", "information search and retrieval", "privacy-preserving data publishing", "anonymization"], "combined": "Anonymizing user profiles for personalized web search We study the problem of anonymizing user profiles so that user privacy is sufficiently protected while the anonymized profiles are still effective in enabling personalized web search. We propose a Bayes-optimal privacy notion to bound the prior and posterior probability of associating a user with an individual term in the anonymized user profile set. We also propose a novel bundling technique that clusters user profiles into groups by taking into account the semantic relationships between the terms while satisfying the privacy constraint. We evaluate our approach through a set of preliminary experiments using real data demonstrating its feasibility and effectiveness. [[EENNDD]] personalized search; information search and retrieval; privacy-preserving data publishing; anonymization"}, "Menganonimkan profil pengguna untuk carian web yang diperibadikan Kami mengkaji masalah menganonimkan profil pengguna sehingga privasi pengguna dilindungi secukupnya sementara profil tanpa nama masih berkesan dalam memungkinkan carian web yang diperibadikan. Kami mencadangkan idea privasi Bayes yang optimum untuk mengikat kebarangkalian sebelumnya dan posterior untuk mengaitkan pengguna dengan istilah individu dalam kumpulan profil pengguna tanpa nama. Kami juga mencadangkan teknik ikatan novel yang menggabungkan profil pengguna ke dalam kumpulan dengan mengambil kira hubungan semantik antara syarat sambil memenuhi kekangan privasi. Kami menilai pendekatan kami melalui satu set eksperimen awal menggunakan data sebenar yang menunjukkan kemungkinan dan keberkesanannya. [[EENNDD]] carian diperibadikan; pencarian dan pengambilan maklumat; penerbitan data yang memelihara privasi; tanpa nama"], [{"string": "The SOWES approach to P2P web search using semantic overlays No contact information provided yet.", "keywords": ["content analysis and indexing", "information search and retrieval", "miscellaneous", "distributed and peer-to-peer search", "semantic overlay networks"], "combined": "The SOWES approach to P2P web search using semantic overlays No contact information provided yet. [[EENNDD]] content analysis and indexing; information search and retrieval; miscellaneous; distributed and peer-to-peer search; semantic overlay networks"}, "Pendekatan SOWES untuk carian web P2P menggunakan hamparan semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] analisis dan pengindeksan kandungan; pencarian dan pengambilan maklumat; pelbagai; edaran dan carian rakan sebaya; rangkaian overlay semantik"], [{"string": "Shout out: integrating news and reader comments A useful approach for enabling computers to automatically create new content is utilizing the text, media, and information already present on the World Wide Web. The newly created content is known as \"machine-generated content\". For example, a machine-generated content system may create a multimedia news show with two animated anchors presenting a news story; one anchor reads the news story with text taken from an existing news article, and the other anchor regularly interrupts with his or her own opinion about the story. In this paper, we present such a system, and describe in detail its strategy for autonomously extracting and selecting the opinions given by the second anchor.", "keywords": ["emotional valence detection", "cosine similarity", "machine-generated content"], "combined": "Shout out: integrating news and reader comments A useful approach for enabling computers to automatically create new content is utilizing the text, media, and information already present on the World Wide Web. The newly created content is known as \"machine-generated content\". For example, a machine-generated content system may create a multimedia news show with two animated anchors presenting a news story; one anchor reads the news story with text taken from an existing news article, and the other anchor regularly interrupts with his or her own opinion about the story. In this paper, we present such a system, and describe in detail its strategy for autonomously extracting and selecting the opinions given by the second anchor. [[EENNDD]] emotional valence detection; cosine similarity; machine-generated content"}, "Berseru: menyatukan berita dan komen pembaca Pendekatan yang berguna untuk membolehkan komputer membuat kandungan baru secara automatik adalah menggunakan teks, media, dan maklumat yang sudah ada di World Wide Web. Kandungan yang baru dibuat dikenali sebagai \"kandungan yang dihasilkan oleh mesin\". Sebagai contoh, sistem kandungan yang dihasilkan oleh mesin boleh membuat rancangan berita multimedia dengan dua sauh animasi yang menyampaikan berita; satu penyiar membaca kisah berita dengan teks yang diambil dari artikel berita yang ada, dan penyiar yang lain kerap mengganggu pendapatnya sendiri mengenai kisah itu. Dalam makalah ini, kami menyajikan sistem seperti itu, dan menjelaskan secara terperinci strateginya untuk secara automatik mengekstrak dan memilih pendapat yang diberikan oleh sauh kedua. [[EENNDD]] pengesanan valensi emosi; kesamaan kosinus; kandungan yang dihasilkan oleh mesin"], [{"string": "Incorporating site-level knowledge to extract structured data from web forums Web forums have become an important data resource for many web applications, but extracting structured data from unstructured web forum pages is still a challenging task due to both complex page layout designs and unrestricted user created posts. In this paper, we study the problem of structured data extraction from various web forum sites. Our target is to find a solution as general as possible to extract structured data, such as post title, post author, post time, and post content from any forum site. In contrast to most existing information extraction methods, which only leverage the knowledge inside an individual page, we incorporate both page-level and site-level knowledge and employ Markov logic networks (MLNs) to effectively integrate all useful evidence by learning their importance automatically. Site-level knowledge includes (1) the linkages among different object pages, such as list pages and post pages, and (2) the interrelationships of pages belonging to the same object. The experimental results on 20 forums show a very encouraging information extraction performance, and demonstrate the ability of the proposed approach on various forums. We also show that the performance is limited if only page-level knowledge is used, while when incorporating the site-level knowledge both precision and recall can be significantly improved.", "keywords": ["information extraction", "site-level knowledge", "markov logic networks", "miscellaneous", "structured data", "web forums"], "combined": "Incorporating site-level knowledge to extract structured data from web forums Web forums have become an important data resource for many web applications, but extracting structured data from unstructured web forum pages is still a challenging task due to both complex page layout designs and unrestricted user created posts. In this paper, we study the problem of structured data extraction from various web forum sites. Our target is to find a solution as general as possible to extract structured data, such as post title, post author, post time, and post content from any forum site. In contrast to most existing information extraction methods, which only leverage the knowledge inside an individual page, we incorporate both page-level and site-level knowledge and employ Markov logic networks (MLNs) to effectively integrate all useful evidence by learning their importance automatically. Site-level knowledge includes (1) the linkages among different object pages, such as list pages and post pages, and (2) the interrelationships of pages belonging to the same object. The experimental results on 20 forums show a very encouraging information extraction performance, and demonstrate the ability of the proposed approach on various forums. We also show that the performance is limited if only page-level knowledge is used, while when incorporating the site-level knowledge both precision and recall can be significantly improved. [[EENNDD]] information extraction; site-level knowledge; markov logic networks; miscellaneous; structured data; web forums"}, "Menggabungkan pengetahuan peringkat laman web untuk mengekstrak data berstruktur dari forum web Forum web telah menjadi sumber data penting untuk banyak aplikasi web, tetapi mengekstrak data berstruktur dari halaman forum web tidak berstruktur masih merupakan tugas yang mencabar kerana reka bentuk susun atur halaman yang rumit dan pengguna yang tidak dibatasi dibuat jawatan. Dalam makalah ini, kami mengkaji masalah pengekstrakan data berstruktur dari pelbagai laman web forum. Sasaran kami adalah untuk mencari jalan penyelesaian yang seluas mungkin untuk mengekstrak data berstruktur, seperti judul pos, pengarang pos, waktu posting, dan isi posting dari mana-mana laman forum. Berbeza dengan kebanyakan kaedah pengekstrakan maklumat yang ada, yang hanya memanfaatkan pengetahuan di dalam satu halaman, kami menggabungkan pengetahuan peringkat halaman dan tahap laman dan menggunakan rangkaian logik Markov (MLN) untuk menyatukan semua bukti berguna secara efektif dengan mempelajari kepentingannya secara automatik. Pengetahuan peringkat laman web merangkumi (1) hubungan antara halaman objek yang berbeza, seperti halaman senarai dan halaman pos, dan (2) hubungan halaman yang tergolong dalam objek yang sama. Hasil eksperimen di 20 forum menunjukkan prestasi pengekstrakan maklumat yang sangat menggembirakan, dan menunjukkan kemampuan pendekatan yang dicadangkan pada berbagai forum. Kami juga menunjukkan bahawa prestasi terhad jika hanya pengetahuan peringkat halaman yang digunakan, sementara ketika memasukkan pengetahuan di peringkat laman web, ketepatan dan penarikan dapat ditingkatkan dengan ketara. [[EENNDD]] pengekstrakan maklumat; pengetahuan peringkat laman web; rangkaian logik markov; pelbagai; data berstruktur; forum web"], [{"string": "Mr. LDA: a flexible large scale topic modeling package using variational inference in MapReduce Latent Dirichlet Allocation (LDA) is a popular topic modeling technique for exploring document collections. Because of the increasing prevalence of large datasets, there is a need to improve the scalability of inference for LDA. In this paper, we introduce a novel and flexible large scale topic modeling package in MapReduce (Mr. LDA). As opposed to other techniques which use Gibbs sampling, our proposed framework uses variational inference, which easily fits into a distributed environment. More importantly, this variational implementation, unlike highly tuned and specialized implementations based on Gibbs sampling, is easily extensible. We demonstrate two extensions of the models possible with this scalable framework: informed priors to guide topic discovery and extracting topics from a multilingual corpus. We compare the scalability of Mr. LDA against Mahout, an existing large scale topic modeling package. Mr. LDA out-performs Mahout both in execution speed and held-out likelihood.", "keywords": ["mapreduce", "topic models", "scalability"], "combined": "Mr. LDA: a flexible large scale topic modeling package using variational inference in MapReduce Latent Dirichlet Allocation (LDA) is a popular topic modeling technique for exploring document collections. Because of the increasing prevalence of large datasets, there is a need to improve the scalability of inference for LDA. In this paper, we introduce a novel and flexible large scale topic modeling package in MapReduce (Mr. LDA). As opposed to other techniques which use Gibbs sampling, our proposed framework uses variational inference, which easily fits into a distributed environment. More importantly, this variational implementation, unlike highly tuned and specialized implementations based on Gibbs sampling, is easily extensible. We demonstrate two extensions of the models possible with this scalable framework: informed priors to guide topic discovery and extracting topics from a multilingual corpus. We compare the scalability of Mr. LDA against Mahout, an existing large scale topic modeling package. Mr. LDA out-performs Mahout both in execution speed and held-out likelihood. [[EENNDD]] mapreduce; topic models; scalability"}, "LDA: pakej pemodelan topik berskala besar yang fleksibel menggunakan inferensi variasi dalam MapReduce Latent Dirichlet Allocation (LDA) adalah teknik pemodelan topik yang popular untuk meneroka koleksi dokumen. Kerana peningkatan prevalensi set data yang besar, ada keperluan untuk meningkatkan skalabilitas inferensi untuk LDA. Dalam makalah ini, kami memperkenalkan pakej pemodelan topik skala besar dan fleksibel dalam MapReduce (Mr. LDA). Berbanding dengan teknik lain yang menggunakan pengambilan sampel Gibbs, kerangka cadangan kami menggunakan inferensi variasi, yang dengan mudah sesuai dengan lingkungan yang diedarkan. Lebih penting lagi, pelaksanaan variasi ini, tidak seperti pelaksanaan yang sangat disesuaikan dan khusus berdasarkan pengambilan sampel Gibbs, mudah dilanjutkan. Kami menunjukkan dua peluasan model yang mungkin dengan kerangka kerja yang dapat diskalakan: maklumat awal sebelum membimbing penemuan topik dan mengekstrak topik dari korpus pelbagai bahasa. Kami membandingkan skalabilitas Mr. LDA terhadap Mahout, pakej pemodelan topik berskala besar yang ada. LDA mengalahkan Mahout dalam kelajuan pelaksanaan dan kemungkinan ditangguhkan. [[EENNDD]] mapreduce; model topik; skalabiliti"], [{"string": "Vertex collocation profiles: subgraph counting for link analysis and prediction We introduce the concept of a vertex collocation profile (VCP) for the purpose of topological link analysis and prediction. VCPs provide nearly complete information about the surrounding local structure of embedded vertex pairs. The VCP approach offers a new tool for domain experts to understand the underlying growth mechanisms in their networks and to analyze link formation mechanisms in the appropriate sociological, biological, physical, or other context. The same resolution that gives VCP its analytical power also enables it to perform well when used in supervised models to discriminate potential new links. We first develop the theory, mathematics, and algorithms underlying VCPs. Then we demonstrate VCP methods performing link prediction competitively with unsupervised and supervised methods across several different network families. We conclude with timing results that introduce the comparative performance of several existing algorithms and the practicability of VCP computations on large networks.", "keywords": ["link prediction", "graph theory", "network analysis", "link analysis"], "combined": "Vertex collocation profiles: subgraph counting for link analysis and prediction We introduce the concept of a vertex collocation profile (VCP) for the purpose of topological link analysis and prediction. VCPs provide nearly complete information about the surrounding local structure of embedded vertex pairs. The VCP approach offers a new tool for domain experts to understand the underlying growth mechanisms in their networks and to analyze link formation mechanisms in the appropriate sociological, biological, physical, or other context. The same resolution that gives VCP its analytical power also enables it to perform well when used in supervised models to discriminate potential new links. We first develop the theory, mathematics, and algorithms underlying VCPs. Then we demonstrate VCP methods performing link prediction competitively with unsupervised and supervised methods across several different network families. We conclude with timing results that introduce the comparative performance of several existing algorithms and the practicability of VCP computations on large networks. [[EENNDD]] link prediction; graph theory; network analysis; link analysis"}, "Profil kolokasi verteks: pengiraan subgraf untuk analisis dan ramalan pautan Kami memperkenalkan konsep profil kolokasi bucu (VCP) untuk tujuan analisis dan ramalan pautan topologi. VCP memberikan maklumat yang hampir lengkap mengenai struktur tempatan sepasang pasangan bucu tertanam. Pendekatan VCP menawarkan alat baru untuk pakar domain untuk memahami mekanisme pertumbuhan yang mendasari dalam rangkaian mereka dan untuk menganalisis mekanisme pembentukan pautan dalam konteks sosiologi, biologi, fizikal, atau yang lain. Resolusi yang sama yang memberi VCP kekuatan analitiknya juga memungkinkannya berkinerja baik ketika digunakan dalam model yang diawasi untuk membedakan pautan baru yang berpotensi. Kami mula-mula mengembangkan teori, matematik, dan algoritma yang mendasari VCP. Kemudian kami menunjukkan kaedah VCP melakukan ramalan pautan secara kompetitif dengan kaedah yang tidak diawasi dan diawasi di beberapa keluarga rangkaian yang berbeza. Kami menyimpulkan dengan keputusan masa yang memperkenalkan prestasi perbandingan beberapa algoritma yang ada dan kebolehlaksanaan pengiraan VCP pada rangkaian besar. [[EENNDD]] ramalan pautan; teori grafik; analisis rangkaian; analisis pautan"], [{"string": "visKQWL, a visual renderer for a semantic web query language KiWi is a semantic Wiki that combines the Wiki philosophy of collaborative content creation with the methods of the Semantic Web in order to enable effective knowledge management.", "keywords": ["wiki", "information search and retrieval", "semantic wikis", "visual query languages", "semantic web", "keyword querying"], "combined": "visKQWL, a visual renderer for a semantic web query language KiWi is a semantic Wiki that combines the Wiki philosophy of collaborative content creation with the methods of the Semantic Web in order to enable effective knowledge management. [[EENNDD]] wiki; information search and retrieval; semantic wikis; visual query languages; semantic web; keyword querying"}, "visKQWL, penyaji visual untuk bahasa pertanyaan web semantik KiWi adalah Wiki semantik yang menggabungkan falsafah Wiki pembuatan kandungan kolaboratif dengan kaedah Web Semantik untuk membolehkan pengurusan pengetahuan berkesan. [[EENNDD]] wiki; pencarian dan pengambilan maklumat; wiki semantik; bahasa pertanyaan visual; web semantik; pertanyaan kata kunci"], [{"string": "Detecting web page structure for adaptive viewing on small form factor devices No contact information provided yet.", "keywords": ["mobile browser", "adaptive hypermedia", "content adaptation"], "combined": "Detecting web page structure for adaptive viewing on small form factor devices No contact information provided yet. [[EENNDD]] mobile browser; adaptive hypermedia; content adaptation"}, "Mengesan struktur halaman web untuk tontonan adaptif pada peranti faktor bentuk kecil Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penyemak imbas mudah alih; hipermedia adaptif; penyesuaian kandungan"], [{"string": "PodCred: a framework for analyzing podcast preference The PodCred framework is a framework for assessing the credibility and quality of podcasts published on the internet. It consists of a series of indicators designed to support prediction of listener preference of one podcast over another, given that both carry comparable informational content. The indicators are grouped into four categories pertaining to the Podcast Content, the Podcaster, the Podcast Context or the Technical Execution of the podcast. We adopt the term \"cred\" as a designation encompassing both credibility (comprising trustworthiness and expertise) and qualitative acceptability to listeners. Our podcast analysis framework is inspired by work on credibility in blogs, another medium dominated by user generated content. The PodCred framework is derived from a review of the literature on credibility for other media, a survey of prescriptive standards for podcasting, and a detailed data analysis of award winning podcasts. The paper concludes with a discussion of future work in which the framework will be applied.", "keywords": ["credibility", "user preferences", "miscellaneous", "quality", "assessment", "podcasts"], "combined": "PodCred: a framework for analyzing podcast preference The PodCred framework is a framework for assessing the credibility and quality of podcasts published on the internet. It consists of a series of indicators designed to support prediction of listener preference of one podcast over another, given that both carry comparable informational content. The indicators are grouped into four categories pertaining to the Podcast Content, the Podcaster, the Podcast Context or the Technical Execution of the podcast. We adopt the term \"cred\" as a designation encompassing both credibility (comprising trustworthiness and expertise) and qualitative acceptability to listeners. Our podcast analysis framework is inspired by work on credibility in blogs, another medium dominated by user generated content. The PodCred framework is derived from a review of the literature on credibility for other media, a survey of prescriptive standards for podcasting, and a detailed data analysis of award winning podcasts. The paper concludes with a discussion of future work in which the framework will be applied. [[EENNDD]] credibility; user preferences; miscellaneous; quality; assessment; podcasts"}, "PodCred: kerangka untuk menganalisis pilihan podcast Kerangka PodCred adalah kerangka untuk menilai kredibiliti dan kualiti podcast yang diterbitkan di internet. Ini terdiri daripada serangkaian indikator yang dirancang untuk menyokong ramalan pilihan pendengar dari satu podcast daripada podcast yang lain, memandangkan kedua-duanya membawa kandungan maklumat yang setanding. Indikator dikelompokkan ke dalam empat kategori yang berkaitan dengan Kandungan Podcast, Podcaster, Konteks Podcast atau Pelaksanaan Teknikal podcast. Kami menggunakan istilah \"cred\" sebagai sebutan yang merangkumi kedua-dua kredibiliti (terdiri dari kepercayaan dan kepakaran) dan penerimaan kualitatif kepada pendengar. Rangka kerja analisis podcast kami diilhamkan oleh kerja kredibiliti dalam blog, medium lain yang dikuasai oleh kandungan yang dihasilkan oleh pengguna. Kerangka PodCred berasal dari tinjauan literatur mengenai kredibiliti untuk media lain, tinjauan mengenai standard preskriptif untuk podcasting, dan analisis data terperinci mengenai podcast pemenang anugerah. Makalah ini diakhiri dengan perbincangan mengenai karya masa depan di mana kerangka kerja akan diterapkan. [[EENNDD]] kredibiliti; pilihan pengguna; pelbagai; kualiti; penilaian; podcast"], [{"string": "An adaptive model for optimizing performance of an incremental web crawler An abstract is not available.", "keywords": ["incremental crawler", "optimization", "scalability", "performance evaluation", "interaction styles", "crawler"], "combined": "An adaptive model for optimizing performance of an incremental web crawler An abstract is not available. [[EENNDD]] incremental crawler; optimization; scalability; performance evaluation; interaction styles; crawler"}, "Model adaptif untuk mengoptimumkan prestasi crawler web tambahan Abstrak tidak tersedia. [[EENNDD]] perangkak tambahan; pengoptimuman; skalabiliti; penilaian prestasi; gaya interaksi; perangkak"], [{"string": "Web image retrieval reranking with multi-view clustering General image retrieval is often carried out by a text-based search engine, such as Google Image Search. In this case, natural language queries are used as input to the search engine. Usually, the user queries are quite ambiguous and the returned results are not well-organized as the ranking often done by the popularity of an image. In order to address these problems, we propose to use both textual and visual contents of retrieved images to reRank web retrieved results. In particular, a machine learning technique, a multi-view clustering algorithm is proposed to reorganize the original results provided by the text-based search engine. Preliminary results validate the effectiveness of the proposed framework.", "keywords": ["reranking", "information search and retrieval", "web image retrieval", "multi-view clustering"], "combined": "Web image retrieval reranking with multi-view clustering General image retrieval is often carried out by a text-based search engine, such as Google Image Search. In this case, natural language queries are used as input to the search engine. Usually, the user queries are quite ambiguous and the returned results are not well-organized as the ranking often done by the popularity of an image. In order to address these problems, we propose to use both textual and visual contents of retrieved images to reRank web retrieved results. In particular, a machine learning technique, a multi-view clustering algorithm is proposed to reorganize the original results provided by the text-based search engine. Preliminary results validate the effectiveness of the proposed framework. [[EENNDD]] reranking; information search and retrieval; web image retrieval; multi-view clustering"}, "Pengambilan semula gambar web dengan pengelompokan multi-pandangan Pengambilan gambar umum sering dilakukan oleh mesin carian berasaskan teks, seperti Carian Imej Google. Dalam kes ini, pertanyaan bahasa semula jadi digunakan sebagai input ke mesin pencari. Biasanya, pertanyaan pengguna agak samar-samar dan hasil yang dikembalikan tidak disusun dengan baik kerana peringkat sering dilakukan oleh populariti gambar. Untuk mengatasi masalah ini, kami mencadangkan untuk menggunakan kedua-dua kandungan teks dan visual gambar yang diambil untuk menyusun semula hasil yang diambil web. Khususnya, teknik pembelajaran mesin, algoritma pengelompokan multi-pandangan dicadangkan untuk menyusun semula hasil asal yang disediakan oleh mesin carian berasaskan teks. Hasil awal mengesahkan keberkesanan rangka kerja yang dicadangkan. [[EENNDD]] kedudukan semula; pencarian dan pengambilan maklumat; pengambilan gambar web; pengelompokan pelbagai pandangan"], [{"string": "Hardening Web browsers against man-in-the-middle and eavesdropping attacks No contact information provided yet.", "keywords": ["eavesdropping attack", "well-in-advance instruction", "man-in-the-middle attack", "ssl", "password", "just-in-time instruction", "certificate", "screen design", "interaction styles", "https", "safe staging", "web browser"], "combined": "Hardening Web browsers against man-in-the-middle and eavesdropping attacks No contact information provided yet. [[EENNDD]] eavesdropping attack; well-in-advance instruction; man-in-the-middle attack; ssl; password; just-in-time instruction; certificate; screen design; interaction styles; https; safe staging; web browser"}, "Mengeras penyemak imbas Web daripada serangan man-in-the-middle dan menguping Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] serangan mengupas; arahan awal; serangan lelaki-di-tengah-tengah; ssl; kata laluan; arahan tepat pada masanya; sijil; reka bentuk skrin; gaya interaksi; https; pementasan selamat; pelayar web"], [{"string": "A constraint extension to scalable vector graphics An abstract is not available.", "keywords": ["constraints", "differential scaling", "three-dimensional graphics and realism", "svg", "multimedia information systems", "scalable vector graphics", "user interfaces", "semantic zooming", "csvg", "interaction"], "combined": "A constraint extension to scalable vector graphics An abstract is not available. [[EENNDD]] constraints; differential scaling; three-dimensional graphics and realism; svg; multimedia information systems; scalable vector graphics; user interfaces; semantic zooming; csvg; interaction"}, "Pelanjutan kekangan untuk grafik vektor yang boleh diskalakan Abstrak tidak tersedia. [[EENNDD]] kekangan; penskalaan pembezaan; grafik dan realisme tiga dimensi; svg; sistem maklumat multimedia; grafik vektor berskala; antara muka pengguna; zum semantik; csvg; interaksi"], [{"string": "Page-level template detection via isotonic smoothing We develop a novel framework for the page-level template detection problem. Our framework is built on two main ideas. The first is theautomatic generation of training data for a classifier that, given apage, assigns a templateness score to every DOM node of the page. The second is the global smoothing of these per-node classifier scores bysolving a regularized isotonic regression problem; the latter follows from a simple yet powerful abstraction of templateness on a page. Our extensive experiments on human-labeled test data show that our approachdetects templates effectively.", "keywords": ["webpage sectioning", "webpage segmentation", "template detection", "miscellaneous", "isotonic regression"], "combined": "Page-level template detection via isotonic smoothing We develop a novel framework for the page-level template detection problem. Our framework is built on two main ideas. The first is theautomatic generation of training data for a classifier that, given apage, assigns a templateness score to every DOM node of the page. The second is the global smoothing of these per-node classifier scores bysolving a regularized isotonic regression problem; the latter follows from a simple yet powerful abstraction of templateness on a page. Our extensive experiments on human-labeled test data show that our approachdetects templates effectively. [[EENNDD]] webpage sectioning; webpage segmentation; template detection; miscellaneous; isotonic regression"}, "Pengesanan templat peringkat halaman melalui pelicinan isotonik Kami mengembangkan kerangka baru untuk masalah pengesanan templat peringkat halaman. Rangka kerja kami dibina berdasarkan dua idea utama. Yang pertama adalah penjanaan data latihan secara automatik untuk pengklasifikasi yang, dengan kadar yang pantas, memberikan skor kerangka pada setiap simpul DOM halaman. Yang kedua adalah kelancaran skor pengkelasan per-node ini dengan menyelesaikan masalah regresi isotonik teratur; yang terakhir mengikuti pengabaian kesederhanaan yang sederhana namun kuat pada halaman. Eksperimen kami yang luas terhadap data ujian berlabel manusia menunjukkan bahawa pendekatan kami mendeteksi templat dengan berkesan. [[EENNDD]] pembahagian halaman web; segmentasi laman web; pengesanan templat; pelbagai; regresi isotonik"], [{"string": "Probabilistic question recommendation for question answering communities User-Interactive Question Answering (QA) communities such as Yahoo! Answers are growing in popularity. However, as these QA sites always have thousands of new questions posted daily, it is difficult for users to find the questions that are of interest to them. Consequently, this may delay the answering of the new questions. This gives rise to question recommendation techniques that help users locate interesting questions. In this paper, we adopt the Probabilistic Latent Semantic Analysis (PLSA) model for question recommendation and propose a novel metric to evaluate the performance of our approach. The experimental results show our recommendation approach is effective.", "keywords": ["plsa", "question recommendation", "question answering"], "combined": "Probabilistic question recommendation for question answering communities User-Interactive Question Answering (QA) communities such as Yahoo! Answers are growing in popularity. However, as these QA sites always have thousands of new questions posted daily, it is difficult for users to find the questions that are of interest to them. Consequently, this may delay the answering of the new questions. This gives rise to question recommendation techniques that help users locate interesting questions. In this paper, we adopt the Probabilistic Latent Semantic Analysis (PLSA) model for question recommendation and propose a novel metric to evaluate the performance of our approach. The experimental results show our recommendation approach is effective. [[EENNDD]] plsa; question recommendation; question answering"}, "Cadangan soalan probabilistik untuk komuniti menjawab soalan Komuniti Menjawab Soalan Interaktif Pengguna (QA) seperti Yahoo! Jawapan semakin popular. Namun, kerana laman web QA ini selalu mempunyai ribuan soalan baru yang disiarkan setiap hari, sukar bagi pengguna untuk mencari soalan yang menarik bagi mereka. Akibatnya, ini mungkin melambatkan menjawab soalan baru. Ini menimbulkan teknik cadangan soalan yang membantu pengguna mencari soalan yang menarik. Dalam makalah ini, kami menggunakan model Probabilistic Latent Semantic Analysis (PLSA) untuk cadangan soalan dan mencadangkan metrik novel untuk menilai prestasi pendekatan kami. Hasil eksperimen menunjukkan pendekatan cadangan kami berkesan. [[EENNDD]] plsa; cadangan soalan; menjawab soalan"], [{"string": "Wikipedia vandalism detection Wikipedia is an online encyclopedia that anyone can access and edit. It has become one of the most important sources of knowledge online and many third party projects rely on it for a wide-range of purposes. The open model of Wikipedia allows pranksters, lobbyists and spammers to attack the integrity of the encyclopedia and this endangers it as a public resource. This is known in the community as vandalism.", "keywords": ["reputation", "natural language processing", "machine learning", "wikipedia vandalism detection"], "combined": "Wikipedia vandalism detection Wikipedia is an online encyclopedia that anyone can access and edit. It has become one of the most important sources of knowledge online and many third party projects rely on it for a wide-range of purposes. The open model of Wikipedia allows pranksters, lobbyists and spammers to attack the integrity of the encyclopedia and this endangers it as a public resource. This is known in the community as vandalism. [[EENNDD]] reputation; natural language processing; machine learning; wikipedia vandalism detection"}, "Pengesanan vandalisme Wikipedia Wikipedia adalah ensiklopedia dalam talian yang boleh diakses dan diedit oleh sesiapa sahaja. Ini telah menjadi salah satu sumber pengetahuan terpenting dalam talian dan banyak projek pihak ketiga bergantung padanya untuk pelbagai tujuan. Model terbuka Wikipedia membolehkan pranksters, pelobi dan spammer menyerang integriti ensiklopedia dan ini membahayakannya sebagai sumber awam. Ini dikenali dalam masyarakat sebagai vandalisme. [[EENNDD]] reputasi; pemprosesan bahasa semula jadi; pembelajaran mesin; pengesanan vandalisme wikipedia"], [{"string": "Structuring and presenting annotated media repositories No contact information provided yet.", "keywords": ["xslt", "style", "xhtml+smil", "semantics", "rdf", "document structure"], "combined": "Structuring and presenting annotated media repositories No contact information provided yet. [[EENNDD]] xslt; style; xhtml+smil; semantics; rdf; document structure"}, "Menyusun dan mempersembahkan repositori media beranotasi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] xslt; gaya; xhtml + senyum; semantik; rdf; struktur dokumen"], [{"string": "On optimal service selection No contact information provided yet.", "keywords": ["service matchmaking", "automatic service composition", "service selection problem", "nonfunctional properties"], "combined": "On optimal service selection No contact information provided yet. [[EENNDD]] service matchmaking; automatic service composition; service selection problem; nonfunctional properties"}, "Pada pemilihan perkhidmatan yang optimum Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perjodohan perkhidmatan; komposisi perkhidmatan automatik; masalah pemilihan perkhidmatan; sifat tidak berfungsi"], [{"string": "Highly scalable web applications with zero-copy data transfer The performance of server-side applications is becoming increasingly important as more applications exploit the Web application model. Extensive work has been done to improve the performance of individual software components such as Web servers and programming language runtimes. This paper describes a novel approach to boost Web application performance by improving inter-process communication between a programming language runtime and Web server runtime. The approach reduces redundant processing for memory copying and the context switch overhead between user space and kernel space by exploiting the zero-copy data transfer methodology, such as the sendfile system call. In order to transparently utilize this optimization feature with existing Web applications, we propose enhancements of the PHP runtime, FastCGI protocol, and Web server. Our proposed approach achieves a 126% performance improvement with micro-benchmarks and a 44% performance improvement for a standard Web benchmark, SPECweb2005.", "keywords": ["scripting", "fastcgi", "zero copy", "web server", "sendfile", "php"], "combined": "Highly scalable web applications with zero-copy data transfer The performance of server-side applications is becoming increasingly important as more applications exploit the Web application model. Extensive work has been done to improve the performance of individual software components such as Web servers and programming language runtimes. This paper describes a novel approach to boost Web application performance by improving inter-process communication between a programming language runtime and Web server runtime. The approach reduces redundant processing for memory copying and the context switch overhead between user space and kernel space by exploiting the zero-copy data transfer methodology, such as the sendfile system call. In order to transparently utilize this optimization feature with existing Web applications, we propose enhancements of the PHP runtime, FastCGI protocol, and Web server. Our proposed approach achieves a 126% performance improvement with micro-benchmarks and a 44% performance improvement for a standard Web benchmark, SPECweb2005. [[EENNDD]] scripting; fastcgi; zero copy; web server; sendfile; php"}, "Aplikasi web yang sangat berskala dengan pemindahan data zero-copy Prestasi aplikasi sisi pelayan menjadi semakin penting kerana lebih banyak aplikasi mengeksploitasi model aplikasi Web. Kerja yang meluas telah dilakukan untuk meningkatkan prestasi komponen perisian individu seperti pelayan Web dan waktu operasi bahasa pengaturcaraan. Makalah ini menjelaskan pendekatan baru untuk meningkatkan prestasi aplikasi Web dengan meningkatkan komunikasi antara proses antara runtime bahasa pengaturcaraan dan runtime pelayan Web. Pendekatan ini mengurangkan pemprosesan berlebihan untuk penyalinan memori dan pertukaran konteks overhead antara ruang pengguna dan ruang kernel dengan mengeksploitasi metodologi pemindahan data zero-copy, seperti panggilan sistem sendfile. Untuk menggunakan fitur pengoptimuman ini secara transparan dengan aplikasi Web yang ada, kami mencadangkan penambahan jangka waktu PHP, protokol FastCGI, dan pelayan Web. Pendekatan yang dicadangkan kami mencapai peningkatan prestasi 126% dengan penanda aras mikro dan peningkatan prestasi 44% untuk penanda aras Web standard, SPECweb2005. [[EENNDD]] skrip; fastcgi; salinan sifar; pelayan web; hantar fail; php"], [{"string": "On the informativeness of cascade and intent-aware effectiveness measures The Maximum Entropy Method provides one technique for validating search engine effectiveness measures. Under this method, the value of an effectiveness measure is used as a constraint to estimate the most likely distribution of relevant documents under a maximum entropy assumption. This inferred distribution may then be compared to the actual distribution to quantify the \"informativeness\" of the measure. The inferred distribution may also be used to estimate values for other effectiveness measures. Previous work focused on traditional effectiveness measures, such as average precision. In this paper, we extend the Maximum Entropy Method to the newer cascade and intent-aware effectiveness measures by considering the dependency of the documents ranked in a results list. These measures are intended to reflect the novelty and diversity of search results in addition to the traditional relevance. Our results indicate that intent-aware measures based on the cascade model are informative in terms of both inferring actual distribution and predicting the values of other retrieval measures.", "keywords": ["effectiveness measures", "measure informativeness", "novelty", "evaluation", "diversity"], "combined": "On the informativeness of cascade and intent-aware effectiveness measures The Maximum Entropy Method provides one technique for validating search engine effectiveness measures. Under this method, the value of an effectiveness measure is used as a constraint to estimate the most likely distribution of relevant documents under a maximum entropy assumption. This inferred distribution may then be compared to the actual distribution to quantify the \"informativeness\" of the measure. The inferred distribution may also be used to estimate values for other effectiveness measures. Previous work focused on traditional effectiveness measures, such as average precision. In this paper, we extend the Maximum Entropy Method to the newer cascade and intent-aware effectiveness measures by considering the dependency of the documents ranked in a results list. These measures are intended to reflect the novelty and diversity of search results in addition to the traditional relevance. Our results indicate that intent-aware measures based on the cascade model are informative in terms of both inferring actual distribution and predicting the values of other retrieval measures. [[EENNDD]] effectiveness measures; measure informativeness; novelty; evaluation; diversity"}, "Mengenai keberkesanan langkah-langkah keberkesanan lata dan sedar-sedar Kaedah Maksimum Entropi menyediakan satu teknik untuk mengesahkan langkah-langkah keberkesanan mesin pencari. Di bawah kaedah ini, nilai ukuran keberkesanan digunakan sebagai kekangan untuk menganggarkan pengedaran dokumen yang paling mungkin di bawah anggapan entropi maksimum. Pengagihan yang disimpulkan ini kemudiannya dapat dibandingkan dengan sebaran sebenar untuk mengukur \"maklumat\" ukuran tersebut. Pengagihan yang disimpulkan juga dapat digunakan untuk menganggarkan nilai untuk ukuran keberkesanan lain. Karya sebelumnya tertumpu pada ukuran keberkesanan tradisional, seperti ketepatan purata. Dalam makalah ini, kami memperluas Kaedah Entropi Maksimum ke langkah keberkesanan yang lebih baru dan keberkesanan yang disengajakan dengan mempertimbangkan ketergantungan dokumen yang berada dalam senarai hasil. Langkah-langkah ini bertujuan untuk mencerminkan kebaruan dan kepelbagaian hasil carian di samping relevansi tradisional. Hasil kajian kami menunjukkan bahawa langkah-langkah yang disengajakan berdasarkan model lata bersifat informatif dari segi kesimpulan mengenai penyebaran sebenar dan meramalkan nilai-nilai tindakan pengambilan yang lain. [[EENNDD]] langkah keberkesanan; mengukur keberkesanan; kebaharuan; penilaian; kepelbagaian"], [{"string": "The design and implementation of the redland RDF application framework An abstract is not available.", "keywords": ["hypertext/hypermedia", "metadata", "rdf", "application framework"], "combined": "The design and implementation of the redland RDF application framework An abstract is not available. [[EENNDD]] hypertext/hypermedia; metadata; rdf; application framework"}, "Reka bentuk dan pelaksanaan kerangka aplikasi RDF redland Abstrak tidak tersedia. [[EENNDD]] hiperteks / hipermedia; metadata; rdf; kerangka aplikasi"], [{"string": "How people use the web on mobile devices This paper describes a series of user studies on how people use the Web via mobile devices. The data primarily comes from contextual inquiries with 47 participants between 2004 and 2007, and is complemented with a phone log analysis of 577 panelists in 2007. We report four key contextual factors in using the Web on mobile devices and propose mobile Web activity taxonomy. The framework contains three user activity categories identical to previous stationary Web studies: information seeking, communication, and transaction, and a new category: personal space extension. The new category refers to the practice that people put their content on the Web for personal access, therefore extending their personal information space.", "keywords": ["content object handling", "activity taxonomy", "mobile web", "personal space extension", "information seeking"], "combined": "How people use the web on mobile devices This paper describes a series of user studies on how people use the Web via mobile devices. The data primarily comes from contextual inquiries with 47 participants between 2004 and 2007, and is complemented with a phone log analysis of 577 panelists in 2007. We report four key contextual factors in using the Web on mobile devices and propose mobile Web activity taxonomy. The framework contains three user activity categories identical to previous stationary Web studies: information seeking, communication, and transaction, and a new category: personal space extension. The new category refers to the practice that people put their content on the Web for personal access, therefore extending their personal information space. [[EENNDD]] content object handling; activity taxonomy; mobile web; personal space extension; information seeking"}, "Cara orang menggunakan web pada peranti mudah alih Makalah ini menerangkan serangkaian kajian pengguna mengenai bagaimana orang menggunakan Web melalui peranti mudah alih. Data terutamanya berasal dari pertanyaan kontekstual dengan 47 peserta antara tahun 2004 dan 2007, dan dilengkapi dengan analisis log telefon 577 panelis pada tahun 2007. Kami melaporkan empat faktor kontekstual utama dalam menggunakan Web pada peranti mudah alih dan mencadangkan taksonomi aktiviti Web mudah alih. Kerangka ini mengandungi tiga kategori aktiviti pengguna yang serupa dengan kajian Web pegun sebelumnya: pencarian maklumat, komunikasi, dan transaksi, dan kategori baru: peluasan ruang peribadi. Kategori baru merujuk kepada amalan bahawa orang meletakkan kandungan mereka di Web untuk akses peribadi, oleh itu memperluas ruang maklumat peribadi mereka. [[EENNDD]] pengendalian objek kandungan; taksonomi aktiviti; web mudah alih; peluasan ruang peribadi; mencari maklumat"], [{"string": "ALVIN: a system for visualizing large networks An abstract is not available.", "keywords": ["network visualization", "visualizing the web", "information search and retrieval", "sampling"], "combined": "ALVIN: a system for visualizing large networks An abstract is not available. [[EENNDD]] network visualization; visualizing the web; information search and retrieval; sampling"}, "ALVIN: sistem untuk memvisualisasikan rangkaian besar Abstrak tidak tersedia. [[EENNDD]] visualisasi rangkaian; menggambarkan web; pencarian dan pengambilan maklumat; persampelan"], [{"string": "Analyzing seller practices in a Brazilian marketplace E-commerce is growing at an exponential rate. In the last decade, there has been an explosion of online commercial activity enabled by World Wide Web (WWW). These days, many consumers are less attracted to online auctions, preferring to buy merchandise quickly using fixed-price negotiations. Sales at Amazon.com, the leader in online sales of fixed-price goods, rose 37% in the first quarter of 2008. At eBay, where auctions make up 58% of the site's sales, revenue rose 14%. In Brazil, probably by cultural influence, online auctions are not been popular. This work presents a characterization and analysis of fixed-price online negotiations. Using actual data from a Brazilian marketplace, we analyze seller practices, considering seller profiles and strategies. We show that different sellers adopt strategies according to their interests, abilities and experience. Moreover, we confirm that choosing a selling strategy is not simple, since it is important to consider the seller's characteristics to evaluate the applicability of a strategy. The work also provides a comparative analysis of some selling practices in Brazil with popular worldwide marketplaces.", "keywords": ["marketplaces", "e-commerce", "e-markets", "electronic commerce", "selling practices"], "combined": "Analyzing seller practices in a Brazilian marketplace E-commerce is growing at an exponential rate. In the last decade, there has been an explosion of online commercial activity enabled by World Wide Web (WWW). These days, many consumers are less attracted to online auctions, preferring to buy merchandise quickly using fixed-price negotiations. Sales at Amazon.com, the leader in online sales of fixed-price goods, rose 37% in the first quarter of 2008. At eBay, where auctions make up 58% of the site's sales, revenue rose 14%. In Brazil, probably by cultural influence, online auctions are not been popular. This work presents a characterization and analysis of fixed-price online negotiations. Using actual data from a Brazilian marketplace, we analyze seller practices, considering seller profiles and strategies. We show that different sellers adopt strategies according to their interests, abilities and experience. Moreover, we confirm that choosing a selling strategy is not simple, since it is important to consider the seller's characteristics to evaluate the applicability of a strategy. The work also provides a comparative analysis of some selling practices in Brazil with popular worldwide marketplaces. [[EENNDD]] marketplaces; e-commerce; e-markets; electronic commerce; selling practices"}, "Menganalisis amalan penjual di pasaran E-dagang Brazil berkembang pada kadar eksponensial. Dalam dekad terakhir, terdapat ledakan aktiviti komersial dalam talian yang diaktifkan oleh World Wide Web (WWW). Hari ini, banyak pengguna kurang tertarik dengan lelongan dalam talian, lebih suka membeli barang dengan cepat menggunakan rundingan harga tetap. Penjualan di Amazon.com, peneraju penjualan barang-barang harga tetap dalam talian, meningkat 37% pada suku pertama 2008. Di eBay, di mana lelongan merangkumi 58% daripada penjualan laman web ini, pendapatan meningkat 14%. Di Brazil, mungkin oleh pengaruh budaya, lelongan dalam talian tidak popular. Karya ini menunjukkan ciri dan analisis rundingan dalam talian harga tetap. Dengan menggunakan data sebenar dari pasaran Brazil, kami menganalisis amalan penjual, dengan mempertimbangkan profil dan strategi penjual. Kami menunjukkan bahawa penjual yang berbeza menggunakan strategi mengikut minat, kemampuan dan pengalaman mereka. Lebih-lebih lagi, kami mengesahkan bahawa memilih strategi jual bukanlah sesuatu yang mudah, kerana penting untuk mempertimbangkan ciri-ciri penjual untuk menilai penerapan strategi. Karya ini juga memberikan analisis perbandingan beberapa amalan penjualan di Brazil dengan pasaran yang popular di seluruh dunia. [[EENNDD]] pasaran; e-dagang; e-pasaran; perdagangan elektronik; amalan menjual"], [{"string": "An investigation of cloning in web applications No contact information provided yet.", "keywords": ["software reuse", "clone analysis", "web engineering", "clones", "software maintenance", "clone metrics", "web applications"], "combined": "An investigation of cloning in web applications No contact information provided yet. [[EENNDD]] software reuse; clone analysis; web engineering; clones; software maintenance; clone metrics; web applications"}, "Penyiasatan pengklonan dalam aplikasi web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penggunaan semula perisian; analisis klon; kejuruteraan web; klon; penyelenggaraan perisian; metrik klon; aplikasi web"], [{"string": "Planetary-scale views on a large instant-messaging network We present a study of anonymized data capturing a month of high-level communication activities within the whole of the Microsoft Messenger instant-messaging system. We examine characteristics and patterns that emerge from the collective dynamics of large numbers of people, rather than the actions and characteristics of individuals. The dataset contains summary properties of 30 billion conversations among 240 million people. From the data, we construct a communication graph with 180 million nodes and 1.3 billion undirected edges, creating the largest social network constructed and analyzed to date. We report on multiple aspects of the dataset and synthesized graph. We find that the graph is well-connected and robust to node removal. We investigate on a planetary-scale the oft-cited report that people are separated by \"six degrees of separation\" and find that the average path length among Messenger users is 6.6. We find that people tend to communicate more with each other when they have similar age, language, and location, and that cross-gender conversations are both more frequent and of longer duration than conversations with the same gender.", "keywords": ["large data", "communication networks", "user demographics", "social networks", "online communication"], "combined": "Planetary-scale views on a large instant-messaging network We present a study of anonymized data capturing a month of high-level communication activities within the whole of the Microsoft Messenger instant-messaging system. We examine characteristics and patterns that emerge from the collective dynamics of large numbers of people, rather than the actions and characteristics of individuals. The dataset contains summary properties of 30 billion conversations among 240 million people. From the data, we construct a communication graph with 180 million nodes and 1.3 billion undirected edges, creating the largest social network constructed and analyzed to date. We report on multiple aspects of the dataset and synthesized graph. We find that the graph is well-connected and robust to node removal. We investigate on a planetary-scale the oft-cited report that people are separated by \"six degrees of separation\" and find that the average path length among Messenger users is 6.6. We find that people tend to communicate more with each other when they have similar age, language, and location, and that cross-gender conversations are both more frequent and of longer duration than conversations with the same gender. [[EENNDD]] large data; communication networks; user demographics; social networks; online communication"}, "Pandangan skala planet pada rangkaian pesanan segera yang besar Kami menyajikan kajian mengenai data tanpa nama yang menangkap sebulan aktiviti komunikasi peringkat tinggi dalam keseluruhan sistem pesanan segera Microsoft Messenger. Kami mengkaji ciri dan corak yang muncul dari dinamika kolektif sejumlah besar orang, dan bukannya tindakan dan ciri-ciri individu. Set data mengandungi ringkasan sifat 30 bilion perbualan di antara 240 juta orang. Dari data tersebut, kami membina grafik komunikasi dengan 180 juta simpul dan 1,3 bilion tepi yang tidak diarahkan, mewujudkan rangkaian sosial terbesar yang dibina dan dianalisis hingga kini. Kami melaporkan pelbagai aspek kumpulan data dan grafik yang disintesis. Kami dapati grafik tersebut dihubungkan dengan baik dan kuat untuk menghilangkan simpul. Kami menyiasat pada skala planet laporan yang sering disebut bahawa orang dipisahkan oleh \"enam darjah pemisahan\" dan mendapati bahawa panjang jalan rata-rata di antara pengguna Messenger adalah 6.6. Kami mendapati bahawa orang cenderung untuk berkomunikasi lebih banyak antara satu sama lain ketika mereka mempunyai usia, bahasa, dan lokasi yang serupa, dan perbualan antara jantina lebih kerap dan lebih lama daripada perbualan dengan jantina yang sama. [[EENNDD]] data besar; rangkaian komunikasi; demografi pengguna; rangkaian sosial; komunikasi dalam talian"], [{"string": "Semantic personalization of web portal contents Enriching Web applications with personalized data is of major interest for facilitating the user access to the published contents, and therefore, for guaranteeing successful user navigation. We propose a conceptual model for extracting personalized recommendations based on user profiling, ontological domain models, and semantic reasoning. The approach offers a high-level representation of the designed application based on a domain-specific metamodel for Web applications called WebML.", "keywords": ["personalization", "conceptual modeling", "ontology", "computer-aided software engineering", "semantic web"], "combined": "Semantic personalization of web portal contents Enriching Web applications with personalized data is of major interest for facilitating the user access to the published contents, and therefore, for guaranteeing successful user navigation. We propose a conceptual model for extracting personalized recommendations based on user profiling, ontological domain models, and semantic reasoning. The approach offers a high-level representation of the designed application based on a domain-specific metamodel for Web applications called WebML. [[EENNDD]] personalization; conceptual modeling; ontology; computer-aided software engineering; semantic web"}, "Pemperibadian semantik kandungan portal web Memperkaya aplikasi Web dengan data yang diperibadikan adalah kepentingan utama untuk memudahkan pengguna mengakses kandungan yang diterbitkan, dan oleh itu, untuk menjamin navigasi pengguna yang berjaya. Kami mencadangkan model konseptual untuk mengambil cadangan yang diperibadikan berdasarkan profil pengguna, model domain ontologi, dan penaakulan semantik. Pendekatan ini menawarkan representasi tingkat tinggi dari aplikasi yang dirancang berdasarkan metododel khusus domain untuk aplikasi Web yang disebut WebML. [[EENNDD]] pemperibadian; pemodelan konsep; ontologi; kejuruteraan perisian berbantukan komputer; web semantik"], [{"string": "Spam double-funnel: connecting web spammers with advertisers Spammers use questionable search engine optimization (SEO) techniques to promote their spam links into top search results. In this paper, we focus on one prevalent type of spam - redirection spam - where one can identify spam pages by the third-party domains that these pages redirect traffic to. We propose a five-layer, double-funnel model for describing end-to-end redirection spam, present a methodology for analyzing the layers, and identify prominent domains on each layer using two sets of commercial keywords. one targeting spammers and the other targeting advertisers. The methodology and findings are useful for search engines to strengthen their ranking algorithms against spam, for legitimate website owners to locate and remove spam doorway pages, and for legitimate advertisers to identify unscrupulous syndicators who serve ads on spam pages.", "keywords": ["web spam", "redirection and cloaking", "search spam", "advertisement syndication"], "combined": "Spam double-funnel: connecting web spammers with advertisers Spammers use questionable search engine optimization (SEO) techniques to promote their spam links into top search results. In this paper, we focus on one prevalent type of spam - redirection spam - where one can identify spam pages by the third-party domains that these pages redirect traffic to. We propose a five-layer, double-funnel model for describing end-to-end redirection spam, present a methodology for analyzing the layers, and identify prominent domains on each layer using two sets of commercial keywords. one targeting spammers and the other targeting advertisers. The methodology and findings are useful for search engines to strengthen their ranking algorithms against spam, for legitimate website owners to locate and remove spam doorway pages, and for legitimate advertisers to identify unscrupulous syndicators who serve ads on spam pages. [[EENNDD]] web spam; redirection and cloaking; search spam; advertisement syndication"}, "Spam double-funnel: menghubungkan web spammer dengan pengiklan Spammer menggunakan teknik pengoptimuman enjin carian (SEO) yang dipersoalkan untuk mempromosikan pautan spam mereka ke hasil carian teratas. Dalam makalah ini, kami memfokuskan pada satu jenis spam yang lazim - spam pengalihan - di mana seseorang dapat mengenal pasti halaman spam oleh domain pihak ketiga tempat halaman ini mengarahkan lalu lintas ke. Kami mencadangkan model lima-lapisan, corong berganda untuk menggambarkan spam pengalihan hujung-ke-hujung, menyajikan metodologi untuk menganalisis lapisan, dan mengenal pasti domain yang menonjol pada setiap lapisan menggunakan dua set kata kunci komersial. satu penyasarkan spam dan yang lain menyasarkan pengiklan. Metodologi dan penemuan berguna untuk mesin pencari untuk memperkuat algoritma peringkat mereka terhadap spam, bagi pemilik laman web yang sah untuk mencari dan membuang halaman pintu spam, dan bagi pengiklan yang sah untuk mengenal pasti sindiket yang tidak bertanggungjawab yang menayangkan iklan di halaman spam. [[EENNDD]] spam web; pengalihan dan penyembunyian; cari spam; sindiket iklan"], [{"string": "Flickr tag recommendation based on collective knowledge Online photo services such as Flickr and Zooomr allow users to share their photos with family, friends, and the online community at large. An important facet of these services is that users manually annotate their photos using so called tags, which describe the contents of the photo or provide additional contextual and semantical information. In this paper we investigate how we can assist users in the tagging phase. The contribution of our research is twofold. We analyse a representative snapshot of Flickr and present the results by means of a tag characterisation focussing on how users tags photos and what information is contained in the tagging. Based on this analysis, we present and evaluate tag recommendation strategies to support the user in the photo annotation task by recommending a set of tags that can be added to the photo. The results of the empirical evaluation show that we can effectively recommend relevant tags for a variety of photos with different levels of exhaustiveness of original tagging.", "keywords": ["photo annotations", "tag recommendation", "content analysis and indexing", "on-line information services", "tag co-occurence", "collective knowledge", "aggregated tag suggestion", "tag characterisation", "flickr"], "combined": "Flickr tag recommendation based on collective knowledge Online photo services such as Flickr and Zooomr allow users to share their photos with family, friends, and the online community at large. An important facet of these services is that users manually annotate their photos using so called tags, which describe the contents of the photo or provide additional contextual and semantical information. In this paper we investigate how we can assist users in the tagging phase. The contribution of our research is twofold. We analyse a representative snapshot of Flickr and present the results by means of a tag characterisation focussing on how users tags photos and what information is contained in the tagging. Based on this analysis, we present and evaluate tag recommendation strategies to support the user in the photo annotation task by recommending a set of tags that can be added to the photo. The results of the empirical evaluation show that we can effectively recommend relevant tags for a variety of photos with different levels of exhaustiveness of original tagging. [[EENNDD]] photo annotations; tag recommendation; content analysis and indexing; on-line information services; tag co-occurence; collective knowledge; aggregated tag suggestion; tag characterisation; flickr"}, "Cadangan tag Flickr berdasarkan pengetahuan kolektif Perkhidmatan foto dalam talian seperti Flickr dan Zooomr membolehkan pengguna berkongsi foto mereka dengan keluarga, rakan, dan komuniti dalam talian secara meluas. Aspek penting dari perkhidmatan ini adalah pengguna secara manual memberi anotasi foto mereka menggunakan tag yang disebut, yang menerangkan kandungan foto atau memberikan maklumat kontekstual dan semantik tambahan. Dalam makalah ini kita menyiasat bagaimana kita dapat membantu pengguna dalam fasa pemberian tag. Sumbangan penyelidikan kami adalah dua kali ganda. Kami menganalisis snapshot perwakilan Flickr dan menyajikan hasilnya dengan karakterisasi tag yang berfokus pada bagaimana pengguna menandai foto dan maklumat apa yang terkandung dalam pemberian tag. Berdasarkan analisis ini, kami menyajikan dan menilai strategi cadangan tag untuk menyokong pengguna dalam tugas penjelasan foto dengan mengesyorkan satu set tag yang dapat ditambahkan ke foto. Hasil penilaian empirikal menunjukkan bahawa kami dapat dengan berkesan mengesyorkan tag yang relevan untuk pelbagai foto dengan tahap ketuntasan yang berbeza dari penandaan asal. [[EENNDD]] anotasi foto; cadangan teg; analisis kandungan dan pengindeksan; perkhidmatan maklumat dalam talian; kejadian bersama tag; pengetahuan kolektif; cadangan teg agregat; pencirian teg; flickr"], [{"string": "Learning how to learn with web contents No contact information provided yet.", "keywords": ["web contents", "hyperspace", "meta-learning", "navigational learning", "learning affordance"], "combined": "Learning how to learn with web contents No contact information provided yet. [[EENNDD]] web contents; hyperspace; meta-learning; navigational learning; learning affordance"}, "Belajar bagaimana belajar dengan kandungan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kandungan web; hiperspace; pembelajaran meta; pembelajaran navigasi; perbelanjaan belajar"], [{"string": "Persistence in web based collaborations No contact information provided yet.", "keywords": ["hci", "user interfaces", "music", "creativity", "collaboration", "group and organization interfaces"], "combined": "Persistence in web based collaborations No contact information provided yet. [[EENNDD]] hci; user interfaces; music; creativity; collaboration; group and organization interfaces"}, "Ketekunan dalam kerjasama berasaskan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] hci; antara muka pengguna; muzik; kreativiti; kerjasama; antara muka kumpulan dan organisasi"], [{"string": "Harnessing the wisdom of crowds: video event detection based on synchronous comments With the recent explosive growth of the number of videos on the Web, it becomes more important to facilitate users' demand for locating their preferred event clips in the lengthy and voluminous programs. Although there has been a great deal of study on generic event detection in recent years, the performance of existing approaches is still far from satisfactory. In this paper, we propose an integrated framework for general event detection. The key idea is that we utilize the synchronous comments to segment the video into clips with semantic text analysis, while taking into account the relationship between the users who write the comments. By borrowing the power of \"the wisdom of crowds\", we experimentally demonstrate that our approach can effectively detect video events.", "keywords": ["information extraction", "tag analysis", "information search and retrieval", "video search"], "combined": "Harnessing the wisdom of crowds: video event detection based on synchronous comments With the recent explosive growth of the number of videos on the Web, it becomes more important to facilitate users' demand for locating their preferred event clips in the lengthy and voluminous programs. Although there has been a great deal of study on generic event detection in recent years, the performance of existing approaches is still far from satisfactory. In this paper, we propose an integrated framework for general event detection. The key idea is that we utilize the synchronous comments to segment the video into clips with semantic text analysis, while taking into account the relationship between the users who write the comments. By borrowing the power of \"the wisdom of crowds\", we experimentally demonstrate that our approach can effectively detect video events. [[EENNDD]] information extraction; tag analysis; information search and retrieval; video search"}, "Memanfaatkan kebijaksanaan orang ramai: pengesanan acara video berdasarkan komen segerak Dengan peningkatan jumlah video di Web yang meletup baru-baru ini, menjadi lebih penting untuk memudahkan permintaan pengguna untuk mencari klip acara pilihan mereka dalam program yang panjang dan besar. Walaupun telah dilakukan banyak kajian mengenai pengesanan peristiwa generik dalam beberapa tahun terakhir, prestasi pendekatan yang ada masih jauh dari memuaskan. Dalam makalah ini, kami mengusulkan kerangka terpadu untuk pengesanan peristiwa umum. Idea utamanya ialah kami menggunakan komen segerak untuk membagi video menjadi klip dengan analisis teks semantik, sambil mengambil kira hubungan antara pengguna yang menulis komen. Dengan meminjam kekuatan \"kebijaksanaan orang banyak\", kami secara eksperimen menunjukkan bahawa pendekatan kami dapat mengesan peristiwa video dengan berkesan. [[EENNDD]] pengekstrakan maklumat; analisis teg; pencarian dan pengambilan maklumat; carian video"], [{"string": "Implementing the media fragments URI specification In this paper, we describe two examples of implementations of the Media Fragments URI specification which is currently being developed by the W3C Media Fragments Working Group. The group's mission is to create standard addressing schemes for media fragments on the Web using Uniform Resource Identifiers (URIs). We describe two scenarios to illustrate the implementations. More specifically, we show how User Agents (UA) will either be able to resolve media fragment URIs without help from the server, or will make use of a media fragments-aware server. Finally, we present some ongoing discussions and issues regarding the implementation of the Media Fragments specification.", "keywords": ["video accessibility", "media fragments", "video url"], "combined": "Implementing the media fragments URI specification In this paper, we describe two examples of implementations of the Media Fragments URI specification which is currently being developed by the W3C Media Fragments Working Group. The group's mission is to create standard addressing schemes for media fragments on the Web using Uniform Resource Identifiers (URIs). We describe two scenarios to illustrate the implementations. More specifically, we show how User Agents (UA) will either be able to resolve media fragment URIs without help from the server, or will make use of a media fragments-aware server. Finally, we present some ongoing discussions and issues regarding the implementation of the Media Fragments specification. [[EENNDD]] video accessibility; media fragments; video url"}, "Menerapkan spesifikasi URI fragmen media Dalam makalah ini, kami menerangkan dua contoh pelaksanaan spesifikasi URI Fragmen Media yang saat ini sedang dikembangkan oleh Kumpulan Kerja Fragmen Media W3C. Misi kumpulan ini adalah untuk membuat skema pengalamatan standard untuk fragmen media di Web menggunakan Uniform Resource Identifiers (URI). Kami menerangkan dua senario untuk menggambarkan pelaksanaannya. Lebih khusus lagi, kami menunjukkan bagaimana Ejen Pengguna (UA) dapat menyelesaikan URI fragmen media tanpa bantuan daripada pelayan, atau akan menggunakan pelayan yang menyedari fragmen media. Akhirnya, kami membentangkan beberapa perbincangan dan permasalahan yang sedang berlangsung mengenai pelaksanaan spesifikasi Fragmen Media. [[EENNDD]] kebolehcapaian video; serpihan media; url video"], [{"string": "Distribution of relevant documents in domain-level aggregates for topic distillation No contact information provided yet.", "keywords": ["web ir", "information search and retrieval", "distribution of relevant documents", "aggregates"], "combined": "Distribution of relevant documents in domain-level aggregates for topic distillation No contact information provided yet. [[EENNDD]] web ir; information search and retrieval; distribution of relevant documents; aggregates"}, "Pengedaran dokumen yang berkaitan dalam agregat peringkat domain untuk penyulingan topik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] laman web; pencarian dan pengambilan maklumat; pengedaran dokumen yang berkaitan; agregat"], [{"string": "Adaptive filtering of advertisements on web pages No contact information provided yet.", "keywords": ["interface agents", "weighted majority", "advertisement filtering"], "combined": "Adaptive filtering of advertisements on web pages No contact information provided yet. [[EENNDD]] interface agents; weighted majority; advertisement filtering"}, "Penyaringan iklan secara adaptif di laman web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ejen antara muka; majoriti berwajaran; tapisan iklan"], [{"string": "A graphical user interface toolkit approach to thin-client computing No contact information provided yet.", "keywords": ["remote method invocation", "client-server systems", "language constructs and features", "user interfaces", "network computing", "distributed systems", "user interface toolkit"], "combined": "A graphical user interface toolkit approach to thin-client computing No contact information provided yet. [[EENNDD]] remote method invocation; client-server systems; language constructs and features; user interfaces; network computing; distributed systems; user interface toolkit"}, "Pendekatan toolkit antara muka pengguna grafik untuk pengkomputeran pelanggan tipis Belum ada maklumat hubungan yang diberikan. [[EENNDD]] permohonan kaedah jauh; sistem pelayan pelanggan; konstruk dan ciri bahasa; antara muka pengguna; pengkomputeran rangkaian; sistem yang diedarkan; kit alat antara muka pengguna"], [{"string": "Improvement of HITS-based algorithms on web documents No contact information provided yet.", "keywords": ["general", "relevance scoring methods", "hits-based algorithms", "information retrieval"], "combined": "Improvement of HITS-based algorithms on web documents No contact information provided yet. [[EENNDD]] general; relevance scoring methods; hits-based algorithms; information retrieval"}, "Peningkatan algoritma berdasarkan HITS pada dokumen web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] umum; kaedah pemarkahan perkaitan; algoritma berdasarkan hits; pengambilan maklumat"], [{"string": "Trains of thought: generating information maps When information is abundant, it becomes increasingly difficult to fit nuggets of knowledge into a single coherent picture. Complex stories spaghetti into branches, side stories, and intertwining narratives. In order to explore these stories, one needs a map to navigate unfamiliar territory. We propose a methodology for creating structured summaries of information, which we call metro maps. Our proposed algorithm generates a concise structured set of documents maximizing coverage of salient pieces of information. Most importantly, metro maps explicitly show the relations among retrieved pieces in a way that captures story development. We first formalize characteristics of good maps and formulate their construction as an optimization problem. Then we provide efficient methods with theoretical guarantees for generating maps. Finally, we integrate user interaction into our framework, allowing users to alter the maps to better reflect their interests. Pilot user studies with a real-world dataset demonstrate that the method is able to produce maps which help users acquire knowledge efficiently.", "keywords": ["general", "content analysis and indexing", "information search and retrieval", "metro maps", "summarization", "information"], "combined": "Trains of thought: generating information maps When information is abundant, it becomes increasingly difficult to fit nuggets of knowledge into a single coherent picture. Complex stories spaghetti into branches, side stories, and intertwining narratives. In order to explore these stories, one needs a map to navigate unfamiliar territory. We propose a methodology for creating structured summaries of information, which we call metro maps. Our proposed algorithm generates a concise structured set of documents maximizing coverage of salient pieces of information. Most importantly, metro maps explicitly show the relations among retrieved pieces in a way that captures story development. We first formalize characteristics of good maps and formulate their construction as an optimization problem. Then we provide efficient methods with theoretical guarantees for generating maps. Finally, we integrate user interaction into our framework, allowing users to alter the maps to better reflect their interests. Pilot user studies with a real-world dataset demonstrate that the method is able to produce maps which help users acquire knowledge efficiently. [[EENNDD]] general; content analysis and indexing; information search and retrieval; metro maps; summarization; information"}, "Keretapi pemikiran: menghasilkan peta maklumat Apabila maklumat banyak, semakin sukar untuk memasukkan nugget pengetahuan ke dalam satu gambar yang koheren. Cerita spageti yang rumit menjadi cabang, cerita sampingan, dan naratif yang saling berkaitan. Untuk meneroka kisah-kisah ini, seseorang memerlukan peta untuk menavigasi wilayah yang tidak dikenali. Kami mencadangkan metodologi untuk membuat ringkasan maklumat yang tersusun, yang kami namakan peta metro. Algoritma yang dicadangkan kami menghasilkan sekumpulan dokumen berstruktur ringkas yang memaksimumkan liputan maklumat yang penting. Yang paling penting, peta metro secara eksplisit menunjukkan hubungan antara potongan yang diambil dengan cara yang dapat menangkap perkembangan cerita. Kami pertama kali memformalkan ciri peta yang baik dan merumuskan pembinaannya sebagai masalah pengoptimuman. Kemudian kami menyediakan kaedah yang cekap dengan jaminan teori untuk menghasilkan peta. Akhirnya, kami mengintegrasikan interaksi pengguna ke dalam kerangka kerja kami, yang membolehkan pengguna mengubah peta untuk menggambarkan minat mereka dengan lebih baik. Kajian pengguna rintis dengan set data dunia nyata menunjukkan bahawa kaedah tersebut dapat menghasilkan peta yang dapat membantu pengguna memperoleh pengetahuan dengan cekap. [[EENNDD]] umum; analisis kandungan dan pengindeksan; carian dan pengambilan maklumat; peta metro; ringkasan; maklumat"], [{"string": "Topic sentiment mixture: modeling facets and opinions in weblogs In this paper, we define the problem of topic-sentiment analysis on Weblogs and propose a novel probabilistic model to capture the mixture of topics and sentiments simultaneously. The proposed Topic-Sentiment Mixture (TSM) model can reveal the latent topical facets in a Weblog collection, the subtopics in the results of an ad hoc query, and their associated sentiments. It could also provide general sentiment models that are applicable to any ad hoc topics. With a specifically designed HMM structure, the sentiment models and topic models estimated with TSM can be utilized to extract topic life cycles and sentiment dynamics. Empirical experiments on different Weblog datasets show that this approach is effective for modeling the topic facets and sentiments and extracting their dynamics from Weblog collections. The TSM model is quite general; it can be applied to any text collections with a mixture of topics and sentiments, thus has many potential applications, such as search result summarization, opinion tracking, and user behavior prediction.", "keywords": ["weblogs", "sentiment analysis", "information search and retrieval", "mixture model", "topic-sentiment mixture", "topic models"], "combined": "Topic sentiment mixture: modeling facets and opinions in weblogs In this paper, we define the problem of topic-sentiment analysis on Weblogs and propose a novel probabilistic model to capture the mixture of topics and sentiments simultaneously. The proposed Topic-Sentiment Mixture (TSM) model can reveal the latent topical facets in a Weblog collection, the subtopics in the results of an ad hoc query, and their associated sentiments. It could also provide general sentiment models that are applicable to any ad hoc topics. With a specifically designed HMM structure, the sentiment models and topic models estimated with TSM can be utilized to extract topic life cycles and sentiment dynamics. Empirical experiments on different Weblog datasets show that this approach is effective for modeling the topic facets and sentiments and extracting their dynamics from Weblog collections. The TSM model is quite general; it can be applied to any text collections with a mixture of topics and sentiments, thus has many potential applications, such as search result summarization, opinion tracking, and user behavior prediction. [[EENNDD]] weblogs; sentiment analysis; information search and retrieval; mixture model; topic-sentiment mixture; topic models"}, "Campuran sentimen topik: aspek pemodelan dan pendapat dalam blog web Dalam makalah ini, kami menentukan masalah analisis sentimen topik di Weblog dan mencadangkan model probabilistik baru untuk menangkap campuran topik dan sentimen secara serentak. Model Topik-Sentimen Campuran (TSM) yang dicadangkan dapat mendedahkan aspek topikal terpendam dalam koleksi Weblog, subtopik dalam hasil pertanyaan ad hoc, dan sentimen yang berkaitan. Ini juga dapat memberikan model sentimen umum yang berlaku untuk topik ad hoc. Dengan struktur HMM yang dirancang khusus, model sentimen dan model topik yang diperkirakan dengan TSM dapat digunakan untuk mengekstrak kitaran hidup topik dan dinamika sentimen. Eksperimen empirikal pada kumpulan data Weblog yang berbeza menunjukkan bahawa pendekatan ini berkesan untuk memodelkan aspek dan sentimen topik dan mengekstrak dinamika mereka dari koleksi Weblog. Model TSM agak umum; ia dapat diterapkan pada koleksi teks apa pun dengan campuran topik dan sentimen, sehingga memiliki banyak aplikasi yang berpotensi, seperti ringkasan hasil pencarian, penjejakan pendapat, dan ramalan perilaku pengguna. [[EENNDD]] blog web; analisis sentimen; carian dan pengambilan maklumat; model campuran; campuran topik-sentimen; model topik"], [{"string": "Structured query suggestion for specialization and parallel movement: effect on search behaviors Query suggestion, which enables the user to revise a query with a single click, has become one of the most fundamental features of Web search engines. However, it is often difficult for the user to choose from a list of query suggestions, and to understand the relation between an input query and suggested ones. In this paper, we propose a new method to present query suggestions to the user, which has been designed to help two popular query reformulation actions, namely, specialization (e.g. from \"nikon\" to \"nikon camera\" ) and parallel movement (e.g. from \"nikon camera\" to \"canon camera\"). Using a query log collected from a popular commercial Web search engine, our prototype called SParQS classifies query suggestions into automatically generated categories and generates a label for each category. Moreover, SParQS presents some new entities as alternatives to the original query (e.g. \"canon\" in response to the query \"nikon\"), together with their query suggestions classified in the same way as the original query's suggestions. We conducted a task-based user study to compare SParQS with a traditional \"flat list\" query suggestion interface. Our results show that the SParQS interface enables subjects to search more successfully than the flat list case, even though query suggestions presented were exactly the same in the two interfaces. In addition, the subjects found the query suggestions more helpful when they were presented in the SParQS interface rather than in a flat list.", "keywords": ["query suggestion", "web search", "information search and retrieval", "query log mining", "search user interface"], "combined": "Structured query suggestion for specialization and parallel movement: effect on search behaviors Query suggestion, which enables the user to revise a query with a single click, has become one of the most fundamental features of Web search engines. However, it is often difficult for the user to choose from a list of query suggestions, and to understand the relation between an input query and suggested ones. In this paper, we propose a new method to present query suggestions to the user, which has been designed to help two popular query reformulation actions, namely, specialization (e.g. from \"nikon\" to \"nikon camera\" ) and parallel movement (e.g. from \"nikon camera\" to \"canon camera\"). Using a query log collected from a popular commercial Web search engine, our prototype called SParQS classifies query suggestions into automatically generated categories and generates a label for each category. Moreover, SParQS presents some new entities as alternatives to the original query (e.g. \"canon\" in response to the query \"nikon\"), together with their query suggestions classified in the same way as the original query's suggestions. We conducted a task-based user study to compare SParQS with a traditional \"flat list\" query suggestion interface. Our results show that the SParQS interface enables subjects to search more successfully than the flat list case, even though query suggestions presented were exactly the same in the two interfaces. In addition, the subjects found the query suggestions more helpful when they were presented in the SParQS interface rather than in a flat list. [[EENNDD]] query suggestion; web search; information search and retrieval; query log mining; search user interface"}, "Cadangan pertanyaan berstruktur untuk pengkhususan dan pergerakan selari: kesan pada tingkah laku carian Cadangan pertanyaan, yang membolehkan pengguna menyemak semula pertanyaan dengan satu klik, telah menjadi salah satu ciri paling penting dari mesin carian Web. Walau bagaimanapun, selalunya sukar bagi pengguna untuk memilih dari senarai cadangan pertanyaan, dan memahami hubungan antara pertanyaan input dan cadangan. Dalam makalah ini, kami mencadangkan kaedah baru untuk mengemukakan cadangan pertanyaan kepada pengguna, yang telah dirancang untuk membantu dua tindakan penyusunan semula pertanyaan yang popular, iaitu pengkhususan (misalnya dari \"nikon\" hingga \"kamera nikon\") dan pergerakan selari (contohnya dari \"kamera nikon\" hingga \"kamera kanon\"). Dengan menggunakan log pertanyaan yang dikumpulkan dari enjin carian Web komersial yang popular, prototaip kami yang dipanggil SParQS mengklasifikasikan cadangan pertanyaan ke dalam kategori yang dihasilkan secara automatik dan menghasilkan label untuk setiap kategori. Lebih-lebih lagi, SParQS menyajikan beberapa entiti baru sebagai alternatif untuk pertanyaan asal (mis. \"Canon\" sebagai tindak balas kepada pertanyaan \"nikon\"), bersama dengan cadangan pertanyaan mereka yang diklasifikasikan dengan cara yang sama seperti cadangan pertanyaan asal. Kami menjalankan kajian pengguna berdasarkan tugas untuk membandingkan SParQS dengan antara muka cadangan pertanyaan \"senarai rata\" tradisional. Hasil kajian kami menunjukkan bahawa antara muka SParQS membolehkan subjek mencari dengan lebih berjaya daripada kes senarai rata, walaupun cadangan pertanyaan yang dikemukakan sama pada kedua-dua antara muka tersebut. Di samping itu, subjek mendapati cadangan pertanyaan lebih berguna apabila disajikan dalam antara muka SParQS dan bukannya dalam senarai rata. [[EENNDD]] cadangan pertanyaan; carian sesawang; carian dan pengambilan maklumat; perlombongan log pertanyaan; cari antara muka pengguna"], [{"string": "Adaptive query routing in peer web search No contact information provided yet.", "keywords": ["adaptive query routing", "information search and retrieval", "performance evaluation", "peer collaborative search", "topical crawlers"], "combined": "Adaptive query routing in peer web search No contact information provided yet. [[EENNDD]] adaptive query routing; information search and retrieval; performance evaluation; peer collaborative search; topical crawlers"}, "Penghalaan pertanyaan adaptif dalam carian web rakan sebaya Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penghalaan pertanyaan adaptif; pencarian dan pengambilan maklumat; penilaian prestasi; carian rakan sebaya; crawler topikal"], [{"string": "Discovering event evolution graphs from newswires No contact information provided yet.", "keywords": ["event evolution graph", "web content mining", "knowledge management", "event evolution"], "combined": "Discovering event evolution graphs from newswires No contact information provided yet. [[EENNDD]] event evolution graph; web content mining; knowledge management; event evolution"}, "Menemui grafik evolusi peristiwa dari kawat baru Belum ada maklumat hubungan yang diberikan. [[EENNDD]] grafik evolusi peristiwa; perlombongan kandungan web; pengurusan pengetahuan; evolusi peristiwa"], [{"string": "Mirror site maintenance based on evolution associations of web directories Mirroring Web sites is a well-known technique commonly used in the Web community. A mirror site should be updated frequently to ensure that it reflects the content of the original site. Existing mirroring tools apply page-level strategies to check each page of a site, which is inefficient and expensive. In this paper, we propose a novel site-level mirror maintenance strategy. Our approach studies the evolution of Web directorystructures and mines association rules between ancestor-descendant Web directories. Discovered rules indicate the evolution correlations between Web directories. Thus, when maintaining the mirror of a Web site (directory), we can optimally skipsubdirectories which are negatively correlated with it in undergoing significant changes. The preliminary experimental results show that our approach improves the efficiency of the mirror maintenance process significantly while sacrificing slightly in keeping the \"freshness\" of the mirrors.", "keywords": ["evolution correlation", "mirror maintenance", "web evolution"], "combined": "Mirror site maintenance based on evolution associations of web directories Mirroring Web sites is a well-known technique commonly used in the Web community. A mirror site should be updated frequently to ensure that it reflects the content of the original site. Existing mirroring tools apply page-level strategies to check each page of a site, which is inefficient and expensive. In this paper, we propose a novel site-level mirror maintenance strategy. Our approach studies the evolution of Web directorystructures and mines association rules between ancestor-descendant Web directories. Discovered rules indicate the evolution correlations between Web directories. Thus, when maintaining the mirror of a Web site (directory), we can optimally skipsubdirectories which are negatively correlated with it in undergoing significant changes. The preliminary experimental results show that our approach improves the efficiency of the mirror maintenance process significantly while sacrificing slightly in keeping the \"freshness\" of the mirrors. [[EENNDD]] evolution correlation; mirror maintenance; web evolution"}, "Penyelenggaraan laman web cermin berdasarkan hubungan evolusi direktori web Laman web Mirroring adalah teknik terkenal yang biasa digunakan dalam komuniti Web. Laman web cermin mesti dikemas kini dengan kerap untuk memastikan bahawa laman web ini mencerminkan kandungan laman web asal. Alat pencerminan yang ada menerapkan strategi peringkat halaman untuk memeriksa setiap halaman laman web, yang tidak cekap dan mahal. Dalam makalah ini, kami mencadangkan strategi pemeliharaan cermin peringkat tapak baru. Pendekatan kami mengkaji evolusi struktur direktori Web dan peraturan perkaitan lombong antara direktori Web leluhur-keturunan. Peraturan yang dijumpai menunjukkan korelasi evolusi antara direktori Web. Oleh itu, ketika mengekalkan cerminan laman web (direktori), kita dapat secara optimum melangkau direktori yang berkorelasi negatif dengannya dalam menjalani perubahan yang ketara. Hasil eksperimen awal menunjukkan bahawa pendekatan kami meningkatkan kecekapan proses penyelenggaraan cermin secara signifikan sambil sedikit berkorban dalam menjaga \"kesegaran\" cermin. [[EENNDD]] korelasi evolusi; penyelenggaraan cermin; evolusi web"], [{"string": "Batch rekeying for secure group communications An abstract is not available.", "keywords": ["secure group communications", "rekeying", "security and protection", "group key management"], "combined": "Batch rekeying for secure group communications An abstract is not available. [[EENNDD]] secure group communications; rekeying; security and protection; group key management"}, "Sekumpulan semula untuk komunikasi kumpulan yang selamat Abstrak tidak tersedia. [[EENNDD]] komunikasi kumpulan selamat; menghidupkan semula; keselamatan dan perlindungan; pengurusan kunci kumpulan"], [{"string": "Characterizing insecure javascript practices on the web JavaScript is an interpreted programming language most often used for enhancing webpage interactivity and functionality. It has powerful capabilities to interact with webpage documents and browser windows, however, it has also opened the door for many browser-based security attacks. Insecure engineering practices of using JavaScript may not directly lead to security breaches, but they can create new attack vectors and greatly increase the risks of browser-based attacks. In this paper, we present the first measurement study on insecure practices of using JavaScript on the Web. Our focus is on the insecure practices of JavaScript inclusion and dynamic generation, and we examine their severity and nature on 6,805 unique websites. Our measurement results reveal that insecure JavaScript practices are common at various websites: (1) at least 66.4% of the measured websites manifest the insecure practices of including JavaScript files from external domains into the top-level documents of their webpages; (2) over 44.4% of the measured websites use the dangerous eval() function to dynamically generate and execute JavaScript code on their webpages; and (3) in JavaScript dynamic generation, using the document.write() method and the innerHTML property is much more popular than using the relatively secure technique of creating script elements via DOM methods. Our analysis indicates that safe alternatives to these insecure practices exist in common cases and ought to be adopted by website developers and administrators for reducing potential security risks.", "keywords": ["unauthorized access", "ast tree matching", "same origin policy", "web engineering", "javascript", "execution-based measurement"], "combined": "Characterizing insecure javascript practices on the web JavaScript is an interpreted programming language most often used for enhancing webpage interactivity and functionality. It has powerful capabilities to interact with webpage documents and browser windows, however, it has also opened the door for many browser-based security attacks. Insecure engineering practices of using JavaScript may not directly lead to security breaches, but they can create new attack vectors and greatly increase the risks of browser-based attacks. In this paper, we present the first measurement study on insecure practices of using JavaScript on the Web. Our focus is on the insecure practices of JavaScript inclusion and dynamic generation, and we examine their severity and nature on 6,805 unique websites. Our measurement results reveal that insecure JavaScript practices are common at various websites: (1) at least 66.4% of the measured websites manifest the insecure practices of including JavaScript files from external domains into the top-level documents of their webpages; (2) over 44.4% of the measured websites use the dangerous eval() function to dynamically generate and execute JavaScript code on their webpages; and (3) in JavaScript dynamic generation, using the document.write() method and the innerHTML property is much more popular than using the relatively secure technique of creating script elements via DOM methods. Our analysis indicates that safe alternatives to these insecure practices exist in common cases and ought to be adopted by website developers and administrators for reducing potential security risks. [[EENNDD]] unauthorized access; ast tree matching; same origin policy; web engineering; javascript; execution-based measurement"}, "Mencirikan amalan javascript yang tidak selamat di web JavaScript adalah bahasa pengaturcaraan yang ditafsirkan yang paling sering digunakan untuk meningkatkan interaktiviti dan fungsi laman web. Ia memiliki kemampuan kuat untuk berinteraksi dengan dokumen halaman web dan tetingkap penyemak imbas, namun, ia juga telah membuka pintu untuk banyak serangan keselamatan berdasarkan penyemak imbas. Amalan kejuruteraan yang tidak selamat menggunakan JavaScript mungkin tidak secara langsung menyebabkan pelanggaran keselamatan, tetapi mereka dapat membuat vektor serangan baru dan meningkatkan risiko serangan berdasarkan penyemak imbas. Dalam makalah ini, kami menyajikan kajian pengukuran pertama mengenai praktik tidak selamat menggunakan JavaScript di Web. Tumpuan kami adalah pada praktik tidak selamat penyertaan JavaScript dan penjanaan dinamik, dan kami mengkaji keparahan dan sifatnya di 6,805 laman web unik. Hasil pengukuran kami menunjukkan bahawa praktik JavaScript yang tidak selamat adalah umum di berbagai laman web: (1) sekurang-kurangnya 66.4% dari laman web yang diukur menunjukkan amalan tidak selamat termasuk memasukkan fail JavaScript dari domain luaran ke dalam dokumen tingkat atas dari laman web mereka; (2) lebih dari 44.4% laman web yang diukur menggunakan fungsi eval () berbahaya untuk menjana dan melaksanakan kod JavaScript secara dinamis di laman web mereka; dan (3) dalam penjanaan dinamik JavaScript, menggunakan kaedah document.write () dan sifat innerHTML jauh lebih popular daripada menggunakan teknik pembuatan elemen skrip yang agak selamat melalui kaedah DOM. Analisis kami menunjukkan bahawa alternatif yang selamat untuk amalan tidak selamat ini ada dalam kes biasa dan harus diguna pakai oleh pembangun laman web dan pentadbir untuk mengurangkan potensi risiko keselamatan. [[EENNDD]] akses yang tidak dibenarkan; padanan pokok ast; dasar asal yang sama; kejuruteraan web; javascript; pengukuran berasaskan pelaksanaan"], [{"string": "Raise semantics at the user level for dynamic and interactive SOA-based portals In this paper, we describe the fully dynamic semantic portal we implemented, integrating Semantic Web technologies and Service Oriented Architecture (SOA). The goals of the portal are twofold: first it helps administrators to easily propose new features in the portal using semantics to ease the orchestration process; secondly it automatically generates a customized user interface for these scenarios. This user interface takes into account different devices and assists end-users in the use of the portal taking benefit of context awareness. All the added-value of this portal is based on a core semantics defined by an ontology. We present here the main features of this portal and how it was implemented using state-of-the-art technologies and frameworks.", "keywords": ["semantic web services", "context", "semantic portal", "soa"], "combined": "Raise semantics at the user level for dynamic and interactive SOA-based portals In this paper, we describe the fully dynamic semantic portal we implemented, integrating Semantic Web technologies and Service Oriented Architecture (SOA). The goals of the portal are twofold: first it helps administrators to easily propose new features in the portal using semantics to ease the orchestration process; secondly it automatically generates a customized user interface for these scenarios. This user interface takes into account different devices and assists end-users in the use of the portal taking benefit of context awareness. All the added-value of this portal is based on a core semantics defined by an ontology. We present here the main features of this portal and how it was implemented using state-of-the-art technologies and frameworks. [[EENNDD]] semantic web services; context; semantic portal; soa"}, "Tingkatkan semantik di peringkat pengguna untuk portal berasaskan SOA dinamik dan interaktif Dalam makalah ini, kami menerangkan portal semantik dinamik sepenuhnya yang kami laksanakan, mengintegrasikan teknologi Web Semantik dan Senibina Berorientasikan Perkhidmatan (SOA). Matlamat portal adalah dua kali ganda: pertama ia membantu pentadbir dengan mudah mencadangkan ciri baru di portal menggunakan semantik untuk memudahkan proses orkestrasi; kedua ia menghasilkan antara muka pengguna yang disesuaikan untuk senario ini secara automatik. Antara muka pengguna ini mengambil kira peranti yang berbeza dan membantu pengguna akhir dalam penggunaan portal memanfaatkan kesedaran konteks. Semua nilai tambah portal ini didasarkan pada semantik inti yang ditentukan oleh ontologi. Kami membentangkan di sini ciri utama portal ini dan bagaimana ia dilaksanakan menggunakan teknologi dan kerangka kerja canggih. [[EENNDD]] perkhidmatan web semantik; konteks; portal semantik; soa"], [{"string": "Web people search: results of the first evaluation and the plan for the second This paper presents the motivation, resources and results for the first Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. Also, we will describe a survey and proposal for a new task, \"attribute extraction\", which is planned for inclusion in the second evaluation, planned for autumn, 2008.", "keywords": ["information extraction", "attributes of people", "person names", "disambiguation"], "combined": "Web people search: results of the first evaluation and the plan for the second This paper presents the motivation, resources and results for the first Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. Also, we will describe a survey and proposal for a new task, \"attribute extraction\", which is planned for inclusion in the second evaluation, planned for autumn, 2008. [[EENNDD]] information extraction; attributes of people; person names; disambiguation"}, "Pencarian orang web: hasil penilaian pertama dan rancangan untuk yang kedua Makalah ini menyajikan motivasi, sumber daya dan hasil untuk tugas Pencarian Orang Web pertama, yang disusun sebagai bagian dari latihan penilaian SemEval-2007. Juga, kami akan menerangkan tinjauan dan cadangan untuk tugas baru, \"pengekstrakan atribut\", yang dirancang untuk dimasukkan dalam penilaian kedua, yang dirancang untuk musim luruh, 2008. [[EENNDD]] pengekstrakan maklumat; sifat orang; nama orang; pelepasan"], [{"string": "HyLiEn: a hybrid approach to general list extraction on the web We consider the problem of automatically extracting general lists from the web. Existing approaches are mostly dependent upon either the underlying HTML markup or the visual structure of the Web page. We present HyLiEn an unsupervised, Hybrid approach for automatic List discovery and Extraction on the Web. It employs general assumptions about the visual rendering of lists, and the structural representation of items contained in them. We show that our method significantly outperforms existing methods.", "keywords": ["web lists", "web mining", "web information integration"], "combined": "HyLiEn: a hybrid approach to general list extraction on the web We consider the problem of automatically extracting general lists from the web. Existing approaches are mostly dependent upon either the underlying HTML markup or the visual structure of the Web page. We present HyLiEn an unsupervised, Hybrid approach for automatic List discovery and Extraction on the Web. It employs general assumptions about the visual rendering of lists, and the structural representation of items contained in them. We show that our method significantly outperforms existing methods. [[EENNDD]] web lists; web mining; web information integration"}, "HyLiEn: pendekatan hibrid untuk pengekstrakan senarai umum di web Kami menganggap masalah mengekstrak senarai umum dari web secara automatik. Pendekatan yang ada kebanyakannya bergantung pada penanda HTML yang mendasari atau struktur visual halaman Web. Kami menyajikan HyLiEn, pendekatan Hybrid tanpa pengawasan untuk penemuan dan Pengekstrakan Senarai automatik di Web. Ini menggunakan andaian umum mengenai rendering visual daftar, dan representasi struktur item yang terkandung di dalamnya. Kami menunjukkan bahawa kaedah kami jauh mengatasi kaedah yang ada. [[EENNDD]] senarai web; perlombongan web; penyatuan maklumat web"], [{"string": "Discovering the staring people from social networks In this paper, we study a novel problem of staring people discovery from social networks, which is concerned with finding people who are not only authoritative but also sociable in the social network. We formalize this problem as an optimization programming problem. Taking the co-author network as a case study, we define three objective functions and propose two methods to combine these objective functions. A genetic algorithm based method is further presented to solve this problem. Experimental results show that the proposed solution can effectively find the staring people from social networks.", "keywords": ["staring people discovery", "learning", "social network"], "combined": "Discovering the staring people from social networks In this paper, we study a novel problem of staring people discovery from social networks, which is concerned with finding people who are not only authoritative but also sociable in the social network. We formalize this problem as an optimization programming problem. Taking the co-author network as a case study, we define three objective functions and propose two methods to combine these objective functions. A genetic algorithm based method is further presented to solve this problem. Experimental results show that the proposed solution can effectively find the staring people from social networks. [[EENNDD]] staring people discovery; learning; social network"}, "Menemui orang-orang yang menatap dari rangkaian sosial Dalam makalah ini, kami mengkaji masalah baru penemuan orang yang menatap dari rangkaian sosial, yang berkaitan dengan mencari orang-orang yang tidak hanya berwibawa tetapi juga bergaul di rangkaian sosial. Kami memformalkan masalah ini sebagai masalah pengaturcaraan pengoptimuman. Mengambil rangkaian pengarang bersama sebagai kajian kes, kami menentukan tiga fungsi objektif dan mencadangkan dua kaedah untuk menggabungkan fungsi objektif ini. Kaedah berdasarkan algoritma genetik selanjutnya dikemukakan untuk menyelesaikan masalah ini. Hasil eksperimen menunjukkan bahawa penyelesaian yang dicadangkan dapat dengan berkesan mencari orang yang menatap dari rangkaian sosial. [[EENNDD]] menatap penemuan orang; belajar; rangkaian sosial"], [{"string": "Instance-based probabilistic reasoning in the semantic web Most of the approaches for dealing with uncertainty in the Semantic Web rely on the principle that this uncertainty is already asserted. In this paper, we propose a new approach to learn and reason about uncertainty in the Semantic Web. Using instance data, we learn the uncertainty of an OWL ontology, and use that information to perform probabilistic reasoning on it. For this purpose, we use Markov logic, a new representation formalism that combines logic with probabilistic graphical models.", "keywords": ["markov logic", "semantic web", "probabilistic reasoning", "uncertainty, fuzzy, and probabilistic reasoning"], "combined": "Instance-based probabilistic reasoning in the semantic web Most of the approaches for dealing with uncertainty in the Semantic Web rely on the principle that this uncertainty is already asserted. In this paper, we propose a new approach to learn and reason about uncertainty in the Semantic Web. Using instance data, we learn the uncertainty of an OWL ontology, and use that information to perform probabilistic reasoning on it. For this purpose, we use Markov logic, a new representation formalism that combines logic with probabilistic graphical models. [[EENNDD]] markov logic; semantic web; probabilistic reasoning; uncertainty, fuzzy, and probabilistic reasoning"}, "Penalaran probabilistik berdasarkan instance dalam web semantik Sebilangan besar pendekatan untuk menangani ketidakpastian dalam Semantik Web bergantung pada prinsip bahawa ketidakpastian ini sudah ditegaskan. Dalam makalah ini, kami mencadangkan pendekatan baru untuk belajar dan memberi alasan mengenai ketidakpastian di Semantik Web. Dengan menggunakan data contoh, kami mempelajari ketidakpastian ontologi OWL, dan menggunakan maklumat tersebut untuk melakukan pertimbangan kemungkinan. Untuk tujuan ini, kami menggunakan logik Markov, sebuah formalisme representasi baru yang menggabungkan logik dengan model grafik probabilistik. [[EENNDD]] logik markov; web semantik; penaakulan probabilistik; ketidakpastian, kabur, dan penaakulan probabilistik"], [{"string": "Video suggestion and discovery for youtube: taking random walks through the view graph The rapid growth of the number of videos in YouTube provides enormous potential for users to find content of interest to them. Unfortunately, given the difficulty of searching videos, the size of the video repository also makes the discovery of new content a daunting task. In this paper, we present a novel method based upon the analysis of the entire user-video graph to provide personalized video suggestions for users. The resulting algorithm, termed Adsorption, provides a simple method to efficiently propagate preference information through a variety of graphs. We extensively test the results of the recommendations on a three month snapshot of live data from YouTube.", "keywords": ["random walks", "collaborative filtering", "label propagation", "video search", "recommendation systems"], "combined": "Video suggestion and discovery for youtube: taking random walks through the view graph The rapid growth of the number of videos in YouTube provides enormous potential for users to find content of interest to them. Unfortunately, given the difficulty of searching videos, the size of the video repository also makes the discovery of new content a daunting task. In this paper, we present a novel method based upon the analysis of the entire user-video graph to provide personalized video suggestions for users. The resulting algorithm, termed Adsorption, provides a simple method to efficiently propagate preference information through a variety of graphs. We extensively test the results of the recommendations on a three month snapshot of live data from YouTube. [[EENNDD]] random walks; collaborative filtering; label propagation; video search; recommendation systems"}, "Cadangan dan penemuan video untuk youtube: berjalan secara rawak melalui grafik paparan Pertumbuhan pesat jumlah video di YouTube memberikan potensi besar bagi pengguna untuk mencari kandungan yang menarik bagi mereka. Sayangnya, mengingat kesukaran mencari video, ukuran repositori video juga menjadikan penemuan kandungan baru menjadi tugas yang menakutkan. Dalam makalah ini, kami menyajikan kaedah baru berdasarkan analisis keseluruhan grafik pengguna-video untuk memberikan cadangan video yang diperibadikan untuk pengguna. Algoritma yang dihasilkan, yang disebut Adsorption, menyediakan kaedah mudah untuk menyebarkan maklumat pilihan dengan berkesan melalui pelbagai grafik. Kami menguji hasil cadangan secara meluas pada cuplikan data langsung tiga bulan dari YouTube. [[EENNDD]] jalan rawak; penapisan kolaboratif; penyebaran label; carian video; sistem cadangan"], [{"string": "A framework for rapid integration of presentation components The development of user interfaces (UIs) is one of the most time-consuming aspects in software development. In this context, the lack of proper reuse mechanisms for UIs is increasingly becoming manifest, especially as software development is more and more moving toward composite applications. In this paper we propose a framework for the integration of stand-alone modules or applications, where integration occurs at the presentation layer. Hence, the final goal is to reduce the effort required for UI development by maximizing reus.", "keywords": ["presentation integration", "presentation composition", "graphical user interfaces", "xpil", "component model", "interaction styles", "user interface", "presentation component"], "combined": "A framework for rapid integration of presentation components The development of user interfaces (UIs) is one of the most time-consuming aspects in software development. In this context, the lack of proper reuse mechanisms for UIs is increasingly becoming manifest, especially as software development is more and more moving toward composite applications. In this paper we propose a framework for the integration of stand-alone modules or applications, where integration occurs at the presentation layer. Hence, the final goal is to reduce the effort required for UI development by maximizing reus. [[EENNDD]] presentation integration; presentation composition; graphical user interfaces; xpil; component model; interaction styles; user interface; presentation component"}, "Kerangka kerja untuk integrasi komponen persembahan yang pesat Pengembangan antara muka pengguna (UI) adalah salah satu aspek yang paling memakan masa dalam pembangunan perisian. Dalam konteks ini, kurangnya mekanisme penggunaan semula yang tepat untuk UI semakin nyata, terutama ketika pengembangan perisian semakin banyak menuju aplikasi komposit. Dalam makalah ini kami mencadangkan kerangka kerja untuk integrasi modul atau aplikasi yang berdiri sendiri, di mana integrasi berlaku pada lapisan persembahan. Oleh itu, tujuan akhir adalah untuk mengurangkan usaha yang diperlukan untuk pembangunan UI dengan memaksimumkan penggunaan semula. [[EENNDD]] penyatuan persembahan; komposisi persembahan; antara muka pengguna grafik; xpil; model komponen; gaya interaksi; antaramuka pengguna; komponen persembahan"], [{"string": "OWL DL vs. OWL flight: conceptual modeling and reasoning for the semantic Web No contact information provided yet.", "keywords": ["systems and software", "ontologies", "description logics", "semantic web", "logic programming"], "combined": "OWL DL vs. OWL flight: conceptual modeling and reasoning for the semantic Web No contact information provided yet. [[EENNDD]] systems and software; ontologies; description logics; semantic web; logic programming"}, "Penerbangan OWL DL vs OWL: pemodelan dan penaakulan konsep untuk Web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] sistem dan perisian; ontologi; logik keterangan; web semantik; pengaturcaraan logik"], [{"string": "Efficient edge-services for colorblind users An abstract is not available.", "keywords": ["vision", "colorblindness", "computers and society", "web accessibility", "edge services"], "combined": "Efficient edge-services for colorblind users An abstract is not available. [[EENNDD]] vision; colorblindness; computers and society; web accessibility; edge services"}, "Perkhidmatan tepi yang cekap untuk pengguna buta warna Abstrak tidak tersedia. [[EENNDD]] penglihatan; buta warna; komputer dan masyarakat; kebolehcapaian laman web; perkhidmatan tepi"], [{"string": "Scalable techniques for document identifier assignment in inverted indexes Web search engines depend on the full-text inverted index data structure. Because the query processing performance is so dependent on the size of the inverted index, a plethora of research has focused on fast end effective techniques for compressing this structure. Recently, several authors have proposed techniques for improving index compression by optimizing the assignment of document identifiers to the documents in the collection, leading to significant reduction in overall index size.", "keywords": ["documentid reassignment", "index compression"], "combined": "Scalable techniques for document identifier assignment in inverted indexes Web search engines depend on the full-text inverted index data structure. Because the query processing performance is so dependent on the size of the inverted index, a plethora of research has focused on fast end effective techniques for compressing this structure. Recently, several authors have proposed techniques for improving index compression by optimizing the assignment of document identifiers to the documents in the collection, leading to significant reduction in overall index size. [[EENNDD]] documentid reassignment; index compression"}, "Teknik berskala untuk penugasan pengecam dokumen dalam indeks terbalik Mesin carian web bergantung pada struktur data indeks terbalik teks penuh. Oleh kerana prestasi pemprosesan pertanyaan sangat bergantung pada ukuran indeks terbalik, sejumlah besar penyelidikan telah menumpukan pada teknik berkesan cepat untuk memampatkan struktur ini. Baru-baru ini, beberapa pengarang telah mencadangkan teknik untuk meningkatkan pemampatan indeks dengan mengoptimumkan penugasan pengenal dokumen ke dokumen dalam koleksi, yang menyebabkan pengurangan ukuran indeks keseluruhan yang signifikan. [[EENNDD]] penugasan dokumenid; pemampatan indeks"], [{"string": "Protecting browser state from web privacy attacks No contact information provided yet.", "keywords": ["privacy", "unauthorized access", "web spoofing", "web browser design", "phishing"], "combined": "Protecting browser state from web privacy attacks No contact information provided yet. [[EENNDD]] privacy; unauthorized access; web spoofing; web browser design; phishing"}, "Melindungi keadaan penyemak imbas dari serangan privasi web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] privasi; akses tidak dibenarkan; penipuan web; reka bentuk penyemak imbas web; pancingan data"], [{"string": "Clustering user queries of a search engine An abstract is not available.", "keywords": ["query clustering", "search engine", "user log", "web data mining"], "combined": "Clustering user queries of a search engine An abstract is not available. [[EENNDD]] query clustering; search engine; user log; web data mining"}, "Menggabungkan pertanyaan pengguna enjin carian Abstrak tidak tersedia. [[EENNDD]] pengumpulan pertanyaan; enjin carian; log pengguna; perlombongan data web"], [{"string": "A classification based framework for concept summarization In this paper we propose a novel classification based framework for finding a small number of images summarizing a concept. Our method exploits metadata information available with the images to get the category information using Latent Dirichlet Allocation. We modify the import vector machine formulation based on kernel logistic regression to solve the underlying classification problem. We show that the import vectors provide a good summary satisfying important properties such as coverage, diversity and balance. Furthermore, the framework allows users to specify desired distributions over category, time etc, that a summary should satisfy. Experimental results show that the proposed method performs better than state-of-the-art summarization methods in terms of satisfying important visual and semantic properties.", "keywords": ["concept and image summarization", "miscellaneous"], "combined": "A classification based framework for concept summarization In this paper we propose a novel classification based framework for finding a small number of images summarizing a concept. Our method exploits metadata information available with the images to get the category information using Latent Dirichlet Allocation. We modify the import vector machine formulation based on kernel logistic regression to solve the underlying classification problem. We show that the import vectors provide a good summary satisfying important properties such as coverage, diversity and balance. Furthermore, the framework allows users to specify desired distributions over category, time etc, that a summary should satisfy. Experimental results show that the proposed method performs better than state-of-the-art summarization methods in terms of satisfying important visual and semantic properties. [[EENNDD]] concept and image summarization; miscellaneous"}, "Kerangka berdasarkan klasifikasi untuk ringkasan konsep Dalam makalah ini, kami mencadangkan kerangka berdasarkan klasifikasi novel untuk mencari sebilangan kecil gambar yang merangkum konsep. Kaedah kami memanfaatkan maklumat metadata yang tersedia dengan gambar untuk mendapatkan maklumat kategori menggunakan Latent Dirichlet Allocation. Kami mengubah formulasi mesin vektor import berdasarkan regresi logistik kernel untuk menyelesaikan masalah klasifikasi yang mendasari. Kami menunjukkan bahawa vektor import memberikan ringkasan yang baik yang memenuhi sifat penting seperti liputan, kepelbagaian dan keseimbangan. Selanjutnya, kerangka ini membolehkan pengguna menentukan pengedaran yang diingini berdasarkan kategori, masa dan lain-lain, yang harus dipenuhi oleh ringkasan. Hasil eksperimen menunjukkan bahawa kaedah yang dicadangkan berkinerja lebih baik daripada kaedah ringkasan canggih dari segi memuaskan sifat visual dan semantik yang penting. [[EENNDD]] konsep dan ringkasan gambar; pelbagai"], [{"string": "Helix: online enterprise data analytics The size, heterogeneity and dynamicity of data within an enterprise makes indexing, integration and analysis of the data increasingly difficult tasks. On the other hand, there has been a massive increase in the amount of high-quality open data available on the Web that could provide invaluable insights to data analysts and business intelligence specialists within the enterprise. The goal of Helix project is to provide users within the enterprise with a platform that allows them to perform online analysis of almost any type and amount of internal data using the power of external knowledge bases available on the Web. Such a platform requires a novel, data-format agnostic indexing mechanism, and light-weight data linking techniques that could link semantically related records across internal and external data sources of various characteristics. We present the initial architecture of our system and discuss several research challenges involved in building such a system.", "keywords": ["linked data", "on-line information services", "semantic link discovery", "data integration", "enterprise data management"], "combined": "Helix: online enterprise data analytics The size, heterogeneity and dynamicity of data within an enterprise makes indexing, integration and analysis of the data increasingly difficult tasks. On the other hand, there has been a massive increase in the amount of high-quality open data available on the Web that could provide invaluable insights to data analysts and business intelligence specialists within the enterprise. The goal of Helix project is to provide users within the enterprise with a platform that allows them to perform online analysis of almost any type and amount of internal data using the power of external knowledge bases available on the Web. Such a platform requires a novel, data-format agnostic indexing mechanism, and light-weight data linking techniques that could link semantically related records across internal and external data sources of various characteristics. We present the initial architecture of our system and discuss several research challenges involved in building such a system. [[EENNDD]] linked data; on-line information services; semantic link discovery; data integration; enterprise data management"}, "Helix: analisis data perusahaan dalam talian Saiz, heterogenitas dan dinamik data dalam syarikat menjadikan pengindeksan, integrasi dan analisis data menjadi tugas yang sukar. Di sisi lain, telah terjadi peningkatan besar-besaran dalam jumlah data terbuka berkualiti tinggi yang tersedia di Web yang dapat memberikan wawasan yang tidak ternilai bagi penganalisis data dan pakar kecerdasan perniagaan dalam perusahaan. Matlamat projek Helix adalah untuk menyediakan pengguna dalam perusahaan dengan platform yang membolehkan mereka melakukan analisis dalam talian mengenai hampir semua jenis dan jumlah data dalaman menggunakan kekuatan pangkalan pengetahuan luaran yang terdapat di Web. Platform sedemikian memerlukan novel, mekanisme pengindeksan agnostik format data, dan teknik penghubung data ringan yang dapat menghubungkan catatan berkaitan semantik di seluruh sumber data dalaman dan luaran dengan pelbagai ciri. Kami membentangkan seni bina awal sistem kami dan membincangkan beberapa cabaran penyelidikan yang terlibat dalam pembinaan sistem tersebut. [[EENNDD]] data yang dipautkan; perkhidmatan maklumat dalam talian; penemuan pautan semantik; penyatuan data; pengurusan data perusahaan"], [{"string": "Two birds with one stone: a graph-based framework for disambiguating and tagging people names in web search The ever growing volume of Web data makes it increasingly challenging to accurately find relevant information about a specific person on the Web. To address the challenge caused by name ambiguity in Web people search, this paper explores a novel graph-based framework to both disambiguate and tag people entities in Web search results. Experimental results demonstrate the effectiveness of the proposed framework in tag discovery and name disambiguation.", "keywords": ["tagging", "general", "clustering", "name disambiguation"], "combined": "Two birds with one stone: a graph-based framework for disambiguating and tagging people names in web search The ever growing volume of Web data makes it increasingly challenging to accurately find relevant information about a specific person on the Web. To address the challenge caused by name ambiguity in Web people search, this paper explores a novel graph-based framework to both disambiguate and tag people entities in Web search results. Experimental results demonstrate the effectiveness of the proposed framework in tag discovery and name disambiguation. [[EENNDD]] tagging; general; clustering; name disambiguation"}, "Dua burung dengan satu batu: kerangka berdasarkan grafik untuk menghilangkan dan menandai nama orang dalam carian web Jumlah data Web yang semakin meningkat menjadikannya semakin sukar untuk mencari maklumat yang relevan mengenai orang tertentu di Web dengan tepat. Untuk mengatasi cabaran yang disebabkan oleh kekaburan nama dalam pencarian orang Web, makalah ini meneroka kerangka kerja berdasarkan grafik baru untuk menghilangkan dan menandai entiti orang dalam hasil carian Web. Hasil eksperimen menunjukkan keberkesanan kerangka kerja yang dicadangkan dalam penemuan tag dan disambiguasi nama. [[EENNDD]] penandaan; umum; pengelompokan; disambiguasi nama"], [{"string": "Piazza: data management infrastructure for semantic web applications No contact information provided yet.", "keywords": ["peer data management systems", "data description languages", "xml", "semantic web"], "combined": "Piazza: data management infrastructure for semantic web applications No contact information provided yet. [[EENNDD]] peer data management systems; data description languages; xml; semantic web"}, "Piazza: infrastruktur pengurusan data untuk aplikasi web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] sistem pengurusan data rakan sebaya; bahasa penerangan data; xml; web semantik"], [{"string": "Towards second and third generation web-based multimedia An abstract is not available.", "keywords": ["transformations", "xslt", "html", "smil", "xml", "multimedia"], "combined": "Towards second and third generation web-based multimedia An abstract is not available. [[EENNDD]] transformations; xslt; html; smil; xml; multimedia"}, "Ke arah multimedia berasaskan web generasi kedua dan ketiga Abstrak tidak tersedia. [[EENNDD]] transformasi; xslt; html; senyum; xml; multimedia"], [{"string": "Estimating sizes of social networks via biased sampling Online social networks have become very popular in recent years and their number of users is already measured in many hundreds of millions. For various commercial and sociological purposes, an independent estimate of their sizes is important. In this work, algorithms for estimating the number of users in such networks are considered. The proposed schemes are also applicable for estimating the sizes of networks' sub-populations. The suggested algorithms interact with the social networks via their public APIs only, and rely on no other external information. Due to obvious traffic and privacy concerns, the number of such interactions is severely limited. We therefore focus on minimizing the number of API interactions needed for producing good size estimates. We adopt the abstraction of social networks as undirected graphs and use random node sampling. By counting the number of collisions or non-unique nodes in the sample, we produce a size estimate. Then, we show analytically that the estimate error vanishes with high probability for smaller number of samples than those required by prior-art algorithms. Moreover, although our algorithms are provably correct for any graph, they excel when applied to social network-like graphs. The proposed algorithms were evaluated on synthetic as well real social networks such as Facebook, IMDB, and DBLP. Our experiments corroborated the theoretical results, and demonstrated the effectiveness of the algorithms.", "keywords": ["undirected graph", "sampling", "population estimator", "nonnumerical algorithms and problems", "social network"], "combined": "Estimating sizes of social networks via biased sampling Online social networks have become very popular in recent years and their number of users is already measured in many hundreds of millions. For various commercial and sociological purposes, an independent estimate of their sizes is important. In this work, algorithms for estimating the number of users in such networks are considered. The proposed schemes are also applicable for estimating the sizes of networks' sub-populations. The suggested algorithms interact with the social networks via their public APIs only, and rely on no other external information. Due to obvious traffic and privacy concerns, the number of such interactions is severely limited. We therefore focus on minimizing the number of API interactions needed for producing good size estimates. We adopt the abstraction of social networks as undirected graphs and use random node sampling. By counting the number of collisions or non-unique nodes in the sample, we produce a size estimate. Then, we show analytically that the estimate error vanishes with high probability for smaller number of samples than those required by prior-art algorithms. Moreover, although our algorithms are provably correct for any graph, they excel when applied to social network-like graphs. The proposed algorithms were evaluated on synthetic as well real social networks such as Facebook, IMDB, and DBLP. Our experiments corroborated the theoretical results, and demonstrated the effectiveness of the algorithms. [[EENNDD]] undirected graph; sampling; population estimator; nonnumerical algorithms and problems; social network"}, "Menganggarkan ukuran rangkaian sosial melalui pensampelan berat sebelah Rangkaian sosial dalam talian menjadi sangat popular dalam beberapa tahun kebelakangan ini dan jumlah pengguna mereka sudah diukur dalam ratusan juta. Untuk pelbagai tujuan komersial dan sosiologi, anggaran bebas bagi ukurannya adalah penting. Dalam karya ini, algoritma untuk menganggarkan jumlah pengguna dalam rangkaian tersebut dipertimbangkan. Skema yang dicadangkan juga berlaku untuk menganggarkan ukuran sub-populasi rangkaian. Algoritma yang dicadangkan berinteraksi dengan rangkaian sosial melalui API awam mereka sahaja, dan tidak bergantung pada maklumat luaran yang lain. Oleh kerana masalah lalu lintas dan privasi yang jelas, jumlah interaksi tersebut sangat terhad. Oleh itu, kami memberi tumpuan untuk meminimumkan bilangan interaksi API yang diperlukan untuk menghasilkan anggaran ukuran yang baik. Kami menggunakan abstraksi rangkaian sosial sebagai grafik tidak diarahkan dan menggunakan pensampelan simpul rawak. Dengan menghitung jumlah perlanggaran atau nod yang tidak unik dalam sampel, kami menghasilkan anggaran ukuran. Kemudian, kami menunjukkan secara analitikal bahawa ralat anggaran hilang dengan kebarangkalian tinggi untuk bilangan sampel yang lebih kecil daripada yang diperlukan oleh algoritma seni sebelumnya. Lebih-lebih lagi, walaupun algoritma kami terbukti betul untuk grafik apa pun, mereka cemerlang apabila diterapkan pada grafik seperti rangkaian sosial. Algoritma yang dicadangkan dinilai pada rangkaian sosial sintetik dan nyata seperti Facebook, IMDB, dan DBLP. Eksperimen kami mengesahkan hasil teori, dan menunjukkan keberkesanan algoritma. [[EENNDD]] graf tidak diarahkan; persampelan; penganggar populasi; algoritma dan masalah bukan berangka; rangkaian sosial"], [{"string": "Growing parallel paths for entity-page discovery In this paper, we use the structural and relational information on the Web to find entity-pages. Specifically, given a Web site and an entity-page (e.g., department and faculty member homepage) we seek to find all of the entity-pages of the same type (e.g., all faculty members in the department). To do this, we propose a web structure mining method which grows parallel paths through the web graph and DOM trees. We show that by utilizing these parallel paths we can efficiently discover all entity-pages of the same type. Finally, we demonstrate the accuracy of our method with a case study on various domains.", "keywords": ["entity pages", "semi-structured data", "web structure mining", "parallel paths"], "combined": "Growing parallel paths for entity-page discovery In this paper, we use the structural and relational information on the Web to find entity-pages. Specifically, given a Web site and an entity-page (e.g., department and faculty member homepage) we seek to find all of the entity-pages of the same type (e.g., all faculty members in the department). To do this, we propose a web structure mining method which grows parallel paths through the web graph and DOM trees. We show that by utilizing these parallel paths we can efficiently discover all entity-pages of the same type. Finally, we demonstrate the accuracy of our method with a case study on various domains. [[EENNDD]] entity pages; semi-structured data; web structure mining; parallel paths"}, "Menambah jalan selari untuk penemuan halaman entiti Dalam makalah ini, kami menggunakan maklumat struktur dan hubungan di Web untuk mencari halaman entiti. Secara khusus, memandangkan laman web dan halaman entiti (mis. Halaman utama dan anggota fakulti) kami berusaha mencari semua halaman entiti dengan jenis yang sama (mis., Semua anggota fakulti di jabatan). Untuk melakukan ini, kami mencadangkan kaedah perlombongan struktur web yang mengembangkan jalan selari melalui grafik web dan pokok DOM. Kami menunjukkan bahawa dengan menggunakan jalan selari ini kita dapat menemui semua halaman entiti dengan jenis yang cekap. Akhirnya, kami menunjukkan ketepatan kaedah kami dengan kajian kes di pelbagai domain. [[EENNDD]] halaman entiti; data separa berstruktur; perlombongan struktur web; jalan selari"], [{"string": "The classroom sentinel: supporting data-driven decision-making in the classroom No contact information provided yet.", "keywords": ["data integration", "teacher cognition", "classroom pattern detection", "computer-managed instruction", "data-driven decision making", "alert generation"], "combined": "The classroom sentinel: supporting data-driven decision-making in the classroom No contact information provided yet. [[EENNDD]] data integration; teacher cognition; classroom pattern detection; computer-managed instruction; data-driven decision making; alert generation"}, "Sentinel bilik darjah: menyokong pembuatan keputusan berdasarkan data di dalam kelas Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penyatuan data; kognisi guru; pengesanan corak bilik darjah; arahan yang dikendalikan oleh komputer; membuat keputusan berdasarkan data; generasi berjaga-jaga"], [{"string": "Generation of multimedia TV news contents for WWW No contact information provided yet.", "keywords": ["tv news", "video ocr", "information retrieval", "content analysis"], "combined": "Generation of multimedia TV news contents for WWW No contact information provided yet. [[EENNDD]] tv news; video ocr; information retrieval; content analysis"}, "Penjanaan kandungan berita TV multimedia untuk WWW Belum ada maklumat hubungan yang diberikan. [[EENNDD]] berita tv; video ocr; pengambilan maklumat; analisis kandungan"], [{"string": "Efficiently querying rdf data in triple stores Efficiently querying RDF data is being an important factor in applying Semantic Web technologies to real-world applications. In this context, many efforts have been made to store and query RDF data in relational database using particular schemas. In this paper, we propose a new scheme to store, index, and query RDF data in triple stores. Graph feature of RDF data is taken into considerations which might help reduce the join costs on the vertical database structure. We would partition RDF triples into overlapped groups, store them in a triple table with one more column of group identity, and build up a signature tree to index them. Based on this infrastructure, a complex RDF query is decomposed into multiple pieces of sub-queries which could be easily filtered into some RDF groups using signature tree index, and finally is evaluated with a composed and optimized SQL with specific constraints. We compare the performance of our method with prior art on typical queries over a large scaled LUBM and UOBM benchmark data (more than 10 million triples). For some extreme cases, they can promote 3 to 4 orders of magnitude.", "keywords": ["rdf", "graph partitioning", "indexing", "signature"], "combined": "Efficiently querying rdf data in triple stores Efficiently querying RDF data is being an important factor in applying Semantic Web technologies to real-world applications. In this context, many efforts have been made to store and query RDF data in relational database using particular schemas. In this paper, we propose a new scheme to store, index, and query RDF data in triple stores. Graph feature of RDF data is taken into considerations which might help reduce the join costs on the vertical database structure. We would partition RDF triples into overlapped groups, store them in a triple table with one more column of group identity, and build up a signature tree to index them. Based on this infrastructure, a complex RDF query is decomposed into multiple pieces of sub-queries which could be easily filtered into some RDF groups using signature tree index, and finally is evaluated with a composed and optimized SQL with specific constraints. We compare the performance of our method with prior art on typical queries over a large scaled LUBM and UOBM benchmark data (more than 10 million triples). For some extreme cases, they can promote 3 to 4 orders of magnitude. [[EENNDD]] rdf; graph partitioning; indexing; signature"}, "Menanyakan data rdf dengan cekap di tiga kedai dengan cekap Menanya data RDF dengan cekap menjadi faktor penting dalam menerapkan teknologi Web Semantik ke aplikasi dunia nyata. Dalam konteks ini, banyak usaha telah dilakukan untuk menyimpan dan meminta data RDF dalam pangkalan data relasional menggunakan skema tertentu. Dalam makalah ini, kami mencadangkan skema baru untuk menyimpan, mengindeks, dan meminta data RDF di tiga kedai. Ciri grafik data RDF dipertimbangkan yang dapat membantu mengurangkan kos bergabung pada struktur pangkalan data menegak. Kami akan membahagi tiga kali ganda RDF ke dalam kumpulan bertindih, menyimpannya dalam jadual tiga dengan satu lajur identiti kumpulan lagi, dan membina pokok tanda tangan untuk mengindeksnya. Berdasarkan infrastruktur ini, pertanyaan RDF yang kompleks diuraikan menjadi beberapa sub-pertanyaan yang dapat disaring dengan mudah ke dalam beberapa kelompok RDF menggunakan indeks pohon tanda tangan, dan akhirnya dievaluasi dengan SQL yang disusun dan dioptimumkan dengan batasan tertentu. Kami membandingkan prestasi kaedah kami dengan seni sebelumnya pada pertanyaan biasa mengenai data penanda aras LUBM dan UOBM berskala besar (lebih daripada 10 juta tiga kali ganda). Untuk beberapa kes yang melampau, mereka dapat mempromosikan 3 hingga 4 pesanan besar. [[EENNDD]] rdf; pembahagian graf; pengindeksan; tandatangan"], [{"string": "Flexible on-device service object replication with replets An increasingly large amount of Web applications employ service objects such as Servlets to generate dynamic and personalized content. Existing caching infrastructures are not well suited for caching such content in mobile environments because of disconnection and weak connection. One possible approach to this problem is to replicate Web-related application logic to client devices. The challenges to this approach are to deal with client devices that exhibit huge divergence in resource availabilities, to support applications that have different data sharing and coherency requirements, and to accommodate the same application under different deployment environments.", "keywords": ["capability", "reconfiguration", "preference", "replication", "synchronization", "service"], "combined": "Flexible on-device service object replication with replets An increasingly large amount of Web applications employ service objects such as Servlets to generate dynamic and personalized content. Existing caching infrastructures are not well suited for caching such content in mobile environments because of disconnection and weak connection. One possible approach to this problem is to replicate Web-related application logic to client devices. The challenges to this approach are to deal with client devices that exhibit huge divergence in resource availabilities, to support applications that have different data sharing and coherency requirements, and to accommodate the same application under different deployment environments. [[EENNDD]] capability; reconfiguration; preference; replication; synchronization; service"}, "Replikasi objek perkhidmatan pada peranti yang fleksibel dengan ulangan Semakin banyak aplikasi Web yang menggunakan objek perkhidmatan seperti Servlet untuk menghasilkan kandungan yang dinamik dan diperibadikan. Prasarana cache yang ada tidak begitu sesuai untuk menyimpan cache kandungan seperti itu di persekitaran mudah alih kerana terputus dan sambungan yang lemah. Salah satu pendekatan yang mungkin untuk masalah ini adalah mereplikasi logik aplikasi berkaitan Web ke peranti pelanggan. Tantangan untuk pendekatan ini adalah untuk menangani perangkat klien yang menunjukkan perbedaan besar dalam ketersediaan sumber daya, untuk mendukung aplikasi yang memiliki keperluan pembagian data dan koherensi yang berbeda, dan untuk mengakomodasi aplikasi yang sama di lingkungan penyebaran yang berbeza. [[EENNDD]] keupayaan; penyusunan semula; pilihan; replikasi; penyegerakan; perkhidmatan"], [{"string": "Exploiting web search to generate synonyms for entities Tasks recognizing named entities such as products, people names, or locations from documents have recently received significant attention in the literature. Many solutions to these tasks assume the existence of reference entity tables. An important challenge that needs to be addressed in the entity extraction task is that of ascertaining whether or not a candidate string approximately matches with a named entity in a given reference table.", "keywords": ["web search", "synonym generation", "entity extraction", "similarity measure"], "combined": "Exploiting web search to generate synonyms for entities Tasks recognizing named entities such as products, people names, or locations from documents have recently received significant attention in the literature. Many solutions to these tasks assume the existence of reference entity tables. An important challenge that needs to be addressed in the entity extraction task is that of ascertaining whether or not a candidate string approximately matches with a named entity in a given reference table. [[EENNDD]] web search; synonym generation; entity extraction; similarity measure"}, "Mengeksploitasi carian web untuk menghasilkan sinonim untuk entiti Tugas mengenali entiti bernama seperti produk, nama orang, atau lokasi dari dokumen baru-baru ini mendapat perhatian yang signifikan dalam literatur. Banyak penyelesaian untuk tugas-tugas ini mengandaikan adanya jadual entiti rujukan. Cabaran penting yang perlu ditangani dalam tugas pengekstrakan entiti adalah memastikan sama ada rentetan calon kira-kira sesuai dengan entiti bernama dalam jadual rujukan tertentu. [[EENNDD]] carian web; penjanaan sinonim; pengekstrakan entiti; ukuran kesamaan"], [{"string": "Targeted disambiguation of ad-hoc, homogeneous sets of named entities In many entity extraction applications, the entities to be recognized are constrained to be from a list of \"target entities\". In many cases, these target entities are (i) ad-hoc, i.e., do not exist in a knowledge base and (ii) homogeneous (e.g., all the entities are IT companies). We study the following novel disambiguation problem in this unique setting: given the candidate mentions of all the target entities, determine which ones are true mentions of a target entity. Prior techniques only consider target entities present in a knowledge base and/or having a rich set of attributes. In this paper, we develop novel techniques that require no knowledge about the entities except their names. Our main insight is to leverage the homogeneity constraint and disambiguate the candidate mentions collectively across all documents. We propose a graph-based model, called MentionRank, for that purpose. Furthermore, if additional knowledge is available for some or all of the entities, our model can leverage it to further improve quality. Our experiments demonstrate the effectiveness of our model. To the best of our knowledge, this is the first work on targeted entity disambiguation for ad-hoc entities.", "keywords": ["mentionrank", "information search and retrieval", "entity extraction", "targeted disambiguation", "named entity disambiguation", "ad-hoc entity identification"], "combined": "Targeted disambiguation of ad-hoc, homogeneous sets of named entities In many entity extraction applications, the entities to be recognized are constrained to be from a list of \"target entities\". In many cases, these target entities are (i) ad-hoc, i.e., do not exist in a knowledge base and (ii) homogeneous (e.g., all the entities are IT companies). We study the following novel disambiguation problem in this unique setting: given the candidate mentions of all the target entities, determine which ones are true mentions of a target entity. Prior techniques only consider target entities present in a knowledge base and/or having a rich set of attributes. In this paper, we develop novel techniques that require no knowledge about the entities except their names. Our main insight is to leverage the homogeneity constraint and disambiguate the candidate mentions collectively across all documents. We propose a graph-based model, called MentionRank, for that purpose. Furthermore, if additional knowledge is available for some or all of the entities, our model can leverage it to further improve quality. Our experiments demonstrate the effectiveness of our model. To the best of our knowledge, this is the first work on targeted entity disambiguation for ad-hoc entities. [[EENNDD]] mentionrank; information search and retrieval; entity extraction; targeted disambiguation; named entity disambiguation; ad-hoc entity identification"}, "Disambiguasi yang disasarkan bagi kumpulan entiti bernama ad-hoc, homogen Dalam banyak aplikasi pengekstrakan entiti, entiti yang akan diakui dibatasi dari senarai \"entiti sasaran\". Dalam banyak kes, entiti sasaran ini (i) ad-hoc, iaitu, tidak wujud dalam pangkalan pengetahuan dan (ii) homogen (mis., Semua entiti tersebut adalah syarikat IT). Kami mengkaji masalah disambiguasi novel berikut dalam suasana unik ini: mengingat calon menyebut semua entiti sasaran, tentukan mana yang benar-benar disebut entiti sasaran. Teknik sebelumnya hanya mempertimbangkan entiti sasaran yang ada di pangkalan pengetahuan dan / atau mempunyai banyak atribut. Dalam makalah ini, kami mengembangkan teknik novel yang tidak memerlukan pengetahuan mengenai entiti kecuali namanya. Wawasan utama kami adalah memanfaatkan kekangan homogenitas dan membongkar sebutan calon secara kolektif di semua dokumen. Kami mencadangkan model berasaskan grafik, yang disebut MentionRank, untuk tujuan tersebut. Tambahan pula, jika pengetahuan tambahan tersedia untuk beberapa atau semua entiti, model kami dapat memanfaatkannya untuk meningkatkan kualiti. Eksperimen kami menunjukkan keberkesanan model kami. Sepengetahuan kami, ini adalah karya pertama mengenai disambiguasi entiti yang disasarkan untuk entiti ad-hoc. [[EENNDD]] peringkat penyebutan; carian dan pengambilan maklumat; pengekstrakan entiti; disambiguasi yang disasarkan; disambiguasi entiti bernama; pengenalan entiti ad-hoc"], [{"string": "Geographical topic discovery and comparison This paper studies the problem of discovering and comparing geographical topics from GPS-associated documents. GPS-associated documents become popular with the pervasiveness of location-acquisition technologies. For example, in Flickr, the geo-tagged photos are associated with tags and GPS locations. In Twitter, the locations of the tweets can be identified by the GPS locations from smart phones. Many interesting concepts, including cultures, scenes, and product sales, correspond to specialized geographical distributions. In this paper, we are interested in two questions: (1) how to discover different topics of interests that are coherent in geographical regions? (2) how to compare several topics across different geographical locations? To answer these questions, this paper proposes and compares three ways of modeling geographical topics: location-driven model, text-driven model, and a novel joint model called LGTA (Latent Geographical Topic Analysis) that combines location and text. To make a fair comparison, we collect several representative datasets from Flickr website including Landscape, Activity, Manhattan, National park, Festival, Car, and Food. The results show that the first two methods work in some datasets but fail in others. LGTA works well in all these datasets at not only finding regions of interests but also providing effective comparisons of the topics across different locations. The results confirm our hypothesis that the geographical distributions can help modeling topics, while topics provide important cues to group different geographical regions.", "keywords": ["geographical topics", "topic comparison", "topic modeling"], "combined": "Geographical topic discovery and comparison This paper studies the problem of discovering and comparing geographical topics from GPS-associated documents. GPS-associated documents become popular with the pervasiveness of location-acquisition technologies. For example, in Flickr, the geo-tagged photos are associated with tags and GPS locations. In Twitter, the locations of the tweets can be identified by the GPS locations from smart phones. Many interesting concepts, including cultures, scenes, and product sales, correspond to specialized geographical distributions. In this paper, we are interested in two questions: (1) how to discover different topics of interests that are coherent in geographical regions? (2) how to compare several topics across different geographical locations? To answer these questions, this paper proposes and compares three ways of modeling geographical topics: location-driven model, text-driven model, and a novel joint model called LGTA (Latent Geographical Topic Analysis) that combines location and text. To make a fair comparison, we collect several representative datasets from Flickr website including Landscape, Activity, Manhattan, National park, Festival, Car, and Food. The results show that the first two methods work in some datasets but fail in others. LGTA works well in all these datasets at not only finding regions of interests but also providing effective comparisons of the topics across different locations. The results confirm our hypothesis that the geographical distributions can help modeling topics, while topics provide important cues to group different geographical regions. [[EENNDD]] geographical topics; topic comparison; topic modeling"}, "Penemuan dan perbandingan topik geografi Makalah ini mengkaji masalah mencari dan membandingkan topik geografi dari dokumen yang berkaitan dengan GPS. Dokumen yang berkaitan dengan GPS menjadi popular dengan penyebaran teknologi pemerolehan lokasi. Sebagai contoh, di Flickr, foto yang diberi tag geo dikaitkan dengan tag dan lokasi GPS. Di Twitter, lokasi tweet dapat dikenal pasti oleh lokasi GPS dari telefon pintar. Banyak konsep menarik, termasuk budaya, pemandangan, dan penjualan produk, sesuai dengan pengedaran geografi khusus. Dalam makalah ini, kita tertarik dengan dua pertanyaan: (1) bagaimana mencari topik minat yang berbeza yang koheren di wilayah geografi? (2) bagaimana membandingkan beberapa topik di pelbagai lokasi geografi? Untuk menjawab soalan-soalan ini, makalah ini mencadangkan dan membandingkan tiga cara pemodelan topik geografi: model berdasarkan lokasi, model berdasarkan teks, dan model gabungan novel yang disebut LGTA (Latent Geographicical Topic Analysis) yang menggabungkan lokasi dan teks. Untuk membuat perbandingan yang adil, kami mengumpulkan beberapa set data perwakilan dari laman web Flickr termasuk Landscape, Activity, Manhattan, National park, Festival, Car, dan Food. Hasilnya menunjukkan bahawa dua kaedah pertama berfungsi dalam beberapa set data tetapi gagal dalam yang lain. LGTA berfungsi dengan baik dalam semua set data ini bukan hanya mencari kawasan minat tetapi juga memberikan perbandingan topik yang berkesan di pelbagai lokasi. Hasilnya mengesahkan hipotesis kami bahawa taburan geografi dapat membantu pemodelan topik, sementara topik memberikan petunjuk penting untuk mengelompokkan kawasan geografi yang berbeza. [[EENNDD]] topik geografi; perbandingan topik; pemodelan topik"], [{"string": "Here, there, and everywhere: correlated online behaviors can lead to overestimates of the effects of advertising Measuring the causal effects of online advertising (adfx) on user behavior is important to the health of the WWW publishing industry. In this paper, using three controlled experiments, we show that observational data frequently lead to incorrect estimates of adfx. The reason, which we label \"activity bias,\" comes from the surprising amount of time-based correlation between the myriad activities that users undertake online. In Experiment 1, users who are exposed to an ad on a given day are much more likely to engage in brand-relevant search queries as compared to their recent history for reasons that had nothing do with the advertisement. In Experiment 2, we show that activity bias occurs for page views across diverse websites. In Experiment 3, we track account sign-ups at a competitor's (of the advertiser) website and find that many more people sign-up on the day they saw an advertisement than on other days, but that the true \"competitive effect\" was minimal. In all three experiments, exposure to a campaign signals doing \"more of everything\" in given period of time, making it difficult to find a suitable \"matched control\" using prior behavior. In such cases, the \"match\" is fundamentally different from the exposed group, and we show how and why observational methods lead to a massive overestimate of adfx in such circumstances.", "keywords": ["advertising effectiveness", "field experiments", "selection bias", "causal inference", "browsing behavior"], "combined": "Here, there, and everywhere: correlated online behaviors can lead to overestimates of the effects of advertising Measuring the causal effects of online advertising (adfx) on user behavior is important to the health of the WWW publishing industry. In this paper, using three controlled experiments, we show that observational data frequently lead to incorrect estimates of adfx. The reason, which we label \"activity bias,\" comes from the surprising amount of time-based correlation between the myriad activities that users undertake online. In Experiment 1, users who are exposed to an ad on a given day are much more likely to engage in brand-relevant search queries as compared to their recent history for reasons that had nothing do with the advertisement. In Experiment 2, we show that activity bias occurs for page views across diverse websites. In Experiment 3, we track account sign-ups at a competitor's (of the advertiser) website and find that many more people sign-up on the day they saw an advertisement than on other days, but that the true \"competitive effect\" was minimal. In all three experiments, exposure to a campaign signals doing \"more of everything\" in given period of time, making it difficult to find a suitable \"matched control\" using prior behavior. In such cases, the \"match\" is fundamentally different from the exposed group, and we show how and why observational methods lead to a massive overestimate of adfx in such circumstances. [[EENNDD]] advertising effectiveness; field experiments; selection bias; causal inference; browsing behavior"}, "Di sini, di mana sahaja dan di mana-mana sahaja: tingkah laku dalam talian yang berkorelasi dapat menyebabkan penilaian berlebihan terhadap kesan iklan Mengukur kesan penyebab iklan dalam talian (adfx) terhadap tingkah laku pengguna adalah penting bagi kesihatan industri penerbitan WWW. Dalam makalah ini, menggunakan tiga eksperimen terkawal, kami menunjukkan bahawa data pemerhatian sering menyebabkan anggaran adfx tidak tepat. Sebabnya, yang kami beri label \"bias aktiviti\", berasal dari banyaknya korelasi berdasarkan masa antara pelbagai aktiviti yang dilakukan pengguna dalam talian. Dalam Eksperimen 1, pengguna yang terdedah kepada iklan pada hari tertentu lebih cenderung untuk terlibat dalam pertanyaan carian yang berkaitan dengan jenama berbanding dengan sejarah terkini mereka untuk alasan yang tidak ada kaitan dengan iklan tersebut. Dalam Eksperimen 2, kami menunjukkan bahawa bias aktiviti berlaku untuk paparan halaman di pelbagai laman web. Dalam Eksperimen 3, kami mengesan pendaftaran akaun di laman web pesaing (pengiklan) dan mendapati bahawa lebih banyak orang mendaftar pada hari mereka melihat iklan daripada pada hari-hari lain, tetapi \"kesan persaingan\" yang sebenarnya adalah minimum . Dalam ketiga eksperimen tersebut, pendedahan kepada kempen memberi isyarat untuk melakukan \"lebih dari segalanya\" dalam jangka masa tertentu, sehingga sukar untuk mencari \"kawalan yang sesuai\" yang sesuai dengan tingkah laku sebelumnya. Dalam kes seperti itu, \"pertandingan\" pada asasnya berbeza dari kumpulan yang terdedah, dan kami menunjukkan bagaimana dan mengapa kaedah pemerhatian membawa kepada penilaian berlebihan adfx dalam keadaan seperti itu. [[EENNDD]] keberkesanan pengiklanan; eksperimen lapangan; bias pemilihan; inferens sebab; tingkah laku melayari"], [{"string": "Visitor awareness in the web An abstract is not available.", "keywords": ["web activity", "augmented reality", "visitor awareness", "web", "web-based interaction", "ambient media", "web awareness"], "combined": "Visitor awareness in the web An abstract is not available. [[EENNDD]] web activity; augmented reality; visitor awareness; web; web-based interaction; ambient media; web awareness"}, "Kesedaran pelawat di web Abstrak tidak tersedia. [[EENNDD]] aktiviti web; augmented reality; kesedaran pelawat; laman web; interaksi berasaskan web; media ambien; kesedaran web"], [{"string": "The language observatory project (LOP) No contact information provided yet.", "keywords": ["web crawler", "language", "language identification", "content analysis and indexing", "character sets", "language digital divide", "scripts"], "combined": "The language observatory project (LOP) No contact information provided yet. [[EENNDD]] web crawler; language; language identification; content analysis and indexing; character sets; language digital divide; scripts"}, "Projek pemerhatian bahasa (LOP) Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perayap web; bahasa; pengenalan bahasa; analisis kandungan dan pengindeksan; set watak; jurang digital bahasa; skrip"], [{"string": "Expressiveness of XSDs: from practice to theory, there and back again No contact information provided yet.", "keywords": ["formal model", "xml schema", "expressiveness"], "combined": "Expressiveness of XSDs: from practice to theory, there and back again No contact information provided yet. [[EENNDD]] formal model; xml schema; expressiveness"}, "Ekspresif XSD: dari praktik ke teori, ada dan kembali lagi Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] model rasmi; skema xml; ekspresi"], [{"string": "Filtering spam e-mail on a global scale No contact information provided yet.", "keywords": ["spam", "junk e-mail", "international e-mail"], "combined": "Filtering spam e-mail on a global scale No contact information provided yet. [[EENNDD]] spam; junk e-mail; international e-mail"}, "Menyaring e-mel spam dalam skala global Belum ada maklumat hubungan yang diberikan. [[EENNDD]] spam; e-mel sampah; e-mel antarabangsa"], [{"string": "BackRank: an alternative for PageRank? No contact information provided yet.", "keywords": ["pagerank", "web analysis", "random walk", "flow", "back button"], "combined": "BackRank: an alternative for PageRank? No contact information provided yet. [[EENNDD]] pagerank; web analysis; random walk; flow; back button"}, "BackRank: alternatif untuk PageRank? Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pagerank; analisis web; jalan rawak; aliran; butang kembali"], [{"string": "Ad-hoc object retrieval in the web of data Semantic Search refers to a loose set of concepts, challenges and techniques having to do with harnessing the information of the growing Web of Data (WoD) for Web search. Here we propose a formal model of one specific semantic search task: ad-hoc object retrieval. We show that this task provides a solid framework to study some of the semantic search problems currently tackled by commercial Web search engines. We connect this task to the traditional ad-hoc document retrieval and discuss appropriate evaluation metrics. Finally, we carry out a realistic evaluation of this task in the context of a Web search application.", "keywords": ["semantic search", "object retrieval", "evaluation"], "combined": "Ad-hoc object retrieval in the web of data Semantic Search refers to a loose set of concepts, challenges and techniques having to do with harnessing the information of the growing Web of Data (WoD) for Web search. Here we propose a formal model of one specific semantic search task: ad-hoc object retrieval. We show that this task provides a solid framework to study some of the semantic search problems currently tackled by commercial Web search engines. We connect this task to the traditional ad-hoc document retrieval and discuss appropriate evaluation metrics. Finally, we carry out a realistic evaluation of this task in the context of a Web search application. [[EENNDD]] semantic search; object retrieval; evaluation"}, "Pengambilan objek ad-hoc di web data Semantic Search merujuk kepada sekumpulan konsep, cabaran dan teknik yang longgar yang berkaitan dengan memanfaatkan maklumat Web of Data (WoD) yang semakin meningkat untuk carian Web. Di sini kami mencadangkan model formal satu tugas carian semantik tertentu: pengambilan objek ad-hoc. Kami menunjukkan bahawa tugas ini menyediakan kerangka kerja yang kukuh untuk mengkaji beberapa masalah pencarian semantik yang kini ditangani oleh mesin carian Web komersial. Kami menghubungkan tugas ini dengan pengambilan dokumen ad-hoc tradisional dan membincangkan metrik penilaian yang sesuai. Akhirnya, kami melakukan penilaian yang realistik terhadap tugas ini dalam konteks aplikasi carian Web. [[EENNDD]] carian semantik; pengambilan objek; penilaian"], [{"string": "Sailer: an effective search engine for unified retrieval of heterogeneous xml and web documents This paper studies the problem of unified ranked retrieval of heterogeneous XML documents and Web data. We propose an effective search engine called Sailer to adaptively and versatilely answer keyword queries over the heterogenous data. We model the Web pages and XML documents as graphs. We propose the concept of pivotal trees to effectively answer keyword queries and present an effective method to identify the top-k pivotal trees with the highest ranks from the graphs. Moreover, we propose effective indexes to facilitate the effective unified ranked retrieval. We have conducted an extensive experimental study using real datasets, and the experimental results show that Sailer achieves both high search efficiency and accuracy, and outperforms the existing approaches significantly.", "keywords": ["unified keyword search", "keyword search", "database applications", "xml", "web pages"], "combined": "Sailer: an effective search engine for unified retrieval of heterogeneous xml and web documents This paper studies the problem of unified ranked retrieval of heterogeneous XML documents and Web data. We propose an effective search engine called Sailer to adaptively and versatilely answer keyword queries over the heterogenous data. We model the Web pages and XML documents as graphs. We propose the concept of pivotal trees to effectively answer keyword queries and present an effective method to identify the top-k pivotal trees with the highest ranks from the graphs. Moreover, we propose effective indexes to facilitate the effective unified ranked retrieval. We have conducted an extensive experimental study using real datasets, and the experimental results show that Sailer achieves both high search efficiency and accuracy, and outperforms the existing approaches significantly. [[EENNDD]] unified keyword search; keyword search; database applications; xml; web pages"}, "Sailer: enjin carian yang berkesan untuk pengambilan bersatu dokumen xml dan web heterogen Makalah ini mengkaji masalah pengambilan peringkat bersatu bagi dokumen XML dan data Web heterogen. Kami mencadangkan mesin carian yang berkesan yang dipanggil Sailer untuk menjawab pertanyaan kata kunci secara adaptif dan serba boleh terhadap data yang heterogen. Kami memodelkan halaman Web dan dokumen XML sebagai grafik. Kami mencadangkan konsep pokok penting untuk menjawab pertanyaan kata kunci dengan berkesan dan memberikan kaedah yang berkesan untuk mengenal pasti pokok penting k-teratas dengan kedudukan tertinggi dari grafik. Lebih-lebih lagi, kami mencadangkan indeks berkesan untuk memudahkan pengambilan peringkat bersatu yang berkesan. Kami telah melakukan kajian eksperimen yang luas menggunakan set data sebenar, dan hasil eksperimen menunjukkan bahawa Sailer mencapai kecekapan dan ketepatan carian yang tinggi, dan mengatasi pendekatan yang ada dengan ketara. [[EENNDD]] carian kata kunci bersatu; carian kata kunci; aplikasi pangkalan data; xml; laman sesawang"], [{"string": "Deriving music theme annotations from user tags Music theme annotations would be really beneficial for supporting retrieval, but are often neglected by users while annotating. Thus, in order to support users in tagging and to fill the gaps in the tag space, in this paper we develop algorithms for recommending theme annotations. Our methods exploit already existing user tags, the lyrics of music tracks, as well as combinations of both. We compare the results for our recommended theme annotations against genre and style recommendations - a much easier and already studied task. We evaluate the quality of our recommended tags against an expert ground truth data set. Our results are promising and provide interesting insights into possible extensions for music tagging systems to support music search.", "keywords": ["theme tag recommendations", "content analysis and indexing", "information search and retrieval", "metadata enrichment", "high-level music descriptors", "collaborative tagging"], "combined": "Deriving music theme annotations from user tags Music theme annotations would be really beneficial for supporting retrieval, but are often neglected by users while annotating. Thus, in order to support users in tagging and to fill the gaps in the tag space, in this paper we develop algorithms for recommending theme annotations. Our methods exploit already existing user tags, the lyrics of music tracks, as well as combinations of both. We compare the results for our recommended theme annotations against genre and style recommendations - a much easier and already studied task. We evaluate the quality of our recommended tags against an expert ground truth data set. Our results are promising and provide interesting insights into possible extensions for music tagging systems to support music search. [[EENNDD]] theme tag recommendations; content analysis and indexing; information search and retrieval; metadata enrichment; high-level music descriptors; collaborative tagging"}, "Mendapatkan anotasi tema muzik dari tag pengguna Anotasi tema muzik akan sangat bermanfaat untuk menyokong pengambilan, tetapi sering diabaikan oleh pengguna semasa memberi penjelasan. Oleh itu, untuk menyokong pengguna dalam pemberian tag dan untuk mengisi jurang di ruang tanda, dalam makalah ini kami mengembangkan algoritma untuk mengesyorkan anotasi tema. Kaedah kami memanfaatkan tag pengguna yang sudah ada, lirik trek muzik, dan juga kombinasi keduanya. Kami membandingkan hasil untuk anotasi tema yang disyorkan dengan cadangan genre dan gaya - tugas yang jauh lebih mudah dan sudah dikaji. Kami menilai kualiti tag yang disyorkan terhadap set data kebenaran pakar. Hasil kami menjanjikan dan memberikan pandangan menarik mengenai kemungkinan sambungan untuk sistem penandaan muzik untuk menyokong carian muzik. [[EENNDD]] cadangan teg tema; analisis kandungan dan pengindeksan; pencarian dan pengambilan maklumat; pengayaan metadata; deskriptor muzik tahap tinggi; penandaan kolaboratif"], [{"string": "Analysis of topic dynamics in web search No contact information provided yet.", "keywords": ["user modeling", "web search", "topic analysis", "topic transition", "information storage and retrieval"], "combined": "Analysis of topic dynamics in web search No contact information provided yet. [[EENNDD]] user modeling; web search; topic analysis; topic transition; information storage and retrieval"}, "Analisis dinamika topik dalam carian web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pemodelan pengguna; carian sesawang; analisis topik; peralihan topik; penyimpanan dan pengambilan maklumat"], [{"string": "Multi-factor clustering for a marketplace search interface Search engines provide a small window to the vast repository of data they index and against which they search. They try their best to return the documents that are of relevance to the user but often a large number of results may be returned. Users struggle to manage this vast result set looking for the items of interest. Clustering search results is one way of alleviating this navigational pain. In this paper we describe a clustering system that enables clustering search results in an online marketplace search system.", "keywords": ["linear", "suffix-tree"], "combined": "Multi-factor clustering for a marketplace search interface Search engines provide a small window to the vast repository of data they index and against which they search. They try their best to return the documents that are of relevance to the user but often a large number of results may be returned. Users struggle to manage this vast result set looking for the items of interest. Clustering search results is one way of alleviating this navigational pain. In this paper we describe a clustering system that enables clustering search results in an online marketplace search system. [[EENNDD]] linear; suffix-tree"}, "Penggabungan pelbagai faktor untuk antara muka carian pasaran Enjin carian menyediakan tetingkap kecil ke banyak data yang mereka indeks dan yang mereka cari. Mereka berusaha sebaik mungkin untuk mengembalikan dokumen yang berkaitan dengan pengguna tetapi selalunya sebilangan besar hasil dapat dikembalikan. Pengguna berjuang untuk menguruskan set hasil yang luas ini untuk mencari item yang menarik. Penggabungan hasil carian adalah salah satu cara untuk meringankan kesakitan navigasi ini. Dalam makalah ini kami menerangkan sistem pengelompokan yang memungkinkan hasil pencarian pengelompokan dalam sistem carian pasar dalam talian. [[EENNDD]] linear; pokok akhiran"], [{"string": "Autopedia: automatic domain-independent Wikipedia article generation This paper proposes a general framework, named Autopedia, to generate high-quality wikipedia articles for given concepts in any domains, by automatically selecting the best wikipedia template consisting the sub-topics to organize the article for the input concept. Experimental results on 4,526 concepts validate the effectiveness of Autopedia, and the wikipedia template selection approach which takes into account both the template quality and the semantic relatedness between the input concept and its sibling concepts, performs the best.", "keywords": ["miscellaneous", "wikipedia", "domain independent", "template selection", "article generation"], "combined": "Autopedia: automatic domain-independent Wikipedia article generation This paper proposes a general framework, named Autopedia, to generate high-quality wikipedia articles for given concepts in any domains, by automatically selecting the best wikipedia template consisting the sub-topics to organize the article for the input concept. Experimental results on 4,526 concepts validate the effectiveness of Autopedia, and the wikipedia template selection approach which takes into account both the template quality and the semantic relatedness between the input concept and its sibling concepts, performs the best. [[EENNDD]] miscellaneous; wikipedia; domain independent; template selection; article generation"}, "Autopedia: penghasilan artikel Wikipedia bebas domain automatik Makalah ini mencadangkan kerangka umum, bernama Autopedia, untuk menghasilkan artikel wikipedia berkualiti tinggi untuk konsep yang diberikan dalam domain apa pun, dengan memilih templat wikipedia terbaik secara automatik yang terdiri daripada sub-topik untuk mengatur artikel untuk konsep input. Hasil eksperimen pada 4.526 konsep mengesahkan keberkesanan Autopedia, dan pendekatan pemilihan templat wikipedia yang mempertimbangkan kualiti templat dan hubungan semantik antara konsep input dan konsep saudara kandungnya, menunjukkan prestasi terbaik. [[EENNDD]] pelbagai; wikipedia; domain bebas; pemilihan templat; penghasilan artikel"], [{"string": "The ScratchPad: sensemaking support for the web The World Wide Web is a powerful platform for a wide range of information tasks. Dramatic advances in technology, such as improved search capabilities and the AJAX application model, have enabled entirely new web-based applications and usage patterns, making many tasks easier to perform than ever before. However, few tools have been developed to assist with sensemaking tasks: complex research behaviors in which users gather and comprehend information from many sources to answer potentially vague, non-procedural questions. Sensemaking tasks are common and include, for example, researching vacation destinations or deciding how to invest. This paper presents the ScratchPad, an extension to the standard browser interface that is designed to capture, organize, and exploit the information discovered while performing a sensemaking task.", "keywords": ["user interfaces", "www", "sensemaking", "visual snalytics", "web browser"], "combined": "The ScratchPad: sensemaking support for the web The World Wide Web is a powerful platform for a wide range of information tasks. Dramatic advances in technology, such as improved search capabilities and the AJAX application model, have enabled entirely new web-based applications and usage patterns, making many tasks easier to perform than ever before. However, few tools have been developed to assist with sensemaking tasks: complex research behaviors in which users gather and comprehend information from many sources to answer potentially vague, non-procedural questions. Sensemaking tasks are common and include, for example, researching vacation destinations or deciding how to invest. This paper presents the ScratchPad, an extension to the standard browser interface that is designed to capture, organize, and exploit the information discovered while performing a sensemaking task. [[EENNDD]] user interfaces; www; sensemaking; visual snalytics; web browser"}, "ScratchPad: sokongan sensemaking untuk web World Wide Web adalah platform yang kuat untuk pelbagai tugas maklumat. Kemajuan teknologi secara dramatik, seperti peningkatan kemampuan carian dan model aplikasi AJAX, telah memungkinkan aplikasi dan corak penggunaan berasaskan web yang sepenuhnya baru, menjadikan banyak tugas lebih mudah dilakukan daripada sebelumnya. Walau bagaimanapun, beberapa alat telah dikembangkan untuk membantu tugas membuat sensem: tingkah laku penyelidikan yang kompleks di mana pengguna mengumpulkan dan memahami maklumat dari banyak sumber untuk menjawab soalan-soalan bukan prosedur yang mungkin samar-samar. Tugas membuat sensasi adalah perkara biasa dan termasuk, misalnya, meneliti destinasi percutian atau memutuskan cara melabur. Makalah ini menyajikan ScratchPad, peluasan ke antara muka penyemak imbas standard yang dirancang untuk menangkap, mengatur, dan memanfaatkan maklumat yang dijumpai ketika melakukan tugas sensemaking. [[EENNDD]] antara muka pengguna; www; membuat pertimbangan; snalytics visual; pelayar web"], [{"string": "Summarization of archived and shared personal photo collections The volume of personal photos hosted on photo archives and social sharing platforms has been increasing exponentially. It is difficult to get an overview of a large collection of personal photos without browsing though the entire database manually. In this research, we propose a framework to generate representative subset summaries from photo collections hosted on web archives or social networks. We define salient properties of an effective photo summary and model summarization as an optimization of these properties, given the size constraints. We also introduce metrics for evaluating photo summaries based on their information content and the ability to satisfy user's information needs. Our experiments show that our summarization framework performs better than baseline algorithms.", "keywords": ["optimization", "summarization", "personal photos", "social networks"], "combined": "Summarization of archived and shared personal photo collections The volume of personal photos hosted on photo archives and social sharing platforms has been increasing exponentially. It is difficult to get an overview of a large collection of personal photos without browsing though the entire database manually. In this research, we propose a framework to generate representative subset summaries from photo collections hosted on web archives or social networks. We define salient properties of an effective photo summary and model summarization as an optimization of these properties, given the size constraints. We also introduce metrics for evaluating photo summaries based on their information content and the ability to satisfy user's information needs. Our experiments show that our summarization framework performs better than baseline algorithms. [[EENNDD]] optimization; summarization; personal photos; social networks"}, "Ringkasan koleksi foto peribadi yang diarkibkan dan dikongsi Jumlah gambar peribadi yang dihoskan di arkib foto dan platform perkongsian sosial telah meningkat dengan pesat. Adalah sukar untuk mendapatkan gambaran keseluruhan koleksi foto peribadi tanpa melayari keseluruhan pangkalan data secara manual. Dalam penyelidikan ini, kami mencadangkan kerangka untuk menghasilkan ringkasan subset perwakilan dari koleksi foto yang dihoskan di arkib web atau rangkaian sosial. Kami menentukan sifat penting dari ringkasan foto yang berkesan dan ringkasan model sebagai pengoptimuman sifat-sifat ini, memandangkan kekangan ukuran. Kami juga memperkenalkan metrik untuk menilai ringkasan foto berdasarkan kandungan maklumat mereka dan kemampuan untuk memenuhi keperluan maklumat pengguna. Eksperimen kami menunjukkan bahawa kerangka ringkasan kami berprestasi lebih baik daripada algoritma asas. [[EENNDD]] pengoptimuman; ringkasan; gambar peribadi; rangkaian sosial"], [{"string": "Migrating web application sessions in mobile computing No contact information provided yet.", "keywords": ["web applications", "mobile computing", "session hand-off"], "combined": "Migrating web application sessions in mobile computing No contact information provided yet. [[EENNDD]] web applications; mobile computing; session hand-off"}, "Memigrasikan sesi aplikasi web dalam pengkomputeran mudah alih Belum ada maklumat hubungan yang diberikan. [[EENNDD]] aplikasi web; pengkomputeran mudah alih; sesi lepas tangan"], [{"string": "DiTaBBu: automating the production of time-based hypermedia content No contact information provided yet.", "keywords": ["multimodality", "digital talking books", "accessibility", "document preparation", "hypertext/hypermedia", "ditabbu", "automatic presentation generation", "user interfaces", "hypermedia"], "combined": "DiTaBBu: automating the production of time-based hypermedia content No contact information provided yet. [[EENNDD]] multimodality; digital talking books; accessibility; document preparation; hypertext/hypermedia; ditabbu; automatic presentation generation; user interfaces; hypermedia"}, "DiTaBBu: mengautomasikan pengeluaran kandungan hypermedia berdasarkan masa Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] multimodaliti; buku bercakap digital; kebolehcapaian; penyediaan dokumen; hiperteks / hipermedia; ditabbu; penjanaan persembahan automatik; antara muka pengguna; hipermedia"], [{"string": "Fast and parallel webpage layout The web browser is a CPU-intensive program. Especially on mobile devices, webpages load too slowly, expending significant time in processing a document's appearance. Due to power constraints, most hardware-driven speedups will come in the form of parallel architectures. This is also true of mobile devices such as phones and e-books. In this paper, we introduce new algorithms for CSS selector matching, layout solving, and font rendering, which represent key components for a fast layout engine. Evaluation on popular sites shows speedups as high as 80x. We also formulate the layout problem with attribute grammars, enabling us to not only parallelize our algorithm but prove that it computes in O(log) time and without reflow.", "keywords": ["multicore", "graphical user interfaces", "html", "font", "mobile", "css", "selector", "attribute grammar", "box model", "layout"], "combined": "Fast and parallel webpage layout The web browser is a CPU-intensive program. Especially on mobile devices, webpages load too slowly, expending significant time in processing a document's appearance. Due to power constraints, most hardware-driven speedups will come in the form of parallel architectures. This is also true of mobile devices such as phones and e-books. In this paper, we introduce new algorithms for CSS selector matching, layout solving, and font rendering, which represent key components for a fast layout engine. Evaluation on popular sites shows speedups as high as 80x. We also formulate the layout problem with attribute grammars, enabling us to not only parallelize our algorithm but prove that it computes in O(log) time and without reflow. [[EENNDD]] multicore; graphical user interfaces; html; font; mobile; css; selector; attribute grammar; box model; layout"}, "Susun atur halaman web yang pantas dan selari Penyemak imbas web adalah program yang berintensifkan CPU. Terutama pada peranti mudah alih, halaman web dimuat terlalu perlahan, menghabiskan masa yang signifikan dalam memproses penampilan dokumen. Oleh kerana kekangan kuasa, kebanyakan kelajuan yang didorong oleh perkakasan akan datang dalam bentuk seni bina selari. Ini juga berlaku untuk peranti mudah alih seperti telefon dan e-buku. Dalam makalah ini, kami memperkenalkan algoritma baru untuk pencocokan pemilih CSS, penyelesaian tata letak, dan rendering font, yang mewakili komponen utama untuk mesin tata letak cepat. Penilaian di laman web popular menunjukkan kelajuan setinggi 80x. Kami juga merumuskan masalah susun atur dengan tatabahasa atribut, yang memungkinkan kami untuk tidak hanya menyelaraskan algoritma kami tetapi membuktikan bahawa ia dikira dalam waktu O (log) dan tanpa reflow. [[EENNDD]] pelbagai warna; antara muka pengguna grafik; html; fon; mudah alih; css; pemilih; tatabahasa atribut; model kotak; susun atur"], [{"string": "Filtering microblogging messages for social tv Social TV was named one of the ten most important emerging technologies in 2010 by the MIT Technology Review. Manufacturers of set-top boxes and televisions have recently started to integrate access to social networks into their products. Some of these systems allow users to read microblogging messages related to the TV program they are currently watching. However, such systems suffer from low precision and recall when they use the title of the show as keywords when retrieving messages, without any additional filtering.", "keywords": ["classification", "microblogging", "filtering", "twitter", "social tv"], "combined": "Filtering microblogging messages for social tv Social TV was named one of the ten most important emerging technologies in 2010 by the MIT Technology Review. Manufacturers of set-top boxes and televisions have recently started to integrate access to social networks into their products. Some of these systems allow users to read microblogging messages related to the TV program they are currently watching. However, such systems suffer from low precision and recall when they use the title of the show as keywords when retrieving messages, without any additional filtering. [[EENNDD]] classification; microblogging; filtering; twitter; social tv"}, "Menyaring mesej mikroblog untuk tv sosial TV sosial dinamakan sebagai salah satu daripada sepuluh teknologi baru yang paling penting muncul pada tahun 2010 oleh MIT Technology Review. Pengilang set-top box dan televisyen baru-baru ini mula mengintegrasikan akses ke rangkaian sosial ke dalam produk mereka. Beberapa sistem ini membolehkan pengguna membaca mesej mikroblog yang berkaitan dengan program TV yang sedang mereka tonton. Walau bagaimanapun, sistem sedemikian mengalami ketepatan rendah dan mengingat ketika mereka menggunakan tajuk rancangan sebagai kata kunci ketika mengambil mesej, tanpa penapisan tambahan. [[EENNDD]] klasifikasi; blog mikro; tapisan; twitter; tv sosial"], [{"string": "SourceRank: relevance and trust assessment for deep web sources based on inter-source agreement One immediate challenge in searching the deep web databases is source selection - i.e. selecting the most relevant web databases for answering a given query. The existing database selection methods (both text and relational) assess the source quality based on the query-similarity-based relevance assessment. When applied to the deep web these methods have two deficiencies. First is that the methods are agnostic to the correctness (trustworthiness) of the sources. Secondly, the query based relevance does not consider the importance of the results. These two considerations are essential for the open collections like the deep web. Since a number of sources provide answers to any query, we conjuncture that the agreements between these answers are likely to be helpful in assessing the importance and the trustworthiness of the sources. We compute the agreement between the sources as the agreement of the answers returned. While computing the agreement, we also measure and compensate for possible collusion between the sources. This adjusted agreement is modeled as a graph with sources at the vertices. On this agreement graph, a quality score of a source that we call SourceRank, is calculated as the stationary visit probability of a random walk. We evaluate SourceRank in multiple domains, including sources in Google Base, with sizes up to 675 sources. We demonstrate that the SourceRank tracks source corruption. Further, our relevance evaluations show that SourceRank improves precision by 22-60% over the Google Base and the other baseline methods. SourceRank has been implemented in a system called Factal.", "keywords": ["source selection", "web integration", "sourcerank", "deep web"], "combined": "SourceRank: relevance and trust assessment for deep web sources based on inter-source agreement One immediate challenge in searching the deep web databases is source selection - i.e. selecting the most relevant web databases for answering a given query. The existing database selection methods (both text and relational) assess the source quality based on the query-similarity-based relevance assessment. When applied to the deep web these methods have two deficiencies. First is that the methods are agnostic to the correctness (trustworthiness) of the sources. Secondly, the query based relevance does not consider the importance of the results. These two considerations are essential for the open collections like the deep web. Since a number of sources provide answers to any query, we conjuncture that the agreements between these answers are likely to be helpful in assessing the importance and the trustworthiness of the sources. We compute the agreement between the sources as the agreement of the answers returned. While computing the agreement, we also measure and compensate for possible collusion between the sources. This adjusted agreement is modeled as a graph with sources at the vertices. On this agreement graph, a quality score of a source that we call SourceRank, is calculated as the stationary visit probability of a random walk. We evaluate SourceRank in multiple domains, including sources in Google Base, with sizes up to 675 sources. We demonstrate that the SourceRank tracks source corruption. Further, our relevance evaluations show that SourceRank improves precision by 22-60% over the Google Base and the other baseline methods. SourceRank has been implemented in a system called Factal. [[EENNDD]] source selection; web integration; sourcerank; deep web"}, "SourceRank: penilaian kesesuaian dan kepercayaan untuk sumber web mendalam berdasarkan perjanjian antara sumber Salah satu cabaran segera dalam mencari pangkalan data web dalam adalah pemilihan sumber - iaitu memilih pangkalan data web yang paling relevan untuk menjawab pertanyaan tertentu. Kaedah pemilihan pangkalan data yang ada (teks dan relasional) menilai kualiti sumber berdasarkan penilaian kesesuaian berdasarkan pertanyaan-kesamaan. Apabila digunakan pada web dalam, kaedah ini mempunyai dua kekurangan. Pertama adalah bahawa kaedahnya agnostik terhadap kebenaran (kepercayaan) sumber. Kedua, perkaitan berdasarkan pertanyaan tidak mempertimbangkan kepentingan hasilnya. Kedua-dua pertimbangan ini penting untuk koleksi terbuka seperti web dalam. Oleh kerana sebilangan sumber memberikan jawapan untuk sebarang pertanyaan, kami menyimpulkan bahawa kesepakatan antara jawapan ini cenderung bermanfaat dalam menilai kepentingan dan kepercayaan sumber tersebut. Kami mengira kesepakatan antara sumber sebagai persetujuan jawapan dikembalikan. Semasa mengira perjanjian, kami juga mengukur dan mengimbangi kemungkinan berlakunya persekutuan antara sumber. Perjanjian yang disesuaikan ini dimodelkan sebagai grafik dengan sumber di bucu. Pada grafik perjanjian ini, skor kualiti sumber yang kita panggil SourceRank, dikira sebagai kebarangkalian lawatan pegun dari jalan rawak. Kami menilai SourceRank dalam beberapa domain, termasuk sumber di Pangkalan Google, dengan ukuran hingga 675 sumber. Kami menunjukkan bahawa SourceRank mengesan kerosakan sumber. Selanjutnya, penilaian kesesuaian kami menunjukkan bahawa SourceRank meningkatkan ketepatan sebanyak 22-60% berbanding Pangkalan Google dan kaedah asas lain. SourceRank telah dilaksanakan dalam sistem yang disebut Factal. [[EENNDD]] pemilihan sumber; penyepaduan web; sourcerank; web dalam"], [{"string": "Using OWL for querying an XML/RDF syntax No contact information provided yet.", "keywords": ["ontologies", "interoperability", "idref-awareness", "schema-awareness", "xpath", "xml", "rdf", "semantic integration"], "combined": "Using OWL for querying an XML/RDF syntax No contact information provided yet. [[EENNDD]] ontologies; interoperability; idref-awareness; schema-awareness; xpath; xml; rdf; semantic integration"}, "Menggunakan OWL untuk meminta sintaks XML / RDF Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] ontologi; saling kendali; kesedaran idref; kesedaran skema; xpath; xml; rdf; integrasi semantik"], [{"string": "Seeing the whole in parts: text summarization for web browsing on handheld devices An abstract is not available.", "keywords": ["ubiquitous computing", "summarization", "pda", "personal digital assistant", "wap", "wireless computing", "interaction styles", "handheld computers", "mobile computing", "portable devices"], "combined": "Seeing the whole in parts: text summarization for web browsing on handheld devices An abstract is not available. [[EENNDD]] ubiquitous computing; summarization; pda; personal digital assistant; wap; wireless computing; interaction styles; handheld computers; mobile computing; portable devices"}, "Melihat keseluruhan bahagian: ringkasan teks untuk melayari web pada peranti genggam Abstrak tidak tersedia. [[EENNDD]] pengkomputeran di mana-mana; ringkasan; pda; pembantu digital peribadi; wap; pengkomputeran tanpa wayar; gaya interaksi; komputer genggam; pengkomputeran mudah alih; peranti mudah alih"], [{"string": "KnowledgeTree: a distributed architecture for adaptive e-learning Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.", "keywords": ["adaptive web", "content re-use", "learning portal", "student model server", "adaptive content service", "learning object metadata", "e-learning"], "combined": "KnowledgeTree: a distributed architecture for adaptive e-learning Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references. [[EENNDD]] adaptive web; content re-use; learning portal; student model server; adaptive content service; learning object metadata; e-learning"}, "KnowledgeTree: seni bina yang diedarkan untuk e-pembelajaran adaptif Catatan: Kesalahan OCR mungkin terdapat dalam Senarai Rujukan ini yang diekstrak dari artikel teks lengkap. ACM memilih untuk mendedahkan Senarai lengkap dan bukan hanya rujukan yang betul dan berkaitan. [[EENNDD]] web adaptif; penggunaan semula kandungan; portal pembelajaran; pelayan model pelajar; perkhidmatan kandungan adaptif; metadata objek pembelajaran; e-pembelajaran"], [{"string": "Hubble: an advanced dynamic folder system for XML No contact information provided yet.", "keywords": ["content navigation", "categorization", "dynamic folder", "xml"], "combined": "Hubble: an advanced dynamic folder system for XML No contact information provided yet. [[EENNDD]] content navigation; categorization; dynamic folder; xml"}, "Hubble: sistem folder dinamik maju untuk XML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] navigasi kandungan; pengkategorian; folder dinamik; xml"], [{"string": "CTR-S: a logic for specifying contracts in semantic web services No contact information provided yet.", "keywords": ["languages and systems", "miscellaneous", "services composition", "web services", "contracts"], "combined": "CTR-S: a logic for specifying contracts in semantic web services No contact information provided yet. [[EENNDD]] languages and systems; miscellaneous; services composition; web services; contracts"}, "CTR-S: logik untuk menentukan kontrak dalam perkhidmatan web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] bahasa dan sistem; pelbagai; komposisi perkhidmatan; perkhidmatan web; kontrak"], [{"string": "Description logic programs: combining logic programs with description logic No contact information provided yet.", "keywords": ["systems and software", "ontologies", "model-theoretic semantics", "interoperability", "knowledge representation", "rules", "inferencing", "description logic", "miscellaneous", "translation", "xml", "semantic web", "rdf", "logic programs", "information integration"], "combined": "Description logic programs: combining logic programs with description logic No contact information provided yet. [[EENNDD]] systems and software; ontologies; model-theoretic semantics; interoperability; knowledge representation; rules; inferencing; description logic; miscellaneous; translation; xml; semantic web; rdf; logic programs; information integration"}, "Huraian program logik: menggabungkan program logik dengan logik keterangan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] sistem dan perisian; ontologi; semantik model-teori; saling kendali; perwakilan pengetahuan; peraturan; membuat kesimpulan; logik keterangan; pelbagai; terjemahan; xml; web semantik; rdf; program logik; penyatuan maklumat"], [{"string": "Automated synthesis of executable web service compositions from BPEL4WS processes No contact information provided yet.", "keywords": ["business processes", "web service composition", "automated synthesis"], "combined": "Automated synthesis of executable web service compositions from BPEL4WS processes No contact information provided yet. [[EENNDD]] business processes; web service composition; automated synthesis"}, "Sintesis automatik komposisi perkhidmatan web yang dapat dilaksanakan dari proses BPEL4WS Belum ada maklumat hubungan yang diberikan. [[EENNDD]] proses perniagaan; komposisi perkhidmatan web; sintesis automatik"], [{"string": "Answering similarity queries in peer-to-peer networks An abstract is not available.", "keywords": ["similarity", "image", "peer-to-peer"], "combined": "Answering similarity queries in peer-to-peer networks An abstract is not available. [[EENNDD]] similarity; image; peer-to-peer"}, "Menjawab pertanyaan kesamaan dalam rangkaian peer-to-peer Abstrak tidak tersedia. [[EENNDD]] persamaan; imej; rakan sebaya"], [{"string": "Scheduling web requests in broadcast environments No contact information provided yet.", "keywords": ["general", "scheduling algorithms", "web", "time constraints", "on-demand broadcast"], "combined": "Scheduling web requests in broadcast environments No contact information provided yet. [[EENNDD]] general; scheduling algorithms; web; time constraints; on-demand broadcast"}, "Menjadualkan permintaan web di persekitaran siaran Belum ada maklumat hubungan yang diberikan. [[EENNDD]] umum; algoritma penjadualan; laman web; kekangan masa; siaran atas permintaan"], [{"string": "Determining the user intent of web search engine queries Determining the user intent of Web searches is a difficult problem due to the sparse data available concerning the searcher. In this paper, we examine a method to determine the user intent underlying Web search engine queries. We qualitatively analyze samples of queries from seven transaction logs from three different Web search engines containing more than five million queries. From this analysis, we identified characteristics of user queries based on three broad classifications of user intent. The classifications of informational, navigational, and transactional represent the type of content destination the searcher desired as expressed by their query. We implemented our classification algorithm and automatically classified a separate Web search engine transaction log of over a million queries submitted by several hundred thousand users. Our findings show that more than 80% of Web queries are informational in nature, with about 10% each being navigational and transactional. In order to validate the accuracy of our algorithm, we manually coded 400 queries and compared the classification to the results from our algorithm. This comparison showed that our automatic classification has an accuracy of 74%. Of the remaining 25% of the queries, the user intent is generally vague or multi-faceted, pointing to the need to for probabilistic classification. We illustrate how knowledge of searcher intent might be used to enhance future Web search engines.", "keywords": ["search engines", "user intent", "web searching", "web queries"], "combined": "Determining the user intent of web search engine queries Determining the user intent of Web searches is a difficult problem due to the sparse data available concerning the searcher. In this paper, we examine a method to determine the user intent underlying Web search engine queries. We qualitatively analyze samples of queries from seven transaction logs from three different Web search engines containing more than five million queries. From this analysis, we identified characteristics of user queries based on three broad classifications of user intent. The classifications of informational, navigational, and transactional represent the type of content destination the searcher desired as expressed by their query. We implemented our classification algorithm and automatically classified a separate Web search engine transaction log of over a million queries submitted by several hundred thousand users. Our findings show that more than 80% of Web queries are informational in nature, with about 10% each being navigational and transactional. In order to validate the accuracy of our algorithm, we manually coded 400 queries and compared the classification to the results from our algorithm. This comparison showed that our automatic classification has an accuracy of 74%. Of the remaining 25% of the queries, the user intent is generally vague or multi-faceted, pointing to the need to for probabilistic classification. We illustrate how knowledge of searcher intent might be used to enhance future Web search engines. [[EENNDD]] search engines; user intent; web searching; web queries"}, "Menentukan maksud pengguna pertanyaan mesin pencari web Menentukan maksud pengguna carian Web adalah masalah yang sukar kerana data yang jarang tersedia mengenai pencari. Dalam makalah ini, kami memeriksa kaedah untuk menentukan maksud pengguna yang mendasari pertanyaan enjin carian Web. Kami secara kualitatif menganalisis sampel pertanyaan dari tujuh log transaksi dari tiga mesin carian Web yang berbeza yang mengandungi lebih daripada lima juta pertanyaan. Dari analisis ini, kami mengenal pasti ciri-ciri pertanyaan pengguna berdasarkan tiga klasifikasi maksud pengguna yang luas. Klasifikasi maklumat, navigasi, dan transaksional mewakili jenis tujuan kandungan yang dicari oleh pencari seperti yang dinyatakan oleh pertanyaan mereka. Kami melaksanakan algoritma klasifikasi kami dan secara automatik mengelaskan log transaksi mesin carian Web yang berasingan dengan lebih dari satu juta pertanyaan yang dikemukakan oleh beberapa ratus ribu pengguna. Penemuan kami menunjukkan bahawa lebih daripada 80% pertanyaan Web bersifat maklumat, dengan masing-masing sekitar 10% adalah navigasi dan transaksi. Untuk mengesahkan ketepatan algoritma kami, kami mengkodkan 400 pertanyaan secara manual dan membandingkan klasifikasi dengan hasil dari algoritma kami. Perbandingan ini menunjukkan bahawa klasifikasi automatik kami mempunyai ketepatan 74%. Dari 25% pertanyaan yang selebihnya, maksud pengguna pada umumnya samar-samar atau pelbagai aspek, menunjukkan perlunya klasifikasi probabilistik. Kami menggambarkan bagaimana pengetahuan tentang maksud pencari dapat digunakan untuk meningkatkan mesin pencari Web masa depan. [[EENNDD]] enjin carian; niat pengguna; carian web; pertanyaan web"], [{"string": "Privacy wizards for social networking sites Privacy is an enormous problem in online social networking sites. While sites such as Facebook allow users fine-grained control over who can see their profiles, it is difficult for average users to specify this kind of detailed policy.", "keywords": ["active learning", "usability", "social network privacy"], "combined": "Privacy wizards for social networking sites Privacy is an enormous problem in online social networking sites. While sites such as Facebook allow users fine-grained control over who can see their profiles, it is difficult for average users to specify this kind of detailed policy. [[EENNDD]] active learning; usability; social network privacy"}, "Penyihir privasi untuk laman rangkaian sosial Privasi adalah masalah besar dalam laman rangkaian sosial dalam talian. Walaupun laman web seperti Facebook membenarkan pengguna mengawal dengan jelas siapa yang dapat melihat profil mereka, sukar bagi pengguna rata-rata untuk menentukan jenis dasar terperinci ini. [[EENNDD]] pembelajaran aktif; kebolehgunaan; privasi rangkaian sosial"], [{"string": "Evaluation with informational and navigational intents Given an ambiguous or underspecified query, search result diversification aims at accomodating different user intents within a single \"entry-point\" result page. However, some intents are informational, for which many relevant pages may help, while others are navigational, for which only one web page is required. We propose new evaluation metrics for search result diversification that considers this distinction, as well as a simple method for comparing the intuitiveness of a given pair of metrics quantitatively. Our main experimental findings are: (a) In terms of discriminative power which reflects statistical reliability, the proposed metrics, DIN#-nDCG and P+Q#, are comparable to intent recall and D#-nDCG, and possibly superior to \u03b1-nDCG; (b) In terms of preference agreement with intent recall, P+Q# is superior to other diversity metrics and therefore may be the most intuitive as a metric that emphasises diversity; and (c) In terms of preference agreement with effective precision, DIN#-nDCG is superior to other diversity metrics and therefore may be the most intuitive as a metric that emphasises relevance. Moreover, DIN#-nDCG may be the most intuitive as a metric that considers both diversity and relevance. In addition, we demonstrate that the randomised Tukey's Honestly Significant Differences test that takes the entire set of available runs into account is substantially more conservative than the paired bootstrap test that only considers one run pair at a time, and therefore recommend the former approach for significance testing when a set of runs is available for evaluation.", "keywords": ["redundancy", "intents", "diversification", "novelty", "information search and retrieval", "evaluation", "metrics"], "combined": "Evaluation with informational and navigational intents Given an ambiguous or underspecified query, search result diversification aims at accomodating different user intents within a single \"entry-point\" result page. However, some intents are informational, for which many relevant pages may help, while others are navigational, for which only one web page is required. We propose new evaluation metrics for search result diversification that considers this distinction, as well as a simple method for comparing the intuitiveness of a given pair of metrics quantitatively. Our main experimental findings are: (a) In terms of discriminative power which reflects statistical reliability, the proposed metrics, DIN#-nDCG and P+Q#, are comparable to intent recall and D#-nDCG, and possibly superior to \u03b1-nDCG; (b) In terms of preference agreement with intent recall, P+Q# is superior to other diversity metrics and therefore may be the most intuitive as a metric that emphasises diversity; and (c) In terms of preference agreement with effective precision, DIN#-nDCG is superior to other diversity metrics and therefore may be the most intuitive as a metric that emphasises relevance. Moreover, DIN#-nDCG may be the most intuitive as a metric that considers both diversity and relevance. In addition, we demonstrate that the randomised Tukey's Honestly Significant Differences test that takes the entire set of available runs into account is substantially more conservative than the paired bootstrap test that only considers one run pair at a time, and therefore recommend the former approach for significance testing when a set of runs is available for evaluation. [[EENNDD]] redundancy; intents; diversification; novelty; information search and retrieval; evaluation; metrics"}, "Penilaian dengan maksud maklumat dan navigasi Memandangkan pertanyaan yang samar-samar atau tidak ditentukan, kepelbagaian hasil carian bertujuan untuk mengakomodasi maksud pengguna yang berbeza dalam satu halaman hasil \"titik masuk\". Walau bagaimanapun, beberapa tujuan adalah maklumat, yang mana banyak halaman yang relevan dapat membantu, sementara yang lain adalah navigasi, yang hanya memerlukan satu halaman web. Kami mencadangkan metrik penilaian baru untuk kepelbagaian hasil carian yang mempertimbangkan perbezaan ini, dan juga kaedah mudah untuk membandingkan intuitif sepasang metrik tertentu secara kuantitatif. Penemuan eksperimen utama kami adalah: (a) Dari segi daya diskriminatif yang mencerminkan kebolehpercayaan statistik, metrik yang dicadangkan, DIN # -nDCG dan P + Q #, setanding dengan penarikan niat dan D # -nDCG, dan mungkin lebih tinggi daripada \u03b1-nDCG ; (b) Dari segi perjanjian pilihan dengan penarikan maksud, P + Q # lebih unggul daripada metrik kepelbagaian lain dan oleh itu mungkin yang paling intuitif sebagai metrik yang menekankan kepelbagaian; dan (c) Dari segi perjanjian pilihan dengan ketepatan yang berkesan, DIN # -nDCG lebih unggul daripada metrik kepelbagaian lain dan oleh itu mungkin yang paling intuitif sebagai metrik yang menekankan perkaitan. Lebih-lebih lagi, DIN # -nDCG mungkin yang paling intuitif sebagai metrik yang mempertimbangkan kepelbagaian dan perkaitan. Di samping itu, kami menunjukkan bahawa ujian Perbezaan Jujur Tukey secara rawak yang mengambil kira keseluruhan set berjalan yang tersedia jauh lebih konservatif daripada ujian bootstrap berpasangan yang hanya menganggap satu pasangan berjalan pada satu masa, dan oleh itu mengesyorkan pendekatan sebelumnya untuk kepentingan ujian apabila satu set larian tersedia untuk penilaian. [[EENNDD]] redundansi; niat; kepelbagaian; kebaharuan; carian dan pengambilan maklumat; penilaian; sukatan"], [{"string": "What is Twitter, a social network or a news media? Twitter, a microblogging service less than three years old, commands more than 41 million users as of July 2009 and is growing fast. Twitter users tweet about any topic within the 140-character limit and follow others to receive their tweets. The goal of this paper is to study the topological characteristics of Twitter and its power as a new medium of information sharing.", "keywords": ["homophily", "pagerank", "online social network", "retweet", "social and behavioral sciences", "degree of separation", "information diffusion", "reciprocity", "twitter", "influential"], "combined": "What is Twitter, a social network or a news media? Twitter, a microblogging service less than three years old, commands more than 41 million users as of July 2009 and is growing fast. Twitter users tweet about any topic within the 140-character limit and follow others to receive their tweets. The goal of this paper is to study the topological characteristics of Twitter and its power as a new medium of information sharing. [[EENNDD]] homophily; pagerank; online social network; retweet; social and behavioral sciences; degree of separation; information diffusion; reciprocity; twitter; influential"}, "Apa itu Twitter, rangkaian sosial atau media berita? Twitter, perkhidmatan microblogging berusia kurang dari tiga tahun, memerintah lebih daripada 41 juta pengguna pada bulan Julai 2009 dan berkembang pesat. Pengguna Twitter tweet mengenai topik apa pun dalam had 140 watak dan ikuti orang lain untuk menerima tweet mereka. Matlamat makalah ini adalah untuk mengkaji ciri topologi Twitter dan kekuatannya sebagai medium perkongsian maklumat baru. [[EENNDD]] homofili; pagerank; rangkaian sosial dalam talian; retweet; sains sosial dan tingkah laku; tahap pemisahan; penyebaran maklumat; timbal balik; twitter; berpengaruh"], [{"string": "Recommendations for the long tail by term-query graph We define a new approach to the query recommendation problem. In particular, our main goal is to design a model enabling the generation of query suggestions also for rare and previously unseen queries. In other words we are targeting queries in the long tail. The model is based on a graph having two sets of nodes: Term nodes, and Query nodes. The graph induces a Markov chain on which a generic random walker starts from a subset of Term nodes, moves along Query nodes, and restarts (with a given probability) only from the same initial subset of Term nodes. Computing the stationary distribution of such a Markov chain is equivalent to extracting the so-called Center-piece Subgraph from the graph associated with the Markov chain itself. Given a query, we extract its terms and we set the restart subset to this term set. Therefore, we do not require a query to have been previously observed for the recommending model to be able to generate suggestions.", "keywords": ["query recommender systems", "web search effectiveness"], "combined": "Recommendations for the long tail by term-query graph We define a new approach to the query recommendation problem. In particular, our main goal is to design a model enabling the generation of query suggestions also for rare and previously unseen queries. In other words we are targeting queries in the long tail. The model is based on a graph having two sets of nodes: Term nodes, and Query nodes. The graph induces a Markov chain on which a generic random walker starts from a subset of Term nodes, moves along Query nodes, and restarts (with a given probability) only from the same initial subset of Term nodes. Computing the stationary distribution of such a Markov chain is equivalent to extracting the so-called Center-piece Subgraph from the graph associated with the Markov chain itself. Given a query, we extract its terms and we set the restart subset to this term set. Therefore, we do not require a query to have been previously observed for the recommending model to be able to generate suggestions. [[EENNDD]] query recommender systems; web search effectiveness"}, "Cadangan untuk jangka panjang dengan grafik jangka istilah Kami menentukan pendekatan baru untuk masalah cadangan pertanyaan. Khususnya, tujuan utama kami adalah untuk merancang model yang memungkinkan penghasilan cadangan pertanyaan untuk pertanyaan yang jarang dan sebelumnya tidak kelihatan. Dengan kata lain, kami menyasarkan pertanyaan dalam keadaan panjang. Model ini berdasarkan pada grafik yang mempunyai dua set nod: Node jangka, dan node Pertanyaan. Grafik mendorong rantaian Markov di mana pejalan kaki rawak generik bermula dari subset node Term, bergerak di sepanjang node Kueri, dan dimulakan semula (dengan kebarangkalian tertentu) hanya dari subset awal yang sama dari node Term. Mengira pengedaran pegun dari rantai Markov sedemikian sama dengan mengekstrak apa yang disebut Sub-potongan Subgraf dari graf yang berkaitan dengan rantai Markov itu sendiri. Dengan adanya pertanyaan, kami mengekstrak istilahnya dan kami menetapkan subset mulakan semula ke set istilah ini. Oleh itu, kami tidak memerlukan pertanyaan yang diperhatikan sebelumnya agar model yang mengesyorkan dapat menghasilkan cadangan. [[EENNDD]] sistem pengesyorkan pertanyaan; keberkesanan carian laman web"], [{"string": "An adaptive crawler for locating hidden-Web entry points In this paper we describe new adaptive crawling strategies to efficiently locate the entry points to hidden-Web sources. The fact that hidden-Web sources are very sparsely distributedmakes the problem of locating them especially challenging. We deal with this problem by using the contents ofpages to focus the crawl on a topic; by prioritizing promisinglinks within the topic; and by also following links that may not lead to immediate benefit. We propose a new frameworkwhereby crawlers automatically learn patterns of promisinglinks and adapt their focus as the crawl progresses, thus greatly reducing the amount of required manual setup andtuning. Our experiments over real Web pages in a representativeset of domains indicate that online learning leadsto significant gains in harvest rates' the adaptive crawlers retrieve up to three times as many forms as crawlers thatuse a fixed focus strategy.", "keywords": ["learning classifiers", "online learning", "hiddenweb", "web crawling strategies"], "combined": "An adaptive crawler for locating hidden-Web entry points In this paper we describe new adaptive crawling strategies to efficiently locate the entry points to hidden-Web sources. The fact that hidden-Web sources are very sparsely distributedmakes the problem of locating them especially challenging. We deal with this problem by using the contents ofpages to focus the crawl on a topic; by prioritizing promisinglinks within the topic; and by also following links that may not lead to immediate benefit. We propose a new frameworkwhereby crawlers automatically learn patterns of promisinglinks and adapt their focus as the crawl progresses, thus greatly reducing the amount of required manual setup andtuning. Our experiments over real Web pages in a representativeset of domains indicate that online learning leadsto significant gains in harvest rates' the adaptive crawlers retrieve up to three times as many forms as crawlers thatuse a fixed focus strategy. [[EENNDD]] learning classifiers; online learning; hiddenweb; web crawling strategies"}, "Crawler adaptif untuk mencari titik masuk Web tersembunyi Dalam makalah ini kami menerangkan strategi merangkak adaptif baru untuk mencari titik masuk ke sumber Web tersembunyi dengan cekap. Kenyataan bahawa sumber-sumber Web tersembunyi jarang sekali menimbulkan masalah untuk mencarinya terutamanya mencabar. Kami menangani masalah ini dengan menggunakan kandungan halaman untuk memfokuskan perayapan pada topik; dengan mengutamakan pautan yang menjanjikan dalam topik; dan juga dengan mengikuti pautan yang mungkin tidak membawa faedah langsung. Kami mencadangkan kerangka baru di mana perayap secara automatik mempelajari corak pautan yang menjanjikan dan menyesuaikan fokus mereka ketika perayapan berlangsung, sehingga dapat mengurangkan jumlah penyediaan dan penyesuaian manual yang diperlukan. Eksperimen kami ke atas laman Web sebenar dalam domain perwakilan menunjukkan bahawa pembelajaran dalam talian membawa peningkatan yang signifikan dalam kadar penuaian 'perayap adaptif mengambil sehingga tiga kali lebih banyak bentuk daripada perayap yang menggunakan strategi fokus tetap. [[EENNDD]] pengkelasan pembelajaran; pembelajaran dalam talian; web tersembunyi; strategi merangkak web"], [{"string": "Geospatial mapping and navigation of the web An abstract is not available.", "keywords": ["geographic information systems", "geospatial information retrieval", "browsers", "navigation", "interaction styles"], "combined": "Geospatial mapping and navigation of the web An abstract is not available. [[EENNDD]] geographic information systems; geospatial information retrieval; browsers; navigation; interaction styles"}, "Pemetaan geografi dan navigasi web Abstrak tidak tersedia. [[EENNDD]] sistem maklumat geografi; pengambilan maklumat geospatial; penyemak imbas; pelayaran; gaya interaksi"], [{"string": "Supporting anonymous location queries in mobile environments with privacygrid This paper presents PrivacyGrid - a framework for supporting anonymous location-based queries in mobile information delivery systems. The PrivacyGrid framework offers three unique capabilities. First, it provides a location privacy protection preference profile model, called location P3P, which allows mobile users to explicitly define their preferred location privacy requirements in terms of both location hiding measures (e.g., location k-anonymity and location l-diversity) and location service quality measures (e.g., maximum spatial resolution and maximum temporal resolution). Second, it provides fast and effective location cloaking algorithms for location k-anonymity and location l-diversity in a mobile environment. We develop dynamic bottom-up and top-down grid cloaking algorithms with the goal of achieving high anonymization success rate and efficiency in terms of both time complexity and maintenance cost. A hybrid approach that carefully combines the strengths of both bottom-up and top-down cloaking approaches to further reduce the average anonymization time is also developed. Last but not the least, PrivacyGrid incorporates temporal cloaking into the location cloaking process to further increase the success rate of location anonymization. We also discuss PrivacyGrid mechanisms for supporting anonymous location queries. Experimental evaluation shows that the PrivacyGrid approach can provide close to optimal location k-anonymity as defined by per user location P3P without introducing significant performance penalties.", "keywords": ["location privacy", "l-diversity", "k-anonymity"], "combined": "Supporting anonymous location queries in mobile environments with privacygrid This paper presents PrivacyGrid - a framework for supporting anonymous location-based queries in mobile information delivery systems. The PrivacyGrid framework offers three unique capabilities. First, it provides a location privacy protection preference profile model, called location P3P, which allows mobile users to explicitly define their preferred location privacy requirements in terms of both location hiding measures (e.g., location k-anonymity and location l-diversity) and location service quality measures (e.g., maximum spatial resolution and maximum temporal resolution). Second, it provides fast and effective location cloaking algorithms for location k-anonymity and location l-diversity in a mobile environment. We develop dynamic bottom-up and top-down grid cloaking algorithms with the goal of achieving high anonymization success rate and efficiency in terms of both time complexity and maintenance cost. A hybrid approach that carefully combines the strengths of both bottom-up and top-down cloaking approaches to further reduce the average anonymization time is also developed. Last but not the least, PrivacyGrid incorporates temporal cloaking into the location cloaking process to further increase the success rate of location anonymization. We also discuss PrivacyGrid mechanisms for supporting anonymous location queries. Experimental evaluation shows that the PrivacyGrid approach can provide close to optimal location k-anonymity as defined by per user location P3P without introducing significant performance penalties. [[EENNDD]] location privacy; l-diversity; k-anonymity"}, "Menyokong pertanyaan lokasi tanpa nama di persekitaran mudah alih dengan privacygrid Makalah ini menyajikan PrivacyGrid - kerangka kerja untuk menyokong pertanyaan berdasarkan lokasi tanpa nama dalam sistem penyampaian maklumat mudah alih. Rangka kerja PrivacyGrid menawarkan tiga keupayaan unik. Pertama, ia menyediakan model profil pilihan perlindungan privasi lokasi, yang disebut lokasi P3P, yang membolehkan pengguna mudah alih secara jelas menentukan keperluan privasi lokasi pilihan mereka dari segi kedua-dua langkah penyembunyian lokasi (misalnya, lokasi k-anonimiti dan lokasi l-kepelbagaian) dan lokasi ukuran kualiti perkhidmatan (contohnya, resolusi spasial maksimum dan resolusi temporal maksimum). Kedua, ia menyediakan algoritma penyelubungan lokasi yang cepat dan berkesan untuk lokasi k-anonimiti dan kepelbagaian lokasi dalam persekitaran mudah alih. Kami mengembangkan algoritma cloaking grid bawah-atas dan atas-bawah yang dinamik dengan tujuan untuk mencapai tahap kejayaan dan kecekapan anonimisasi yang tinggi dari segi kerumitan masa dan kos penyelenggaraan. Pendekatan hibrid yang dengan hati-hati menggabungkan kekuatan pendekatan cloaking bawah-atas dan atas-bawah untuk mengurangkan lagi masa anonimisasi juga dikembangkan. Akhir kata, PrivacyGrid menggabungkan tempo cloaking ke dalam proses cloaking lokasi untuk meningkatkan lagi tahap keberhasilan anonimisasi lokasi. Kami juga membincangkan mekanisme PrivacyGrid untuk menyokong pertanyaan lokasi tanpa nama. Penilaian eksperimental menunjukkan bahawa pendekatan PrivacyGrid dapat memberikan jarak dekat dengan lokasi k-anonim yang optimum seperti yang ditentukan oleh setiap lokasi pengguna P3P tanpa mengenakan hukuman prestasi yang signifikan. [[EENNDD]] privasi lokasi; l-kepelbagaian; k-anonimiti"], [{"string": "Visualization of Geo-annotated pictures in mobile phones In this work, a novel mobile browser for geo-referenced pictures is introduced and described. We use the term browser to denote a system aimed at browsing pictures selected from a large set like Internet photo sharing services. The criteria to filter a subset of pictures to browse are three: the user's actual position, the user's actual heading, and the user's preferences. In this work we only focus on the first two criteria leaving the integration of user's preferences for future developments.", "keywords": ["gps", "geo-browsing", "compass", "mobile photo browsing", "maps", "arduino", "interaction styles"], "combined": "Visualization of Geo-annotated pictures in mobile phones In this work, a novel mobile browser for geo-referenced pictures is introduced and described. We use the term browser to denote a system aimed at browsing pictures selected from a large set like Internet photo sharing services. The criteria to filter a subset of pictures to browse are three: the user's actual position, the user's actual heading, and the user's preferences. In this work we only focus on the first two criteria leaving the integration of user's preferences for future developments. [[EENNDD]] gps; geo-browsing; compass; mobile photo browsing; maps; arduino; interaction styles"}, "Visualisasi gambar beranotasi Geo dalam telefon bimbit Dalam karya ini, penyemak imbas mudah alih baru untuk gambar yang dirujuk secara geo diperkenalkan dan dijelaskan. Kami menggunakan istilah penyemak imbas untuk menunjukkan sistem yang bertujuan untuk melihat-lihat gambar yang dipilih dari satu set besar seperti perkhidmatan perkongsian foto Internet. Kriteria untuk menyaring subkumpulan gambar untuk dilihat adalah tiga: kedudukan sebenar pengguna, tajuk sebenar pengguna, dan pilihan pengguna. Dalam karya ini kita hanya memfokuskan pada dua kriteria pertama yang meninggalkan integrasi pilihan pengguna untuk perkembangan masa depan. [[EENNDD]] gps; melayari geo; kompas; penyemakan imbas foto mudah alih; peta; arduino; gaya interaksi"], [{"string": "Object views: fine-grained sharing in browsers Browsers do not currently support the secure sharing of JavaScript objects between principals. We present this problem as the need for object views, which are consistent and controllable versions of objects. Multiple views can be made for the same object and customized for the recipients. We implement object views with a JavaScript library that wraps shared objects and interposes on all access attempts. The security challenge is to fully mediate access to objects shared through a view and prevent privilege escalation. We discuss how object views can be deployed in two settings: same-origin sharing with rewriting-based JavaScript isolation systems like Google Caja, and inter-origin sharing between browser frames over a message-passing channel.", "keywords": ["same origin policy", "remote object", "membrane", "javascript", "reference monitor", "web security", "security and protection", "coding tools and techniques", "ecmascript", "browser", "aspect", "wrapper"], "combined": "Object views: fine-grained sharing in browsers Browsers do not currently support the secure sharing of JavaScript objects between principals. We present this problem as the need for object views, which are consistent and controllable versions of objects. Multiple views can be made for the same object and customized for the recipients. We implement object views with a JavaScript library that wraps shared objects and interposes on all access attempts. The security challenge is to fully mediate access to objects shared through a view and prevent privilege escalation. We discuss how object views can be deployed in two settings: same-origin sharing with rewriting-based JavaScript isolation systems like Google Caja, and inter-origin sharing between browser frames over a message-passing channel. [[EENNDD]] same origin policy; remote object; membrane; javascript; reference monitor; web security; security and protection; coding tools and techniques; ecmascript; browser; aspect; wrapper"}, "Paparan objek: perkongsian terperinci dalam penyemak imbas Penyemak imbas pada masa ini tidak menyokong perkongsian objek JavaScript yang selamat antara pengetua. Kami mengemukakan masalah ini sebagai keperluan untuk melihat objek, yang merupakan versi objek yang konsisten dan terkawal. Pelbagai paparan boleh dibuat untuk objek yang sama dan disesuaikan untuk penerima. Kami menerapkan pandangan objek dengan pustaka JavaScript yang membungkus objek bersama dan menghentikan semua percubaan akses. Cabaran keselamatan adalah untuk memediasi sepenuhnya akses ke objek yang dikongsi melalui pandangan dan mencegah peningkatan hak istimewa. Kami membincangkan bagaimana pandangan objek dapat digunakan dalam dua tetapan: perkongsian asal yang sama dengan sistem pengasingan JavaScript berdasarkan penulisan semula seperti Google Caja, dan perkongsian antara asal antara bingkai penyemak imbas melalui saluran penyampaian mesej. [[EENNDD]] dasar asal yang sama; objek terpencil; membran; javascript; monitor rujukan; keselamatan web; keselamatan dan perlindungan; alat dan teknik pengekodan; ekmaskrip; penyemak imbas; aspek; pembalut"], [{"string": "Csurf: a context-driven non-visual web-browser Web sites are designed for graphical mode of interaction. Sighted users can \"cut to the chase\" and quickly identify relevant information in Web pages. On the contrary, individuals with visual disabilities have to use screen-readers tobrowse the Web. As screen-readers process pages sequentially and read through everything, Web browsing can become strenuous and time-consuming. Although, the use ofshortcuts and searching offers some improvements, the problem still remains. In this paper, we address the problemof information overload in non-visual Web access using thenotion of context. Our prototype system, CSurf, embodyingour approach, provides the usual features of a screen-reader.However, when a user follows a link, CSurf captures thecontext of the link using a simple topic-boundary detectiontechnique, and uses it to identify relevant information onthe next page with the help of a Support Vector Machine, astatistical machine-learning model. Then, CSurf reads the Web page starting from the most relevant section, identifiedby the model. We conducted a series experiments to evaluate the performance of CSurf against the state-of-the-artscreen-reader, JAWS. Our results show that the use of context can potentially save browsing time and substantiallyimprove browsing experience of visually disabled people.", "keywords": ["hearsay", "non-visual", "context", "csurf", "hypertext/hypermedia", "voice browser", "user interfaces", "partitioning", "web accessibility", "semantic blocks", "screen-reader"], "combined": "Csurf: a context-driven non-visual web-browser Web sites are designed for graphical mode of interaction. Sighted users can \"cut to the chase\" and quickly identify relevant information in Web pages. On the contrary, individuals with visual disabilities have to use screen-readers tobrowse the Web. As screen-readers process pages sequentially and read through everything, Web browsing can become strenuous and time-consuming. Although, the use ofshortcuts and searching offers some improvements, the problem still remains. In this paper, we address the problemof information overload in non-visual Web access using thenotion of context. Our prototype system, CSurf, embodyingour approach, provides the usual features of a screen-reader.However, when a user follows a link, CSurf captures thecontext of the link using a simple topic-boundary detectiontechnique, and uses it to identify relevant information onthe next page with the help of a Support Vector Machine, astatistical machine-learning model. Then, CSurf reads the Web page starting from the most relevant section, identifiedby the model. We conducted a series experiments to evaluate the performance of CSurf against the state-of-the-artscreen-reader, JAWS. Our results show that the use of context can potentially save browsing time and substantiallyimprove browsing experience of visually disabled people. [[EENNDD]] hearsay; non-visual; context; csurf; hypertext/hypermedia; voice browser; user interfaces; partitioning; web accessibility; semantic blocks; screen-reader"}, "Csurf: laman web penyemak imbas bukan visual berasaskan konteks yang dirancang untuk mod interaksi grafik. Pengguna yang melihat dapat \"mengejar\" dan dengan cepat mengenal pasti maklumat yang relevan di halaman Web. Sebaliknya, individu yang mempunyai masalah penglihatan harus menggunakan pembaca skrin untuk menjelajah Web. Oleh kerana pembaca skrin memproses halaman secara berurutan dan membaca semuanya, melayari laman web boleh menjadi berat dan memakan masa. Walaupun, penggunaan jalan pintas dan pencarian menawarkan beberapa penambahbaikan, masalahnya masih ada. Dalam makalah ini, kami menangani masalah keterlaluan maklumat dalam akses Web bukan visual menggunakan pergerakan konteks. Sistem prototaip kami, CSurf, merangkumi pendekatan kami, menyediakan ciri biasa pembaca skrin. Walau bagaimanapun, apabila pengguna mengikuti pautan, CSurf menangkap konteks pautan menggunakan teknik pengesanan sempadan topik yang mudah, dan menggunakannya untuk mengenal pasti maklumat yang relevan mengenai halaman seterusnya dengan bantuan Mesin Vektor Sokongan, model pembelajaran mesin astatis. Kemudian, CSurf membaca halaman Web bermula dari bahagian yang paling relevan, yang dikenal pasti oleh model. Kami melakukan eksperimen siri untuk menilai prestasi CSurf terhadap pembaca skrin seni terkini, JAWS. Hasil kajian kami menunjukkan bahawa penggunaan konteks berpotensi menjimatkan masa penyemakan imbas dan meningkatkan pengalaman penyemakan imbas orang kurang upaya. [[EENNDD]] khabar angin; bukan visual; konteks; luncur; hiperteks / hipermedia; penyemak imbas suara; antara muka pengguna; pembahagian; kebolehcapaian laman web; blok semantik; pembaca skrin"], [{"string": "Protecting electronic commerce from distributed denial-of-service attacks No contact information provided yet.", "keywords": ["quality of service", "security and protection", "electronic commerce", "financial", "denial of service"], "combined": "Protecting electronic commerce from distributed denial-of-service attacks No contact information provided yet. [[EENNDD]] quality of service; security and protection; electronic commerce; financial; denial of service"}, "Melindungi perdagangan elektronik dari serangan penolakan perkhidmatan yang diedarkan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kualiti perkhidmatan; keselamatan dan perlindungan; perdagangan elektronik; kewangan; penolakan perkhidmatan"], [{"string": "Talking about data: sharing richly structured information through blogs and wikis The web has dramatically enhanced people's ability to communicate ideas, knowledge, and opinions. But the authoring tools that most people understand, blogs and wikis, primarily guide users toward authoring text. In this work, we show that substantial gains in expressivity and communication would accrue if people could easily share richly structured information in meaningful visualizations. We then describe several extensions we have created for blogs and wikis that enable users to publish, share, and aggregate such structured information using the same workflows they apply to text. In particular, we aim to preserve those attributes that make blogs and wikis so effective: one-click access to the information, one-click publishing of content, natural authoring interfaces, and the ability to easily copy-and-paste information and visualizations from other sources.", "keywords": ["visualization", "blogs", "wikis", "linked data"], "combined": "Talking about data: sharing richly structured information through blogs and wikis The web has dramatically enhanced people's ability to communicate ideas, knowledge, and opinions. But the authoring tools that most people understand, blogs and wikis, primarily guide users toward authoring text. In this work, we show that substantial gains in expressivity and communication would accrue if people could easily share richly structured information in meaningful visualizations. We then describe several extensions we have created for blogs and wikis that enable users to publish, share, and aggregate such structured information using the same workflows they apply to text. In particular, we aim to preserve those attributes that make blogs and wikis so effective: one-click access to the information, one-click publishing of content, natural authoring interfaces, and the ability to easily copy-and-paste information and visualizations from other sources. [[EENNDD]] visualization; blogs; wikis; linked data"}, "Bercakap mengenai data: berkongsi maklumat berstruktur yang kaya melalui blog dan wiki Web telah meningkatkan keupayaan pengguna untuk menyampaikan idea, pengetahuan, dan pendapat secara dramatik. Tetapi alat pengarang yang difahami oleh kebanyakan orang, blog dan wiki, terutama membimbing pengguna ke arah menulis teks. Dalam karya ini, kami menunjukkan bahawa keuntungan ekspresif dan komunikasi yang besar akan bertambah jika orang dapat dengan mudah berkongsi maklumat yang terstruktur dalam visualisasi yang bermakna. Kami kemudian menerangkan beberapa peluasan yang telah kami buat untuk blog dan wiki yang membolehkan pengguna menerbitkan, berkongsi, dan mengumpulkan maklumat berstruktur tersebut menggunakan aliran kerja yang sama seperti yang mereka gunakan untuk teks. Khususnya, kami bertujuan untuk mengekalkan atribut yang menjadikan blog dan wiki sangat berkesan: akses satu klik ke maklumat, penerbitan satu klik kandungan, antara muka pengarang semula jadi, dan keupayaan untuk menyalin dan menampal maklumat dan visualisasi dengan mudah dari sumber lain. [[EENNDD]] visualisasi; blog; wiki; data terpaut"], [{"string": "A dual-mode user interface for accessing 3D content on the world wide web The Web evolved from a text-based system to the current rich and interactive medium that supports images, 2D graphics, audio and video. The major media type that is still missing is 3D graphics. Although various approaches have been proposed (most notably VRML/X3D), they have not been widely adopted. One reason for the limited acceptance is the lack of 3D interaction techniques that are optimal for the hypertext-based Web interface. We present a novel strategy for accessing integrated information spaces, where hypertext and 3D graphics data are simultaneously available and linked. We introduce a user interface that has two modes between which a user can switch anytime: the driven by simple hypertext-based interactions \"don't-make-me-think\" mode, where a 3D scene is embedded in hypertext and the more immersive 3D \"take-me-to-the-Wonderland\" mode, which immerses the hypertextual annotations into the 3D scene. A user study is presented, which characterizes the user interface in terms of its efficiency and usability.", "keywords": ["user interfaces", "hypertext", "3d web", "user interface", "3d graphics"], "combined": "A dual-mode user interface for accessing 3D content on the world wide web The Web evolved from a text-based system to the current rich and interactive medium that supports images, 2D graphics, audio and video. The major media type that is still missing is 3D graphics. Although various approaches have been proposed (most notably VRML/X3D), they have not been widely adopted. One reason for the limited acceptance is the lack of 3D interaction techniques that are optimal for the hypertext-based Web interface. We present a novel strategy for accessing integrated information spaces, where hypertext and 3D graphics data are simultaneously available and linked. We introduce a user interface that has two modes between which a user can switch anytime: the driven by simple hypertext-based interactions \"don't-make-me-think\" mode, where a 3D scene is embedded in hypertext and the more immersive 3D \"take-me-to-the-Wonderland\" mode, which immerses the hypertextual annotations into the 3D scene. A user study is presented, which characterizes the user interface in terms of its efficiency and usability. [[EENNDD]] user interfaces; hypertext; 3d web; user interface; 3d graphics"}, "Antara muka pengguna dwi-mod untuk mengakses kandungan 3D di web seluruh dunia Web berkembang dari sistem berasaskan teks ke media yang kaya dan interaktif semasa yang menyokong gambar, grafik 2D, audio dan video. Jenis media utama yang masih ada ialah grafik 3D. Walaupun pelbagai pendekatan telah diusulkan (terutama VRML / X3D), pendekatan tersebut belum banyak digunakan. Salah satu sebab untuk penerimaan terhad adalah kurangnya teknik interaksi 3D yang optimum untuk antara muka Web berasaskan hiperteks. Kami menyajikan strategi baru untuk mengakses ruang maklumat bersepadu, di mana data grafik hiperteks dan 3D secara serentak tersedia dan dihubungkan. Kami memperkenalkan antara muka pengguna yang mempunyai dua mod di mana pengguna boleh beralih bila-bila masa: didorong oleh mod interaksi berasaskan hiperteks sederhana \"jangan buat-saya-pikir\", di mana adegan 3D tertanam dalam hiperteks dan yang lebih mendalam Mod \"take-me-to-the-Wonderland\" 3D, yang memasukkan anotasi hiperteksual ke dalam pemandangan 3D. Satu kajian pengguna disajikan, yang mencirikan antara muka pengguna dari segi kecekapan dan kegunaannya. [[EENNDD]] antara muka pengguna; hiperteks; Web 3d; antaramuka pengguna; Grafik 3d"], [{"string": "Querying and maintaining a compact XML storage As XML database sizes grow, the amount of space used for storing the data and auxiliary data structures becomes a major factor in query and update performance. This paper presents a new storage scheme for XML data that supports all navigational operations in near constant time. In addition to supporting efficient queries, the space requirement of the proposed scheme is within a constant factor of the information theoretic minimum, while insertions and deletions can be performed in near constant time as well. As a result, the proposed structure features a small memory footprint that increases cache locality, whilst still supporting standard APIs, such as DOM, and necessary database operations, such as queries and updates, efficiently. Analysis and experiments show that the proposed structure is space and time efficient.", "keywords": ["compact storage", "storage optimization", "query processing", "information storage", "xml"], "combined": "Querying and maintaining a compact XML storage As XML database sizes grow, the amount of space used for storing the data and auxiliary data structures becomes a major factor in query and update performance. This paper presents a new storage scheme for XML data that supports all navigational operations in near constant time. In addition to supporting efficient queries, the space requirement of the proposed scheme is within a constant factor of the information theoretic minimum, while insertions and deletions can be performed in near constant time as well. As a result, the proposed structure features a small memory footprint that increases cache locality, whilst still supporting standard APIs, such as DOM, and necessary database operations, such as queries and updates, efficiently. Analysis and experiments show that the proposed structure is space and time efficient. [[EENNDD]] compact storage; storage optimization; query processing; information storage; xml"}, "Meminta dan mengekalkan penyimpanan XML yang padat Seiring bertambahnya ukuran pangkalan data XML, jumlah ruang yang digunakan untuk menyimpan data dan struktur data tambahan menjadi faktor utama permintaan dan kemas kini prestasi. Makalah ini membentangkan skema penyimpanan baru untuk data XML yang menyokong semua operasi navigasi dalam waktu yang hampir berterusan. Sebagai tambahan untuk menyokong pertanyaan yang cekap, keperluan ruang dari skema yang dicadangkan berada dalam faktor tetap dari teori minimum maklumat, sementara penyisipan dan penghapusan juga dapat dilakukan dalam waktu yang hampir tetap. Akibatnya, struktur yang dicadangkan menampilkan jejak memori kecil yang meningkatkan lokasi cache, sementara masih menyokong API standard, seperti DOM, dan operasi pangkalan data yang diperlukan, seperti pertanyaan dan kemas kini, dengan cekap. Analisis dan eksperimen menunjukkan bahawa struktur yang dicadangkan adalah ruang dan masa yang cekap. [[EENNDD]] storan padat; pengoptimuman storan; pemprosesan pertanyaan; penyimpanan maklumat; xml"], [{"string": "A framework for coordinated multi-modal browsing with multiple clients No contact information provided yet.", "keywords": ["web proxy", "multi-modal browsing", "graphical user interfaces"], "combined": "A framework for coordinated multi-modal browsing with multiple clients No contact information provided yet. [[EENNDD]] web proxy; multi-modal browsing; graphical user interfaces"}, "Kerangka kerja untuk melayari pelbagai mod yang diselaraskan dengan beberapa pelanggan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] proksi web; melayari pelbagai mod; antara muka pengguna grafik"], [{"string": "Information spreading in context Information spreading processes are central to human interactions. Despite recent studies in online domains, little is known about factors that could affect the dissemination of a single piece of information. In this paper, we address this challenge by combining two related but distinct datasets, collected from a large scale privacy-preserving distributed social sensor system. We find that the social and organizational context significantly impacts to whom and how fast people forward information. Yet the structures within spreading processes can be well captured by a simple stochastic branching model, indicating surprising independence of context. Our results build the foundation of future predictive models of information flow and provide significant insights towards design of communication platforms.", "keywords": ["context", "information spread", "network science", "social networks", "structure"], "combined": "Information spreading in context Information spreading processes are central to human interactions. Despite recent studies in online domains, little is known about factors that could affect the dissemination of a single piece of information. In this paper, we address this challenge by combining two related but distinct datasets, collected from a large scale privacy-preserving distributed social sensor system. We find that the social and organizational context significantly impacts to whom and how fast people forward information. Yet the structures within spreading processes can be well captured by a simple stochastic branching model, indicating surprising independence of context. Our results build the foundation of future predictive models of information flow and provide significant insights towards design of communication platforms. [[EENNDD]] context; information spread; network science; social networks; structure"}, "Penyebaran maklumat dalam konteks Proses penyebaran maklumat merupakan pusat interaksi manusia. Walaupun terdapat kajian terbaru dalam domain dalam talian, tidak banyak yang diketahui mengenai faktor-faktor yang boleh mempengaruhi penyebaran satu maklumat. Dalam makalah ini, kami menangani cabaran ini dengan menggabungkan dua set data yang berkaitan tetapi berbeza, yang dikumpulkan dari sistem sensor sosial yang diedarkan untuk menjaga privasi skala besar. Kami mendapati bahawa konteks sosial dan organisasi memberi kesan yang signifikan kepada siapa dan seberapa pantas orang menyampaikan maklumat. Namun struktur dalam proses penyebaran dapat ditangkap dengan baik oleh model percabangan stokastik sederhana, yang menunjukkan kebebasan konteks yang mengejutkan. Hasil kami membina asas model ramalan aliran maklumat masa depan dan memberikan pandangan yang signifikan terhadap reka bentuk platform komunikasi. [[EENNDD]] konteks; penyebaran maklumat; sains rangkaian; rangkaian sosial; struktur"], [{"string": "Semantic resource management for the web: an e-learning application No contact information provided yet.", "keywords": ["computer uses in education", "document capture", "knowledge management", "semantic web", "e-learning"], "combined": "Semantic resource management for the web: an e-learning application No contact information provided yet. [[EENNDD]] computer uses in education; document capture; knowledge management; semantic web; e-learning"}, "Pengurusan sumber semantik untuk web: aplikasi e-pembelajaran Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penggunaan komputer dalam pendidikan; tangkapan dokumen; pengurusan pengetahuan; web semantik; e-pembelajaran"], [{"string": "Keynote talk No contact information provided yet.", "keywords": ["on-line information services", "world wide web"], "combined": "Keynote talk No contact information provided yet. [[EENNDD]] on-line information services; world wide web"}, "Ucapan utama Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perkhidmatan maklumat dalam talian; web seluruh dunia"], [{"string": "On ranking techniques for desktop search This paper addresses the desktop search problem by considering varioustechniques for ranking results of a search query over thefile system. First, basic ranking techniques, which are based ona single file feature (e.g., file name, file content, access date, etc.)are considered. Next, two learning-based ranking schemes are presented, and are shown to be significantly more effective than the basic ranking methods. Finally, a novel ranking technique, based on query selectiveness is considered,for use during the cold-start period of the system. This method isalso shown to be empirically effective, even though it does notinvolve any learning.", "keywords": ["personal information management", "information search and retrieval", "ranking", "desktop search"], "combined": "On ranking techniques for desktop search This paper addresses the desktop search problem by considering varioustechniques for ranking results of a search query over thefile system. First, basic ranking techniques, which are based ona single file feature (e.g., file name, file content, access date, etc.)are considered. Next, two learning-based ranking schemes are presented, and are shown to be significantly more effective than the basic ranking methods. Finally, a novel ranking technique, based on query selectiveness is considered,for use during the cold-start period of the system. This method isalso shown to be empirically effective, even though it does notinvolve any learning. [[EENNDD]] personal information management; information search and retrieval; ranking; desktop search"}, "Mengenai teknik pemeringkatan untuk carian desktop Makalah ini mengatasi masalah pencarian desktop dengan mempertimbangkan teknik-teknik varioust untuk pemeringkatan hasil dari pertanyaan pencarian terhadap sistem file. Pertama, teknik peringkat asas, yang berdasarkan pada ciri satu fail (misalnya, nama fail, kandungan fail, tarikh akses, dll.) Dipertimbangkan. Seterusnya, dua skema peringkat berdasarkan pembelajaran disajikan, dan terbukti jauh lebih berkesan daripada kaedah peringkat asas. Akhirnya, teknik pemeringkatan novel, berdasarkan selektif pertanyaan dipertimbangkan, untuk digunakan dalam tempoh permulaan sistem. Kaedah ini juga terbukti berkesan secara empirik, walaupun tidak melibatkan pembelajaran. [[EENNDD]] pengurusan maklumat peribadi; pencarian dan pengambilan maklumat; peringkat; carian desktop"], [{"string": "MedSearch: a specialized search engine for medical information People are thirsty for medical information. Existing Web search engines cannot handle medical search well because they do not consider its special requirements. Often a medical information searcher is uncertain about his exact questions and unfamiliar with medical terminology. Therefore, he prefers to pose long queries, describing his symptoms and situation in plain English, and receive comprehensive, relevant information from search results. This paper presents MedSearch, a specialized medical Web search engine, to address these challenges. MedSearch can assist ordinary Internet users to search for medical information, by accepting queries of extended length, providing diversified search results, and suggesting related medical phrases.", "keywords": ["medical web search engine", "medical query"], "combined": "MedSearch: a specialized search engine for medical information People are thirsty for medical information. Existing Web search engines cannot handle medical search well because they do not consider its special requirements. Often a medical information searcher is uncertain about his exact questions and unfamiliar with medical terminology. Therefore, he prefers to pose long queries, describing his symptoms and situation in plain English, and receive comprehensive, relevant information from search results. This paper presents MedSearch, a specialized medical Web search engine, to address these challenges. MedSearch can assist ordinary Internet users to search for medical information, by accepting queries of extended length, providing diversified search results, and suggesting related medical phrases. [[EENNDD]] medical web search engine; medical query"}, "MedSearch: enjin carian khusus untuk maklumat perubatan Orang ramai haus akan maklumat perubatan. Enjin carian Web yang ada tidak dapat menangani carian perubatan dengan baik kerana tidak mempertimbangkan keperluan khasnya. Selalunya pencari maklumat perubatan tidak pasti mengenai soalannya yang tepat dan tidak biasa dengan istilah perubatan. Oleh itu, dia lebih suka mengemukakan pertanyaan panjang, menjelaskan gejala dan situasinya dalam bahasa Inggeris biasa, dan menerima maklumat yang relevan dan komprehensif dari hasil carian. Makalah ini menyajikan MedSearch, mesin carian Web perubatan khusus, untuk menangani cabaran ini. MedSearch dapat membantu pengguna Internet biasa untuk mencari maklumat perubatan, dengan menerima pertanyaan yang panjang, memberikan hasil carian yang pelbagai, dan mencadangkan frasa perubatan yang berkaitan. [[EENNDD]] enjin carian web perubatan; pertanyaan perubatan"], [{"string": "Answering relationship queries on the web Finding relationships between entities on the Web, e.g., the connections between different places or the commonalities of people, is a novel and challenging problem. Existing Web search engines excel in keyword matching and document ranking, but they cannot well handle many relationship queries. This paper proposes a new method for answering relationship queries on two entities. Our method first respectively retrieves the top Web pages for either entity from a Web search engine. It then matches these Web pages and generates an ordered list of Web page pairs. Each Web page pair consists of one Web page for either entity. The top ranked Web page pairs are likely to contain the relationships between the two entities. One main challenge in the ranking process is to effectively filter out the large amount of noise in the Web pages without losing much useful information. To achieve this, our method assigns appropriate weights to terms in Web pages and intelligently identifies the potential connecting terms that capture the relationships between the two entities. Only those top potential connecting terms with large weights are used to rank Web page pairs. Finally, the top ranked Web page pairs are presented to the searcher. For each such pair, the query terms and the top potential connecting terms are properly highlighted so that the relationships between the two entities can be easily identified. We implemented a prototype on top of the Google search engine and evaluated it under a wide variety of query scenarios. The experimental results show that our method is effective at finding important relationships with low overhead.", "keywords": ["web search", "relationship query"], "combined": "Answering relationship queries on the web Finding relationships between entities on the Web, e.g., the connections between different places or the commonalities of people, is a novel and challenging problem. Existing Web search engines excel in keyword matching and document ranking, but they cannot well handle many relationship queries. This paper proposes a new method for answering relationship queries on two entities. Our method first respectively retrieves the top Web pages for either entity from a Web search engine. It then matches these Web pages and generates an ordered list of Web page pairs. Each Web page pair consists of one Web page for either entity. The top ranked Web page pairs are likely to contain the relationships between the two entities. One main challenge in the ranking process is to effectively filter out the large amount of noise in the Web pages without losing much useful information. To achieve this, our method assigns appropriate weights to terms in Web pages and intelligently identifies the potential connecting terms that capture the relationships between the two entities. Only those top potential connecting terms with large weights are used to rank Web page pairs. Finally, the top ranked Web page pairs are presented to the searcher. For each such pair, the query terms and the top potential connecting terms are properly highlighted so that the relationships between the two entities can be easily identified. We implemented a prototype on top of the Google search engine and evaluated it under a wide variety of query scenarios. The experimental results show that our method is effective at finding important relationships with low overhead. [[EENNDD]] web search; relationship query"}, "Menjawab pertanyaan hubungan di web Mencari hubungan antara entiti di Web, misalnya, hubungan antara tempat yang berlainan atau kesamaan orang, adalah masalah baru dan mencabar. Mesin carian Web yang ada unggul dalam pencocokan kata kunci dan kedudukan dokumen, tetapi mereka tidak dapat menangani banyak pertanyaan hubungan dengan baik. Makalah ini mencadangkan kaedah baru untuk menjawab pertanyaan hubungan pada dua entiti. Kaedah kami masing-masing mendapatkan halaman Web teratas untuk kedua-dua entiti dari enjin carian Web. Kemudian sepadan dengan halaman Web ini dan menghasilkan senarai pasangan halaman Web yang teratur. Setiap pasangan halaman Web terdiri daripada satu halaman Web untuk kedua entiti. Pasangan laman web peringkat teratas cenderung mengandungi hubungan antara kedua-dua entiti tersebut. Satu cabaran utama dalam proses pemeringkatan adalah dengan berkesan menyaring sejumlah besar kebisingan di laman Web tanpa kehilangan banyak maklumat yang berguna. Untuk mencapai ini, kaedah kami memberikan bobot yang sesuai untuk istilah di laman Web dan mengenal pasti istilah penyambungan yang berpotensi menangkap hubungan antara kedua-dua entiti tersebut. Hanya istilah penghubung berpotensi teratas dengan bobot besar yang digunakan untuk menentukan pasangan halaman Web. Akhirnya, pasangan halaman Web peringkat teratas dipersembahkan kepada pencari. Untuk setiap pasangan tersebut, istilah pertanyaan dan istilah penghubung berpotensi teratas disorot dengan betul sehingga hubungan antara kedua-dua entiti dapat dikenali dengan mudah. Kami melaksanakan prototaip di atas enjin carian Google dan mengevaluasinya di bawah pelbagai senario pertanyaan. Hasil eksperimen menunjukkan bahawa kaedah kami berkesan dalam mencari hubungan penting dengan overhead rendah. [[EENNDD]] carian web; pertanyaan hubungan"], [{"string": "Advertising keyword generation using active learning This paper proposes an efficient relevance feedback based interactive model for keyword generation in sponsored search advertising. We formulate the ranking of relevant terms as a supervised learning problem and suggest new terms for the seed by leveraging user relevance feedback information. Active learning is employed to select the most informative samples from a set of candidate terms for user labeling. Experiments show our approach improves the relevance of generated terms significantly with little user effort required.", "keywords": ["on-line information services", "active learning", "sponsored search", "keyword generation"], "combined": "Advertising keyword generation using active learning This paper proposes an efficient relevance feedback based interactive model for keyword generation in sponsored search advertising. We formulate the ranking of relevant terms as a supervised learning problem and suggest new terms for the seed by leveraging user relevance feedback information. Active learning is employed to select the most informative samples from a set of candidate terms for user labeling. Experiments show our approach improves the relevance of generated terms significantly with little user effort required. [[EENNDD]] on-line information services; active learning; sponsored search; keyword generation"}, "Iklan penjanaan kata kunci menggunakan pembelajaran aktif Makalah ini mencadangkan model interaktif berasaskan maklum balas relevan yang berkesan untuk penjanaan kata kunci dalam iklan carian tajaan. Kami merumuskan peringkat istilah yang relevan sebagai masalah pembelajaran yang diawasi dan mencadangkan istilah baru untuk benih dengan memanfaatkan maklumat maklum balas relevan pengguna. Pembelajaran aktif digunakan untuk memilih sampel yang paling bermaklumat dari satu set syarat calon untuk pelabelan pengguna. Eksperimen menunjukkan pendekatan kami meningkatkan relevansi istilah yang dihasilkan dengan ketara dengan usaha pengguna yang sedikit diperlukan. [[EENNDD]] perkhidmatan maklumat dalam talian; pembelajaran aktif; carian tajaan; penghasilan kata kunci"], [{"string": "Consideration set generation in commerce search In commerce search, the set of products returned by a search engine often forms the basis for all user interactions leading up to a potential transaction on the web. Such a set of products is known as the consideration set. In this study, we consider the problem of generating consideration set of products in commerce search so as to maximize user satisfaction. One of the key features of commerce search that we exploit in our study is the association of a set of important attributes with the products and a set of specified attributes with the user queries. Those important attributes not used in the query are treated as unspecified. The attribute space admits a natural definition of user satisfaction via user preferences on the attributes and their values, viz. require that the surfaced products be close to the specified attribute values in the query, and diverse with respect to the unspecified attributes. We model this as a general Max-Sum Dispersion problem wherein we are given a set of n nodes in a metric space and the objective is to select a subset of nodes with total cost at most a given budget, and maximize the sum of the pairwise distances between the selected nodes. In our setting, each node denotes a product, the cost of a node being inversely proportional to its relevance with respect to specified attributes. The distance between two nodes quantifies the diversity with respect to the unspecified attributes. The problem is NP-hard and a 2-approximation was previously known only when all the nodes have unit cost.", "keywords": ["novelty", "information search and retrieval", "facility dispersion", "relevance", "approximation algorithms"], "combined": "Consideration set generation in commerce search In commerce search, the set of products returned by a search engine often forms the basis for all user interactions leading up to a potential transaction on the web. Such a set of products is known as the consideration set. In this study, we consider the problem of generating consideration set of products in commerce search so as to maximize user satisfaction. One of the key features of commerce search that we exploit in our study is the association of a set of important attributes with the products and a set of specified attributes with the user queries. Those important attributes not used in the query are treated as unspecified. The attribute space admits a natural definition of user satisfaction via user preferences on the attributes and their values, viz. require that the surfaced products be close to the specified attribute values in the query, and diverse with respect to the unspecified attributes. We model this as a general Max-Sum Dispersion problem wherein we are given a set of n nodes in a metric space and the objective is to select a subset of nodes with total cost at most a given budget, and maximize the sum of the pairwise distances between the selected nodes. In our setting, each node denotes a product, the cost of a node being inversely proportional to its relevance with respect to specified attributes. The distance between two nodes quantifies the diversity with respect to the unspecified attributes. The problem is NP-hard and a 2-approximation was previously known only when all the nodes have unit cost. [[EENNDD]] novelty; information search and retrieval; facility dispersion; relevance; approximation algorithms"}, "Pembentukan set pertimbangan dalam pencarian perdagangan Dalam carian perdagangan, sekumpulan produk yang dikembalikan oleh mesin pencari sering menjadi dasar untuk semua interaksi pengguna yang mengarah ke potensi transaksi di web. Set produk sedemikian dikenali sebagai set pertimbangan. Dalam kajian ini, kami mempertimbangkan masalah menghasilkan set pertimbangan produk dalam pencarian perdagangan sehingga dapat memaksimumkan kepuasan pengguna. Salah satu ciri utama carian perdagangan yang kami eksploitasi dalam kajian kami adalah perkaitan satu set atribut penting dengan produk dan satu set atribut yang ditentukan dengan pertanyaan pengguna. Atribut penting yang tidak digunakan dalam pertanyaan dianggap tidak ditentukan. Ruang atribut mengakui definisi semula jadi kepuasan pengguna melalui pilihan pengguna terhadap atribut dan nilai-nilai mereka, iaitu. menghendaki produk yang dilampirkan mendekati nilai atribut yang ditentukan dalam pertanyaan, dan beragam berkaitan dengan atribut yang tidak ditentukan. Kami memodelkannya sebagai masalah Penyebaran Jumlah Maksimum yang umum di mana kita diberi satu set node dalam ruang metrik dan objektifnya adalah memilih subset nod dengan jumlah kos paling banyak anggaran, dan memaksimumkan jumlah pasangan jarak antara nod yang dipilih. Dalam tetapan kami, setiap simpul menandakan produk, kos simpul berbanding terbalik dengan kaitannya dengan atribut yang ditentukan. Jarak antara dua nod mengukur kepelbagaian berkenaan dengan atribut yang tidak ditentukan. Masalahnya adalah NP-hard dan penghampiran 2 sebelumnya hanya diketahui apabila semua nod mempunyai kos seunit. [[EENNDD]] kebaruan; carian dan pengambilan maklumat; penyebaran kemudahan; perkaitan; algoritma penghampiran"], [{"string": "sMash: semantic-based mashup navigation for data API network With the proliferation of data APIs, it is not uncommon that users who have no clear ideas about data APIs will encounter difficulties to build Mashups to satisfy their requirements. In this paper, we present a semantic-based mashup navigation system, sMash that makes mashup building easy by constructing and visualizing a real-life data API network. We build a sample network by gathering more than 300 popular APIs and find that the relationships between them are so complex that our system will play an important role in navigating users and give them inspiration to build interesting mashups easily. The system is accessible at: http://www.dart.zju.edu.cn/mashup.", "keywords": ["semantic", "data api network", "mashup navigation", "miscellaneous", "social"], "combined": "sMash: semantic-based mashup navigation for data API network With the proliferation of data APIs, it is not uncommon that users who have no clear ideas about data APIs will encounter difficulties to build Mashups to satisfy their requirements. In this paper, we present a semantic-based mashup navigation system, sMash that makes mashup building easy by constructing and visualizing a real-life data API network. We build a sample network by gathering more than 300 popular APIs and find that the relationships between them are so complex that our system will play an important role in navigating users and give them inspiration to build interesting mashups easily. The system is accessible at: http://www.dart.zju.edu.cn/mashup. [[EENNDD]] semantic; data api network; mashup navigation; miscellaneous; social"}, "sMash: navigasi mashup berasaskan semantik untuk rangkaian data API Dengan percambahan API data, tidak jarang pengguna yang tidak mempunyai idea yang jelas tentang API data akan menghadapi kesulitan untuk membina Mashup untuk memenuhi keperluan mereka. Dalam makalah ini, kami menyajikan sistem navigasi mashup berbasis semantik, sMash yang menjadikan bangunan mashup mudah dengan membina dan memvisualisasikan rangkaian API data kehidupan nyata. Kami membina rangkaian sampel dengan mengumpulkan lebih dari 300 API popular dan mendapati bahawa hubungan antara mereka sangat kompleks sehingga sistem kami akan memainkan peranan penting dalam menavigasi pengguna dan memberi mereka inspirasi untuk membina mashup yang menarik dengan mudah. Sistem ini boleh diakses di: http://www.dart.zju.edu.cn/mashup. [[EENNDD]] semantik; rangkaian api data; navigasi mashup; pelbagai; sosial"], [{"string": "Staging transformations for multimodal web interaction management No contact information provided yet.", "keywords": ["program transformations", "out-of-turn interaction", "web dialogs", "mixed-initiative interaction"], "combined": "Staging transformations for multimodal web interaction management No contact information provided yet. [[EENNDD]] program transformations; out-of-turn interaction; web dialogs; mixed-initiative interaction"}, "Transformasi berperingkat untuk pengurusan interaksi web multimodal Belum ada maklumat hubungan yang diberikan. [[EENNDD]] transformasi program; interaksi di luar giliran; dialog web; interaksi inisiatif campuran"], [{"string": "Hybrid keyword search auctions Search auctions have become a dominant source of revenue generation on the Internet. Such auctions have typically used per-click bidding and pricing. We propose the use of hybrid auctions where an advertiser can make a per-impression as well as a per-click bid, and the auctioneer then chooses one of the two as the pricing mechanism. We assume that the advertiser and the auctioneer both have separate beliefs (called priors) on the click-probability of an advertisement. We first prove that the hybrid auction is truthful, assuming that the advertisers are risk-neutral. We then show that this auction is superior to the existing per-click auction in multiple ways: We show that risk-seeking advertisers will choose only a per-impression bid whereas risk-averse advertisers will choose only a per-click bid, and argue that both kind of advertisers arise naturally. Hence, the ability to bid in a hybrid fashion is important to account for the risk characteristics of the advertisers. For obscure keywords, the auctioneer is unlikely to have a very sharp prior on the click-probabilities. In such situations, we show that having the extra information from the advertisers in the form of a per-impression bid can result in significantly higher revenue. An advertiser who believes that its click-probability is much higher than the auctioneer's estimate can use per-impression bids to correct the auctioneer's prior without incurring any extra cost. The hybrid auction can allow the advertiser and auctioneer to implement complex dynamic programming strategies to deal with the uncertainty in the click-probability using the same basic auction. The per-click and per-impression bidding schemes can only be used to implement two extreme cases of these strategies. As Internet commerce matures, we need more sophisticated pricing models to exploit all the information held by each of the participants. We believe that hybrid auctions could be an important step in this direction. The hybrid auction easily extends to multiple slots, and is also applicable to scenarios where the hybrid bidding is per-impression and per-action (i.e. CPM and CPA), or per-click and per-action (i.e. CPC and CPA).", "keywords": ["internet", "keyword auctions", "nonnumerical algorithms and problems", "mechanism design"], "combined": "Hybrid keyword search auctions Search auctions have become a dominant source of revenue generation on the Internet. Such auctions have typically used per-click bidding and pricing. We propose the use of hybrid auctions where an advertiser can make a per-impression as well as a per-click bid, and the auctioneer then chooses one of the two as the pricing mechanism. We assume that the advertiser and the auctioneer both have separate beliefs (called priors) on the click-probability of an advertisement. We first prove that the hybrid auction is truthful, assuming that the advertisers are risk-neutral. We then show that this auction is superior to the existing per-click auction in multiple ways: We show that risk-seeking advertisers will choose only a per-impression bid whereas risk-averse advertisers will choose only a per-click bid, and argue that both kind of advertisers arise naturally. Hence, the ability to bid in a hybrid fashion is important to account for the risk characteristics of the advertisers. For obscure keywords, the auctioneer is unlikely to have a very sharp prior on the click-probabilities. In such situations, we show that having the extra information from the advertisers in the form of a per-impression bid can result in significantly higher revenue. An advertiser who believes that its click-probability is much higher than the auctioneer's estimate can use per-impression bids to correct the auctioneer's prior without incurring any extra cost. The hybrid auction can allow the advertiser and auctioneer to implement complex dynamic programming strategies to deal with the uncertainty in the click-probability using the same basic auction. The per-click and per-impression bidding schemes can only be used to implement two extreme cases of these strategies. As Internet commerce matures, we need more sophisticated pricing models to exploit all the information held by each of the participants. We believe that hybrid auctions could be an important step in this direction. The hybrid auction easily extends to multiple slots, and is also applicable to scenarios where the hybrid bidding is per-impression and per-action (i.e. CPM and CPA), or per-click and per-action (i.e. CPC and CPA). [[EENNDD]] internet; keyword auctions; nonnumerical algorithms and problems; mechanism design"}, "Lelong carian kata kunci hibrid Lelong carian telah menjadi sumber penjanaan pendapatan yang dominan di Internet. Lelongan seperti ini biasanya menggunakan penawaran dan harga per klik. Kami mencadangkan penggunaan lelang hibrid di mana pengiklan dapat membuat tawaran per tayangan serta tawaran per klik, dan pelelong kemudian memilih salah satu dari keduanya sebagai mekanisme penetapan harga. Kami mengandaikan bahawa pengiklan dan pelelong kedua-duanya mempunyai kepercayaan yang terpisah (disebut sebelumnya) mengenai kemungkinan klik iklan. Mula-mula kami membuktikan bahawa lelong kacukan itu benar, dengan anggapan bahawa pengiklan tidak berisiko. Kami kemudian menunjukkan bahawa lelongan ini lebih unggul daripada lelang per klik yang ada dalam pelbagai cara: Kami menunjukkan bahawa pengiklan yang mencari risiko hanya akan memilih tawaran per tayangan sedangkan pengiklan yang menghindari risiko hanya akan memilih tawaran per klik, dan berpendapat bahawa kedua-dua jenis pengiklan timbul secara semula jadi. Oleh itu, kemampuan untuk membuat tawaran secara hibrid adalah penting untuk mengambil kira ciri risiko pengiklan. Untuk kata kunci yang tidak jelas, pelelong tidak mungkin mempunyai kecerdasan yang lebih tinggi sebelum kemungkinan klik. Dalam situasi seperti itu, kami menunjukkan bahawa memiliki maklumat tambahan dari pengiklan dalam bentuk tawaran per tayangan dapat menghasilkan pendapatan yang jauh lebih tinggi. Pengiklan yang percaya bahawa kebarangkalian kliknya jauh lebih tinggi daripada anggaran pelelong boleh menggunakan tawaran per tera untuk membetulkan pelelong sebelumnya tanpa menanggung kos tambahan. Lelong hibrid boleh membolehkan pengiklan dan pelelong melaksanakan strategi pengaturcaraan dinamik yang kompleks untuk menangani ketidakpastian kemungkinan klik menggunakan lelong asas yang sama. Skema penawaran per klik dan per kesan hanya dapat digunakan untuk melaksanakan dua kes ekstrim strategi ini. Apabila perdagangan Internet semakin matang, kita memerlukan model harga yang lebih canggih untuk memanfaatkan semua maklumat yang dimiliki oleh setiap peserta. Kami percaya bahawa lelong hibrid boleh menjadi langkah penting ke arah ini. Lelongan hibrid dengan mudah merangkumi beberapa slot, dan juga berlaku untuk senario di mana penawaran hibrid adalah per tayangan dan per tindakan (iaitu CPM dan CPA), atau per klik dan per tindakan (iaitu CPC dan CPA). [[EENNDD]] internet; lelongan kata kunci; algoritma dan masalah bukan berangka; reka bentuk mekanisme"], [{"string": "Design of a crawler with bounded bandwidth No contact information provided yet.", "keywords": ["bandwidth optimization", "parallel web crawlers", "information storage and retrieval"], "combined": "Design of a crawler with bounded bandwidth No contact information provided yet. [[EENNDD]] bandwidth optimization; parallel web crawlers; information storage and retrieval"}, "Reka bentuk crawler dengan lebar jalur terikat Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengoptimuman lebar jalur; perayap web selari; penyimpanan dan pengambilan maklumat"], [{"string": "Subspace: secure cross-domain communication for web mashups Combining data and code from third-party sources has enabled a new wave of web mashups that add creativity and functionality to web applications. However, browsers are poorly designed to pass data between domains, often forcing web developers to abandon security in the name of functionality. To address this deficiency, we developed Subspace, a cross-domain communication mechanism that allows efficient communication across domains without sacrificing security. Our prototype requires only a small JavaScript library, and works across all major browsers. We believe Subspace can serve as a new secure communication primitive for web mashups.", "keywords": ["unauthorized access", "same origin policy", "access control", "web services", "trust"], "combined": "Subspace: secure cross-domain communication for web mashups Combining data and code from third-party sources has enabled a new wave of web mashups that add creativity and functionality to web applications. However, browsers are poorly designed to pass data between domains, often forcing web developers to abandon security in the name of functionality. To address this deficiency, we developed Subspace, a cross-domain communication mechanism that allows efficient communication across domains without sacrificing security. Our prototype requires only a small JavaScript library, and works across all major browsers. We believe Subspace can serve as a new secure communication primitive for web mashups. [[EENNDD]] unauthorized access; same origin policy; access control; web services; trust"}, "Subspace: komunikasi rentas domain yang selamat untuk mashup web Menggabungkan data dan kod dari sumber pihak ketiga telah memungkinkan gelombang baru mashup web yang menambahkan kreativiti dan fungsi ke aplikasi web. Walau bagaimanapun, penyemak imbas tidak dirancang dengan baik untuk menyebarkan data antara domain, sering memaksa pembangun web untuk meninggalkan keselamatan atas nama fungsi. Untuk mengatasi kekurangan ini, kami mengembangkan Subspace, mekanisme komunikasi rentas domain yang memungkinkan komunikasi yang efisien di seluruh domain tanpa mengorbankan keamanan. Prototaip kami hanya memerlukan pustaka JavaScript kecil, dan berfungsi di semua penyemak imbas utama. Kami percaya Subspace dapat berfungsi sebagai primitif komunikasi selamat baru untuk mashup web. [[EENNDD]] akses tanpa kebenaran; dasar asal yang sama; kawalan akses; perkhidmatan web; kepercayaan"], [{"string": "2lip: the step towards the web3d The World Wide Web allows users to create and publish a variety of resources, including multimedia ones. Most of the contemporary best practices for designing web interfaces, however, do not take into account the 3D techniques.", "keywords": ["user interfaces", "wikipedia", "web3d", "wpf"], "combined": "2lip: the step towards the web3d The World Wide Web allows users to create and publish a variety of resources, including multimedia ones. Most of the contemporary best practices for designing web interfaces, however, do not take into account the 3D techniques. [[EENNDD]] user interfaces; wikipedia; web3d; wpf"}, "2lip: langkah menuju ke web3d World Wide Web membolehkan pengguna membuat dan menerbitkan pelbagai sumber, termasuk multimedia. Sebilangan besar amalan terbaik kontemporari untuk merancang antara muka web, bagaimanapun, tidak mengambil kira teknik 3D. [[EENNDD]] antara muka pengguna; wikipedia; web3d; wpf"], [{"string": "The role of social networks in information diffusion Online social networking technologies enable individuals to simultaneously share information with any number of peers. Quantifying the causal effect of these mediums on the dissemination of information requires not only identification of who influences whom, but also of whether individuals would still propagate information in the absence of social signals about that information. We examine the role of social networks in online information diffusion with a large-scale field experiment that randomizes exposure to signals about friends' information sharing among 253 million subjects in situ. Those who are exposed are significantly more likely to spread information, and do so sooner than those who are not exposed. We further examine the relative role of strong and weak ties in information propagation. We show that, although stronger ties are individually more influential, it is the more abundant weak ties who are responsible for the propagation of novel information. This suggests that weak ties may play a more dominant role in the dissemination of information online than currently believed.", "keywords": ["causality", "social influence", "tie strength"], "combined": "The role of social networks in information diffusion Online social networking technologies enable individuals to simultaneously share information with any number of peers. Quantifying the causal effect of these mediums on the dissemination of information requires not only identification of who influences whom, but also of whether individuals would still propagate information in the absence of social signals about that information. We examine the role of social networks in online information diffusion with a large-scale field experiment that randomizes exposure to signals about friends' information sharing among 253 million subjects in situ. Those who are exposed are significantly more likely to spread information, and do so sooner than those who are not exposed. We further examine the relative role of strong and weak ties in information propagation. We show that, although stronger ties are individually more influential, it is the more abundant weak ties who are responsible for the propagation of novel information. This suggests that weak ties may play a more dominant role in the dissemination of information online than currently believed. [[EENNDD]] causality; social influence; tie strength"}, "Peranan rangkaian sosial dalam penyebaran maklumat Teknologi rangkaian sosial dalam talian membolehkan individu berkongsi maklumat secara serentak dengan sebilangan rakan sebaya. Mengukur kesan kausal media ini pada penyebaran maklumat bukan sahaja memerlukan pengenalpastian siapa yang mempengaruhi siapa, tetapi juga sama ada individu akan menyebarkan maklumat sekiranya tidak ada isyarat sosial mengenai maklumat tersebut. Kami mengkaji peranan rangkaian sosial dalam penyebaran maklumat dalam talian dengan eksperimen lapangan berskala besar yang secara rawak memaparkan isyarat mengenai perkongsian maklumat rakan di antara 253 juta subjek di situ. Mereka yang terdedah secara signifikan lebih cenderung menyebarkan maklumat, dan melakukannya lebih cepat daripada mereka yang tidak terdedah. Kami seterusnya mengkaji peranan relatif hubungan kuat dan lemah dalam penyebaran maklumat. Kami menunjukkan bahawa, walaupun hubungan yang lebih kuat secara individu lebih berpengaruh, hubungan yang lebih kuat adalah pertalian yang bertanggungjawab untuk menyebarkan maklumat baru. Ini menunjukkan bahawa hubungan yang lemah mungkin memainkan peranan yang lebih dominan dalam penyebaran maklumat secara dalam talian daripada yang diyakini sekarang. [[EENNDD]] sebab akibat; pengaruh sosial; kekuatan tali leher"], [{"string": "Web projections: learning from contextual subgraphs of the web Graphical relationships among Web pages have been exploited inmethods for ranking search results. To date, specific graphicalproperties have been used in these analyses. We introduce a WebProjection methodology that generalizes prior efforts of graphicalrelationships of the web in several ways. With the approach, wecreate subgraphs by projecting sets of pages and domains onto thelarger web graph, and then use machine learning to constructpredictive models that consider graphical properties as evidence. Wedescribe the method and then present experiments that illustrate theconstruction of predictive models of search result quality and userquery reformulation.", "keywords": ["query reformulation", "web graph", "web search", "web projection", "contextual subgraph"], "combined": "Web projections: learning from contextual subgraphs of the web Graphical relationships among Web pages have been exploited inmethods for ranking search results. To date, specific graphicalproperties have been used in these analyses. We introduce a WebProjection methodology that generalizes prior efforts of graphicalrelationships of the web in several ways. With the approach, wecreate subgraphs by projecting sets of pages and domains onto thelarger web graph, and then use machine learning to constructpredictive models that consider graphical properties as evidence. Wedescribe the method and then present experiments that illustrate theconstruction of predictive models of search result quality and userquery reformulation. [[EENNDD]] query reformulation; web graph; web search; web projection; contextual subgraph"}, "Unjuran web: belajar dari subgraf kontekstual web Hubungan grafik antara laman web telah dimanfaatkan kaedah untuk menentukan hasil carian. Sehingga kini, sifat grafik khusus telah digunakan dalam analisis ini. Kami memperkenalkan metodologi WebProjection yang menggeneralisasi usaha-usaha sebelumnya mengenai hubungan grafik web dengan beberapa cara. Dengan pendekatan tersebut, buat subgraf dengan memproyeksikan set halaman dan domain ke grafik web yang lebih tinggi, dan kemudian gunakan pembelajaran mesin untuk membina model ramalan yang menganggap sifat grafik sebagai bukti. Jelaskan kaedah dan kemudian tunjukkan eksperimen yang menggambarkan pembinaan model ramalan kualiti hasil carian dan penyusunan semula pertanyaan pengguna. [[EENNDD]] penyusunan semula pertanyaan; grafik web; carian sesawang; unjuran web; subgraf kontekstual"], [{"string": "Keys for XML An abstract is not available.", "keywords": ["relative keys", "logical design", "keys", "xml"], "combined": "Keys for XML An abstract is not available. [[EENNDD]] relative keys; logical design; keys; xml"}, "Kekunci untuk XML Abstrak tidak tersedia. [[EENNDD]] kunci relatif; reka bentuk logik; kunci; xml"], [{"string": "Detecting soft errors by redirection classification A soft error redirection is a URL redirection to a page that returns the HTTP status code 200 (OK) but has actually no relevant content to the client request. Since such redirections degrade the performance of web search engines in many ways, it is highly desirable to remove as many of them as possible. We propose a novel approach to detect soft error redirections by analyzing redirection logs collected during crawling operation. Experimental results on huge crawl data show that our measure can classify soft error redirections effectively.", "keywords": ["search engine", "soft 404", "information search and retrieval", "spam", "url redirection"], "combined": "Detecting soft errors by redirection classification A soft error redirection is a URL redirection to a page that returns the HTTP status code 200 (OK) but has actually no relevant content to the client request. Since such redirections degrade the performance of web search engines in many ways, it is highly desirable to remove as many of them as possible. We propose a novel approach to detect soft error redirections by analyzing redirection logs collected during crawling operation. Experimental results on huge crawl data show that our measure can classify soft error redirections effectively. [[EENNDD]] search engine; soft 404; information search and retrieval; spam; url redirection"}, "Mengesan kesalahan lembut dengan klasifikasi pengalihan Redirection kesalahan lembut adalah pengalihan URL ke halaman yang mengembalikan kod status HTTP 200 (OK) tetapi sebenarnya tidak mempunyai kandungan yang relevan dengan permintaan klien. Oleh kerana pengalihan seperti itu menurunkan prestasi enjin carian web dalam banyak cara, sangat wajar untuk membuang seberapa banyak yang mungkin. Kami mencadangkan pendekatan baru untuk mengesan pengalihan ralat lembut dengan menganalisis log pengalihan yang dikumpulkan semasa operasi merangkak. Hasil eksperimen pada data perayapan besar menunjukkan bahawa ukuran kami dapat mengklasifikasikan pengalihan ralat lembut dengan berkesan. [[EENNDD]] enjin carian; lembut 404; pencarian dan pengambilan maklumat; spam; pengalihan url"], [{"string": "Mining the peanut gallery: opinion extraction and semantic classification of product reviews No contact information provided yet.", "keywords": ["document classification", "opinion mining"], "combined": "Mining the peanut gallery: opinion extraction and semantic classification of product reviews No contact information provided yet. [[EENNDD]] document classification; opinion mining"}, "Melombong galeri kacang: pengambilan pendapat dan semantik klasifikasi ulasan produk Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengelasan dokumen; perlombongan pendapat"], [{"string": "Web data extraction based on partial tree alignment No contact information provided yet.", "keywords": ["miscellaneous", "wrapper", "data extraction", "data record extraction"], "combined": "Web data extraction based on partial tree alignment No contact information provided yet. [[EENNDD]] miscellaneous; wrapper; data extraction; data record extraction"}, "Pengekstrakan data web berdasarkan penjajaran separa pokok Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pelbagai; pembungkus; pengekstrakan data; pengekstrakan rekod data"], [{"string": "The credibility of the posted information in a recommendation system based on a map No contact information provided yet.", "keywords": ["gis", "credibility", "recommendation", "navigation", "posting", "types of systems"], "combined": "The credibility of the posted information in a recommendation system based on a map No contact information provided yet. [[EENNDD]] gis; credibility; recommendation; navigation; posting; types of systems"}, "Kebolehpercayaan maklumat yang disiarkan dalam sistem cadangan berdasarkan peta Belum ada maklumat hubungan yang diberikan. [[EENNDD]] gis; kredibiliti; cadangan; pelayaran; pengeposan; jenis sistem"], [{"string": "Searching the workplace web No contact information provided yet.", "keywords": ["information search and retrieval"], "combined": "Searching the workplace web No contact information provided yet. [[EENNDD]] information search and retrieval"}, "Mencari di laman web tempat kerja Belum ada maklumat hubungan. [[EENNDD]] carian dan pengambilan maklumat"], [{"string": "Merkle tree authentication of HTTP responses No contact information provided yet.", "keywords": ["merkle hash tree", "authenticity", "web content distribution"], "combined": "Merkle tree authentication of HTTP responses No contact information provided yet. [[EENNDD]] merkle hash tree; authenticity; web content distribution"}, "Pengesahan pokok Merkle untuk respons HTTP Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] merkle hash tree; kesahihan; pengedaran kandungan web"], [{"string": "Summarization of online image collections via implicit feedback The availability of map interfaces and location-aware devices makes a growing amount of unstructured, geo-referenced information available on the Web. In particular, over twelve million geo-referenced photos are now available on Flickr, a popular photo-sharing website. We show a method to analyze the Flickr data and generate aggregate knowledge in the form of \"representative tags\" for arbitrary areas in the world. We display these tags on a map interface in an interactive web application along with images associated with each tag. We then use the implicit feedback of the aggregate user interactions with the tags and images to learn which images best describe the area shown on the map.", "keywords": ["image summarization", "implicit feedback", "miscellaneous", "geotagged", "tagging", "visualization", "photographs", "geo-referenced data"], "combined": "Summarization of online image collections via implicit feedback The availability of map interfaces and location-aware devices makes a growing amount of unstructured, geo-referenced information available on the Web. In particular, over twelve million geo-referenced photos are now available on Flickr, a popular photo-sharing website. We show a method to analyze the Flickr data and generate aggregate knowledge in the form of \"representative tags\" for arbitrary areas in the world. We display these tags on a map interface in an interactive web application along with images associated with each tag. We then use the implicit feedback of the aggregate user interactions with the tags and images to learn which images best describe the area shown on the map. [[EENNDD]] image summarization; implicit feedback; miscellaneous; geotagged; tagging; visualization; photographs; geo-referenced data"}, "Ringkasan koleksi gambar dalam talian melalui maklum balas tersirat Ketersediaan antara muka peta dan peranti yang menyedari lokasi menjadikan semakin banyak maklumat yang tidak tersusun dan rujukan geo tersedia di Web. Khususnya, lebih daripada dua belas juta foto yang dirujuk geo kini tersedia di Flickr, sebuah laman web perkongsian foto yang popular. Kami menunjukkan kaedah untuk menganalisis data Flickr dan menghasilkan pengetahuan agregat dalam bentuk \"tag perwakilan\" untuk kawasan sewenang-wenang di dunia. Kami memaparkan tag ini di antara muka peta dalam aplikasi web interaktif bersama dengan gambar yang berkaitan dengan setiap tag. Kami kemudian menggunakan maklum balas tersirat dari interaksi pengguna agregat dengan teg dan gambar untuk mengetahui gambar mana yang paling sesuai menggambarkan kawasan yang ditunjukkan di peta. [[EENNDD]] ringkasan gambar; maklum balas tersirat; pelbagai; geoteg; penandaan; visualisasi; gambar; data rujukan geo"], [{"string": "Learning to re-rank: query-dependent image re-ranking using click data Our objective is to improve the performance of keyword based image search engines by re-ranking their original results. To this end, we address three limitations of existing search engines in this paper. First, there is no straight-forward, fully automated way of going from textual queries to visual features. Image search engines therefore primarily rely on static and textual features for ranking. Visual features are mainly used for secondary tasks such as finding similar images. Second, image rankers are trained on query-image pairs labeled with relevance judgments determined by human experts. Such labels are well known to be noisy due to various factors including ambiguous queries, unknown user intent and subjectivity in human judgments. This leads to learning a sub-optimal ranker. Finally, a static ranker is typically built to handle disparate user queries. The ranker is therefore unable to adapt its parameters to suit the query at hand which again leads to sub-optimal results. We demonstrate that all of these problems can be mitigated by employing a re-ranking algorithm that leverages aggregate user click data.", "keywords": ["information search and retrieval", "click data", "image search", "image re-ranking"], "combined": "Learning to re-rank: query-dependent image re-ranking using click data Our objective is to improve the performance of keyword based image search engines by re-ranking their original results. To this end, we address three limitations of existing search engines in this paper. First, there is no straight-forward, fully automated way of going from textual queries to visual features. Image search engines therefore primarily rely on static and textual features for ranking. Visual features are mainly used for secondary tasks such as finding similar images. Second, image rankers are trained on query-image pairs labeled with relevance judgments determined by human experts. Such labels are well known to be noisy due to various factors including ambiguous queries, unknown user intent and subjectivity in human judgments. This leads to learning a sub-optimal ranker. Finally, a static ranker is typically built to handle disparate user queries. The ranker is therefore unable to adapt its parameters to suit the query at hand which again leads to sub-optimal results. We demonstrate that all of these problems can be mitigated by employing a re-ranking algorithm that leverages aggregate user click data. [[EENNDD]] information search and retrieval; click data; image search; image re-ranking"}, "Belajar untuk menilai semula: pemeringkatan gambar yang bergantung kepada pertanyaan menggunakan data klik Objektif kami adalah untuk meningkatkan prestasi mesin pencari gambar berdasarkan kata kunci dengan menyusun semula hasil semula. Untuk tujuan ini, kami menangani tiga batasan enjin carian yang ada dalam makalah ini. Pertama, tidak ada jalan langsung yang sepenuhnya automatik dari pertanyaan teks ke ciri visual. Oleh itu, enjin carian imej bergantung pada ciri statik dan teks untuk peringkat. Ciri visual terutama digunakan untuk tugas sekunder seperti mencari gambar yang serupa. Kedua, pemeringkat gambar dilatih pada pasangan gambar-pertanyaan yang dilabelkan dengan pertimbangan relevan yang ditentukan oleh pakar manusia. Label sedemikian terkenal bising kerana pelbagai faktor termasuk pertanyaan yang tidak jelas, niat pengguna yang tidak diketahui dan subjektiviti dalam penilaian manusia. Ini membawa kepada pembelajaran pemeringkatan yang kurang optimum. Akhirnya, pemeringkat statik biasanya dibina untuk menangani pertanyaan pengguna yang berbeza. Oleh itu, pemeringkat tidak dapat menyesuaikan parameternya agar sesuai dengan pertanyaan yang sekali lagi membawa kepada hasil yang tidak optimum. Kami menunjukkan bahawa semua masalah ini dapat dikurangkan dengan menggunakan algoritma pemeringkatan semula yang memanfaatkan data klik pengguna agregat. [[EENNDD]] carian dan pengambilan maklumat; klik data; carian gambar; pemeringkatan semula gambar"], [{"string": "Mining models of human activities from the web No contact information provided yet.", "keywords": ["activity models", "activity inference", "rfid", "web mining"], "combined": "Mining models of human activities from the web No contact information provided yet. [[EENNDD]] activity models; activity inference; rfid; web mining"}, "Model perlombongan aktiviti manusia dari web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] model aktiviti; inferens aktiviti; rfid; perlombongan web"], [{"string": "Efficiently evaluating graph constraints in content-based publish/subscribe We introduce the problem of evaluating graph constraints in content-based publish/subscribe (pub/sub) systems. This problem formulation extends traditional content-based pub/sub systems in the following manner: publishers and subscribers are connected via a (logical) directed graph G with node and edge constraints, which limits the set of valid paths between them. Such graph constraints can be used to model a Web advertising exchange (where there may be restrictions on how advertising networks can connect advertisers and publishers) and content delivery problems in social networks (where there may be restrictions on how information can be shared via the social graph). In this context, we develop efficient algorithms for evaluating graph constraints over arbitrary directed graphs G. We also present experimental results that demonstrate the effectiveness and scalability of the proposed algorithms using a realistic dataset from Yahoo!'s Web advertising exchange.", "keywords": ["graph constraints", "pub/sub", "indexing"], "combined": "Efficiently evaluating graph constraints in content-based publish/subscribe We introduce the problem of evaluating graph constraints in content-based publish/subscribe (pub/sub) systems. This problem formulation extends traditional content-based pub/sub systems in the following manner: publishers and subscribers are connected via a (logical) directed graph G with node and edge constraints, which limits the set of valid paths between them. Such graph constraints can be used to model a Web advertising exchange (where there may be restrictions on how advertising networks can connect advertisers and publishers) and content delivery problems in social networks (where there may be restrictions on how information can be shared via the social graph). In this context, we develop efficient algorithms for evaluating graph constraints over arbitrary directed graphs G. We also present experimental results that demonstrate the effectiveness and scalability of the proposed algorithms using a realistic dataset from Yahoo!'s Web advertising exchange. [[EENNDD]] graph constraints; pub/sub; indexing"}, "Menilai kekangan grafik dengan cekap dalam penerbitan / langganan berasaskan kandungan Kami memperkenalkan masalah penilaian kekangan grafik dalam sistem penerbitan / langganan (pub / sub) berasaskan kandungan. Perumusan masalah ini memperluas sistem pub / sub berasaskan kandungan tradisional dengan cara berikut: penerbit dan pelanggan dihubungkan melalui (logik) graf G yang diarahkan dengan kekangan nod dan tepi, yang membatasi set jalan yang sah di antara mereka. Kekangan grafik seperti itu dapat digunakan untuk memodelkan pertukaran iklan Web (di mana mungkin ada sekatan tentang bagaimana rangkaian iklan dapat menghubungkan pengiklan dan penerbit) dan masalah penyampaian kandungan di rangkaian sosial (di mana mungkin ada sekatan tentang bagaimana maklumat dapat dikongsi melalui sosial graf). Dalam konteks ini, kami mengembangkan algoritma yang cekap untuk menilai batasan grafik berbanding graf yang diarahkan secara sewenang-wenang G. Kami juga membentangkan hasil eksperimen yang menunjukkan keberkesanan dan skalabilitas algoritma yang dicadangkan menggunakan set data yang realistik dari pertukaran iklan Web Yahoo !. [[EENNDD]] kekangan grafik; pub / sub; pengindeksan"], [{"string": "Beyond position bias: examining result attractiveness as a source of presentation bias in clickthrough data Leveraging clickthrough data has become a popular approach for evaluating and optimizing information retrieval systems. Although data is plentiful, one must take care when interpreting clicks, since user behavior can be affected by various sources of presentation bias. While the issue of position bias in clickthrough data has been the topic of much study, other presentation bias effects have received comparatively little attention. For instance, since users must decide whether to click on a result based on its summary (e.g., the title, URL and abstract), one might expect clicks to favor \"more attractive\" results. In this paper, we examine result summary attractiveness as a potential source of presentation bias. This study distinguishes itself from prior work by aiming to detect systematic biases in click behavior due to attractive summaries inflating perceived relevance. Our experiments conducted on the Google web search engine show substantial evidence of presentation bias in clicks towards results with more attractive titles.", "keywords": ["click logs", "implicit feedback", "presentation bias"], "combined": "Beyond position bias: examining result attractiveness as a source of presentation bias in clickthrough data Leveraging clickthrough data has become a popular approach for evaluating and optimizing information retrieval systems. Although data is plentiful, one must take care when interpreting clicks, since user behavior can be affected by various sources of presentation bias. While the issue of position bias in clickthrough data has been the topic of much study, other presentation bias effects have received comparatively little attention. For instance, since users must decide whether to click on a result based on its summary (e.g., the title, URL and abstract), one might expect clicks to favor \"more attractive\" results. In this paper, we examine result summary attractiveness as a potential source of presentation bias. This study distinguishes itself from prior work by aiming to detect systematic biases in click behavior due to attractive summaries inflating perceived relevance. Our experiments conducted on the Google web search engine show substantial evidence of presentation bias in clicks towards results with more attractive titles. [[EENNDD]] click logs; implicit feedback; presentation bias"}, "Bias kedudukan luar: memeriksa daya tarikan hasil sebagai sumber penyampaian bias dalam data klik-tayang Memanfaatkan data klik-tayang telah menjadi pendekatan yang popular untuk menilai dan mengoptimumkan sistem pengambilan maklumat. Walaupun data berlimpah, seseorang mesti berhati-hati ketika menafsirkan klik, kerana tingkah laku pengguna dapat dipengaruhi oleh pelbagai sumber bias persembahan. Walaupun isu bias kedudukan dalam data klik-tayang menjadi topik banyak kajian, kesan bias persembahan lain mendapat perhatian yang relatif sedikit. Sebagai contoh, oleh kerana pengguna mesti memutuskan untuk mengklik hasil berdasarkan ringkasannya (mis., Judul, URL dan abstrak), seseorang mungkin mengharapkan klik untuk menggemari hasil yang \"lebih menarik\". Dalam makalah ini, kami mengkaji daya tarikan ringkasan hasil sebagai sumber bias persembahan yang berpotensi. Kajian ini membezakan dirinya dari pekerjaan sebelumnya dengan bertujuan untuk mengesan bias sistematik dalam tingkah laku klik kerana ringkasan yang menarik meningkatkan perkaitan yang dirasakan. Eksperimen kami yang dilakukan di mesin carian web Google menunjukkan bukti yang besar mengenai penyampaian bias dalam klik terhadap hasil dengan tajuk yang lebih menarik. [[EENNDD]] log klik; maklum balas tersirat; berat sebelah persembahan"], [{"string": "Privacy diffusion on the web: a longitudinal perspective For the last few years we have been studying the diffusion of private information for users as they visit various Web sites triggering data gathering aggregation by third parties. This paper reports on our longitudinal study consisting of multiple snapshots of our examination of such diffusion over four years. We examine the various technical ways by which third-party aggregators acquire data and the depth of user-related information acquired. We study techniques for protecting privacy diffusion as well as limitations of such techniques. We introduce the concept of secondary privacy damage. Our results show increasing aggregation of user-related data by a steadily decreasing number of entities. A handful of companies are able to track users' movement across almost all of the popular Web sites. Virtually all the protection techniques have significant limitations highlighting the seriousness of the problem and the need for alternate solutions.", "keywords": ["privacy", "applications", "privacy enhancing technologies"], "combined": "Privacy diffusion on the web: a longitudinal perspective For the last few years we have been studying the diffusion of private information for users as they visit various Web sites triggering data gathering aggregation by third parties. This paper reports on our longitudinal study consisting of multiple snapshots of our examination of such diffusion over four years. We examine the various technical ways by which third-party aggregators acquire data and the depth of user-related information acquired. We study techniques for protecting privacy diffusion as well as limitations of such techniques. We introduce the concept of secondary privacy damage. Our results show increasing aggregation of user-related data by a steadily decreasing number of entities. A handful of companies are able to track users' movement across almost all of the popular Web sites. Virtually all the protection techniques have significant limitations highlighting the seriousness of the problem and the need for alternate solutions. [[EENNDD]] privacy; applications; privacy enhancing technologies"}, "Penyebaran privasi di web: perspektif membujur Selama beberapa tahun terakhir kami telah mengkaji penyebaran maklumat peribadi untuk pengguna ketika mereka mengunjungi pelbagai laman web yang mencetuskan pengumpulan data oleh pihak ketiga. Makalah ini melaporkan kajian membujur kami yang terdiri daripada beberapa gambaran mengenai pemeriksaan penyebaran kami selama empat tahun. Kami mengkaji pelbagai cara teknikal dengan cara agregator pihak ketiga memperoleh data dan kedalaman maklumat berkaitan pengguna yang diperoleh. Kami mengkaji teknik untuk melindungi penyebaran privasi serta batasan teknik tersebut. Kami memperkenalkan konsep kerosakan privasi sekunder. Hasil kami menunjukkan peningkatan agregat data berkaitan pengguna dengan jumlah entiti yang semakin menurun. Sebilangan kecil syarikat dapat mengesan pergerakan pengguna di hampir semua laman web yang popular. Hampir semua teknik perlindungan mempunyai batasan yang ketara yang menunjukkan betapa seriusnya masalah dan perlunya penyelesaian alternatif. [[EENNDD]] privasi; permohonan; teknologi meningkatkan privasi"], [{"string": "Towards a flash search engine based on expressive semantics No contact information provided yet.", "keywords": ["eigenvector", "search engine", "classification", "flash retrieval", "web application", "expressive semantics"], "combined": "Towards a flash search engine based on expressive semantics No contact information provided yet. [[EENNDD]] eigenvector; search engine; classification; flash retrieval; web application; expressive semantics"}, "Ke arah mesin pencari kilat berdasarkan semantik ekspresif Belum ada maklumat hubungan yang diberikan. [[EENNDD]] eigenvector; enjin carian; pengelasan; pengambilan kilat; aplikasi sesawang; semantik ekspresif"], [{"string": "Using annotations in enterprise search No contact information provided yet.", "keywords": ["community ranking", "enterprise search", "anchortext", "information search and retrieval"], "combined": "Using annotations in enterprise search No contact information provided yet. [[EENNDD]] community ranking; enterprise search; anchortext; information search and retrieval"}, "Menggunakan anotasi dalam carian perusahaan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kedudukan komuniti; carian perusahaan; sauh; pencarian dan pencarian maklumat"], [{"string": "Beyond XML and RDF: the versatile web query language xcerpt No contact information provided yet.", "keywords": ["web", "versatility", "xml", "xcerpt", "rdf"], "combined": "Beyond XML and RDF: the versatile web query language xcerpt No contact information provided yet. [[EENNDD]] web; versatility; xml; xcerpt; rdf"}, "Di luar XML dan RDF: xcerpt bahasa pertanyaan web serba boleh Belum ada maklumat hubungan yang disediakan. [[EENNDD]] web; serba boleh; xml; xcerpt; rdf"], [{"string": "Crosslanguage blog mining and trend visualisation People use weblogs to express thoughts, present ideas and share knowledge, therefore weblogs are extraordinarily valuable resources, amongs others, for trend analysis. Trends are derived from the chronological sequence of blog post count per topic. The comparison with a reference corpus allows qualitative statements over identified trends. We propose a crosslanguage blog mining and trend visualisation system to analyse blogs across languages and topics. The trend visualisation facilitates the identification of trends and the comparison with the reference news article corpus. To prove the correctness of our system we computed the correlation between trends in blogs and news articles for a subset of blogs and topics. The evaluation corroborated our hypothesis of a high correlation coefficient for these subsets and therefore the correctness of our system for different languages and topics is proven.", "keywords": ["miscellaneous", "blog mining", "trend visualisation", "crosslanguage"], "combined": "Crosslanguage blog mining and trend visualisation People use weblogs to express thoughts, present ideas and share knowledge, therefore weblogs are extraordinarily valuable resources, amongs others, for trend analysis. Trends are derived from the chronological sequence of blog post count per topic. The comparison with a reference corpus allows qualitative statements over identified trends. We propose a crosslanguage blog mining and trend visualisation system to analyse blogs across languages and topics. The trend visualisation facilitates the identification of trends and the comparison with the reference news article corpus. To prove the correctness of our system we computed the correlation between trends in blogs and news articles for a subset of blogs and topics. The evaluation corroborated our hypothesis of a high correlation coefficient for these subsets and therefore the correctness of our system for different languages and topics is proven. [[EENNDD]] miscellaneous; blog mining; trend visualisation; crosslanguage"}, "Penambangan silang bahasa dan visualisasi trend Orang menggunakan blog untuk mengekspresikan pemikiran, mengemukakan idea dan berkongsi pengetahuan, oleh itu blog web adalah sumber yang sangat berharga, antara lain, untuk analisis trend. Trend diperoleh daripada urutan kronologi jumlah catatan blog setiap topik. Perbandingan dengan korpus rujukan membolehkan penyataan kualitatif terhadap trend yang dikenal pasti. Kami mencadangkan sistem perlombongan bahasa silang bahasa dan visualisasi trend untuk menganalisis blog dalam pelbagai bahasa dan topik. Visualisasi tren memudahkan pengenalpastian tren dan perbandingan dengan korpus artikel berita rujukan. Untuk membuktikan ketepatan sistem kami, kami mengira korelasi antara trend dalam blog dan artikel berita untuk subset blog dan topik. Penilaian ini membuktikan hipotesis kami mengenai pekali korelasi yang tinggi untuk subset ini dan oleh itu kebenaran sistem kami untuk pelbagai bahasa dan topik terbukti. [[EENNDD]] pelbagai; perlombongan blog; visualisasi trend; bahasa silang"], [{"string": "A self organizing document map algorithm for large scale hyperlinked data inspired by neuronal migration Web document clustering is one of the research topics that is being pursued continuously due to the large variety of applications. Since Web documents usually have variety and diversity in terms of domains, content and quality, one of the technical difficulties is to find a reasonable number and size of clusters. In this research, we pay attention to SOMs (Self Organizing Maps) because of their capability of visualized clustering that helps users to investigate characteristics of data in detail. The SOM is widely known as a \"scalable\" algorithm because of its capability to handle large numbers of records. However, it is effective only when the vectors are small and dense. Although several research efforts on making the SOM scalable have been conducted, technical issues on scalability and performance for sparse high-dimensional data such as hyperlinked documents still remain. In this paper, we introduce MIGSOM, an SOM algorithm inspired by a recent discovery on neuronal migration. The two major advantages of MIGSOM are its scalability for sparse high-dimensional data and its clustering visualization functionality. In this paper, we describe the algorithm and implementation, and show the practicality of the algorithm by applying MIGSOM to a huge scale real data set: Wikipedia's hyperlink data.", "keywords": ["learning", "link analysis", "wikipedia", "som", "visualization", "clustering"], "combined": "A self organizing document map algorithm for large scale hyperlinked data inspired by neuronal migration Web document clustering is one of the research topics that is being pursued continuously due to the large variety of applications. Since Web documents usually have variety and diversity in terms of domains, content and quality, one of the technical difficulties is to find a reasonable number and size of clusters. In this research, we pay attention to SOMs (Self Organizing Maps) because of their capability of visualized clustering that helps users to investigate characteristics of data in detail. The SOM is widely known as a \"scalable\" algorithm because of its capability to handle large numbers of records. However, it is effective only when the vectors are small and dense. Although several research efforts on making the SOM scalable have been conducted, technical issues on scalability and performance for sparse high-dimensional data such as hyperlinked documents still remain. In this paper, we introduce MIGSOM, an SOM algorithm inspired by a recent discovery on neuronal migration. The two major advantages of MIGSOM are its scalability for sparse high-dimensional data and its clustering visualization functionality. In this paper, we describe the algorithm and implementation, and show the practicality of the algorithm by applying MIGSOM to a huge scale real data set: Wikipedia's hyperlink data. [[EENNDD]] learning; link analysis; wikipedia; som; visualization; clustering"}, "Algoritma peta dokumen pengorganisasian sendiri untuk data hiperpautan berskala besar yang diilhami oleh penghimpunan dokumen Web penghijrahan neuron adalah salah satu topik penyelidikan yang sedang dilakukan secara berterusan kerana pelbagai aplikasi. Oleh kerana dokumen Web biasanya mempunyai kepelbagaian dan kepelbagaian dari segi domain, kandungan dan kualiti, salah satu kesulitan teknikal adalah mencari jumlah dan ukuran kelompok yang munasabah. Dalam penyelidikan ini, kami memperhatikan SOM (Self Organizing Maps) kerana kemampuan mereka dalam pengelompokan visualisasi yang membantu pengguna menyelidiki ciri data secara terperinci. SOM dikenali sebagai algoritma \"berskala\" kerana kemampuannya untuk mengendalikan sejumlah besar rekod. Walau bagaimanapun, ia hanya berkesan apabila vektornya kecil dan padat. Walaupun telah dilakukan beberapa upaya penelitian untuk membuat SOM yang dapat diskalakan, masalah teknis mengenai skalabilitas dan prestasi untuk data dimensi tinggi yang jarang seperti dokumen bertautan tetap ada. Dalam makalah ini, kami memperkenalkan MIGSOM, algoritma SOM yang diilhami oleh penemuan baru mengenai migrasi neuron. Dua kelebihan utama MIGSOM adalah skalabilitasnya untuk data dimensi tinggi yang jarang dan fungsi visualisasi pengelompokannya. Dalam makalah ini, kami menerangkan algoritma dan pelaksanaannya, dan menunjukkan kepraktisan algoritma dengan menerapkan MIGSOM ke set data nyata skala besar: data hiperpautan Wikipedia. [[EENNDD]] pembelajaran; analisis pautan; wikipedia; som; visualisasi; pengelompokan"], [{"string": "The web of topics: discovering the topology of topic evolution in a corpus In this paper we study how to discover the evolution of topics over time in a time-stamped document collection. Our approach is uniquely designed to capture the rich topology of topic evolution inherent in the corpus. Instead of characterizing the evolving topics at fixed time points, we conceptually define a topic as a quantized unit of evolutionary change in content and discover topics with the time of their appearance in the corpus. Discovered topics are then connected to form a topic evolution graph using a measure derived from the underlying document network. Our approach allows inhomogeneous distribution of topics over time and does not impose any topological restriction in topic evolution graphs. We evaluate our algorithm on the ACM corpus.", "keywords": ["general", "language model", "information search and retrieval", "topic evolution", "topology", "citation network"], "combined": "The web of topics: discovering the topology of topic evolution in a corpus In this paper we study how to discover the evolution of topics over time in a time-stamped document collection. Our approach is uniquely designed to capture the rich topology of topic evolution inherent in the corpus. Instead of characterizing the evolving topics at fixed time points, we conceptually define a topic as a quantized unit of evolutionary change in content and discover topics with the time of their appearance in the corpus. Discovered topics are then connected to form a topic evolution graph using a measure derived from the underlying document network. Our approach allows inhomogeneous distribution of topics over time and does not impose any topological restriction in topic evolution graphs. We evaluate our algorithm on the ACM corpus. [[EENNDD]] general; language model; information search and retrieval; topic evolution; topology; citation network"}, "Web topik: menemui topologi evolusi topik dalam korpus Dalam makalah ini kita mengkaji bagaimana untuk mengetahui evolusi topik dari masa ke masa dalam koleksi dokumen yang dicap waktu. Pendekatan kami dirancang secara unik untuk menangkap topologi evolusi topik yang terdapat dalam korpus. Daripada mencirikan topik yang sedang berkembang pada titik waktu tetap, kami secara konseptual mendefinisikan topik sebagai unit kuantiti perubahan evolusi dalam kandungan dan menemui topik dengan waktu kemunculannya di korpus. Topik yang ditemui kemudian disambungkan untuk membentuk grafik evolusi topik menggunakan ukuran yang berasal dari rangkaian dokumen yang mendasari. Pendekatan kami memungkinkan penyebaran topik yang tidak homogen dari masa ke masa dan tidak mengenakan sekatan topologi dalam grafik evolusi topik. Kami menilai algoritma kami pada korpus ACM. [[EENNDD]] umum; model bahasa; pencarian dan pengambilan maklumat; evolusi topik; topologi; rangkaian petikan"], [{"string": "Analysis of community structure in Wikipedia We present the results of a community detection analysis of the Wikipedia graph. Distinct communities in Wikipedia contain semantically closely related articles. The central topic of a community can be identified using PageRank. Extracted communities can be organized hierarchically similar to manually created Wikipedia category structure.", "keywords": ["wikipedia", "graph analysis", "community detection"], "combined": "Analysis of community structure in Wikipedia We present the results of a community detection analysis of the Wikipedia graph. Distinct communities in Wikipedia contain semantically closely related articles. The central topic of a community can be identified using PageRank. Extracted communities can be organized hierarchically similar to manually created Wikipedia category structure. [[EENNDD]] wikipedia; graph analysis; community detection"}, "Analisis struktur komuniti di Wikipedia Kami membentangkan hasil analisis pengesanan komuniti grafik Wikipedia. Komuniti yang berbeza di Wikipedia mengandungi artikel yang berkaitan secara semantik. Topik utama komuniti dapat dikenal pasti menggunakan PageRank. Komuniti yang diekstrak boleh disusun secara hierarki serupa dengan struktur kategori Wikipedia yang dibuat secara manual. [[EENNDD]] wikipedia; analisis grafik; pengesanan masyarakat"], [{"string": "Understanding and combating link farming in the twitter social network Recently, Twitter has emerged as a popular platform for discovering real-time information on the Web, such as news stories and people's reaction to them. Like the Web, Twitter has become a target for link farming, where users, especially spammers, try to acquire large numbers of follower links in the social network. Acquiring followers not only increases the size of a user's direct audience, but also contributes to the perceived influence of the user, which in turn impacts the ranking of the user's tweets by search engines.", "keywords": ["pagerank", "collusionrank", "spam", "security and protection", "twitter", "link farming"], "combined": "Understanding and combating link farming in the twitter social network Recently, Twitter has emerged as a popular platform for discovering real-time information on the Web, such as news stories and people's reaction to them. Like the Web, Twitter has become a target for link farming, where users, especially spammers, try to acquire large numbers of follower links in the social network. Acquiring followers not only increases the size of a user's direct audience, but also contributes to the perceived influence of the user, which in turn impacts the ranking of the user's tweets by search engines. [[EENNDD]] pagerank; collusionrank; spam; security and protection; twitter; link farming"}, "Memahami dan memerangi hubungan pautan di rangkaian sosial twitter Baru-baru ini, Twitter muncul sebagai platform popular untuk menemui maklumat masa nyata di Web, seperti berita dan reaksi orang terhadap mereka. Seperti Web, Twitter telah menjadi sasaran untuk memautkan pautan, di mana pengguna, terutama spammer, berusaha memperoleh sejumlah besar pautan pengikut di rangkaian sosial. Memperoleh pengikut bukan hanya meningkatkan ukuran khalayak langsung pengguna, tetapi juga memberikan sumbangan terhadap pengaruh pengguna yang dirasakan, yang seterusnya mempengaruhi peringkat tweet pengguna oleh mesin pencari. [[EENNDD]] pagerank; peringkat kolusi; spam; keselamatan dan perlindungan; twitter; pautan pertanian"], [{"string": "A semantic approach for designing business protocols No contact information provided yet.", "keywords": ["web services", "business process composition", "commitments"], "combined": "A semantic approach for designing business protocols No contact information provided yet. [[EENNDD]] web services; business process composition; commitments"}, "Pendekatan semantik untuk merancang protokol perniagaan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perkhidmatan web; komposisi proses perniagaan; komitmen"], [{"string": "FLUX: fuzzy content and structure matching of XML range queries No contact information provided yet.", "keywords": ["xpath", "range query", "xml database"], "combined": "FLUX: fuzzy content and structure matching of XML range queries No contact information provided yet. [[EENNDD]] xpath; range query; xml database"}, "FLUX: kandungan kabur dan padanan struktur pertanyaan pelbagai XML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] xpath; pertanyaan pelbagai; pangkalan data xml"], [{"string": "A game theoretic formulation of the service provisioning problem in cloud systems Cloud computing is an emerging paradigm which allows the on-demand delivering of software, hardware, and data as services. As cloud-based services are more numerous and dynamic, the development of efficient service provisioning policies become increasingly challenging. Game theoretic approaches have shown to gain a thorough analytical understanding of the service provisioning problem.", "keywords": ["generalized nash equilibrium", "resource allocation", "game theory"], "combined": "A game theoretic formulation of the service provisioning problem in cloud systems Cloud computing is an emerging paradigm which allows the on-demand delivering of software, hardware, and data as services. As cloud-based services are more numerous and dynamic, the development of efficient service provisioning policies become increasingly challenging. Game theoretic approaches have shown to gain a thorough analytical understanding of the service provisioning problem. [[EENNDD]] generalized nash equilibrium; resource allocation; game theory"}, "Rumusan teori permainan mengenai masalah penyediaan perkhidmatan dalam sistem awan Pengkomputeran awan adalah paradigma yang muncul yang memungkinkan penyampaian perisian, perkakasan, dan data berdasarkan permintaan sebagai perkhidmatan. Oleh kerana perkhidmatan berasaskan awan lebih banyak dan dinamik, pengembangan dasar penyediaan perkhidmatan yang cekap menjadi semakin mencabar. Pendekatan teori permainan telah terbukti dapat memperoleh pemahaman analitik yang mendalam mengenai masalah penyediaan perkhidmatan. [[EENNDD]] keseimbangan nash umum; peruntukan sumber; teori permainan"], [{"string": "N for the price of 1: bundling web objects for more efficient content delivery An abstract is not available.", "keywords": ["standards", "network protocols", "delta encoding", "persistent connections", "http", "web performance"], "combined": "N for the price of 1: bundling web objects for more efficient content delivery An abstract is not available. [[EENNDD]] standards; network protocols; delta encoding; persistent connections; http; web performance"}, "N untuk harga 1: menggabungkan objek web untuk penyampaian kandungan yang lebih cekap Abstrak tidak tersedia. [[EENNDD]] standard; protokol rangkaian; pengekodan delta; sambungan berterusan; http; prestasi web"], [{"string": "Mixed-initiative, multi-source information assistants An abstract is not available.", "keywords": ["web"], "combined": "Mixed-initiative, multi-source information assistants An abstract is not available. [[EENNDD]] web"}, "Pembantu maklumat pelbagai sumber, inisiatif campuran Abstrak tidak tersedia. [[EENNDD]] web"], [{"string": "Communication design for electronic negotiations on the basis of XML schema An abstract is not available.", "keywords": ["electronic negotiation", "ontology", "computer-aided software engineering", "xml", "application framework"], "combined": "Communication design for electronic negotiations on the basis of XML schema An abstract is not available. [[EENNDD]] electronic negotiation; ontology; computer-aided software engineering; xml; application framework"}, "Reka bentuk komunikasi untuk rundingan elektronik berdasarkan skema XML Abstrak tidak tersedia. [[EENNDD]] rundingan elektronik; ontologi; kejuruteraan perisian berbantukan komputer; xml; kerangka aplikasi"], [{"string": "Pragmatic evaluation of folksonomies Recently, a number of algorithms have been proposed to obtain hierarchical structures - so-called folksonomies - from social tagging data. Work on these algorithms is in part driven by a belief that folksonomies are useful for tasks such as: (a) Navigating social tagging systems and (b) Acquiring semantic relationships between tags. While the promises and pitfalls of the latter have been studied to some extent, we know very little about the extent to which folksonomies are pragmatically useful for navigating social tagging systems. This paper sets out to address this gap by presenting and applying a pragmatic framework for evaluating folksonomies. We model exploratory navigation of a tagging system as decentralized search on a network of tags. Evaluation is based on the fact that the performance of a decentralized search algorithm depends on the quality of the background knowledge used. The key idea of our approach is to use hierarchical structures learned by folksonomy algorithm as background knowledge for decentralized search. Utilizing decentralized search on tag networks in combination with different folksonomies as hierarchical background knowledge allows us to evaluate navigational tasks in social tagging systems. Our experiments with four state-of-the-art folksonomy algorithms on five different social tagging datasets reveal that existing folksonomy algorithms exhibit significant, previously undiscovered, differences with regard to their utility for navigation. Our results are relevant for engineers aiming to improve navigability of social tagging systems and for scientists aiming to evaluate different folksonomy algorithms from a pragmatic perspective.", "keywords": ["decentralized search", "folksonomies", "evaluation"], "combined": "Pragmatic evaluation of folksonomies Recently, a number of algorithms have been proposed to obtain hierarchical structures - so-called folksonomies - from social tagging data. Work on these algorithms is in part driven by a belief that folksonomies are useful for tasks such as: (a) Navigating social tagging systems and (b) Acquiring semantic relationships between tags. While the promises and pitfalls of the latter have been studied to some extent, we know very little about the extent to which folksonomies are pragmatically useful for navigating social tagging systems. This paper sets out to address this gap by presenting and applying a pragmatic framework for evaluating folksonomies. We model exploratory navigation of a tagging system as decentralized search on a network of tags. Evaluation is based on the fact that the performance of a decentralized search algorithm depends on the quality of the background knowledge used. The key idea of our approach is to use hierarchical structures learned by folksonomy algorithm as background knowledge for decentralized search. Utilizing decentralized search on tag networks in combination with different folksonomies as hierarchical background knowledge allows us to evaluate navigational tasks in social tagging systems. Our experiments with four state-of-the-art folksonomy algorithms on five different social tagging datasets reveal that existing folksonomy algorithms exhibit significant, previously undiscovered, differences with regard to their utility for navigation. Our results are relevant for engineers aiming to improve navigability of social tagging systems and for scientists aiming to evaluate different folksonomy algorithms from a pragmatic perspective. [[EENNDD]] decentralized search; folksonomies; evaluation"}, "Penilaian pragmatik folksonomi Baru-baru ini, sejumlah algoritma telah dicadangkan untuk memperoleh struktur hierarki - yang disebut folksonomies - dari data penandaan sosial. Mengusahakan algoritma ini sebahagiannya didorong oleh kepercayaan bahawa folksonomi berguna untuk tugas seperti: (a) Menavigasi sistem penandaan sosial dan (b) Memperolehi hubungan semantik antara tag. Walaupun janji-janji dan perangkap yang terakhir telah dikaji sampai batas tertentu, kita tahu sedikit tentang sejauh mana folksonomi berguna secara pragmatik untuk menavigasi sistem penandaan sosial. Makalah ini bertujuan untuk mengatasi jurang ini dengan membentangkan dan menerapkan kerangka kerja pragmatik untuk menilai folksonomi. Kami memodelkan navigasi penerokaan sistem penandaan sebagai carian terdesentralisasi pada rangkaian tag. Penilaian berdasarkan fakta bahawa prestasi algoritma carian terdesentralisasi bergantung pada kualiti pengetahuan latar belakang yang digunakan. Idea utama pendekatan kami adalah dengan menggunakan struktur hierarki yang dipelajari oleh algoritma folksonomi sebagai pengetahuan latar belakang untuk pencarian terdesentralisasi. Menggunakan carian terdesentralisasi pada rangkaian tag dalam kombinasi dengan folksonomies yang berbeza kerana pengetahuan latar belakang hierarki membolehkan kita menilai tugas navigasi dalam sistem penandaan sosial. Eksperimen kami dengan empat algoritma folksonomi canggih pada lima set data penandaan sosial yang berbeza menunjukkan bahawa algoritma folksonomi yang ada menunjukkan perbezaan yang ketara, yang sebelumnya tidak ditemui, berkenaan dengan kegunaannya untuk navigasi. Hasil kami relevan untuk jurutera yang bertujuan untuk meningkatkan kemampuan navigasi sistem penandaan sosial dan untuk saintis yang bertujuan untuk menilai algoritma folksonomi yang berbeza dari perspektif pragmatik. [[EENNDD]] carian terdesentralisasi; folksonomi; penilaian"], [{"string": "G-ToPSS: fast filtering of graph-based metadata No contact information provided yet.", "keywords": ["systems and software", "information search and retrieval", "content-based routing", "publish/subscribe", "information dissemination", "rdf", "graph matching"], "combined": "G-ToPSS: fast filtering of graph-based metadata No contact information provided yet. [[EENNDD]] systems and software; information search and retrieval; content-based routing; publish/subscribe; information dissemination; rdf; graph matching"}, "G-ToPSS: penyaringan pantas metadata berasaskan grafik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] sistem dan perisian; pencarian dan pengambilan maklumat; penghalaan berdasarkan kandungan; terbitkan / melanggan; penyebaran maklumat; rdf; pemadanan grafik"], [{"string": "On measuring the quality of Wikipedia articles This paper discusses an approach to modeling and measuring information quality of Wikipedia articles. The approach is based on the idea that the quality of Wikipedia articles with distinctly different profiles needs to be measured using different information quality models. We report on our initial study, which involved two categories of Wikipedia articles: \"stabilized\" (those, whose content has not undergone major changes for a significant period of time) and \"controversial\" (the articles, which have undergone vandalism, revert wars, or whose content is subject to internal discussions between Wikipedia editors). We present simple information quality models and compare their performance on a subset of Wikipedia articles with the information quality evaluations provided by human users. Our experiment shows, that using special-purpose models for information quality captures user sentiment about Wikipedia articles better than using a single model for both categories of articles.", "keywords": ["wikipedia", "quality", "models"], "combined": "On measuring the quality of Wikipedia articles This paper discusses an approach to modeling and measuring information quality of Wikipedia articles. The approach is based on the idea that the quality of Wikipedia articles with distinctly different profiles needs to be measured using different information quality models. We report on our initial study, which involved two categories of Wikipedia articles: \"stabilized\" (those, whose content has not undergone major changes for a significant period of time) and \"controversial\" (the articles, which have undergone vandalism, revert wars, or whose content is subject to internal discussions between Wikipedia editors). We present simple information quality models and compare their performance on a subset of Wikipedia articles with the information quality evaluations provided by human users. Our experiment shows, that using special-purpose models for information quality captures user sentiment about Wikipedia articles better than using a single model for both categories of articles. [[EENNDD]] wikipedia; quality; models"}, "Mengukur kualiti artikel Wikipedia Makalah ini membincangkan pendekatan untuk memodelkan dan mengukur kualiti maklumat artikel Wikipedia. Pendekatan ini didasarkan pada idea bahawa kualiti artikel Wikipedia dengan profil yang berbeza berbeza harus diukur dengan menggunakan model kualiti maklumat yang berbeza. Kami melaporkan kajian awal kami, yang melibatkan dua kategori artikel Wikipedia: \"stabil\" (yang, yang kandungannya tidak mengalami perubahan besar untuk jangka masa yang panjang) dan \"kontroversial\" (artikel, yang telah mengalami vandalisme, mengembalikan perang , atau yang kandungannya menjadi bahan perbincangan dalaman antara penyunting Wikipedia). Kami menyajikan model kualiti maklumat yang mudah dan membandingkan prestasinya pada subkumpulan artikel Wikipedia dengan penilaian kualiti maklumat yang diberikan oleh pengguna manusia. Eksperimen kami menunjukkan, bahawa menggunakan model tujuan khas untuk kualiti maklumat menangkap sentimen pengguna mengenai artikel Wikipedia lebih baik daripada menggunakan model tunggal untuk kedua kategori artikel. [[EENNDD]] wikipedia; kualiti; model"], [{"string": "Active objects: actions for entity-centric search We introduce an entity-centric search experience, called Active Objects, in which entity-bearing queries are paired with actions that can be performed on the entities. For example, given a query for a specific flashlight, we aim to present actions such as reading reviews, watching demo videos, and finding the best price online. In an annotation study conducted over a random sample of user query sessions, we found that a large proportion of queries in query logs involve actions on entities, calling for an automatic approach to identifying relevant actions for entity-bearing queries. In this paper, we pose the problem of finding actions that can be performed on entities as the problem of probabilistic inference in a graphical model that captures how an entity bearing query is generated. We design models of increasing complexity that capture latent factors such as entity type and intended actions that determine how a user writes a query in a search box, and the URL that they click on. Given a large collection of real-world queries and clicks from a commercial search engine, the models are learned efficiently through maximum likelihood estimation using an EM algorithm. Given a new query, probabilistic inference enables recommendation of a set of pertinent actions and hosts. We propose an evaluation methodology for measuring the relevance of our recommended actions, and show empirical evidence of the quality and the diversity of the discovered actions.", "keywords": ["active objects", "web search", "query log mining", "actions", "entity-centric search"], "combined": "Active objects: actions for entity-centric search We introduce an entity-centric search experience, called Active Objects, in which entity-bearing queries are paired with actions that can be performed on the entities. For example, given a query for a specific flashlight, we aim to present actions such as reading reviews, watching demo videos, and finding the best price online. In an annotation study conducted over a random sample of user query sessions, we found that a large proportion of queries in query logs involve actions on entities, calling for an automatic approach to identifying relevant actions for entity-bearing queries. In this paper, we pose the problem of finding actions that can be performed on entities as the problem of probabilistic inference in a graphical model that captures how an entity bearing query is generated. We design models of increasing complexity that capture latent factors such as entity type and intended actions that determine how a user writes a query in a search box, and the URL that they click on. Given a large collection of real-world queries and clicks from a commercial search engine, the models are learned efficiently through maximum likelihood estimation using an EM algorithm. Given a new query, probabilistic inference enables recommendation of a set of pertinent actions and hosts. We propose an evaluation methodology for measuring the relevance of our recommended actions, and show empirical evidence of the quality and the diversity of the discovered actions. [[EENNDD]] active objects; web search; query log mining; actions; entity-centric search"}, "Objek aktif: tindakan untuk pencarian entiti-sentris Kami memperkenalkan pengalaman pencarian berpusat pada entiti, yang disebut Objek Aktif, di mana pertanyaan yang mengandungi entiti dipasangkan dengan tindakan yang dapat dilakukan pada entiti. Sebagai contoh, apabila diberikan pertanyaan untuk lampu suluh tertentu, kami bertujuan untuk menunjukkan tindakan seperti membaca ulasan, menonton video demo, dan mencari harga terbaik dalam talian. Dalam kajian anotasi yang dilakukan melalui sampel rawak sesi pertanyaan pengguna, kami mendapati bahawa sebilangan besar pertanyaan dalam log pertanyaan melibatkan tindakan pada entiti, meminta pendekatan automatik untuk mengenal pasti tindakan yang relevan untuk pertanyaan yang berkaitan dengan entiti. Dalam makalah ini, kami mengemukakan masalah mencari tindakan yang dapat dilakukan pada entiti sebagai masalah inferensi probabilistik dalam model grafik yang menangkap bagaimana pertanyaan yang mengandungi entiti dihasilkan. Kami merancang model peningkatan kerumitan yang menangkap faktor pendam seperti jenis entiti dan tindakan yang dimaksudkan yang menentukan bagaimana pengguna menulis pertanyaan dalam kotak carian, dan URL yang mereka klik. Memandangkan banyak koleksi pertanyaan dan klik di dunia nyata dari mesin carian komersial, model dipelajari dengan cekap melalui anggaran kemungkinan maksimum menggunakan algoritma EM. Dengan adanya pertanyaan baru, inferensi probabilistik memungkinkan cadangan sekumpulan tindakan dan host yang bersangkutan. Kami mencadangkan metodologi penilaian untuk mengukur relevansi tindakan yang kami sarankan, dan menunjukkan bukti empirikal mengenai kualiti dan kepelbagaian tindakan yang ditemui. [[EENNDD]] objek aktif; carian sesawang; perlombongan log pertanyaan; tindakan; carian entiti-berpusat"], [{"string": "Efficient k-nearest neighbor graph construction for generic similarity measures K-Nearest Neighbor Graph (K-NNG) construction is an important operation with many web related applications, including collaborative filtering, similarity search, and many others in data mining and machine learning. Existing methods for K-NNG construction either do not scale, or are specific to certain similarity measures. We present NN-Descent, a simple yet efficient algorithm for approximate K-NNG construction with arbitrary similarity measures. Our method is based on local search, has minimal space overhead and does not rely on any shared global index. Hence, it is especially suitable for large-scale applications where data structures need to be distributed over the network. We have shown with a variety of datasets and similarity measures that the proposed method typically converges to above 90% recall with each point comparing only to several percent of the whole dataset on average.", "keywords": ["arbitrary similarity measure", "k-nearest neighbor graph", "information storage and retrieval", "iterative method"], "combined": "Efficient k-nearest neighbor graph construction for generic similarity measures K-Nearest Neighbor Graph (K-NNG) construction is an important operation with many web related applications, including collaborative filtering, similarity search, and many others in data mining and machine learning. Existing methods for K-NNG construction either do not scale, or are specific to certain similarity measures. We present NN-Descent, a simple yet efficient algorithm for approximate K-NNG construction with arbitrary similarity measures. Our method is based on local search, has minimal space overhead and does not rely on any shared global index. Hence, it is especially suitable for large-scale applications where data structures need to be distributed over the network. We have shown with a variety of datasets and similarity measures that the proposed method typically converges to above 90% recall with each point comparing only to several percent of the whole dataset on average. [[EENNDD]] arbitrary similarity measure; k-nearest neighbor graph; information storage and retrieval; iterative method"}, "Pembinaan grafik jiran k-terdekat yang cekap untuk ukuran kesamaan generik Pembinaan K-Nearest Neighbor Graph (K-NNG) adalah operasi penting dengan banyak aplikasi berkaitan web, termasuk penapisan kolaboratif, pencarian kesamaan, dan banyak lagi yang lain dalam perlombongan data dan pembelajaran mesin. Kaedah yang ada untuk pembinaan K-NNG sama ada tidak mengikut skala, atau khusus untuk langkah-langkah kesamaan tertentu. Kami menyajikan NN-Descent, algoritma yang ringkas namun cekap untuk perkiraan pembinaan K-NNG dengan langkah-langkah kesamaan sewenang-wenangnya. Kaedah kami berdasarkan carian tempatan, mempunyai overhead ruang minimum dan tidak bergantung pada indeks global bersama. Oleh itu, sangat sesuai untuk aplikasi berskala besar di mana struktur data perlu diedarkan melalui rangkaian. Kami telah menunjukkan dengan pelbagai set data dan ukuran kesamaan bahawa kaedah yang dicadangkan biasanya menyatukan di atas 90% penarikan balik dengan setiap titik hanya membandingkan beberapa persen dari keseluruhan dataset. [[EENNDD]] ukuran kesamaan sewenang-wenangnya; graf jiran terdekat-k; penyimpanan dan pengambilan maklumat; kaedah berulang"], [{"string": "Geographic locations of web servers No contact information provided yet.", "keywords": ["cctld", "traceroute", "number of hops", "digital-divide", "nic registration information", "africa", "offshore server", "response time", "geographic location of servers"], "combined": "Geographic locations of web servers No contact information provided yet. [[EENNDD]] cctld; traceroute; number of hops; digital-divide; nic registration information; africa; offshore server; response time; geographic location of servers"}, "Lokasi geografi pelayan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] cctld; traceroute; bilangan hop; digital-divide; maklumat pendaftaran yang baik; afrika; pelayan luar pesisir; masa tindak balas; lokasi pelayan geografi"], [{"string": "Estimating web site readability using content extraction Nowadays, information is primarily searched on the WWW. From a user perspective, the readability is an important criterion for measuring the accessibility and thereby the quality of an information. We show that modern content extraction algorithms help to estimate the readability of a web document quite accurate.", "keywords": ["content extraction", "usability", "readability", "content analysis and indexing"], "combined": "Estimating web site readability using content extraction Nowadays, information is primarily searched on the WWW. From a user perspective, the readability is an important criterion for measuring the accessibility and thereby the quality of an information. We show that modern content extraction algorithms help to estimate the readability of a web document quite accurate. [[EENNDD]] content extraction; usability; readability; content analysis and indexing"}, "Mengira kebolehbacaan laman web menggunakan pengekstrakan kandungan Pada masa ini, maklumat terutamanya dicari di WWW. Dari perspektif pengguna, kebolehbacaan adalah kriteria penting untuk mengukur kebolehaksesan dan dengan itu kualiti maklumat. Kami menunjukkan bahawa algoritma pengekstrakan kandungan moden membantu mengira kebolehbacaan dokumen web dengan tepat. [[EENNDD]] pengekstrakan kandungan; kebolehgunaan; kebolehbacaan; analisis kandungan dan pengindeksan"], [{"string": "Web accessibility: a broader view Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.", "keywords": ["input devices and strategies", "handicapped persons/special needs", "standards", "web accessibility", "user interface"], "combined": "Web accessibility: a broader view Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references. [[EENNDD]] input devices and strategies; handicapped persons/special needs; standards; web accessibility; user interface"}, "Kebolehcapaian web: pandangan yang lebih luas Catatan: Kesalahan OCR mungkin terdapat dalam Senarai Rujukan ini yang diekstrak dari artikel teks lengkap. ACM memilih untuk mendedahkan Senarai lengkap dan bukan hanya rujukan yang betul dan berkaitan. [[EENNDD]] peranti input dan strategi; orang kurang upaya / keperluan khas; standard; kebolehcapaian laman web; antaramuka pengguna"], [{"string": "Information flow modeling based on diffusion rate for prediction and ranking Information flows in a network where individuals influence each other. The diffusion rate captures how efficiently the information can diffuse among the users in the network. We propose an information flow model that leverages diffusion rates for: (1) prediction . identify where information should flow to, and (2) ranking . identify who will most quickly receive the information. For prediction, we measure how likely information will propagate from a specific sender to a specific receiver during a certain time period. Accordingly a rate-based recommendation algorithm is proposed that predicts who will most likely receive the information during a limited time period. For ranking, we estimate the expected time for information diffusion to reach a specific user in a network. Subsequently, a DiffusionRank algorithm is proposed that ranks users based on how quickly information will flow to them. Experiments on two datasets demonstrate the effectiveness of the proposed algorithms to both improve the recommendation performance and rank users by the efficiency of information flow.", "keywords": ["collaborative filtering", "recommendation", "social influence", "information flow", "web ranking", "diffusion of innovation", "continuous-time markov chain"], "combined": "Information flow modeling based on diffusion rate for prediction and ranking Information flows in a network where individuals influence each other. The diffusion rate captures how efficiently the information can diffuse among the users in the network. We propose an information flow model that leverages diffusion rates for: (1) prediction . identify where information should flow to, and (2) ranking . identify who will most quickly receive the information. For prediction, we measure how likely information will propagate from a specific sender to a specific receiver during a certain time period. Accordingly a rate-based recommendation algorithm is proposed that predicts who will most likely receive the information during a limited time period. For ranking, we estimate the expected time for information diffusion to reach a specific user in a network. Subsequently, a DiffusionRank algorithm is proposed that ranks users based on how quickly information will flow to them. Experiments on two datasets demonstrate the effectiveness of the proposed algorithms to both improve the recommendation performance and rank users by the efficiency of information flow. [[EENNDD]] collaborative filtering; recommendation; social influence; information flow; web ranking; diffusion of innovation; continuous-time markov chain"}, "Pemodelan aliran maklumat berdasarkan kadar penyebaran untuk ramalan dan peringkat Aliran maklumat dalam rangkaian di mana individu saling mempengaruhi. Kadar penyebaran menangkap seberapa cekap maklumat dapat menyebarkan di antara pengguna di rangkaian. Kami mencadangkan model aliran maklumat yang memanfaatkan kadar penyebaran untuk: (1) ramalan. mengenal pasti ke mana maklumat harus mengalir, dan (2) peringkat. kenal pasti siapa yang akan menerima maklumat dengan pantas. Untuk ramalan, kami mengukur seberapa besar kemungkinan maklumat akan disebarkan dari pengirim tertentu ke penerima tertentu dalam jangka masa tertentu. Oleh itu, algoritma cadangan berdasarkan kadar dicadangkan yang meramalkan siapa yang kemungkinan besar akan menerima maklumat tersebut dalam jangka masa yang terhad. Untuk peringkat, kami menganggarkan jangka masa penyebaran maklumat untuk menjangkau pengguna tertentu dalam rangkaian. Selepas itu, algoritma DiffusionRank dicadangkan yang memberi peringkat pengguna berdasarkan seberapa cepat maklumat akan mengalir kepada mereka. Eksperimen pada dua set data menunjukkan keberkesanan algoritma yang dicadangkan untuk meningkatkan prestasi cadangan dan memberi peringkat pengguna dengan kecekapan aliran maklumat. [[EENNDD]] penapisan kolaboratif; cadangan; pengaruh sosial; aliran maklumat; kedudukan laman web; penyebaran inovasi; rantaian markov masa berterusan"], [{"string": "Constructing folksonomies by integrating structured metadata Aggregating many personal hierarchies into a common taxonomy, also known as a folksonomy, presents several challenges due to its sparseness, ambiguity, noise, and inconsistency. We describe an approach to folksonomy learning based on relational clustering that addresses these challenges by exploiting structured metadata contained in personal hierarchies. Our approach clusters similar hierarchies using their structure and tag statistics, then incrementally weaves them into a deeper, bushier tree. We study folksonomy learning using social metadata extracted from the photo-sharing site Flickr. We evaluate the learned folksonomy quantitatively by automatically comparing it to a reference taxonomy created by the Open Directory Project. Our empirical results suggest that the proposed approach improves upon the state-of-the-art folksonomy learning method.", "keywords": ["folksonomies", "collective knowledge"], "combined": "Constructing folksonomies by integrating structured metadata Aggregating many personal hierarchies into a common taxonomy, also known as a folksonomy, presents several challenges due to its sparseness, ambiguity, noise, and inconsistency. We describe an approach to folksonomy learning based on relational clustering that addresses these challenges by exploiting structured metadata contained in personal hierarchies. Our approach clusters similar hierarchies using their structure and tag statistics, then incrementally weaves them into a deeper, bushier tree. We study folksonomy learning using social metadata extracted from the photo-sharing site Flickr. We evaluate the learned folksonomy quantitatively by automatically comparing it to a reference taxonomy created by the Open Directory Project. Our empirical results suggest that the proposed approach improves upon the state-of-the-art folksonomy learning method. [[EENNDD]] folksonomies; collective knowledge"}, "Membangun folksonomi dengan mengintegrasikan metadata berstruktur Menggabungkan banyak hierarki peribadi ke dalam taksonomi biasa, juga dikenal sebagai folksonomi, menghadirkan beberapa cabaran kerana kelangkaan, kekaburan, kebisingan, dan ketidakkonsistenan. Kami menerangkan pendekatan untuk pembelajaran folksonomi berdasarkan pengelompokan relasional yang menangani cabaran ini dengan memanfaatkan metadata berstruktur yang terkandung dalam hierarki peribadi. Pendekatan kami mengumpulkan hierarki yang serupa dengan menggunakan struktur dan statistik tag mereka, kemudian secara bertahap menenunnya menjadi pohon yang lebih dalam dan lebat. Kami mempelajari pembelajaran folksonomi menggunakan metadata sosial yang diekstrak dari laman perkongsian foto Flickr. Kami menilai folksonomi yang dipelajari secara kuantitatif dengan membandingkannya secara automatik dengan taksonomi rujukan yang dibuat oleh Open Directory Project. Hasil empirik kami menunjukkan bahawa pendekatan yang dicadangkan akan bertambah baik dengan kaedah pembelajaran folksonomi yang canggih. [[EENNDD]] folksonomi; pengetahuan kolektif"], [{"string": "Mining search engine query logs for query recommendation No contact information provided yet.", "keywords": ["session", "recommendation", "mining", "query logs"], "combined": "Mining search engine query logs for query recommendation No contact information provided yet. [[EENNDD]] session; recommendation; mining; query logs"}, "Log pertanyaan carian mesin lombong untuk cadangan pertanyaan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] sesi; cadangan; perlombongan; log pertanyaan"], [{"string": "Automated object persistence for JavaScript Traditionally web applications have required an internet connection in order to work with data. Browsers have lacked any mechanisms to allow web applications to operate offline with a set of data to provide constant access to applications. Recently, through browser plug-ins such as Google Gears, browsers have gained the ability to persist data for offline use. However, until now it's been difficult for a web developer using these plug-ins to manage persisting data both locally for offline use and in the internet cloud due to: synchronization requirements, managing throughput and latency to the cloud, and making it work within the confines of a standards-compliant web browser. Historically in non-browser environments, programming language environments have offered automated object persistence to shield the developer from these complexities. In our research we have created a framework which introduces automated persistence of data objects for JavaScript utilizing the internet. Unlike traditional object persistence solutions, ours relies only on existing or forthcoming internet standards and does not rely upon specific runtime mechanisms such as OS or interpreter/compiler support. A new design was required in order to be suitable to the internet's unique characteristics of varying connection quality and a browser's specific restrictions. We validate our approach using benchmarks which show that our framework can handle thousands of data objects automatically, reducing the amount of work needed by developers to support offline Web applications.", "keywords": ["html5", "javascript", "json", "object persistence", "web storage", "object-oriented programming"], "combined": "Automated object persistence for JavaScript Traditionally web applications have required an internet connection in order to work with data. Browsers have lacked any mechanisms to allow web applications to operate offline with a set of data to provide constant access to applications. Recently, through browser plug-ins such as Google Gears, browsers have gained the ability to persist data for offline use. However, until now it's been difficult for a web developer using these plug-ins to manage persisting data both locally for offline use and in the internet cloud due to: synchronization requirements, managing throughput and latency to the cloud, and making it work within the confines of a standards-compliant web browser. Historically in non-browser environments, programming language environments have offered automated object persistence to shield the developer from these complexities. In our research we have created a framework which introduces automated persistence of data objects for JavaScript utilizing the internet. Unlike traditional object persistence solutions, ours relies only on existing or forthcoming internet standards and does not rely upon specific runtime mechanisms such as OS or interpreter/compiler support. A new design was required in order to be suitable to the internet's unique characteristics of varying connection quality and a browser's specific restrictions. We validate our approach using benchmarks which show that our framework can handle thousands of data objects automatically, reducing the amount of work needed by developers to support offline Web applications. [[EENNDD]] html5; javascript; json; object persistence; web storage; object-oriented programming"}, "Ketekunan objek automatik untuk JavaScript Aplikasi web secara tradisional memerlukan sambungan internet untuk berfungsi dengan data. Penyemak imbas tidak mempunyai mekanisme apa pun untuk membolehkan aplikasi web beroperasi di luar talian dengan sekumpulan data untuk memberikan akses berterusan ke aplikasi. Baru-baru ini, melalui pemalam penyemak imbas seperti Google Gears, penyemak imbas telah memperoleh kemampuan untuk mengekalkan data untuk penggunaan luar talian. Namun, hingga kini sukar bagi pembangun web menggunakan pemalam ini untuk menguruskan data yang berterusan baik secara tempatan untuk penggunaan luar talian dan di awan internet kerana: keperluan penyegerakan, menguruskan throughput dan latensi ke awan, dan membuatnya berfungsi dalam menghadkan penyemak imbas web yang mematuhi piawaian. Secara sejarah dalam persekitaran bukan penyemak imbas, persekitaran bahasa pengaturcaraan telah menawarkan ketekunan objek automatik untuk melindungi pengembang dari kerumitan ini. Dalam penyelidikan kami, kami telah membuat kerangka kerja yang memperkenalkan ketekunan objek data secara automatik untuk JavaScript menggunakan internet. Tidak seperti penyelesaian ketekunan objek tradisional, kami hanya bergantung pada standard internet yang ada atau yang akan datang dan tidak bergantung pada mekanisme jangka masa tertentu seperti OS atau sokongan jurubahasa / penyusun. Reka bentuk baru diperlukan agar sesuai dengan ciri unik internet dengan kualiti sambungan yang berbeza-beza dan sekatan khusus penyemak imbas. Kami mengesahkan pendekatan kami menggunakan penanda aras yang menunjukkan bahawa kerangka kerja kami dapat menangani ribuan objek data secara automatik, mengurangkan jumlah pekerjaan yang diperlukan oleh pembangun untuk menyokong aplikasi Web luar talian. [[EENNDD]] html5; javascript; json; ketekunan objek; storan web; pengaturcaraan berorientasikan objek"], [{"string": "Towards network-aware service composition in the cloud Service-Oriented Computing (SOC) enables the composition of loosely coupled services provided with varying Quality of Service (QoS) levels. Selecting a (near-)optimal set of services for a composition in terms of QoS is crucial when many functionally equivalent services are available. With the advent of Cloud Computing, both the number of such services and their distribution across the network are rising rapidly, increasing the impact of the network on the QoS of such compositions. Despite this, current approaches do not differentiate between the QoS of services themselves and the QoS of the network. Therefore, the computed latency differs substantially from the actual latency, resulting in suboptimal QoS for service compositions in the cloud. Thus, we propose a network-aware approach that handles the QoS of services and the QoS of the network independently. First, we build a network model in order to estimate the network latency between arbitrary services and potential users. Our selection algorithm then leverages this model to find compositions that will result in a low latency given an employed execution policy. In our evaluation, we show that our approach efficiently computes compositions with much lower latency than current approaches.", "keywords": ["network", "optimization", "service composition", "web services", "cloud", "qos"], "combined": "Towards network-aware service composition in the cloud Service-Oriented Computing (SOC) enables the composition of loosely coupled services provided with varying Quality of Service (QoS) levels. Selecting a (near-)optimal set of services for a composition in terms of QoS is crucial when many functionally equivalent services are available. With the advent of Cloud Computing, both the number of such services and their distribution across the network are rising rapidly, increasing the impact of the network on the QoS of such compositions. Despite this, current approaches do not differentiate between the QoS of services themselves and the QoS of the network. Therefore, the computed latency differs substantially from the actual latency, resulting in suboptimal QoS for service compositions in the cloud. Thus, we propose a network-aware approach that handles the QoS of services and the QoS of the network independently. First, we build a network model in order to estimate the network latency between arbitrary services and potential users. Our selection algorithm then leverages this model to find compositions that will result in a low latency given an employed execution policy. In our evaluation, we show that our approach efficiently computes compositions with much lower latency than current approaches. [[EENNDD]] network; optimization; service composition; web services; cloud; qos"}, "Ke arah komposisi perkhidmatan yang sedar rangkaian di Cloud Service-Oriented Computing (SOC) memungkinkan komposisi perkhidmatan yang digabungkan secara longgar yang disediakan dengan tahap Quality of Service (QoS) yang berbeza-beza. Memilih (hampir-) set perkhidmatan optimum untuk komposisi dari segi QoS sangat penting apabila terdapat banyak perkhidmatan yang setara dengan fungsi. Dengan munculnya Cloud Computing, baik jumlah perkhidmatan tersebut dan pengedarannya di seluruh rangkaian meningkat dengan pesat, meningkatkan kesan rangkaian pada QoS komposisi tersebut. Walaupun begitu, pendekatan semasa tidak membezakan antara QoS perkhidmatan itu sendiri dan QoS rangkaian. Oleh itu, latensi yang dikira berbeza jauh dari latency sebenarnya, sehingga QoS suboptimal untuk komposisi perkhidmatan di cloud. Oleh itu, kami mencadangkan pendekatan sedar jaringan yang menangani perkhidmatan QoS dan QoS rangkaian secara bebas. Pertama, kami membina model rangkaian untuk menganggarkan latensi rangkaian antara perkhidmatan sewenang-wenang dan pengguna berpotensi. Algoritma pilihan kami kemudian memanfaatkan model ini untuk mencari komposisi yang akan menghasilkan latensi rendah memandangkan dasar pelaksanaan yang digunakan. Dalam penilaian kami, kami menunjukkan bahawa pendekatan kami dengan berkesan menghitung komposisi dengan kependaman yang jauh lebih rendah daripada pendekatan semasa. [[EENNDD]] rangkaian; pengoptimuman; komposisi perkhidmatan; perkhidmatan web; awan; qos"], [{"string": "Predicting positive and negative links in online social networks We study online social networks in which relationships can be either positive (indicating relations such as friendship) or negative (indicating relations such as opposition or antagonism). Such a mix of positive and negative links arise in a variety of online settings; we study datasets from Epinions, Slashdot and Wikipedia. We find that the signs of links in the underlying social networks can be predicted with high accuracy, using models that generalize across this diverse range of sites. These models provide insight into some of the fundamental principles that drive the formation of signed links in networks, shedding light on theories of balance and status from social psychology; they also suggest social computing applications by which the attitude of one user toward another can be estimated from evidence provided by their relationships with other members of the surrounding social network.", "keywords": ["status theory", "structural balance", "trust", "distrust", "negative edges", "signed networks", "positive edges"], "combined": "Predicting positive and negative links in online social networks We study online social networks in which relationships can be either positive (indicating relations such as friendship) or negative (indicating relations such as opposition or antagonism). Such a mix of positive and negative links arise in a variety of online settings; we study datasets from Epinions, Slashdot and Wikipedia. We find that the signs of links in the underlying social networks can be predicted with high accuracy, using models that generalize across this diverse range of sites. These models provide insight into some of the fundamental principles that drive the formation of signed links in networks, shedding light on theories of balance and status from social psychology; they also suggest social computing applications by which the attitude of one user toward another can be estimated from evidence provided by their relationships with other members of the surrounding social network. [[EENNDD]] status theory; structural balance; trust; distrust; negative edges; signed networks; positive edges"}, "Meramalkan hubungan positif dan negatif dalam rangkaian sosial dalam talian Kami mengkaji rangkaian sosial dalam talian di mana hubungan boleh menjadi positif (menunjukkan hubungan seperti persahabatan) atau negatif (menunjukkan hubungan seperti penentangan atau antagonisme). Campuran pautan positif dan negatif seperti itu timbul dalam pelbagai tetapan dalam talian; kami mengkaji set data dari Epinions, Slashdot dan Wikipedia. Kami mendapati bahawa tanda-tanda pautan di rangkaian sosial yang mendasari dapat diramalkan dengan ketepatan yang tinggi, menggunakan model yang menyamaratakan di pelbagai laman web ini. Model-model ini memberikan pandangan mengenai beberapa prinsip asas yang mendorong pembentukan pautan yang ditandatangani dalam rangkaian, menjelaskan teori keseimbangan dan status dari psikologi sosial; mereka juga mencadangkan aplikasi pengkomputeran sosial di mana sikap satu pengguna terhadap pengguna lain dapat diperkirakan dari bukti yang diberikan oleh hubungan mereka dengan anggota lain dari rangkaian sosial di sekitarnya. [[EENNDD]] teori status; keseimbangan struktur; kepercayaan; tidak percaya; tepi negatif; rangkaian yang ditandatangani; tepi positif"], [{"string": "Web page summarization using dynamic content No contact information provided yet.", "keywords": ["general", "web page summarization", "change detection", "web document"], "combined": "Web page summarization using dynamic content No contact information provided yet. [[EENNDD]] general; web page summarization; change detection; web document"}, "Ringkasan halaman web menggunakan kandungan dinamik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] umum; ringkasan laman web; pengesanan perubahan; dokumen web"], [{"string": "Towards lightweight and efficient DDOS attacks detection for web server In this poster, based on our previous work in building a lightweight DDoS (Distributed Denial-of-Services) attacks detection mechanism for web server using TCM-KNN (Transductive Confidence Machines for K-Nearest Neighbors) and genetic algorithm based instance selection methods, we further propose a more efficient and effective instance selection method, named E-FCM (Extend Fuzzy C-Means). By using this method, we can obtain much cheaper training time for TCM-KNN while ensuring high detection performance. Therefore, the optimized mechanism is more suitable for lightweight DDoS attacks detection in real network environment.", "keywords": ["web server anomaly detection", "security and protection", "e-fcm algorithm", "instance selection", "tcm-knn algorithm"], "combined": "Towards lightweight and efficient DDOS attacks detection for web server In this poster, based on our previous work in building a lightweight DDoS (Distributed Denial-of-Services) attacks detection mechanism for web server using TCM-KNN (Transductive Confidence Machines for K-Nearest Neighbors) and genetic algorithm based instance selection methods, we further propose a more efficient and effective instance selection method, named E-FCM (Extend Fuzzy C-Means). By using this method, we can obtain much cheaper training time for TCM-KNN while ensuring high detection performance. Therefore, the optimized mechanism is more suitable for lightweight DDoS attacks detection in real network environment. [[EENNDD]] web server anomaly detection; security and protection; e-fcm algorithm; instance selection; tcm-knn algorithm"}, "Ke arah pengesanan serangan DDOS yang ringan dan cekap untuk pelayan web Dalam poster ini, berdasarkan karya kami sebelumnya dalam membina mekanisme pengesanan serangan DDoS (Distribusi Denial-of-Services) ringan untuk pelayan web menggunakan TCM-KNN (Mesin Keyakinan Transduktif untuk K-Nearest Jiran) dan kaedah pemilihan contoh berdasarkan algoritma genetik, kami seterusnya mencadangkan kaedah pemilihan contoh yang lebih cekap dan berkesan, bernama E-FCM (Extend Fuzzy C-Means). Dengan menggunakan kaedah ini, kita dapat memperoleh masa latihan yang jauh lebih murah untuk TCM-KNN sambil memastikan prestasi pengesanan yang tinggi. Oleh itu, mekanisme yang dioptimumkan lebih sesuai untuk pengesanan serangan DDoS ringan di persekitaran rangkaian sebenar. [[EENNDD]] pengesanan anomali pelayan web; keselamatan dan perlindungan; algoritma e-fcm; pemilihan contoh; algoritma tcm-knn"], [{"string": "TruRank: taking PageRank to the limit No contact information provided yet.", "keywords": ["pagerank", "discrete mathematics", "web graph"], "combined": "TruRank: taking PageRank to the limit No contact information provided yet. [[EENNDD]] pagerank; discrete mathematics; web graph"}, "TruRank: membawa PageRank ke had Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pagerank; matematik diskrit; grafik web"], [{"string": "WebPod: persistent Web browsing sessions with pocketable storage devices No contact information provided yet.", "keywords": ["portable storage", "web browsing", "virtualization", "process migration"], "combined": "WebPod: persistent Web browsing sessions with pocketable storage devices No contact information provided yet. [[EENNDD]] portable storage; web browsing; virtualization; process migration"}, "WebPod: sesi penyemakan imbas Web yang berterusan dengan peranti penyimpanan yang boleh dikemas. Belum ada maklumat hubungan yang diberikan. [[EENNDD]] storan mudah alih; melayari laman web; maya; proses penghijrahan"], [{"string": "Quality driven web services composition No contact information provided yet.", "keywords": ["web services", "service composition", "qos"], "combined": "Quality driven web services composition No contact information provided yet. [[EENNDD]] web services; service composition; qos"}, "Komposisi perkhidmatan web berdasarkan kualiti Belum ada maklumat hubungan yang diberikan [[EENNDD]] perkhidmatan web; komposisi perkhidmatan; qos"], [{"string": "Rated aspect summarization of short comments Web 2.0 technologies have enabled more and more people to freely comment on different kinds of entities (e.g. sellers, products, services). The large scale of information poses the need and challenge of automatic summarization. In many cases, each of the user-generated short comments comes with an overall rating. In this paper, we study the problem of generating a ``rated aspect summary'' of short comments, which is a decomposed view of the overall ratings for the major aspects so that a user could gain different perspectives towards the target entity. We formally define the problem and decompose the solution into three steps. We demonstrate the effectiveness of our methods by using eBay sellers' feedback comments. We also quantitatively evaluate each step of our methods and study how well human agree on such a summarization task. The proposed methods are quite general and can be used to generate rated aspect summary automatically given any collection of short comments each associated with an overall rating.", "keywords": ["short comments", "information search and retrieval", "rating prediction", "rated aspect summarization"], "combined": "Rated aspect summarization of short comments Web 2.0 technologies have enabled more and more people to freely comment on different kinds of entities (e.g. sellers, products, services). The large scale of information poses the need and challenge of automatic summarization. In many cases, each of the user-generated short comments comes with an overall rating. In this paper, we study the problem of generating a ``rated aspect summary'' of short comments, which is a decomposed view of the overall ratings for the major aspects so that a user could gain different perspectives towards the target entity. We formally define the problem and decompose the solution into three steps. We demonstrate the effectiveness of our methods by using eBay sellers' feedback comments. We also quantitatively evaluate each step of our methods and study how well human agree on such a summarization task. The proposed methods are quite general and can be used to generate rated aspect summary automatically given any collection of short comments each associated with an overall rating. [[EENNDD]] short comments; information search and retrieval; rating prediction; rated aspect summarization"}, "Ringkasan aspek ringkas komen pendek Teknologi Web 2.0 telah membolehkan lebih banyak orang memberi komen secara bebas mengenai pelbagai jenis entiti (mis. Penjual, produk, perkhidmatan). Skala maklumat yang besar menimbulkan keperluan dan cabaran ringkasan automatik. Dalam banyak kes, setiap komen pendek yang dihasilkan pengguna dilengkapi dengan penilaian keseluruhan. Dalam makalah ini, kami mengkaji masalah menghasilkan \"ringkasan aspek dinilai\" dari komen pendek, yang merupakan pandangan terurai dari keseluruhan penilaian untuk aspek utama sehingga pengguna dapat memperoleh perspektif yang berbeza terhadap entiti sasaran. Kami secara formal menentukan masalah dan menguraikan penyelesaiannya menjadi tiga langkah. Kami menunjukkan keberkesanan kaedah kami dengan menggunakan komen maklum balas penjual eBay. Kami juga menilai secara kuantitatif setiap langkah kaedah kami dan mengkaji sejauh mana manusia bersetuju dengan tugas ringkasan tersebut. Kaedah yang dicadangkan agak umum dan boleh digunakan untuk menghasilkan ringkasan aspek yang diberi nilai secara automatik diberikan koleksi komen pendek yang masing-masing berkaitan dengan penilaian keseluruhan. [[EENNDD]] komen pendek; carian dan pengambilan maklumat; ramalan penilaian; ringkasan aspek dinilai"], [{"string": "Distributed cooperative Apache web server An abstract is not available.", "keywords": ["dc-apache", "servers", "scalable web server", "replication", "apache", "load balancing", "www", "distributed web server"], "combined": "Distributed cooperative Apache web server An abstract is not available. [[EENNDD]] dc-apache; servers; scalable web server; replication; apache; load balancing; www; distributed web server"}, "Pelayan web Apache koperasi yang diedarkan Abstrak tidak tersedia. [[EENNDD]] dc-apache; pelayan; pelayan web berskala; replikasi; apache; pengimbangan beban; www; pelayan web yang diedarkan"], [{"string": "Cyclone: an encyclopedic web search site No contact information provided yet.", "keywords": ["organization", "on-line information services", "web search", "natural language processing", "information search and retrieval", "encyclopedias", "extraction"], "combined": "Cyclone: an encyclopedic web search site No contact information provided yet. [[EENNDD]] organization; on-line information services; web search; natural language processing; information search and retrieval; encyclopedias; extraction"}, "Cyclone: laman web carian ensiklopedik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] organisasi; perkhidmatan maklumat dalam talian; carian sesawang; pemprosesan bahasa semula jadi; pencarian dan pengambilan maklumat; ensiklopedia; pengekstrakan"], [{"string": "Threshold selection for web-page classification with highly skewed class distribution We propose a novel cost-efficient approach to threshold selection for binary web-page classification problems with imbalanced class distributions. In many binary-classification tasks the distribution of classes is highly skewed. In such problems, using uniform random sampling in constructing sample sets for threshold setting requires large sample sizes in order to include a statistically sufficient number of examples of the minority class. On the other hand, manually labeling examples is expensive and budgetary considerations require that the size of sample sets be limited. These conflicting requirements make threshold selection a challenging problem. Our method of sample-set construction is a novel approach based on stratified sampling, in which manually labeled examples are expanded to reflect the true class distribution of the web-page population. Our experimental results show that using false positive rate as the criterion for threshold setting results in lower-variance threshold estimates than using other widely used accuracy measures such as F1 and precision.", "keywords": ["threshold selection", "web-page classification", "miscellaneous", "skewed class distribution", "binary classifier", "stratified sampling"], "combined": "Threshold selection for web-page classification with highly skewed class distribution We propose a novel cost-efficient approach to threshold selection for binary web-page classification problems with imbalanced class distributions. In many binary-classification tasks the distribution of classes is highly skewed. In such problems, using uniform random sampling in constructing sample sets for threshold setting requires large sample sizes in order to include a statistically sufficient number of examples of the minority class. On the other hand, manually labeling examples is expensive and budgetary considerations require that the size of sample sets be limited. These conflicting requirements make threshold selection a challenging problem. Our method of sample-set construction is a novel approach based on stratified sampling, in which manually labeled examples are expanded to reflect the true class distribution of the web-page population. Our experimental results show that using false positive rate as the criterion for threshold setting results in lower-variance threshold estimates than using other widely used accuracy measures such as F1 and precision. [[EENNDD]] threshold selection; web-page classification; miscellaneous; skewed class distribution; binary classifier; stratified sampling"}, "Pemilihan ambang untuk klasifikasi halaman web dengan pengedaran kelas yang sangat miring. Kami mencadangkan pendekatan kos efektif novel untuk pemilihan ambang untuk masalah klasifikasi halaman web binari dengan pengedaran kelas yang tidak seimbang. Dalam banyak tugas pengkelasan binari pengagihan kelas sangat condong. Dalam masalah seperti itu, menggunakan persampelan rawak seragam dalam membina set sampel untuk penetapan ambang memerlukan ukuran sampel yang besar untuk memasukkan sejumlah contoh kelas minoriti yang cukup statistik. Sebaliknya, contoh pelabelan secara manual adalah mahal dan pertimbangan anggaran memerlukan ukuran set sampel terhad. Keperluan yang bertentangan ini menjadikan pemilihan ambang menjadi masalah yang mencabar. Kaedah pembinaan set sampel kami adalah pendekatan baru berdasarkan pensampelan berstrata, di mana contoh berlabel secara manual diperluas untuk mencerminkan taburan kelas sebenar populasi halaman web. Hasil eksperimen kami menunjukkan bahawa menggunakan kadar positif palsu sebagai kriteria penetapan ambang menghasilkan anggaran ambang varians lebih rendah daripada menggunakan ukuran ketepatan lain yang banyak digunakan seperti F1 dan ketepatan. [[EENNDD]] pemilihan ambang; pengelasan laman web; pelbagai; pembahagian kelas condong; pengkelasan binari; persampelan berstrata"], [{"string": "Large scale multi-label classification via metalabeler The explosion of online content has made the management of such content non-trivial. Web-related tasks such as web page categorization, news filtering, query categorization, tag recommendation, etc. often involve the construction of multi-label categorization systems on a large scale. Existing multi-label classification methods either do not scale or have unsatisfactory performance. In this work, we propose MetaLabeler to automatically determine the relevant set of labels for each instance without intensive human involvement or expensive cross-validation. Extensive experiments conducted on benchmark data show that the MetaLabeler tends to outperform existing methods. Moreover, MetaLabeler scales to millions of multi-labeled instances and can be deployed easily. This enables us to apply the MetaLabeler to a large scale query categorization problem in Yahoo!, yielding a significant improvement in performance.", "keywords": ["hierarchical classification", "metalabeler", "query categorization", "meta model", "large scale", "multi-label classification"], "combined": "Large scale multi-label classification via metalabeler The explosion of online content has made the management of such content non-trivial. Web-related tasks such as web page categorization, news filtering, query categorization, tag recommendation, etc. often involve the construction of multi-label categorization systems on a large scale. Existing multi-label classification methods either do not scale or have unsatisfactory performance. In this work, we propose MetaLabeler to automatically determine the relevant set of labels for each instance without intensive human involvement or expensive cross-validation. Extensive experiments conducted on benchmark data show that the MetaLabeler tends to outperform existing methods. Moreover, MetaLabeler scales to millions of multi-labeled instances and can be deployed easily. This enables us to apply the MetaLabeler to a large scale query categorization problem in Yahoo!, yielding a significant improvement in performance. [[EENNDD]] hierarchical classification; metalabeler; query categorization; meta model; large scale; multi-label classification"}, "Klasifikasi pelbagai label berskala besar melalui label logam Meletupnya kandungan dalam talian menjadikan pengurusan kandungan tersebut tidak remeh. Tugas yang berkaitan dengan web seperti pengkategorian halaman web, penyaringan berita, pengkategorian pertanyaan, cadangan tag, dan lain-lain sering kali melibatkan pembinaan sistem pengkategorian pelbagai label pada skala besar. Kaedah klasifikasi pelbagai label yang ada sama ada tidak mengikut skala atau mempunyai prestasi yang tidak memuaskan. Dalam karya ini, kami mencadangkan MetaLabeler menentukan secara automatik set label yang relevan untuk setiap kejadian tanpa penglibatan manusia yang intensif atau pengesahan silang yang mahal. Eksperimen ekstensif yang dilakukan pada data penanda aras menunjukkan bahawa MetaLabeler cenderung mengatasi kaedah yang ada. Lebih-lebih lagi, MetaLabeler menimbang berjuta-juta contoh berlabel pelbagai dan dapat digunakan dengan mudah. Ini membolehkan kita menerapkan MetaLabeler untuk masalah pengkategorian pertanyaan skala besar di Yahoo !, menghasilkan peningkatan prestasi yang ketara. [[EENNDD]] klasifikasi hierarki; pelabel logam; pengkategorian pertanyaan; model meta; skala besar; klasifikasi pelbagai label"], [{"string": "Using the wisdom of the crowds for keyword generation In the sponsored search model, search engines are paid by businesses that are interested in displaying ads for their site alongside the search results. Businesses bid for keywords, and their ad is displayed when the keyword is queried to the search engine. An important problem in this process is 'keyword generation': given a business that is interested in launching a campaign, suggest keywords that are related to that campaign. We address this problem by making use of the query logs of the search engine. We identify queries related to a campaign by exploiting the associations between queries and URLs as they are captured by the user's clicks. These queries form good keyword suggestions since they capture the \"wisdom of the crowd\" as to what is related to a site. We formulate the problem as a semi-supervised learning problem, and propose algorithms within the Markov Random Field model. We perform experiments with real query logs, and we demonstrate that our algorithms scale to large query logs and produce meaningful results.", "keywords": ["absorbing random walks", "sponsored search", "markov random fields", "query click logs", "keyword generation"], "combined": "Using the wisdom of the crowds for keyword generation In the sponsored search model, search engines are paid by businesses that are interested in displaying ads for their site alongside the search results. Businesses bid for keywords, and their ad is displayed when the keyword is queried to the search engine. An important problem in this process is 'keyword generation': given a business that is interested in launching a campaign, suggest keywords that are related to that campaign. We address this problem by making use of the query logs of the search engine. We identify queries related to a campaign by exploiting the associations between queries and URLs as they are captured by the user's clicks. These queries form good keyword suggestions since they capture the \"wisdom of the crowd\" as to what is related to a site. We formulate the problem as a semi-supervised learning problem, and propose algorithms within the Markov Random Field model. We perform experiments with real query logs, and we demonstrate that our algorithms scale to large query logs and produce meaningful results. [[EENNDD]] absorbing random walks; sponsored search; markov random fields; query click logs; keyword generation"}, "Menggunakan kebijaksanaan orang ramai untuk menghasilkan kata kunci Dalam model carian yang ditaja, mesin carian dibayar oleh perniagaan yang berminat untuk memaparkan iklan untuk laman web mereka di samping hasil carian. Perniagaan menawar kata kunci, dan iklan mereka dipaparkan ketika kata kunci tersebut disoal ke mesin pencari. Masalah penting dalam proses ini adalah 'pembuatan kata kunci': memandangkan perniagaan yang berminat untuk melancarkan kempen, cadangkan kata kunci yang berkaitan dengan kempen tersebut. Kami mengatasi masalah ini dengan menggunakan log pertanyaan mesin pencari. Kami mengenal pasti pertanyaan yang berkaitan dengan kempen dengan memanfaatkan hubungan antara pertanyaan dan URL kerana ia ditangkap oleh klik pengguna. Pertanyaan ini membentuk cadangan kata kunci yang baik kerana mereka merangkumi \"kebijaksanaan orang ramai\" mengenai apa yang berkaitan dengan laman web. Kami merumuskan masalah sebagai masalah pembelajaran semi-diawasi, dan mencadangkan algoritma dalam model Markov Random Field. Kami melakukan eksperimen dengan log pertanyaan sebenar, dan kami menunjukkan bahawa algoritma kami memperbesar log pertanyaan besar dan menghasilkan hasil yang bermakna. [[EENNDD]] menyerap jalan rawak; carian tajaan; medan rawak markov; log klik pertanyaan; penghasilan kata kunci"], [{"string": "All your contacts are belong to us: automated identity theft attacks on social networks Social networking sites have been increasingly gaining popularity. Well-known sites such as Facebook have been reporting growth rates as high as 3% per week. Many social networking sites have millions of registered users who use these sites to share photographs, contact long-lost friends, establish new business contacts and to keep in touch. In this paper, we investigate how easy it would be for a potential attacker to launch automated crawling and identity theft attacks against a number of popular social networking sites in order to gain access to a large volume of personal user information. The first attack we present is the automated identity theft of existing user profiles and sending of friend requests to the contacts of the cloned victim. The hope, from the attacker's point of view, is that the contacted users simply trust and accept the friend request. By establishing a friendship relationship with the contacts of a victim, the attacker is able to access the sensitive personal information provided by them. In the second, more advanced attack we present, we show that it is effective and feasible to launch an automated, cross-site profile cloning attack. In this attack, we are able to automatically create a forged profile in a network where the victim is not registered yet and contact the victim's friends who are registered on both networks. Our experimental results with real users show that the automated attacks we present are effective and feasible in practice.", "keywords": ["general", "identity theft", "social network security"], "combined": "All your contacts are belong to us: automated identity theft attacks on social networks Social networking sites have been increasingly gaining popularity. Well-known sites such as Facebook have been reporting growth rates as high as 3% per week. Many social networking sites have millions of registered users who use these sites to share photographs, contact long-lost friends, establish new business contacts and to keep in touch. In this paper, we investigate how easy it would be for a potential attacker to launch automated crawling and identity theft attacks against a number of popular social networking sites in order to gain access to a large volume of personal user information. The first attack we present is the automated identity theft of existing user profiles and sending of friend requests to the contacts of the cloned victim. The hope, from the attacker's point of view, is that the contacted users simply trust and accept the friend request. By establishing a friendship relationship with the contacts of a victim, the attacker is able to access the sensitive personal information provided by them. In the second, more advanced attack we present, we show that it is effective and feasible to launch an automated, cross-site profile cloning attack. In this attack, we are able to automatically create a forged profile in a network where the victim is not registered yet and contact the victim's friends who are registered on both networks. Our experimental results with real users show that the automated attacks we present are effective and feasible in practice. [[EENNDD]] general; identity theft; social network security"}, "Semua kenalan anda adalah milik kami: serangan kecurian identiti automatik di rangkaian sosial Laman rangkaian sosial semakin popular. Laman web terkenal seperti Facebook telah melaporkan kadar pertumbuhan setinggi 3% setiap minggu. Banyak laman rangkaian sosial mempunyai berjuta-juta pengguna berdaftar yang menggunakan laman web ini untuk berkongsi gambar, menghubungi rakan-rakan yang sudah lama hilang, menjalin hubungan perniagaan baru dan terus berhubungan. Dalam makalah ini, kami menyelidiki betapa mudahnya penyerang berpotensi melancarkan serangan perayapan automatik dan pencurian identiti terhadap sejumlah laman rangkaian sosial yang popular untuk mendapatkan akses ke sejumlah besar maklumat pengguna peribadi. Serangan pertama yang kami sajikan adalah pencurian identiti automatik profil pengguna yang ada dan pengiriman permintaan rakan ke kenalan mangsa yang diklon. Harapan, dari sudut pandang penyerang, adalah agar pengguna yang dihubungi hanya mempercayai dan menerima permintaan teman. Dengan menjalin hubungan persahabatan dengan kenalan mangsa, penyerang dapat mengakses maklumat peribadi sensitif yang diberikan oleh mereka. Pada serangan kedua yang lebih maju yang kami sampaikan, kami menunjukkan bahawa adalah efektif dan layak untuk melancarkan serangan pengklonan profil silang secara automatik. Dalam serangan ini, kita dapat membuat profil palsu secara automatik dalam rangkaian di mana mangsa belum didaftarkan dan menghubungi rakan mangsa yang terdaftar di kedua-dua rangkaian tersebut. Hasil percubaan kami dengan pengguna sebenar menunjukkan bahawa serangan automatik yang kami sajikan berkesan dan dapat dilaksanakan dalam praktiknya. [[EENNDD]] umum; kecurian identiti; keselamatan rangkaian sosial"], [{"string": "Dynamics of bidding in a P2P lending service: effects of herding and predicting loan success Online peer-to-peer (P2P) lending services are a new type of social platform that enables individuals borrow and lend money directly from one to another. In this paper, we study the dynamics of bidding behavior in a P2P loan auction website, Prosper.com. We investigate the change of various attributes of loan requesting listings over time, such as the interest rate and the number of bids. We observe that there is herding behavior during bidding, and for most of the listings, the numbers of bids they receive reach spikes at very similar time points. We explain these phenomena by showing that there are economic and social factors that lenders take into account when deciding to bid on a listing. We also observe that the profits the lenders make are tied with their bidding preferences. Finally, we build a model based on the temporal progression of the bidding, that reliably predicts the success of a loan request listing, as well as whether a loan will be paid back or not.", "keywords": ["user behavior", "dynamics", "miscellaneous", "peer-to-peer lending service", "auction"], "combined": "Dynamics of bidding in a P2P lending service: effects of herding and predicting loan success Online peer-to-peer (P2P) lending services are a new type of social platform that enables individuals borrow and lend money directly from one to another. In this paper, we study the dynamics of bidding behavior in a P2P loan auction website, Prosper.com. We investigate the change of various attributes of loan requesting listings over time, such as the interest rate and the number of bids. We observe that there is herding behavior during bidding, and for most of the listings, the numbers of bids they receive reach spikes at very similar time points. We explain these phenomena by showing that there are economic and social factors that lenders take into account when deciding to bid on a listing. We also observe that the profits the lenders make are tied with their bidding preferences. Finally, we build a model based on the temporal progression of the bidding, that reliably predicts the success of a loan request listing, as well as whether a loan will be paid back or not. [[EENNDD]] user behavior; dynamics; miscellaneous; peer-to-peer lending service; auction"}, "Dinamika penawaran dalam perkhidmatan pinjaman P2P: kesan penggembalaan dan meramalkan kejayaan pinjaman Perkhidmatan pinjaman peer-to-peer (P2P) dalam talian adalah jenis platform sosial baru yang membolehkan individu meminjam dan meminjamkan wang secara langsung dari satu sama lain. Dalam makalah ini, kami mengkaji dinamika tingkah laku penawaran dalam laman web lelongan pinjaman P2P, Prosper.com. Kami menyiasat perubahan pelbagai atribut senarai permintaan pinjaman dari masa ke masa, seperti kadar faedah dan jumlah tawaran. Kami memerhatikan bahawa terdapat tingkah laku penggembalaan semasa menawar, dan untuk kebanyakan senarai, jumlah tawaran yang mereka terima mencapai lonjakan pada titik waktu yang hampir sama. Kami menerangkan fenomena ini dengan menunjukkan bahawa terdapat faktor ekonomi dan sosial yang diambil kira oleh pemberi pinjaman semasa membuat keputusan untuk membuat penyenaraian. Kami juga melihat bahawa keuntungan yang diberikan oleh pemberi pinjaman berkaitan dengan pilihan penawaran mereka. Akhirnya, kami membina model berdasarkan perkembangan sementara penawaran, yang dengan pasti dapat meramalkan kejayaan penyenaraian permintaan pinjaman, dan juga sama ada pinjaman akan dibayar balik atau tidak. [[EENNDD]] tingkah laku pengguna; dinamik; pelbagai; perkhidmatan pinjaman peer-to-peer; lelong"], [{"string": "Winner takes all: competing viruses or ideas on fair-play networks Given two competing products (or memes, or viruses etc.) spreading over a given network, can we predict what will happen at the end, that is, which product will 'win', in terms of highest market share? One may naively expect that the better product (stronger virus) will just have a larger footprint, proportional to the quality ratio of the products (or strength ratio of the viruses). However, we prove the surprising result that, under realistic conditions, for any graph topology, the stronger virus completely wipes-out the weaker one, thus not merely 'winning' but 'taking it all'. In addition to the proofs, we also demonstrate our result with simulations over diverse, real graph topologies, including the social-contact graph of the city of Portland OR (about 31 million edges and 1 million nodes) and internet AS router graphs. Finally, we also provide real data about competing products from Google-Insights, like Facebook-Myspace, and we show again that they agree with our analysis.", "keywords": ["cascades", "winner-takes-all", "competition", "epidemics"], "combined": "Winner takes all: competing viruses or ideas on fair-play networks Given two competing products (or memes, or viruses etc.) spreading over a given network, can we predict what will happen at the end, that is, which product will 'win', in terms of highest market share? One may naively expect that the better product (stronger virus) will just have a larger footprint, proportional to the quality ratio of the products (or strength ratio of the viruses). However, we prove the surprising result that, under realistic conditions, for any graph topology, the stronger virus completely wipes-out the weaker one, thus not merely 'winning' but 'taking it all'. In addition to the proofs, we also demonstrate our result with simulations over diverse, real graph topologies, including the social-contact graph of the city of Portland OR (about 31 million edges and 1 million nodes) and internet AS router graphs. Finally, we also provide real data about competing products from Google-Insights, like Facebook-Myspace, and we show again that they agree with our analysis. [[EENNDD]] cascades; winner-takes-all; competition; epidemics"}, "Pemenang mengambil semua: virus atau idea yang bersaing di rangkaian permainan adil Memandangkan dua produk yang bersaing (atau meme, atau virus dll) tersebar di rangkaian tertentu, bolehkah kita meramalkan apa yang akan berlaku pada akhirnya, iaitu produk mana yang akan 'menang ', dari segi bahagian pasaran tertinggi? Seseorang mungkin dengan naif mengharapkan bahawa produk yang lebih baik (virus yang lebih kuat) akan mempunyai jejak yang lebih besar, sebanding dengan nisbah kualiti produk (atau nisbah kekuatan virus). Walau bagaimanapun, kami membuktikan hasil yang mengejutkan bahawa, dalam keadaan yang realistik, untuk sebarang topologi grafik, virus yang lebih kuat menghapuskan virus yang lebih lemah, dengan itu bukan hanya 'menang' tetapi 'mengambil semuanya'. Sebagai tambahan kepada bukti, kami juga menunjukkan hasil kami dengan simulasi mengenai pelbagai, topologi grafik sebenar, termasuk grafik hubungan sosial di bandar Portland OR (sekitar 31 juta pinggir dan 1 juta nod) dan grafik penghala internet AS. Akhirnya, kami juga memberikan data sebenar mengenai produk pesaing dari Google-Insights, seperti Facebook-Myspace, dan kami menunjukkan sekali lagi bahawa mereka bersetuju dengan analisis kami. [[EENNDD]] lata; pemenang-mengambil-semua; pertandingan; wabak"], [{"string": "Generalized fact-finding Once information retrieval has located a document, and information extraction has provided its contents, how do we know whether we should actually believe it? Fact-finders are a state-of-the-art class of algorithms that operate in a manner analogous to Kleinberg's Hubs and Authorities, iteratively computing the trustworthiness of an information source as a function of the believability of the claims it makes, and the believability of a claim as a function of the trustworthiness of those sources asserting it. However, as fact-finders consider only \"who claims what\", they ignore a great deal of relevant background and contextual information. We present a framework for \"lifting\" (generalizing) the fact-finding process, allowing us to elegantly incorporate knowledge such as the confidence of the information extractor and the attributes of the information sources. Experiments demonstrate that leveraging this information significantly improves performance over existing, \"unlifted\" fact-finding algorithms.", "keywords": ["trust", "fact-finders", "graph algorithms", "data integration"], "combined": "Generalized fact-finding Once information retrieval has located a document, and information extraction has provided its contents, how do we know whether we should actually believe it? Fact-finders are a state-of-the-art class of algorithms that operate in a manner analogous to Kleinberg's Hubs and Authorities, iteratively computing the trustworthiness of an information source as a function of the believability of the claims it makes, and the believability of a claim as a function of the trustworthiness of those sources asserting it. However, as fact-finders consider only \"who claims what\", they ignore a great deal of relevant background and contextual information. We present a framework for \"lifting\" (generalizing) the fact-finding process, allowing us to elegantly incorporate knowledge such as the confidence of the information extractor and the attributes of the information sources. Experiments demonstrate that leveraging this information significantly improves performance over existing, \"unlifted\" fact-finding algorithms. [[EENNDD]] trust; fact-finders; graph algorithms; data integration"}, "Pencari fakta umum Setelah pencarian maklumat telah menemukan dokumen, dan pengekstrakan maklumat telah memberikan kandungannya, bagaimana kita tahu sama ada kita mesti mempercayainya? Pencari fakta adalah kelas algoritma canggih yang beroperasi dengan cara yang serupa dengan Hub dan Pihak Berkuasa Kleinberg, secara berulang mengira kebolehpercayaan sumber maklumat sebagai fungsi dari kebolehpercayaan tuntutan yang dibuatnya, dan kebolehpercayaan tuntutan sebagai fungsi dari kebolehpercayaan sumber-sumber yang menegaskannya. Namun, sebagai pencari fakta hanya mempertimbangkan \"yang menuntut apa\", mereka mengabaikan banyak latar belakang dan maklumat kontekstual yang relevan. Kami menyajikan kerangka kerja untuk \"mengangkat\" (menggeneralisasikan) proses mencari fakta, yang memungkinkan kami menggabungkan pengetahuan dengan elegan seperti keyakinan pengekstrak maklumat dan sifat sumber maklumat. Eksperimen menunjukkan bahawa memanfaatkan maklumat ini secara signifikan meningkatkan prestasi berbanding algoritma pencari fakta yang ada, \"tidak terangkat\". [[EENNDD]] kepercayaan; pencari fakta; algoritma grafik; penyatuan data"], [{"string": "Two-stream indexing for spoken web search This paper presents two-stream processing of audio to index the audio content for Spoken Web search. The first stream indexes the meta-data associated with a particular audio document. The meta-data is usually very sparse, but accurate. This therefore results in a high-precision, low-recall index. The second stream uses a novel language-independent speech recognition to generate text to be indexed. Owing to the multiple languages and the noise in user generated content on the Spoken Web, the speech recognition accuracy of such systems is not high, thus they result in a low-precision, high-recall index. The paper attempts to use these two complementary streams to generate a combined index to increase the precision-recall performance in audio content search.", "keywords": ["mobile phone", "audio search", "literacy", "multimedia information systems", "developing regions", "information search and retrieval", "world wide telecom web", "spoken web"], "combined": "Two-stream indexing for spoken web search This paper presents two-stream processing of audio to index the audio content for Spoken Web search. The first stream indexes the meta-data associated with a particular audio document. The meta-data is usually very sparse, but accurate. This therefore results in a high-precision, low-recall index. The second stream uses a novel language-independent speech recognition to generate text to be indexed. Owing to the multiple languages and the noise in user generated content on the Spoken Web, the speech recognition accuracy of such systems is not high, thus they result in a low-precision, high-recall index. The paper attempts to use these two complementary streams to generate a combined index to increase the precision-recall performance in audio content search. [[EENNDD]] mobile phone; audio search; literacy; multimedia information systems; developing regions; information search and retrieval; world wide telecom web; spoken web"}, "Pengindeksan dua aliran untuk carian web lisan Kertas ini menyajikan pemprosesan dua aliran audio untuk mengindeks kandungan audio untuk carian Web Lisan. Aliran pertama mengindeks data meta yang berkaitan dengan dokumen audio tertentu. Meta-data biasanya sangat jarang, tetapi tepat. Oleh itu, ini menghasilkan indeks penarikan rendah dengan ketepatan tinggi. Aliran kedua menggunakan pengenalan ucapan bebas bahasa novel untuk menghasilkan teks yang akan diindeks. Oleh kerana banyak bahasa dan kebisingan dalam kandungan yang dihasilkan pengguna di Spoken Web, ketepatan pengecaman pertuturan sistem seperti itu tidak tinggi, sehingga menghasilkan indeks penarikan tinggi dengan ketepatan rendah. Makalah ini berusaha menggunakan dua aliran pelengkap ini untuk menghasilkan indeks gabungan untuk meningkatkan prestasi penarikan semula ketepatan dalam carian kandungan audio. [[EENNDD]] telefon bimbit; carian audio; celik huruf; sistem maklumat multimedia; wilayah membangun; pencarian dan pengambilan maklumat; web telekomunikasi seluruh dunia; web lisan"], [{"string": "Integrating value-based requirement engineering models to webml using vip business modeling framework Requirement engineering (RE) is emerging as an increasingly important discipline for supporting Web application development, as these are designed to satisfy diverse stakeholder needs, additional functional, information, multimedia and usability requirements as compared to traditional software applications. Moreover, when considering innovative e-commerce applications, value-based RE is an extremely relevant methodology which exploits the concept of economic value during the RE activity. In contrast, most of the methodologies proposed for the development of Web applications, primarily focus on the system design, and paying less attention to the RE, and specifically to value-based RE. Focusing this aspect, the paper presents integration of value-based RE models to WebML models using our recently proposed VIP Business Modeling Framework [1]. We also analyze the framework's potential in linking other modeling approaches, and argue about its significant integration potential with various E-R/OO-based, process aware Web modeling approaches.", "keywords": ["general", "business information model", "vip", "webml", "business process model", "business value model"], "combined": "Integrating value-based requirement engineering models to webml using vip business modeling framework Requirement engineering (RE) is emerging as an increasingly important discipline for supporting Web application development, as these are designed to satisfy diverse stakeholder needs, additional functional, information, multimedia and usability requirements as compared to traditional software applications. Moreover, when considering innovative e-commerce applications, value-based RE is an extremely relevant methodology which exploits the concept of economic value during the RE activity. In contrast, most of the methodologies proposed for the development of Web applications, primarily focus on the system design, and paying less attention to the RE, and specifically to value-based RE. Focusing this aspect, the paper presents integration of value-based RE models to WebML models using our recently proposed VIP Business Modeling Framework [1]. We also analyze the framework's potential in linking other modeling approaches, and argue about its significant integration potential with various E-R/OO-based, process aware Web modeling approaches. [[EENNDD]] general; business information model; vip; webml; business process model; business value model"}, "Mengintegrasikan model kejuruteraan keperluan berasaskan nilai ke webml menggunakan kerangka pemodelan perniagaan vip Keperluan kejuruteraan (RE) muncul sebagai disiplin yang semakin penting untuk menyokong pembangunan aplikasi Web, kerana ini dirancang untuk memenuhi pelbagai keperluan pihak berkepentingan, fungsi tambahan, maklumat, multimedia dan kegunaan keperluan berbanding dengan aplikasi perisian tradisional. Lebih-lebih lagi, ketika mempertimbangkan aplikasi e-dagang yang inovatif, RE berdasarkan nilai adalah metodologi yang sangat relevan yang memanfaatkan konsep nilai ekonomi semasa aktiviti RE. Sebaliknya, kebanyakan metodologi yang diusulkan untuk pengembangan aplikasi Web, terutama berfokus pada desain sistem, dan kurang memperhatikan RE, dan khusus untuk RE berbasis nilai. Dengan memfokuskan aspek ini, makalah ini menyajikan integrasi model RE berdasarkan nilai ke model WebML menggunakan Kerangka Pemodelan Perniagaan VIP kami yang baru-baru ini dicadangkan [1]. Kami juga menganalisis potensi kerangka kerja dalam menghubungkan pendekatan pemodelan lain, dan berdebat tentang potensi integrasi yang signifikan dengan pelbagai pendekatan pemodelan Web yang berasaskan proses, berdasarkan E-R / OO. [[EENNDD]] umum; model maklumat perniagaan; vip; laman web; model proses perniagaan; model nilai perniagaan"], [{"string": "Similarity spreading: a unified framework for similarity calculation of interrelated objects No contact information provided yet.", "keywords": ["similarity spreading", "mutual reinforcement", "interrelated"], "combined": "Similarity spreading: a unified framework for similarity calculation of interrelated objects No contact information provided yet. [[EENNDD]] similarity spreading; mutual reinforcement; interrelated"}, "Penyebaran kesamaan: kerangka bersatu untuk pengiraan kesamaan objek yang saling berkaitan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kesamaan merebak; pengukuhan bersama; saling berkaitan"], [{"string": "Simulation, verification and automated composition of web services No contact information provided yet.", "keywords": ["ontologies", "representations", "web service composition", "automated reasoning", "web services", "semantic web", "daml", "distributed systems"], "combined": "Simulation, verification and automated composition of web services No contact information provided yet. [[EENNDD]] ontologies; representations; web service composition; automated reasoning; web services; semantic web; daml; distributed systems"}, "Simulasi, pengesahan dan komposisi perkhidmatan web automatik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ontologi; perwakilan; komposisi perkhidmatan web; penaakulan automatik; perkhidmatan web; web semantik; celaka; sistem yang diedarkan"], [{"string": "A client-aware dispatching algorithm for web clusters providing multiple services An abstract is not available.", "keywords": ["clusters", "load balancing", "distributed systems", "dispatching algorithms"], "combined": "A client-aware dispatching algorithm for web clusters providing multiple services An abstract is not available. [[EENNDD]] clusters; load balancing; distributed systems; dispatching algorithms"}, "Algoritma penghantaran yang peka dengan pelanggan untuk kluster web yang menyediakan pelbagai perkhidmatan Abstrak tidak tersedia. [[EENNDD]] kluster; pengimbangan beban; sistem yang diedarkan; algoritma penghantaran"], [{"string": "A system for principled matchmaking in an electronic marketplace No contact information provided yet.", "keywords": ["description logics", "knowledge representation", "decision support", "e-commerce", "matchmaking"], "combined": "A system for principled matchmaking in an electronic marketplace No contact information provided yet. [[EENNDD]] description logics; knowledge representation; decision support; e-commerce; matchmaking"}, "Sistem perjodohan berprinsip di pasaran elektronik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] logik keterangan; perwakilan pengetahuan; sokongan keputusan; e-dagang; jodoh"], [{"string": "XML design for relational storage Design principles for XML schemas that eliminate redundancies and avoid update anomalies have been studied recently. Several normal forms, generalizing those for relational databases, have been proposed. All of them, however, are based on the assumption of anative XML storage, while in practice most of XML data is stored inrelational databases.", "keywords": ["xml data", "equality-generating dependencies", "relational storage", "functional dependencies"], "combined": "XML design for relational storage Design principles for XML schemas that eliminate redundancies and avoid update anomalies have been studied recently. Several normal forms, generalizing those for relational databases, have been proposed. All of them, however, are based on the assumption of anative XML storage, while in practice most of XML data is stored inrelational databases. [[EENNDD]] xml data; equality-generating dependencies; relational storage; functional dependencies"}, "Reka bentuk XML untuk penyimpanan hubungan Prinsip reka bentuk untuk skema XML yang menghilangkan kelebihan dan mengelakkan anomali kemas kini telah dikaji baru-baru ini. Beberapa bentuk normal, menggeneralisasikannya untuk pangkalan data hubungan, telah diusulkan. Namun, kesemuanya didasarkan pada anggapan penyimpanan XML anatif, sementara dalam praktiknya kebanyakan data XML disimpan dalam pangkalan data yang tidak berkaitan. [[EENNDD]] data xml; kebergantungan menjana persamaan; simpanan hubungan; kebergantungan berfungsi"], [{"string": "Model-directed web transactions under constrained modalities No contact information provided yet.", "keywords": ["content adaption", "web transaction", "assisstive device"], "combined": "Model-directed web transactions under constrained modalities No contact information provided yet. [[EENNDD]] content adaption; web transaction; assisstive device"}, "Urus niaga web yang diarahkan model di bawah kaedah yang terhad Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] penyesuaian kandungan; transaksi web; alat bantu"], [{"string": "TCOZ approach to semantic web services design No contact information provided yet.", "keywords": ["tcoz", "daml-s", "daml+oil", "semantic web"], "combined": "TCOZ approach to semantic web services design No contact information provided yet. [[EENNDD]] tcoz; daml-s; daml+oil; semantic web"}, "Pendekatan TCOZ untuk reka bentuk perkhidmatan web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] tcoz; celaka-s; daml + minyak; web semantik"], [{"string": "Online feedback by tests and reporting for elearning and certification programs No contact information provided yet.", "keywords": ["online tests", "blended learning", "elearning"], "combined": "Online feedback by tests and reporting for elearning and certification programs No contact information provided yet. [[EENNDD]] online tests; blended learning; elearning"}, "Maklum balas dalam talian melalui ujian dan pelaporan untuk program pembelajaran dan pensijilan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ujian dalam talian; pembelajaran yang disesuaikan; pembelajaran"], [{"string": "Fine grained content-based adaptation mechanism for providing high end-user quality of experience with adaptive hypermedia systems No contact information provided yet.", "keywords": ["computer uses in education", "end-user quality of experience", "distance education", "content-based adaptation mechanism", "adaptive hypermedia"], "combined": "Fine grained content-based adaptation mechanism for providing high end-user quality of experience with adaptive hypermedia systems No contact information provided yet. [[EENNDD]] computer uses in education; end-user quality of experience; distance education; content-based adaptation mechanism; adaptive hypermedia"}, "Mekanisme penyesuaian berasaskan kandungan halus untuk memberikan pengalaman pengalaman pengguna akhir yang tinggi dengan sistem hipermedia adaptif Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] penggunaan komputer dalam pendidikan; kualiti pengalaman pengguna akhir; pendidikan jarak jauh; mekanisme penyesuaian berasaskan kandungan; hipermedia adaptif"], [{"string": "Empirical comparison of algorithms for network community detection Detecting clusters or communities in large real-world graphs such as large social or information networks is a problem of considerable interest. In practice, one typically chooses an objective function that captures the intuition of a network cluster as set of nodes with better internal connectivity than external connectivity, and then one applies approximation algorithms or heuristics to extract sets of nodes that are related to the objective function and that \"look like\" good communities for the application of interest.", "keywords": ["community structure", "conductance", "graph partitioning", "flow-based methods", "spectral methods"], "combined": "Empirical comparison of algorithms for network community detection Detecting clusters or communities in large real-world graphs such as large social or information networks is a problem of considerable interest. In practice, one typically chooses an objective function that captures the intuition of a network cluster as set of nodes with better internal connectivity than external connectivity, and then one applies approximation algorithms or heuristics to extract sets of nodes that are related to the objective function and that \"look like\" good communities for the application of interest. [[EENNDD]] community structure; conductance; graph partitioning; flow-based methods; spectral methods"}, "Perbandingan empirik algoritma untuk pengesanan komuniti rangkaian Mengesan kluster atau komuniti dalam grafik dunia nyata yang besar seperti rangkaian sosial atau maklumat yang besar adalah masalah yang cukup menarik. Dalam praktiknya, seseorang biasanya memilih fungsi objektif yang menangkap intuisi kluster rangkaian sebagai set nod dengan sambungan dalaman yang lebih baik daripada sambungan luaran, dan kemudian seseorang menggunakan algoritma penghampiran atau heuristik untuk mengekstrak set nod yang berkaitan dengan fungsi objektif dan bahawa \"kelihatan seperti\" komuniti yang baik untuk aplikasi minat [[EENNDD]] struktur komuniti; kekonduksian; pembahagian graf; kaedah berasaskan aliran; kaedah spektrum"], [{"string": "Extraction and search of chemical formulae in text documents on the web Often scientists seek to search for articles on the Web related to a particular chemical. When a scientist searches for a chemical formula using a search engine today, she gets articles where the exact keyword string expressing the chemical formula is found. Searching for the exact occurrence of keywords during searching results in two problems for this domain: a) if the author searches for CH4 and the article has H4C, the article is not returned, and b) ambiguous searches like \"He\" return all documents where Helium is mentioned as well as documents where the pronoun \"he\" occurs. To remedy these deficiencies, we propose a chemical formula search engine. To build a chemical formula search engine, we must solve the following problems: 1) extract chemical formulae from text documents, 2) index chemical formulae, and 3) designranking functions for the chemical formulae. Furthermore, query models are introduced for formula search, and for each a scoring scheme based on features of partial formulae is proposed tomeasure the relevance of chemical formulae and queries. We evaluate algorithms for identifying chemical formulae in documents using classification methods based on Support Vector Machines(SVM), and a probabilistic model based on conditional random fields (CRF). Different methods for SVM and CRF to tune the trade-off between recall and precision forim balanced data are proposed to improve the overall performance. A feature selection method based on frequency and discrimination isused to remove uninformative and redundant features. Experiments show that our approaches to chemical formula extraction work well, especially after trade-off tuning. The results also demonstrate that feature selection can reduce the index size without changing ranked query results much.", "keywords": ["ranking", "query models", "feature boosting", "conditional random fields", "chemical formula", "entity extraction", "feature selection", "similarity search", "support vector machines"], "combined": "Extraction and search of chemical formulae in text documents on the web Often scientists seek to search for articles on the Web related to a particular chemical. When a scientist searches for a chemical formula using a search engine today, she gets articles where the exact keyword string expressing the chemical formula is found. Searching for the exact occurrence of keywords during searching results in two problems for this domain: a) if the author searches for CH4 and the article has H4C, the article is not returned, and b) ambiguous searches like \"He\" return all documents where Helium is mentioned as well as documents where the pronoun \"he\" occurs. To remedy these deficiencies, we propose a chemical formula search engine. To build a chemical formula search engine, we must solve the following problems: 1) extract chemical formulae from text documents, 2) index chemical formulae, and 3) designranking functions for the chemical formulae. Furthermore, query models are introduced for formula search, and for each a scoring scheme based on features of partial formulae is proposed tomeasure the relevance of chemical formulae and queries. We evaluate algorithms for identifying chemical formulae in documents using classification methods based on Support Vector Machines(SVM), and a probabilistic model based on conditional random fields (CRF). Different methods for SVM and CRF to tune the trade-off between recall and precision forim balanced data are proposed to improve the overall performance. A feature selection method based on frequency and discrimination isused to remove uninformative and redundant features. Experiments show that our approaches to chemical formula extraction work well, especially after trade-off tuning. The results also demonstrate that feature selection can reduce the index size without changing ranked query results much. [[EENNDD]] ranking; query models; feature boosting; conditional random fields; chemical formula; entity extraction; feature selection; similarity search; support vector machines"}, "Pengekstrakan dan pencarian formula kimia dalam dokumen teks di web Selalunya para saintis berusaha untuk mencari artikel di Web yang berkaitan dengan bahan kimia tertentu. Ketika seorang saintis mencari formula kimia menggunakan mesin pencari hari ini, dia mendapat artikel di mana rentetan kata kunci yang tepat yang menyatakan formula kimia dijumpai. Mencari kata kunci yang tepat semasa mencari hasil dalam dua masalah untuk domain ini: a) jika pengarang mencari CH4 dan artikel tersebut mempunyai H4C, artikel itu tidak dikembalikan, dan b) carian yang tidak jelas seperti \"Dia\" mengembalikan semua dokumen di mana Helium disebutkan serta dokumen di mana kata ganti \"dia\" berlaku. Untuk mengatasi kekurangan ini, kami mencadangkan mesin carian formula kimia. Untuk membina mesin carian formula kimia, kita mesti menyelesaikan masalah berikut: 1) mengekstrak formula kimia dari dokumen teks, 2) formula kimia indeks, dan 3) merancang fungsi untuk formula kimia. Selanjutnya, model pertanyaan diperkenalkan untuk pencarian formula, dan untuk masing-masing skema pemarkahan berdasarkan ciri formula separa dicadangkan untuk memastikan relevansi formula dan pertanyaan kimia. Kami menilai algoritma untuk mengenal pasti formula kimia dalam dokumen menggunakan kaedah klasifikasi berdasarkan Mesin Sokongan Vektor (SVM), dan model probabilistik berdasarkan bidang rawak bersyarat (CRF). Kaedah yang berbeza untuk SVM dan CRF untuk menyesuaikan pertukaran antara penarikan balik dan data keseimbangan forim yang tepat dicadangkan untuk meningkatkan prestasi keseluruhan. Kaedah pemilihan ciri berdasarkan frekuensi dan diskriminasi digunakan untuk menghilangkan ciri yang tidak berinformasi dan berlebihan. Eksperimen menunjukkan bahawa pendekatan kami terhadap pengekstrakan formula kimia berfungsi dengan baik, terutamanya setelah penalaan pertukaran. Hasilnya juga menunjukkan bahawa pemilihan ciri dapat mengurangi ukuran indeks tanpa mengubah hasil kueri peringkat. [[EENNDD]] kedudukan; model pertanyaan; peningkatan ciri; medan rawak bersyarat; formula kimia; pengekstrakan entiti; pemilihan ciri; carian kesamaan; mesin vektor sokongan"], [{"string": "Visualizing tags over time No contact information provided yet.", "keywords": ["interval covering", "temporal evolution", "tags", "information search and retrieval", "social media", "visualization", "flickr"], "combined": "Visualizing tags over time No contact information provided yet. [[EENNDD]] interval covering; temporal evolution; tags; information search and retrieval; social media; visualization; flickr"}, "Visualisasi tag dari masa ke masa Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] selang selang; evolusi temporal; tag; pencarian dan pengambilan maklumat; media sosial; visualisasi; flickr"], [{"string": "Web-scale classification with naive bayes Traditional Naive Bayes Classifier performs miserably on web-scale taxonomies. In this paper, we investigate the reasons behind such bad performance. We discover that the low performance are not completely caused by the intrinsic limitations of Naive Bayes, but mainly comes from two largely ignored problems: contradiction pair problem and discriminative evidence cancelation problem. We propose modifications that can alleviate the two problems while preserving the advantages of Naive Bayes. The experimental results show our modified Naive Bayes can significantly improve the performance on real web-scale taxonomies.", "keywords": ["learning", "web-scale taxonomy", "naive bayes"], "combined": "Web-scale classification with naive bayes Traditional Naive Bayes Classifier performs miserably on web-scale taxonomies. In this paper, we investigate the reasons behind such bad performance. We discover that the low performance are not completely caused by the intrinsic limitations of Naive Bayes, but mainly comes from two largely ignored problems: contradiction pair problem and discriminative evidence cancelation problem. We propose modifications that can alleviate the two problems while preserving the advantages of Naive Bayes. The experimental results show our modified Naive Bayes can significantly improve the performance on real web-scale taxonomies. [[EENNDD]] learning; web-scale taxonomy; naive bayes"}, "Klasifikasi skala web dengan bayes naif Tradisional Naive Bayes Classifier menunjukkan prestasi buruk pada taksonomi skala web. Dalam makalah ini, kami menyiasat sebab-sebab di sebalik prestasi buruk seperti itu. Kami mendapati bahawa prestasi rendah tidak sepenuhnya disebabkan oleh batasan intrinsik Naive Bayes, tetapi berpunca daripada dua masalah yang diabaikan: masalah pasangan percanggahan dan masalah pembatalan bukti yang diskriminatif. Kami mencadangkan pengubahsuaian yang dapat mengurangkan dua masalah sambil mengekalkan kelebihan Naive Bayes. Hasil eksperimen menunjukkan Naive Bayes yang diubahsuai dapat meningkatkan prestasi secara signifikan pada taksonomi skala web sebenar. [[EENNDD]] pembelajaran; taksonomi skala web; bayes naif"], [{"string": "A probabilistic semantic approach for discovering web services Service discovery is one of challenging issues in Service-Oriented computing. Currently, most of the existing service discovering and matching approaches are based on keywords-based strategy. However, this method is inefficient and time-consuming. In this paper, we present a novel approach for discovering web services. Based on the current dominating mechanisms of discovering and describing Web Services with UDDI and WSDL, the proposed approach utilizes Probabilistic Latent Semantic Analysis (PLSA) to capture semantic concepts hidden behind words in the query and advertisements in services so that services matching is expected to carry out at concept level. We also present related algorithms and preliminary experiments to evaluate the effectiveness of our approach.", "keywords": ["on-line information services", "web service", "web services matching"], "combined": "A probabilistic semantic approach for discovering web services Service discovery is one of challenging issues in Service-Oriented computing. Currently, most of the existing service discovering and matching approaches are based on keywords-based strategy. However, this method is inefficient and time-consuming. In this paper, we present a novel approach for discovering web services. Based on the current dominating mechanisms of discovering and describing Web Services with UDDI and WSDL, the proposed approach utilizes Probabilistic Latent Semantic Analysis (PLSA) to capture semantic concepts hidden behind words in the query and advertisements in services so that services matching is expected to carry out at concept level. We also present related algorithms and preliminary experiments to evaluate the effectiveness of our approach. [[EENNDD]] on-line information services; web service; web services matching"}, "Pendekatan semantik probabilistik untuk mencari perkhidmatan web Penemuan perkhidmatan adalah salah satu masalah yang mencabar dalam pengkomputeran Berorientasikan Perkhidmatan. Pada masa ini, kebanyakan pendekatan penemuan dan pencocokan perkhidmatan yang ada didasarkan pada strategi berdasarkan kata kunci. Walau bagaimanapun, kaedah ini tidak cekap dan memakan masa. Dalam makalah ini, kami menyajikan pendekatan baru untuk mencari perkhidmatan web. Berdasarkan mekanisme mendominasi semasa mencari dan menerangkan Perkhidmatan Web dengan UDDI dan WSDL, pendekatan yang dicadangkan menggunakan Analisis Semantik Latar Belakang Probabilistik (PLSA) untuk menangkap konsep semantik yang tersembunyi di sebalik kata-kata dalam pertanyaan dan iklan dalam perkhidmatan sehingga pencocokan perkhidmatan diharapkan dapat dilakukan keluar pada tahap konsep. Kami juga membentangkan algoritma dan eksperimen awal yang berkaitan untuk menilai keberkesanan pendekatan kami. [[EENNDD]] perkhidmatan maklumat dalam talian; perkhidmatan web; pemadanan perkhidmatan web"], [{"string": "Exception handling in workflow-driven Web applications No contact information provided yet.", "keywords": ["exceptions", "navigation behavior", "failure", "computer-aided software engineering", "workflow", "web applications"], "combined": "Exception handling in workflow-driven Web applications No contact information provided yet. [[EENNDD]] exceptions; navigation behavior; failure; computer-aided software engineering; workflow; web applications"}, "Pengendalian pengecualian dalam aplikasi Web yang didorong oleh aliran kerja Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengecualian; tingkah laku navigasi; kegagalan; kejuruteraan perisian berbantukan komputer; aliran kerja; aplikasi web"], [{"string": "A storage and indexing framework for p2p systems No contact information provided yet.", "keywords": ["peer-to-peer", "p2p", "indexing framework"], "combined": "A storage and indexing framework for p2p systems No contact information provided yet. [[EENNDD]] peer-to-peer; p2p; indexing framework"}, "Rangka kerja penyimpanan dan pengindeksan untuk sistem p2p Belum ada maklumat hubungan yang diberikan. [[EENNDD]] rakan sebaya; p2p; kerangka pengindeksan"], [{"string": "Personal TV viewing by using live chat as metadata No contact information provided yet.", "keywords": ["live chat", "semantic analysis", "fusion of broadcast and web content", "viewpoint", "metadata generation", "video", "viewer", "digest"], "combined": "Personal TV viewing by using live chat as metadata No contact information provided yet. [[EENNDD]] live chat; semantic analysis; fusion of broadcast and web content; viewpoint; metadata generation; video; viewer; digest"}, "Tontonan TV peribadi dengan menggunakan sembang langsung sebagai metadata Belum ada maklumat hubungan yang diberikan. [[EENNDD]] sembang langsung; analisis semantik; penyatuan kandungan siaran dan web; sudut pandangan; penjanaan metadata; video; penonton; mencerna"], [{"string": "Web-assisted annotation, semantic indexing and search of television and radio news No contact information provided yet.", "keywords": ["media archiving", "semantic annotation", "topical segmentation", "probabilistic algorithms", "web search", "natural language processing", "automatic speech recognition", "key-phrase extraction", "semantic web", "multimedia"], "combined": "Web-assisted annotation, semantic indexing and search of television and radio news No contact information provided yet. [[EENNDD]] media archiving; semantic annotation; topical segmentation; probabilistic algorithms; web search; natural language processing; automatic speech recognition; key-phrase extraction; semantic web; multimedia"}, "Anotasi berbantukan web, pengindeksan semantik dan carian berita televisyen dan radio Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengarkiban media; anotasi semantik; segmentasi topikal; algoritma probabilistik; carian sesawang; pemprosesan bahasa semula jadi; pengecaman pertuturan automatik; pengekstrakan frasa kunci; web semantik; multimedia"], [{"string": "Automatic web service composition with abstraction and refinement The behavioral description based Web Service Composition (WSC) problem aims at the automatic construction of a coordinator web service that controls a set of web services to reach a goal state. However, solving the WSC problem exactly with a realistic model is doubly-exponential in the number of variables in web service descriptions. In this paper, we propose a novel efficient approximation-based algorithm using automatic abstraction and refinement to dramatically reduce the number of variables needed to solve the problem.", "keywords": ["refinement", "service composition", "abstraction"], "combined": "Automatic web service composition with abstraction and refinement The behavioral description based Web Service Composition (WSC) problem aims at the automatic construction of a coordinator web service that controls a set of web services to reach a goal state. However, solving the WSC problem exactly with a realistic model is doubly-exponential in the number of variables in web service descriptions. In this paper, we propose a novel efficient approximation-based algorithm using automatic abstraction and refinement to dramatically reduce the number of variables needed to solve the problem. [[EENNDD]] refinement; service composition; abstraction"}, "Komposisi perkhidmatan web automatik dengan pengambilan dan penyempurnaan Masalah Komposisi Perkhidmatan Web (WSC) berdasarkan deskripsi tingkah laku bertujuan untuk pembinaan perkhidmatan web penyelaras automatik yang mengawal sekumpulan perkhidmatan web untuk mencapai keadaan tujuan. Walau bagaimanapun, menyelesaikan masalah WSC tepat dengan model realistik adalah dua kali ganda eksponen dalam jumlah pemboleh ubah dalam deskripsi perkhidmatan web. Dalam makalah ini, kami mengusulkan algoritma berdasarkan pendekatan baru yang efisien menggunakan abstraksi dan penyempurnaan automatik untuk mengurangkan secara drastik jumlah pemboleh ubah yang diperlukan untuk menyelesaikan masalah. [[EENNDD]] penyempurnaan; komposisi perkhidmatan; pengabstrakan"], [{"string": "Communication as information-seeking: the case for mobile social software for developing regions In this paper, we describe several findings from a multi-year, multi-method study of how information and communication technologies have been adopted and adapted in Central Asia. We have found that mobile phone usage is outpacing the rate of Internet adoption, that access to the Internet is primarily through public access sites carrying with it issues regarding privacy and surveillance, that people rely on their social networks as information sources, that public institutions tend to be fairly weak as citizen resources, and that information seeking and communication are conflated in people's usage patterns with different technologies. In addition, in the developed world social networking software has grown rapidly and shown itself to have significant potential for mobilizing a population. Based on the collection of findings from Central Asia and observing patterns of technology usage in other parts of the world, our research leads to the conclusion that exploring mobile social software holds significant potential as an ICT that meshes well with preexisting patterns of communication and information seeking and also leverages the most predominant pattern of technology adoption. Many of the findings from this research echo results from studies in other geographic areas, and so we anticipate that much of this research will be relevant to developing regions generally.", "keywords": ["developing world", "mobile social software", "internet", "central asia", "design ethnography", "international", "social networks", "emerging markets", "mobile devices", "usage patterns", "icts", "sms", "cell phones", "cross-cultural"], "combined": "Communication as information-seeking: the case for mobile social software for developing regions In this paper, we describe several findings from a multi-year, multi-method study of how information and communication technologies have been adopted and adapted in Central Asia. We have found that mobile phone usage is outpacing the rate of Internet adoption, that access to the Internet is primarily through public access sites carrying with it issues regarding privacy and surveillance, that people rely on their social networks as information sources, that public institutions tend to be fairly weak as citizen resources, and that information seeking and communication are conflated in people's usage patterns with different technologies. In addition, in the developed world social networking software has grown rapidly and shown itself to have significant potential for mobilizing a population. Based on the collection of findings from Central Asia and observing patterns of technology usage in other parts of the world, our research leads to the conclusion that exploring mobile social software holds significant potential as an ICT that meshes well with preexisting patterns of communication and information seeking and also leverages the most predominant pattern of technology adoption. Many of the findings from this research echo results from studies in other geographic areas, and so we anticipate that much of this research will be relevant to developing regions generally. [[EENNDD]] developing world; mobile social software; internet; central asia; design ethnography; international; social networks; emerging markets; mobile devices; usage patterns; icts; sms; cell phones; cross-cultural"}, "Komunikasi sebagai pencarian maklumat: kes untuk perisian sosial mudah alih untuk membangun wilayah Dalam makalah ini, kami menjelaskan beberapa penemuan dari kajian multi-kaedah, multi-kaedah bagaimana teknologi maklumat dan komunikasi telah diadopsi dan diadaptasi di Asia Tengah. Kami mendapati bahawa penggunaan telefon bimbit melebihi kadar penggunaan Internet, bahawa akses ke Internet terutama melalui laman web akses awam yang membawa masalah berkaitan privasi dan pengawasan, bahawa orang bergantung pada rangkaian sosial mereka sebagai sumber maklumat, yang cenderung oleh institusi awam cukup lemah sebagai sumber daya warga negara, dan bahawa pencarian maklumat dan komunikasi digabungkan dalam corak penggunaan orang dengan teknologi yang berbeza. Di samping itu, di dunia maju perisian rangkaian sosial telah berkembang pesat dan menunjukkan dirinya berpotensi besar untuk menggerakkan populasi. Berdasarkan koleksi penemuan dari Asia Tengah dan mengamati corak penggunaan teknologi di bahagian lain dunia, penyelidikan kami membawa kepada kesimpulan bahawa meneroka perisian sosial mudah alih berpotensi besar sebagai ICT yang sesuai dengan corak komunikasi dan pencarian maklumat yang sudah ada sebelumnya dan juga memanfaatkan corak penggunaan teknologi yang paling utama. Sebilangan besar penemuan dari penyelidikan ini adalah hasil kajian di kawasan geografi lain, dan oleh itu kami menjangkakan bahawa banyak penyelidikan ini akan relevan dengan wilayah berkembang pada umumnya. [[EENNDD]] dunia membangun; perisian sosial mudah alih; internet; Asia Tengah; etnografi reka bentuk; antarabangsa; rangkaian sosial; pasaran baru muncul; peranti mudah alih; corak penggunaan; icts; sms; telefon bimbit; silang budaya"], [{"string": "Evaluating strategies for similarity search on the web No contact information provided yet.", "keywords": ["evaluation", "related pages", "similarity search", "open directory project", "search"], "combined": "Evaluating strategies for similarity search on the web No contact information provided yet. [[EENNDD]] evaluation; related pages; similarity search; open directory project; search"}, "Menilai strategi untuk carian kesamaan di web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penilaian; halaman berkaitan; carian kesamaan; projek direktori terbuka; cari"], [{"string": "Einstein: physicist or vegetarian? summarizing semantic type graphs for knowledge discovery The Web and, in particular, knowledge-sharing communities such as Wikipedia contain a huge amount of information encompassing disparate and diverse fields. Knowledge bases such as DBpedia or Yago represent the data in a concise and more structured way bearing the potential of bringing database tools to Web Search. The wealth of data, however, poses the challenge of how to retrieve important and valuable information, which is often intertwined with trivial and less important details. This calls for an efficient and automatic summarization method.", "keywords": ["summarization", "semantic search", "knowledge bases"], "combined": "Einstein: physicist or vegetarian? summarizing semantic type graphs for knowledge discovery The Web and, in particular, knowledge-sharing communities such as Wikipedia contain a huge amount of information encompassing disparate and diverse fields. Knowledge bases such as DBpedia or Yago represent the data in a concise and more structured way bearing the potential of bringing database tools to Web Search. The wealth of data, however, poses the challenge of how to retrieve important and valuable information, which is often intertwined with trivial and less important details. This calls for an efficient and automatic summarization method. [[EENNDD]] summarization; semantic search; knowledge bases"}, "Einstein: ahli fizik atau vegetarian? merangkum grafik jenis semantik untuk penemuan pengetahuan Web dan, khususnya, komuniti perkongsian pengetahuan seperti Wikipedia mengandungi sejumlah besar maklumat merangkumi bidang yang berbeza dan pelbagai. Pangkalan pengetahuan seperti DBpedia atau Yago mewakili data secara ringkas dan lebih tersusun yang berpotensi membawa alat pangkalan data ke Carian Web. Kekayaan data, bagaimanapun, menimbulkan tantangan bagaimana mendapatkan maklumat penting dan berharga, yang sering dihubungkan dengan perincian yang remeh dan tidak penting. Ini memerlukan kaedah ringkasan yang cekap dan automatik. [[EENNDD]] ringkasan; carian semantik; asas pengetahuan"], [{"string": "Ranking a stream of news No contact information provided yet.", "keywords": ["information extraction", "news engines", "news ranking"], "combined": "Ranking a stream of news No contact information provided yet. [[EENNDD]] information extraction; news engines; news ranking"}, "Peringkat aliran berita Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengekstrakan maklumat; enjin berita; peringkat berita"], [{"string": "DEW: DNS-enhanced web for faster content delivery No contact information provided yet.", "keywords": ["network architecture and design"], "combined": "DEW: DNS-enhanced web for faster content delivery No contact information provided yet. [[EENNDD]] network architecture and design"}, "DEW: Web yang disempurnakan DNS untuk penghantaran kandungan yang lebih pantas Belum ada maklumat hubungan yang diberikan. [[EENNDD]] seni bina dan reka bentuk rangkaian"], [{"string": "Web service interfaces No contact information provided yet.", "keywords": ["formal verification", "web service substitutivity", "web services", "formal specification", "web service compatibility", "web service interfaces"], "combined": "Web service interfaces No contact information provided yet. [[EENNDD]] formal verification; web service substitutivity; web services; formal specification; web service compatibility; web service interfaces"}, "Antara muka perkhidmatan web Belum ada maklumat hubungan yang diberikan [[EENNDD]] pengesahan rasmi; penggantian perkhidmatan web; perkhidmatan web; spesifikasi rasmi; keserasian perkhidmatan web; antara muka perkhidmatan web"], [{"string": "Incremental maintenance for materialized XPath/XSLT views No contact information provided yet.", "keywords": ["xslt", "xpath", "xml", "materialized view", "view maintenance"], "combined": "Incremental maintenance for materialized XPath/XSLT views No contact information provided yet. [[EENNDD]] xslt; xpath; xml; materialized view; view maintenance"}, "Penyelenggaraan tambahan untuk paparan XPath / XSLT terwujud Belum ada maklumat hubungan yang diberikan. [[EENNDD]] xslt; xpath; xml; pandangan terwujud; penyelenggaraan pandangan"], [{"string": "Time-based contextualized-news browser (t-cnb) No contact information provided yet.", "keywords": ["contextualized news articles", "web browser", "topic graph", "miscellaneous"], "combined": "Time-based contextualized-news browser (t-cnb) No contact information provided yet. [[EENNDD]] contextualized news articles; web browser; topic graph; miscellaneous"}, "Penyemak imbas berita berdasarkan konteks (t-cnb) Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] artikel berita kontekstual; pelayar web; grafik topik; pelbagai"], [{"string": "The anatomy of a news search engine No contact information provided yet.", "keywords": ["information search and retrieval", "extraction", "syndication", "news search engines", "information"], "combined": "The anatomy of a news search engine No contact information provided yet. [[EENNDD]] information search and retrieval; extraction; syndication; news search engines; information"}, "Anatomi enjin carian berita Belum ada maklumat hubungan yang diberikan. [[EENNDD]] carian dan pengambilan maklumat; pengekstrakan; sindiket; enjin carian berita; maklumat"], [{"string": "Adaptive bidding for display advertising Motivated by the emergence of auction-based marketplaces for display ads such as the Right Media Exchange, we study the design of a bidding agent that implements a display advertising campaign by bidding in such a marketplace. The bidding agent must acquire a given number of impressions with a given target spend, when the highest external bid in the marketplace is drawn from an unknown distribution P. The quantity and spend constraints arise from the fact that display ads are usually sold on a CPM basis. We consider both the full information setting, where the winning price in each auction is announced publicly, and the partially observable setting where only the winner obtains information about the distribution; these differ in the penalty incurred by the agent while attempting to learn the distribution. We provide algorithms for both settings, and prove performance guarantees using bounds on uniform closeness from statistics, and techniques from online learning. We experimentally evaluate these algorithms: both algorithms perform very well with respect to both target quantity and spend; further, our algorithm for the partially observable case performs nearly as well as that for the fully observable setting despite the higher penalty incurred during learning.", "keywords": ["adaptive bidding", "general", "display advertising", "concentration bounds", "guess-then-double algorithms", "guaranteed delivery"], "combined": "Adaptive bidding for display advertising Motivated by the emergence of auction-based marketplaces for display ads such as the Right Media Exchange, we study the design of a bidding agent that implements a display advertising campaign by bidding in such a marketplace. The bidding agent must acquire a given number of impressions with a given target spend, when the highest external bid in the marketplace is drawn from an unknown distribution P. The quantity and spend constraints arise from the fact that display ads are usually sold on a CPM basis. We consider both the full information setting, where the winning price in each auction is announced publicly, and the partially observable setting where only the winner obtains information about the distribution; these differ in the penalty incurred by the agent while attempting to learn the distribution. We provide algorithms for both settings, and prove performance guarantees using bounds on uniform closeness from statistics, and techniques from online learning. We experimentally evaluate these algorithms: both algorithms perform very well with respect to both target quantity and spend; further, our algorithm for the partially observable case performs nearly as well as that for the fully observable setting despite the higher penalty incurred during learning. [[EENNDD]] adaptive bidding; general; display advertising; concentration bounds; guess-then-double algorithms; guaranteed delivery"}, "Bidding adaptif untuk iklan paparan Dimotivasi oleh munculnya pasar berasaskan lelong untuk iklan bergambar seperti Right Media Exchange, kami mempelajari reka bentuk ejen pembidaan yang menerapkan kempen iklan display dengan menawar di pasar semacam itu. Ejen pembida mesti memperoleh sejumlah tera dengan sasaran perbelanjaan tertentu, apabila tawaran luaran tertinggi di pasaran diambil dari sebaran P. yang tidak diketahui. Kekangan kuantiti dan perbelanjaan timbul dari kenyataan bahawa iklan paparan biasanya dijual pada BPS asas. Kami mempertimbangkan kedua-dua tetapan maklumat lengkap, di mana harga kemenangan dalam setiap lelongan diumumkan secara terbuka, dan pengaturan yang dapat dilihat sebahagiannya di mana hanya pemenang yang memperoleh maklumat mengenai pengedaran; ini berbeza dalam penalti yang dikenakan oleh ejen semasa cuba mengetahui pengedarannya. Kami menyediakan algoritma untuk kedua-dua tetapan, dan membuktikan jaminan prestasi dengan menggunakan jarak yang hampir sama dari statistik, dan teknik dari pembelajaran dalam talian. Kami secara eksperimen menilai algoritma ini: kedua-dua algoritma menunjukkan prestasi yang baik sehubungan dengan kuantiti dan perbelanjaan sasaran; selanjutnya, algoritma kami untuk kes separa yang dapat dilihat berfungsi hampir sama dengan yang dapat dilihat sepenuhnya walaupun terdapat hukuman yang lebih tinggi yang dikenakan semasa belajar. [[EENNDD]] pembidaan adaptif; umum; iklan paparan; had tumpuan; algoritma dugaan-kemudian-dua; penghantaran dijamin"], [{"string": "Improving mobile internet usability An abstract is not available.", "keywords": ["mobile internet", "hci", "usability", "wap", "portable devices"], "combined": "Improving mobile internet usability An abstract is not available. [[EENNDD]] mobile internet; hci; usability; wap; portable devices"}, "Meningkatkan kebolehgunaan internet mudah alih Abstrak tidak tersedia. [[EENNDD]] internet mudah alih; hci; kebolehgunaan; wap; peranti mudah alih"], [{"string": "A class-feature-centroid classifier for text categorization Automated text categorization is an important technique for many web applications, such as document indexing, document filtering, and cataloging web resources. Many different approaches have been proposed for the automated text categorization problem. Among them, centroid-based approaches have the advantages of short training time and testing time due to its computational efficiency. As a result, centroid-based classifiers have been widely used in many web applications. However, the accuracy of centroid-based classifiers is inferior to SVM, mainly because centroids found during construction are far from perfect locations.", "keywords": ["inner-class", "inter-class", "text classification", "centroid", "denormalized cosine measure"], "combined": "A class-feature-centroid classifier for text categorization Automated text categorization is an important technique for many web applications, such as document indexing, document filtering, and cataloging web resources. Many different approaches have been proposed for the automated text categorization problem. Among them, centroid-based approaches have the advantages of short training time and testing time due to its computational efficiency. As a result, centroid-based classifiers have been widely used in many web applications. However, the accuracy of centroid-based classifiers is inferior to SVM, mainly because centroids found during construction are far from perfect locations. [[EENNDD]] inner-class; inter-class; text classification; centroid; denormalized cosine measure"}, "Pengklasifikasi kelas-ciri-sentroid untuk pengkategorian teks Pengkategorian teks automatik adalah teknik penting untuk banyak aplikasi web, seperti pengindeksan dokumen, penyaringan dokumen, dan pengkatalogan sumber web. Banyak pendekatan yang berbeza telah diusulkan untuk masalah pengkategorian teks automatik. Antaranya, pendekatan berasaskan centroid mempunyai kelebihan waktu latihan yang pendek dan masa ujian kerana kecekapan pengiraannya. Hasilnya, pengkelasan berasaskan centroid telah digunakan secara meluas dalam banyak aplikasi web. Walau bagaimanapun, ketepatan pengkelasan berasaskan centroid lebih rendah daripada SVM, terutamanya kerana centroid yang dijumpai semasa pembinaan jauh dari lokasi yang sempurna. [[EENNDD]] kelas dalaman; antara kelas; pengelasan teks; sentroid; ukuran kosinus denormalisasi"], [{"string": "Utility analysis for topically biased PageRank PageRank is known to be an efficient metric for computing general document importance in the Web. While commonly used as a one-size-fits-all measure, the ability to produce topically biased ranks has not yet been fully explored in detail. In particular, it was still unclear to what granularity of \"topic\" the computation of biased page ranks makes sense. In this paper we present the results of a thorough quantitative and qualitative analysis of biasing PageRank on Open Directory categories. We show that the MAP quality of Biased PageRank generally increases with the ODP level up to a certain point, thus sustaining the usage of more specialized categories to bias PageRank on, in order to improve topic specific search.", "keywords": ["biased pagerank", "personalized search", "information search and retrieval", "open directory"], "combined": "Utility analysis for topically biased PageRank PageRank is known to be an efficient metric for computing general document importance in the Web. While commonly used as a one-size-fits-all measure, the ability to produce topically biased ranks has not yet been fully explored in detail. In particular, it was still unclear to what granularity of \"topic\" the computation of biased page ranks makes sense. In this paper we present the results of a thorough quantitative and qualitative analysis of biasing PageRank on Open Directory categories. We show that the MAP quality of Biased PageRank generally increases with the ODP level up to a certain point, thus sustaining the usage of more specialized categories to bias PageRank on, in order to improve topic specific search. [[EENNDD]] biased pagerank; personalized search; information search and retrieval; open directory"}, "Analisis utiliti untuk PageRank yang berat sebelah secara topikal PageRank dikenali sebagai metrik yang cekap untuk mengira kepentingan dokumen umum dalam Web. Walaupun biasanya digunakan sebagai ukuran satu-ukuran-semua, kemampuan untuk menghasilkan jajaran bias topikal belum diterokai secara terperinci. Secara khusus, masih belum jelas butiran \"topik\" apa yang dihitung pengiraan peringkat halaman yang berat sebelah. Dalam makalah ini kami menyajikan hasil analisis kuantitatif dan kualitatif menyeluruh untuk memihak kategori PageRank pada Open Directory. Kami menunjukkan bahawa kualiti MAP dari Bias PageRank pada umumnya meningkat dengan tingkat ODP hingga titik tertentu, sehingga mempertahankan penggunaan kategori yang lebih khusus untuk bias PageRank, untuk meningkatkan pencarian khusus topik. [[EENNDD]] pagerank berat sebelah; carian diperibadikan; pencarian dan pengambilan maklumat; buka direktori"], [{"string": "Impact of search engines on page popularity No contact information provided yet.", "keywords": ["pagerank", "information search and retrieval", "miscellaneous", "random surfer model", "search engine's impact", "change in pagerank", "web evolution"], "combined": "Impact of search engines on page popularity No contact information provided yet. [[EENNDD]] pagerank; information search and retrieval; miscellaneous; random surfer model; search engine's impact; change in pagerank; web evolution"}, "Kesan enjin carian pada populariti halaman Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pagerank; pencarian dan pengambilan maklumat; pelbagai; model surfer rawak; kesan enjin carian; perubahan dalam pagerank; evolusi web"], [{"string": "Distributed graph pattern matching Graph simulation has been adopted for pattern matching to reduce the complexity and capture the need of novel applications. With the rapid development of the Web and social networks, data is typically distributed over multiple machines. Hence a natural question raised is how to evaluate graph simulation on distributed data. To our knowledge, no such distributed algorithms are in place yet. This paper settles this question by providing evaluation algorithms and optimizations for graph simulation in a distributed setting. (1) We study the impacts of components and data locality on the evaluation of graph simulation. (2) We give an analysis of a large class of distributed algorithms, captured by a message-passing model, for graph simulation. We also identify three complexity measures: visit times, makespan and data shipment, for analyzing the distributed algorithms, and show that these measures are essentially controversial with each other. (3) We propose distributed algorithms and optimization techniques that exploit the properties of graph simulation and the analyses of distributed algorithms. (4) We experimentally verify the effectiveness and efficiency of these algorithms, using both real-life and synthetic data.", "keywords": ["distributed algorithms", "graph querying", "graph simulation"], "combined": "Distributed graph pattern matching Graph simulation has been adopted for pattern matching to reduce the complexity and capture the need of novel applications. With the rapid development of the Web and social networks, data is typically distributed over multiple machines. Hence a natural question raised is how to evaluate graph simulation on distributed data. To our knowledge, no such distributed algorithms are in place yet. This paper settles this question by providing evaluation algorithms and optimizations for graph simulation in a distributed setting. (1) We study the impacts of components and data locality on the evaluation of graph simulation. (2) We give an analysis of a large class of distributed algorithms, captured by a message-passing model, for graph simulation. We also identify three complexity measures: visit times, makespan and data shipment, for analyzing the distributed algorithms, and show that these measures are essentially controversial with each other. (3) We propose distributed algorithms and optimization techniques that exploit the properties of graph simulation and the analyses of distributed algorithms. (4) We experimentally verify the effectiveness and efficiency of these algorithms, using both real-life and synthetic data. [[EENNDD]] distributed algorithms; graph querying; graph simulation"}, "Pencocokan pola grafik yang diedarkan Simulasi grafik telah digunakan untuk pemadanan corak untuk mengurangkan kerumitan dan menangkap keperluan aplikasi baru. Dengan perkembangan pesat Web dan rangkaian sosial, data biasanya diedarkan melalui beberapa mesin. Oleh itu, persoalan semula jadi yang dikemukakan adalah bagaimana menilai simulasi grafik pada data yang diedarkan. Setahu kami, belum ada algoritma yang diedarkan. Makalah ini menyelesaikan soalan ini dengan memberikan algoritma penilaian dan pengoptimuman untuk simulasi grafik dalam suasana yang diedarkan. (1) Kami mengkaji kesan komponen dan lokasi data pada penilaian simulasi grafik. (2) Kami memberikan analisis kelas besar algoritma terdistribusi, diambil oleh model penyampaian pesan, untuk simulasi grafik. Kami juga mengenal pasti tiga langkah kerumitan: masa lawatan, jangka masa dan penghantaran data, untuk menganalisis algoritma yang diedarkan, dan menunjukkan bahawa langkah-langkah ini pada dasarnya kontroversial antara satu sama lain. (3) Kami mengusulkan algoritma terdistribusi dan teknik pengoptimuman yang memanfaatkan sifat simulasi grafik dan analisis algoritma tersebar. (4) Kami secara eksperimen mengesahkan keberkesanan dan kecekapan algoritma ini, menggunakan data kehidupan sebenar dan sintetik. [[EENNDD]] algoritma diedarkan; pertanyaan grafik; simulasi grafik"], [{"string": "An economic model of the worldwide web No contact information provided yet.", "keywords": ["general", "power laws", "market", "utility function", "game theory", "web search", "price of anarchy", "economic model"], "combined": "An economic model of the worldwide web No contact information provided yet. [[EENNDD]] general; power laws; market; utility function; game theory; web search; price of anarchy; economic model"}, "Model ekonomi di seluruh dunia Belum ada maklumat hubungan yang diberikan. [[EENNDD]] umum; undang-undang kuasa; pasaran; fungsi utiliti; teori permainan; carian sesawang; harga anarki; model ekonomi"], [{"string": "Finding authorities and hubs from link structures on the World Wide Web An abstract is not available.", "keywords": ["web searching", "hubs", "kleinberg's algorithm", "threshold", "bayesian", "link analysis", "authorities", "salsa"], "combined": "Finding authorities and hubs from link structures on the World Wide Web An abstract is not available. [[EENNDD]] web searching; hubs; kleinberg's algorithm; threshold; bayesian; link analysis; authorities; salsa"}, "Mencari pihak berkuasa dan hub dari struktur pautan di World Wide Web Abstrak tidak tersedia. [[EENNDD]] carian web; hab; algoritma kleinberg; ambang; bayesian; analisis pautan; pihak berkuasa; salsa"], [{"string": "Support concepts for Web navigation: a cognitive engineering approach An abstract is not available.", "keywords": ["world wide web", "usability", "cognitive engineering", "network user interface"], "combined": "Support concepts for Web navigation: a cognitive engineering approach An abstract is not available. [[EENNDD]] world wide web; usability; cognitive engineering; network user interface"}, "Konsep sokongan untuk navigasi Web: pendekatan kejuruteraan kognitif Abstrak tidak tersedia. [[EENNDD]] web seluruh dunia; kebolehgunaan; kejuruteraan kognitif; antara muka pengguna rangkaian"], [{"string": "Less talk, more rock: automated organization of community-contributed collections of concert videos We describe a system for synchronization and organization of user-contributed content from live music events. We start with a set of short video clips taken at a single event by multiple contributors, who were using a varied set of capture devices. Using audio fingerprints, we synchronize these clips such that overlapping clips can be displayed simultaneously. Furthermore, we use the timing and link structure generated by the synchronization algorithm to improve the findability and representation of the event content, including identifying key moments of interest and descriptive text for important captured segments of the show. We also identify the preferred audio track when multiple clips overlap. We thus create a much improved representation of the event that builds on the automatic content match. Our work demonstrates important principles in the use of content analysis techniques for social media content on the Web, and applies those principles in the domain of live music capture.", "keywords": ["audio fingerprinting", "miscellaneous", "video", "synchronization", "social media"], "combined": "Less talk, more rock: automated organization of community-contributed collections of concert videos We describe a system for synchronization and organization of user-contributed content from live music events. We start with a set of short video clips taken at a single event by multiple contributors, who were using a varied set of capture devices. Using audio fingerprints, we synchronize these clips such that overlapping clips can be displayed simultaneously. Furthermore, we use the timing and link structure generated by the synchronization algorithm to improve the findability and representation of the event content, including identifying key moments of interest and descriptive text for important captured segments of the show. We also identify the preferred audio track when multiple clips overlap. We thus create a much improved representation of the event that builds on the automatic content match. Our work demonstrates important principles in the use of content analysis techniques for social media content on the Web, and applies those principles in the domain of live music capture. [[EENNDD]] audio fingerprinting; miscellaneous; video; synchronization; social media"}, "Kurang bercakap, lebih hebat: organisasi automatik koleksi video konsert yang disumbangkan oleh komuniti Kami menerangkan sistem penyegerakan dan organisasi kandungan yang disumbangkan pengguna dari acara muzik langsung. Kami mulakan dengan satu set klip video pendek yang diambil pada satu acara oleh beberapa penyumbang, yang menggunakan rangkaian perangkat tangkapan yang bervariasi. Dengan menggunakan cap jari audio, kami menyegerakkan klip ini sehingga klip bertindih dapat dipaparkan secara serentak. Selanjutnya, kami menggunakan struktur masa dan pautan yang dihasilkan oleh algoritma penyegerakan untuk meningkatkan kebolehcari dan perwakilan kandungan acara, termasuk mengenal pasti momen-momen penting dan teks deskriptif untuk segmen tayangan penting yang ditangkap. Kami juga mengenal pasti trek audio pilihan apabila banyak klip bertindih. Oleh itu, kami membuat perwakilan acara yang jauh lebih baik berdasarkan padanan kandungan automatik. Karya kami menunjukkan prinsip-prinsip penting dalam penggunaan teknik analisis kandungan untuk kandungan media sosial di Web, dan menerapkan prinsip-prinsip tersebut dalam domain penangkapan muzik langsung. [[EENNDD]] cap jari audio; pelbagai; video; penyegerakan; media sosial"], [{"string": "Enabling full service surrogates using the portable channel representation An abstract is not available.", "keywords": ["content distribution", "web server", "surrogate", "replication", "mirroring", "dynamic content", "portability"], "combined": "Enabling full service surrogates using the portable channel representation An abstract is not available. [[EENNDD]] content distribution; web server; surrogate; replication; mirroring; dynamic content; portability"}, "Mengaktifkan pengganti perkhidmatan penuh menggunakan perwakilan saluran mudah alih Abstrak tidak tersedia. [[EENNDD]] pengedaran kandungan; pelayan web; pengganti; replikasi; pencerminan; kandungan dinamik; kemudahalihan"], [{"string": "The slashdot zoo: mining a social network with negative edges We analyse the corpus of user relationships of the Slashdot technology news site. The data was collected from the Slashdot Zoo feature where users of the website can tag other users as friends and foes, providing positive and negative endorsements. We adapt social network analysis techniques to the problem of negative edge weights. In particular, we consider signed variants of global network characteristics such as the clustering coefficient, node-level characteristics such as centrality and popularity measures, and link-level characteristics such as distances and similarity measures. We evaluate these measures on the task of identifying unpopular users, as well as on the task of predicting the sign of links and show that the network exhibits multiplicative transitivity which allows algebraic methods based on matrix multiplication to be used. We compare our methods to traditional methods which are only suitable for positively weighted edges.", "keywords": ["general", "slashdot zoo", "negative edge", "link prediction", "social network"], "combined": "The slashdot zoo: mining a social network with negative edges We analyse the corpus of user relationships of the Slashdot technology news site. The data was collected from the Slashdot Zoo feature where users of the website can tag other users as friends and foes, providing positive and negative endorsements. We adapt social network analysis techniques to the problem of negative edge weights. In particular, we consider signed variants of global network characteristics such as the clustering coefficient, node-level characteristics such as centrality and popularity measures, and link-level characteristics such as distances and similarity measures. We evaluate these measures on the task of identifying unpopular users, as well as on the task of predicting the sign of links and show that the network exhibits multiplicative transitivity which allows algebraic methods based on matrix multiplication to be used. We compare our methods to traditional methods which are only suitable for positively weighted edges. [[EENNDD]] general; slashdot zoo; negative edge; link prediction; social network"}, "Zoo slashdot: melombong rangkaian sosial dengan kelebihan negatif Kami menganalisis korporat hubungan pengguna laman berita teknologi Slashdot. Data dikumpulkan dari ciri Slashdot Zoo di mana pengguna laman web dapat menandai pengguna lain sebagai rakan dan musuh, memberikan sokongan positif dan negatif. Kami menyesuaikan teknik analisis rangkaian sosial dengan masalah pemberat kelebihan negatif. Khususnya, kami mempertimbangkan varian ditandai ciri rangkaian global seperti pekali pengelompokan, ciri tahap nod seperti ukuran sentraliti dan populariti, dan ciri tahap pautan seperti jarak dan ukuran kesamaan. Kami menilai langkah-langkah ini pada tugas mengenal pasti pengguna yang tidak popular, dan juga pada tugas untuk meramalkan tanda pautan dan menunjukkan bahawa rangkaian tersebut memperlihatkan transitiviti berganda yang memungkinkan kaedah algebra berdasarkan pendaraban matriks digunakan. Kami membandingkan kaedah kami dengan kaedah tradisional yang hanya sesuai untuk tepi berwajaran positif. [[EENNDD]] umum; zoo slashdot; kelebihan negatif; ramalan pautan; rangkaian sosial"], [{"string": "Topical TrustRank: using topicality to combat web spam No contact information provided yet.", "keywords": ["pagerank", "information search and retrieval", "spam", "web search engine", "trustrank"], "combined": "Topical TrustRank: using topicality to combat web spam No contact information provided yet. [[EENNDD]] pagerank; information search and retrieval; spam; web search engine; trustrank"}, "Topical TrustRank: menggunakan topikaliti untuk memerangi spam web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pagerank; pencarian dan pengambilan maklumat; spam; enjin carian web; amanah"], [{"string": "Image annotation using clickthrough data Automatic image annotation using supervised learning is performed by concept classifiers trained on labelled example images. This work proposes the use of clickthrough data collected from search logs as a source for the automatic generation of concept training data, thus avoiding the expensive manual annotation effort. We investigate and evaluate this approach using a collection of 97,628 photographic images. The results indicate that the contribution of search log based training data is positive; in particular, the combination of manual and automatically generated training data outperforms the use of manual data alone. It is therefore possible to use clickthrough data to perform large-scale image annotation with little manual annotation effort or, depending on performance, using only the automatically generated training data. The datasets used as well as an extensive presentation of the experimental results can be accessed at http://olympus.ee.auth.gr/~diou/civr2009/.", "keywords": ["supervised learning", "content analysis and indexing", "clickthrough data", "image annotation", "implicit feedback", "collective knowledge", "search logs", "concepts"], "combined": "Image annotation using clickthrough data Automatic image annotation using supervised learning is performed by concept classifiers trained on labelled example images. This work proposes the use of clickthrough data collected from search logs as a source for the automatic generation of concept training data, thus avoiding the expensive manual annotation effort. We investigate and evaluate this approach using a collection of 97,628 photographic images. The results indicate that the contribution of search log based training data is positive; in particular, the combination of manual and automatically generated training data outperforms the use of manual data alone. It is therefore possible to use clickthrough data to perform large-scale image annotation with little manual annotation effort or, depending on performance, using only the automatically generated training data. The datasets used as well as an extensive presentation of the experimental results can be accessed at http://olympus.ee.auth.gr/~diou/civr2009/. [[EENNDD]] supervised learning; content analysis and indexing; clickthrough data; image annotation; implicit feedback; collective knowledge; search logs; concepts"}, "Anotasi gambar menggunakan data klik lalu Anotasi gambar automatik menggunakan pembelajaran diawasi dilakukan oleh pengkelasan konsep yang dilatih pada gambar contoh berlabel. Karya ini mencadangkan penggunaan data klik-tayang yang dikumpulkan dari log carian sebagai sumber untuk penjanaan automatik data latihan konsep, sekali gus mengelakkan usaha penjelasan manual yang mahal. Kami menyiasat dan menilai pendekatan ini menggunakan koleksi 97,628 gambar fotografi. Hasil kajian menunjukkan bahawa sumbangan data latihan berdasarkan log carian adalah positif; khususnya, gabungan data latihan yang dihasilkan secara manual dan automatik mengatasi penggunaan data manual sahaja. Oleh itu, adalah mungkin untuk menggunakan data klik-tayang untuk melakukan anotasi gambar berskala besar dengan sedikit usaha anotasi manual atau, bergantung pada prestasi, hanya menggunakan data latihan yang dihasilkan secara automatik. Set data yang digunakan serta pembentangan hasil eksperimen yang luas dapat diakses di http://olympus.ee.auth.gr/~diou/civr2009/. [[EENNDD]] pembelajaran yang diselia; analisis kandungan dan pengindeksan; data klik lalu; anotasi gambar; maklum balas tersirat; pengetahuan kolektif; log carian; konsep"], [{"string": "The Yin/Yang web: XML syntax and RDF semantics No contact information provided yet.", "keywords": ["knowledge representation formalisms and methods", "xml", "model theory", "data models", "semantic web", "rdf"], "combined": "The Yin/Yang web: XML syntax and RDF semantics No contact information provided yet. [[EENNDD]] knowledge representation formalisms and methods; xml; model theory; data models; semantic web; rdf"}, "Laman web Yin / Yang: Sintaksis XML dan semantik RDF Belum ada maklumat hubungan yang diberikan. [[EENNDD]] formalisme dan kaedah perwakilan pengetahuan; xml; teori model; model data; web semantik; rdf"], [{"string": "Abstracting application-level web security No contact information provided yet.", "keywords": ["application-level web security", "security policy description language", "component-based design"], "combined": "Abstracting application-level web security No contact information provided yet. [[EENNDD]] application-level web security; security policy description language; component-based design"}, "Abstrak keselamatan web peringkat aplikasi [[EENNDD]] keselamatan web peringkat aplikasi; bahasa perihal dasar keselamatan; reka bentuk berasaskan komponen"], [{"string": "Comparative study of clustering techniques for short text documents We compare various document clustering techniques including K-means, SVD-based method and a graph-based approach and their performance on short text data collected from Twitter. We define a measure for evaluating the cluster error with these techniques. Observations show that graph-based approach using affinity propagation performs best in clustering short text data with minimal cluster error.", "keywords": ["short text", "k-means", "affinity propagation", "svd"], "combined": "Comparative study of clustering techniques for short text documents We compare various document clustering techniques including K-means, SVD-based method and a graph-based approach and their performance on short text data collected from Twitter. We define a measure for evaluating the cluster error with these techniques. Observations show that graph-based approach using affinity propagation performs best in clustering short text data with minimal cluster error. [[EENNDD]] short text; k-means; affinity propagation; svd"}, "Kajian perbandingan teknik pengelompokan untuk dokumen teks pendek Kami membandingkan pelbagai teknik pengelompokan dokumen termasuk cara-K, kaedah berdasarkan SVD dan pendekatan berdasarkan grafik dan prestasi mereka pada data teks pendek yang dikumpulkan dari Twitter. Kami menentukan ukuran untuk menilai ralat kluster dengan teknik ini. Pemerhatian menunjukkan bahawa pendekatan berasaskan grafik menggunakan penyebaran afiniti berkinerja terbaik dalam pengelompokan data teks pendek dengan ralat kluster minimum. [[EENNDD]] teks pendek; k-bermaksud; penyebaran pertalian; svd"], [{"string": "Image annotation using search and mining technologies No contact information provided yet.", "keywords": ["image annotation", "search result clustering", "hash indexing"], "combined": "Image annotation using search and mining technologies No contact information provided yet. [[EENNDD]] image annotation; search result clustering; hash indexing"}, "Anotasi gambar menggunakan teknologi carian dan perlombongan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] anotasi gambar; pengelompokan hasil carian; pengindeksan hash"], [{"string": "ViBE: virtual biology experiments An abstract is not available.", "keywords": ["computer-assisted instruction", "distributed learning", "software design", "elicitation methods", "virtual laboratories"], "combined": "ViBE: virtual biology experiments An abstract is not available. [[EENNDD]] computer-assisted instruction; distributed learning; software design; elicitation methods; virtual laboratories"}, "ViBE: eksperimen biologi maya Abstrak tidak tersedia. [[EENNDD]] arahan berbantukan komputer; pembelajaran yang diedarkan; reka bentuk perisian; kaedah elokasi; makmal maya"], [{"string": "Answering order-based queries over XML data No contact information provided yet.", "keywords": ["holistic algorithms", "xml", "order-based queries"], "combined": "Answering order-based queries over XML data No contact information provided yet. [[EENNDD]] holistic algorithms; xml; order-based queries"}, "Menjawab pertanyaan berdasarkan pesanan melalui data XML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] algoritma holistik; xml; pertanyaan berdasarkan pesanan"], [{"string": "Modeling click-through based word-pairs for web search Statistical translation models and latent semantic analysis (LSA) are two effective approaches to exploit click-through data for web search ranking. This paper presents two document ranking models that combine both approaches by explicitly modeling word-pairs. The first model, called PairModel, is a monolingual ranking model based on word pairs that are derived from click-through data. It maps queries and documents into a concept space spanned by these word pairs. The second model, called Bilingual Paired Topic Model (BPTM), uses bilingual word pairs and jointly models a bilingual query-document collection. This model maps queries and documents in multiple languages into a lower dimensional semantic subspace. Experimental results on web search task show that they significantly outperform the state-of-the-art baseline models, and the best result is obtained by interpolating PairModel and BPTM.", "keywords": ["translation model", "clickthrough data", "latent semantic analysis", "web search", "multilingual ir", "information search and retrieval", "topic models"], "combined": "Modeling click-through based word-pairs for web search Statistical translation models and latent semantic analysis (LSA) are two effective approaches to exploit click-through data for web search ranking. This paper presents two document ranking models that combine both approaches by explicitly modeling word-pairs. The first model, called PairModel, is a monolingual ranking model based on word pairs that are derived from click-through data. It maps queries and documents into a concept space spanned by these word pairs. The second model, called Bilingual Paired Topic Model (BPTM), uses bilingual word pairs and jointly models a bilingual query-document collection. This model maps queries and documents in multiple languages into a lower dimensional semantic subspace. Experimental results on web search task show that they significantly outperform the state-of-the-art baseline models, and the best result is obtained by interpolating PairModel and BPTM. [[EENNDD]] translation model; clickthrough data; latent semantic analysis; web search; multilingual ir; information search and retrieval; topic models"}, "Pemodelan pasangan kata berdasarkan klik untuk carian web Model terjemahan statistik dan analisis semantik laten (LSA) adalah dua pendekatan berkesan untuk mengeksploitasi data klik-tayang untuk kedudukan carian web. Makalah ini mengemukakan dua model pemeringkatan dokumen yang menggabungkan kedua pendekatan dengan memodelkan pasangan kata secara eksplisit. Model pertama, yang disebut PairModel, adalah model peringkat monolingual berdasarkan pasangan kata yang berasal dari data klik-tayang. Ini memetakan pertanyaan dan dokumen ke ruang konsep yang dibentangkan oleh pasangan kata ini. Model kedua, yang disebut Bilingual Paired Topic Model (BPTM), menggunakan pasangan kata dwibahasa dan bersama-sama memodelkan koleksi pertanyaan-dwibahasa. Model ini memetakan pertanyaan dan dokumen dalam pelbagai bahasa ke ruang bawah semantik dimensi yang lebih rendah. Hasil eksperimen pada tugas carian web menunjukkan bahawa mereka secara signifikan mengungguli model garis dasar canggih, dan hasil terbaik diperoleh dengan menginterpolasi PairModel dan BPTM. [[EENNDD]] model terjemahan; data klik lalu; analisis semantik laten; carian sesawang; ir pelbagai bahasa; carian dan pengambilan maklumat; model topik"], [{"string": "eBag: a ubiquitous Web infrastructure for nomadic learning No contact information provided yet.", "keywords": ["xlink", "hypertext/hypermedia", "adaptive", "webdav", "ebag", "hycon", "context-aware"], "combined": "eBag: a ubiquitous Web infrastructure for nomadic learning No contact information provided yet. [[EENNDD]] xlink; hypertext/hypermedia; adaptive; webdav; ebag; hycon; context-aware"}, "eBag: infrastruktur Web di mana-mana untuk pembelajaran nomad Tidak ada maklumat hubungan yang disediakan. [[EENNDD]] xlink; hiperteks / hipermedia; adaptif; webdav; ebag; hycon; peka konteks"], [{"string": "SmartInt: using mined attribute dependencies to integrate fragmented web databases Many web databases can be seen as providing partial and overlapping information about entities in the world. To answer queries effectively, we need to integrate the information about the individual entities that are fragmented over multiple sources. At first blush this is just the inverse of traditional database normalization problem - rather than go from a universal relation to normalized tables, we want to reconstruct the universal relation given the tables (sources). The standard way of reconstructing the entities will involve joining the tables. Unfortunately, because of the autonomous and decentralized way in which the sources are populated, they often do not have Primary Key - Foreign Key relations. While tables do share attributes, direct joins over these shared attributes can result in reconstruction of many spurious entities thus seriously compromising precision. We present a unified approach that supports intelligent retrieval over fragmented web databases by mining and using inter-table dependencies. Experiments with the prototype implementation, SmartInt, show that its retrieval strikes a good balance between precision and recall.", "keywords": ["loss of pk-fk", "web databases", "entity completion"], "combined": "SmartInt: using mined attribute dependencies to integrate fragmented web databases Many web databases can be seen as providing partial and overlapping information about entities in the world. To answer queries effectively, we need to integrate the information about the individual entities that are fragmented over multiple sources. At first blush this is just the inverse of traditional database normalization problem - rather than go from a universal relation to normalized tables, we want to reconstruct the universal relation given the tables (sources). The standard way of reconstructing the entities will involve joining the tables. Unfortunately, because of the autonomous and decentralized way in which the sources are populated, they often do not have Primary Key - Foreign Key relations. While tables do share attributes, direct joins over these shared attributes can result in reconstruction of many spurious entities thus seriously compromising precision. We present a unified approach that supports intelligent retrieval over fragmented web databases by mining and using inter-table dependencies. Experiments with the prototype implementation, SmartInt, show that its retrieval strikes a good balance between precision and recall. [[EENNDD]] loss of pk-fk; web databases; entity completion"}, "SmartInt: menggunakan pergantungan atribut mined untuk mengintegrasikan pangkalan data web yang terpecah-pecah. Banyak pangkalan data web dapat dilihat sebagai memberikan maklumat separa dan tumpang tindih mengenai entiti di dunia. Untuk menjawab pertanyaan dengan berkesan, kita perlu menyatukan maklumat mengenai entiti individu yang berpecah kepada pelbagai sumber. Pada mulanya, ini hanyalah kebalikan dari masalah normalisasi pangkalan data tradisional - daripada beralih dari hubungan universal ke jadual dinormalisasi, kita ingin membina semula hubungan sejagat memandangkan jadual (sumber). Kaedah standard untuk membina semula entiti akan melibatkan penyertaan jadual. Malangnya, kerana cara autonomi dan desentralisasi di mana sumber-sumber dihuni, mereka sering tidak mempunyai hubungan Kunci Utama - Kunci Asing. Walaupun jadual berkongsi atribut, penyertaan langsung ke atas atribut yang dikongsi ini dapat menghasilkan pembinaan semula banyak entiti palsu sehingga secara serius menjejaskan ketepatan. Kami menyajikan pendekatan terpadu yang menyokong pengambilan cerdas ke atas pangkalan data web yang terpecah-pecah dengan melombong dan menggunakan pergantungan antara jadual. Eksperimen dengan pelaksanaan prototaip, SmartInt, menunjukkan bahawa pengambilannya mencapai keseimbangan yang baik antara ketepatan dan penarikan balik. [[EENNDD]] kehilangan pk-fk; pangkalan data web; penyelesaian entiti"], [{"string": "Accelerated focused crawling through online relevance feedback No contact information provided yet.", "keywords": ["hypertext/hypermedia", "document object model", "reinforcement learning", "focused crawling"], "combined": "Accelerated focused crawling through online relevance feedback No contact information provided yet. [[EENNDD]] hypertext/hypermedia; document object model; reinforcement learning; focused crawling"}, "Perayapan fokus yang dipercepat melalui maklum balas perkaitan dalam talian Belum ada maklumat hubungan yang diberikan. [[EENNDD]] hiperteks / hipermedia; model objek dokumen; pembelajaran pengukuhan; fokus merangkak"], [{"string": "Search result re-ranking based on gap between search queries and social tags Both search engine click-through log and social annotation have been utilized as user feedback for search result re-ranking. However, to our best knowledge, no previous study has explored the correlation between these two factors for the task of search result ranking. In this paper, we show that the gap between search queries and social tags of the same Web page can well reflect its user preference score. Motivated by this observation, we propose a novel algorithm, called Query-Tag-Gap (QTG), to re-rank search results for better user satisfaction. Intuitively, on one hand, the search users' intentions are generally described by their queries before they read the search results. On the other hand, the Web annotators semantically tag Web pages after they read the content of the pages. The difference between users' recognition of the same page before and after they read it is a good reflection of user satisfaction. In this extended abstract, we formally define the query set and tag set of the same page as users' pre- and post- knowledge respectively. We empirically show the strong correlation between user satisfaction and user's knowledge gap before and after reading the page. Based on this gap, experiments have shown outstanding performance of our proposed QTG algorithm in search result re-ranking.", "keywords": ["social tagging", "information search and retrieval", "search result ranking", "query log"], "combined": "Search result re-ranking based on gap between search queries and social tags Both search engine click-through log and social annotation have been utilized as user feedback for search result re-ranking. However, to our best knowledge, no previous study has explored the correlation between these two factors for the task of search result ranking. In this paper, we show that the gap between search queries and social tags of the same Web page can well reflect its user preference score. Motivated by this observation, we propose a novel algorithm, called Query-Tag-Gap (QTG), to re-rank search results for better user satisfaction. Intuitively, on one hand, the search users' intentions are generally described by their queries before they read the search results. On the other hand, the Web annotators semantically tag Web pages after they read the content of the pages. The difference between users' recognition of the same page before and after they read it is a good reflection of user satisfaction. In this extended abstract, we formally define the query set and tag set of the same page as users' pre- and post- knowledge respectively. We empirically show the strong correlation between user satisfaction and user's knowledge gap before and after reading the page. Based on this gap, experiments have shown outstanding performance of our proposed QTG algorithm in search result re-ranking. [[EENNDD]] social tagging; information search and retrieval; search result ranking; query log"}, "Pemeringkatan semula hasil carian berdasarkan jurang antara pertanyaan carian dan tag sosial Kedua-dua log klik-tayang mesin pencari dan anotasi sosial telah digunakan sebagai maklum balas pengguna untuk pemeringkatan hasil carian. Namun, sepanjang pengetahuan kami, tidak ada kajian sebelumnya yang meneroka hubungan antara dua faktor ini untuk tugas pemeringkatan hasil carian. Dalam makalah ini, kami menunjukkan bahawa jurang antara pertanyaan carian dan tag sosial dari laman Web yang sama dapat menggambarkan skor pilihan penggunanya. Bermotivasi dengan pemerhatian ini, kami mencadangkan algoritma novel, yang disebut Query-Tag-Gap (QTG), untuk menilai semula hasil carian untuk kepuasan pengguna yang lebih baik. Secara intuitif, di satu pihak, maksud pengguna carian umumnya dijelaskan oleh pertanyaan mereka sebelum mereka membaca hasil carian. Sebaliknya, anotator Web secara semantik menandakan halaman Web setelah mereka membaca kandungan halaman. Perbezaan antara pengiktirafan pengguna terhadap halaman yang sama sebelum dan sesudah membacanya adalah cerminan kepuasan pengguna yang baik. Dalam abstrak yang diperluas ini, kami secara formal menentukan set pertanyaan dan set tag halaman yang sama dengan pengetahuan sebelum dan sesudah pengguna. Kami secara empirik menunjukkan korelasi yang kuat antara kepuasan pengguna dan jurang pengetahuan pengguna sebelum dan sesudah membaca halaman. Berdasarkan jurang ini, eksperimen telah menunjukkan prestasi yang luar biasa dari algoritma QTG yang dicadangkan kami dalam pemeringkatan semula hasil carian. [[EENNDD]] penandaan sosial; carian dan pengambilan maklumat; kedudukan hasil carian; log pertanyaan"], [{"string": "Collaborative filtering for orkut communities: discovery of user latent behavior Users of social networking services can connect with each other by forming communities for online interaction. Yet as the number of communities hosted by such websites grows over time, users have even greater need for effective community recommendations in order to meet more users. In this paper, we investigate two algorithms from very different domains and evaluate their effectiveness for personalized community recommendation. First is association rule mining (ARM), which discovers associations between sets of communities that are shared across many users. Second is latent Dirichlet allocation (LDA), which models user-community co-occurrences using latent aspects. In comparing LDA with ARM, we are interested in discovering whether modeling low-rank latent structure is more effective for recommendations than directly mining rules from the observed data. We experiment on an Orkut data set consisting of 492,104 users and 118,002 communities. Our empirical comparisons using the top-k recommendations metric show that LDA performs consistently better than ARM for the community recommendation task when recommending a list of 4 or more communities. However, for recommendation lists of up to 3 communities, ARM is still a bit better. We analyze examples of the latent information learned by LDA to explain this finding. To efficiently handle the large-scale data set, we parallelize LDA on distributed computers and demonstrate our parallel implementation's scalability with varying numbers of machines.", "keywords": ["collaborative filtering", "recommender systems", "latent topic models", "association rule mining"], "combined": "Collaborative filtering for orkut communities: discovery of user latent behavior Users of social networking services can connect with each other by forming communities for online interaction. Yet as the number of communities hosted by such websites grows over time, users have even greater need for effective community recommendations in order to meet more users. In this paper, we investigate two algorithms from very different domains and evaluate their effectiveness for personalized community recommendation. First is association rule mining (ARM), which discovers associations between sets of communities that are shared across many users. Second is latent Dirichlet allocation (LDA), which models user-community co-occurrences using latent aspects. In comparing LDA with ARM, we are interested in discovering whether modeling low-rank latent structure is more effective for recommendations than directly mining rules from the observed data. We experiment on an Orkut data set consisting of 492,104 users and 118,002 communities. Our empirical comparisons using the top-k recommendations metric show that LDA performs consistently better than ARM for the community recommendation task when recommending a list of 4 or more communities. However, for recommendation lists of up to 3 communities, ARM is still a bit better. We analyze examples of the latent information learned by LDA to explain this finding. To efficiently handle the large-scale data set, we parallelize LDA on distributed computers and demonstrate our parallel implementation's scalability with varying numbers of machines. [[EENNDD]] collaborative filtering; recommender systems; latent topic models; association rule mining"}, "Penapisan kolaboratif untuk komuniti orkut: penemuan tingkah laku pendam pengguna Pengguna perkhidmatan rangkaian sosial dapat berhubung antara satu sama lain dengan membentuk komuniti untuk interaksi dalam talian. Namun seiring dengan bertambahnya jumlah komuniti yang dihoskan oleh laman web tersebut dari masa ke masa, pengguna memerlukan lebih banyak cadangan komuniti yang berkesan agar dapat bertemu dengan lebih banyak pengguna. Dalam makalah ini, kami menyiasat dua algoritma dari domain yang sangat berbeza dan menilai keberkesanannya untuk cadangan komuniti yang diperibadikan. Pertama adalah perlombongan peraturan persatuan (ARM), yang menemui persatuan antara kumpulan komuniti yang dikongsi di banyak pengguna. Kedua adalah peruntukan Dirichlet laten (LDA), yang memodelkan kejadian bersama komuniti pengguna dengan menggunakan aspek pendam. Dalam membandingkan LDA dengan ARM, kami berminat untuk mengetahui sama ada pemodelan struktur laten peringkat rendah lebih berkesan untuk cadangan daripada kaedah perlombongan secara langsung dari data yang diperhatikan. Kami bereksperimen pada kumpulan data Orkut yang terdiri daripada 492,104 pengguna dan 118,002 komuniti. Perbandingan empirikal kami menggunakan metrik cadangan top-k menunjukkan bahawa LDA berprestasi lebih baik secara konsisten daripada ARM untuk tugas cadangan komuniti ketika mengesyorkan senarai 4 komuniti atau lebih. Walau bagaimanapun, untuk senarai cadangan sehingga 3 komuniti, ARM masih sedikit lebih baik. Kami menganalisis contoh maklumat laten yang dipelajari oleh LDA untuk menjelaskan penemuan ini. Untuk mengendalikan set data berskala besar dengan cekap, kami menyelaraskan LDA pada komputer yang diedarkan dan menunjukkan skalabiliti pelaksanaan selari kami dengan jumlah mesin yang berbeza-beza. [[EENNDD]] penapisan kolaboratif; sistem pengesyorkan; model topik pendam; perlombongan peraturan persatuan"], [{"string": "Online curriculum on the semantic Web: the CSD-UoC portal for peer-to-peer e-learning No contact information provided yet.", "keywords": ["jetspeed portlets", "e-learning portals", "semantic web", "ieee-lom"], "combined": "Online curriculum on the semantic Web: the CSD-UoC portal for peer-to-peer e-learning No contact information provided yet. [[EENNDD]] jetspeed portlets; e-learning portals; semantic web; ieee-lom"}, "Kurikulum dalam talian di Web semantik: portal CSD-UoC untuk e-pembelajaran peer-to-peer Tidak ada maklumat hubungan yang disediakan. [[EENNDD]] portlet jetspeed; portal e-pembelajaran; web semantik; ieee-lom"], [{"string": "An environment for collaborative content acquisition and editing by coordinated ubiquitous devices No contact information provided yet.", "keywords": ["ubiquitous network", "ubiquitous computing", "embedded content", "functional web", "software architectures", "multiple device operating", "rfid"], "combined": "An environment for collaborative content acquisition and editing by coordinated ubiquitous devices No contact information provided yet. [[EENNDD]] ubiquitous network; ubiquitous computing; embedded content; functional web; software architectures; multiple device operating; rfid"}, "Persekitaran untuk pemerolehan dan penyuntingan kandungan secara kolaboratif oleh peranti di mana-mana yang diselaraskan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] rangkaian di mana-mana; pengkomputeran di mana-mana; kandungan terbenam; web berfungsi; seni bina perisian; pelbagai peranti beroperasi; rfid"], [{"string": "Factorizing personalized Markov chains for next-basket recommendation Recommender systems are an important component of many websites. Two of the most popular approaches are based on matrix factorization (MF) and Markov chains (MC). MF methods learn the general taste of a user by factorizing the matrix over observed user-item preferences. On the other hand, MC methods model sequential behavior by learning a transition graph over items that is used to predict the next action based on the recent actions of a user. In this paper, we present a method bringing both approaches together. Our method is based on personalized transition graphs over underlying Markov chains. That means for each user an own transition matrix is learned - thus in total the method uses a transition cube. As the observations for estimating the transitions are usually very limited, our method factorizes the transition cube with a pairwise interaction model which is a special case of the Tucker Decomposition. We show that our factorized personalized MC (FPMC) model subsumes both a common Markov chain and the normal matrix factorization model. For learning the model parameters, we introduce an adaption of the Bayesian Personalized Ranking (BPR) framework for sequential basket data. Empirically, we show that our FPMC model outperforms both the common matrix factorization and the unpersonalized MC model both learned with and without factorization.", "keywords": ["markov chain", "matrix factorization", "basket recommendation"], "combined": "Factorizing personalized Markov chains for next-basket recommendation Recommender systems are an important component of many websites. Two of the most popular approaches are based on matrix factorization (MF) and Markov chains (MC). MF methods learn the general taste of a user by factorizing the matrix over observed user-item preferences. On the other hand, MC methods model sequential behavior by learning a transition graph over items that is used to predict the next action based on the recent actions of a user. In this paper, we present a method bringing both approaches together. Our method is based on personalized transition graphs over underlying Markov chains. That means for each user an own transition matrix is learned - thus in total the method uses a transition cube. As the observations for estimating the transitions are usually very limited, our method factorizes the transition cube with a pairwise interaction model which is a special case of the Tucker Decomposition. We show that our factorized personalized MC (FPMC) model subsumes both a common Markov chain and the normal matrix factorization model. For learning the model parameters, we introduce an adaption of the Bayesian Personalized Ranking (BPR) framework for sequential basket data. Empirically, we show that our FPMC model outperforms both the common matrix factorization and the unpersonalized MC model both learned with and without factorization. [[EENNDD]] markov chain; matrix factorization; basket recommendation"}, "Memfaktorkan rantaian Markov yang diperibadikan untuk cadangan keranjang seterusnya Sistem penyarankan adalah komponen penting dalam banyak laman web. Dua pendekatan yang paling popular adalah berdasarkan matrik factorization (MF) dan Markov chain (MC). Kaedah MF mempelajari selera umum pengguna dengan memfaktorkan matriks berbanding pilihan item pengguna yang diperhatikan. Sebaliknya, kaedah MC memodelkan tingkah laku berurutan dengan mempelajari grafik peralihan ke atas item yang digunakan untuk meramalkan tindakan seterusnya berdasarkan tindakan pengguna baru-baru ini. Dalam makalah ini, kami menyajikan satu kaedah yang menyatukan kedua pendekatan tersebut. Kaedah kami berdasarkan grafik peralihan yang diperibadikan di atas rantai Markov yang mendasari. Ini bermaksud untuk setiap pengguna matriks peralihan sendiri dipelajari - dengan demikian secara keseluruhan kaedah menggunakan kubus peralihan. Oleh kerana pemerhatian untuk menganggarkan peralihan biasanya sangat terhad, kaedah kami memfaktorkan kubus peralihan dengan model interaksi berpasangan yang merupakan kes khas Penguraian Tucker. Kami menunjukkan bahawa model MC (FPMC) yang diperibadikan berdasarkan rantai Markov biasa dan model pemfaktoran matriks biasa. Untuk mempelajari parameter model, kami memperkenalkan penyesuaian kerangka Bayesian Personalized Ranking (BPR) untuk data keranjang berurutan. Secara empirikal, kami menunjukkan bahawa model FPMC kami mengungguli faktorisasi matriks biasa dan model MC yang tidak dipersonalisasi yang dipelajari dengan dan tanpa pemfaktoran. [[EENNDD]] rantaian markov; pemfaktoran matriks; cadangan bakul"], [{"string": "pTHINC: a thin-client architecture for mobile wireless web No contact information provided yet.", "keywords": ["remote display", "mobility", "pervasive web", "thin-client computing"], "combined": "pTHINC: a thin-client architecture for mobile wireless web No contact information provided yet. [[EENNDD]] remote display; mobility; pervasive web; thin-client computing"}, "pTHINC: seni bina pelanggan tipis untuk web tanpa wayar mudah alih Belum ada maklumat hubungan yang diberikan. [[EENNDD]] paparan jauh; mobiliti; web yang meluas; pengkomputeran pelanggan tipis"], [{"string": "Scholarly publishing and argument in hyperspace No contact information provided yet.", "keywords": ["scholarly interpretation", "digital libraries", "hypertext/hypermedia", "collaborative web", "modeling debate", "scientific publishing"], "combined": "Scholarly publishing and argument in hyperspace No contact information provided yet. [[EENNDD]] scholarly interpretation; digital libraries; hypertext/hypermedia; collaborative web; modeling debate; scientific publishing"}, "Penerbitan dan hujah ilmiah di ruang angkasa Belum ada maklumat hubungan yang diberikan. [[EENNDD]] tafsiran ilmiah; perpustakaan digital; hiperteks / hipermedia; web kolaboratif; perbahasan pemodelan; penerbitan ilmiah"], [{"string": "The XML web: a first study No contact information provided yet.", "keywords": ["xml documents", "statistical analysis", "structural properties", "xml web"], "combined": "The XML web: a first study No contact information provided yet. [[EENNDD]] xml documents; statistical analysis; structural properties; xml web"}, "Web XML: kajian pertama Belum ada maklumat hubungan. [[EENNDD]] dokumen xml; Analisis statistik; sifat struktur; laman web xml"], [{"string": "Surfing the web by site No contact information provided yet.", "keywords": ["large scale systems", "novel browsing paradigms", "web navigation strategies"], "combined": "Surfing the web by site No contact information provided yet. [[EENNDD]] large scale systems; novel browsing paradigms; web navigation strategies"}, "Melayari laman web melalui laman web Belum ada maklumat hubungan. [[EENNDD]] sistem berskala besar; paradigma melayari novel; strategi navigasi web"], [{"string": "Data quality in web archiving Web archives preserve the history of Web sites and have high long-term value for media and business analysts. Such archives are maintained by periodically re-crawling entire Web sites of interest. From an archivist's point of view, the ideal case to ensure highest possible data quality of the archive would be to \"freeze\" the complete contents of an entire Web site during the time span of crawling and capturing the site. Of course, this is practically infeasible. To comply with the politeness specification of a Web site, the crawler needs to pause between subsequent http requests in order to avoid unduly high load on the site's http server. As a consequence, capturing a large Web site may span hours or even days, which increases the risk that contents collected so far are incoherent with the parts that are still to be crawled. This paper introduces a model for identifying coherent sections of an archive and, thus, measuring the data quality in Web archiving. Additionally, we present a crawling strategy that aims to ensure archive coherence by minimizing the diffusion of Web site captures. Preliminary experiments demonstrate the usefulness of the model and the effectiveness of the strategy.", "keywords": ["temporal coherence", "web archiving", "data quality", "content analysis and indexing"], "combined": "Data quality in web archiving Web archives preserve the history of Web sites and have high long-term value for media and business analysts. Such archives are maintained by periodically re-crawling entire Web sites of interest. From an archivist's point of view, the ideal case to ensure highest possible data quality of the archive would be to \"freeze\" the complete contents of an entire Web site during the time span of crawling and capturing the site. Of course, this is practically infeasible. To comply with the politeness specification of a Web site, the crawler needs to pause between subsequent http requests in order to avoid unduly high load on the site's http server. As a consequence, capturing a large Web site may span hours or even days, which increases the risk that contents collected so far are incoherent with the parts that are still to be crawled. This paper introduces a model for identifying coherent sections of an archive and, thus, measuring the data quality in Web archiving. Additionally, we present a crawling strategy that aims to ensure archive coherence by minimizing the diffusion of Web site captures. Preliminary experiments demonstrate the usefulness of the model and the effectiveness of the strategy. [[EENNDD]] temporal coherence; web archiving; data quality; content analysis and indexing"}, "Kualiti data dalam pengarkiban web Arkib web menyimpan sejarah laman web dan mempunyai nilai jangka panjang yang tinggi untuk penganalisis media dan perniagaan. Arkib sedemikian dikendalikan dengan merangkak semula secara berkala seluruh laman web yang menarik. Dari sudut pandang arkib, kes yang sesuai untuk memastikan kualiti data arkib yang paling tinggi adalah \"membekukan\" kandungan lengkap dari seluruh laman web semasa jangka masa merangkak dan menangkap laman web ini. Sudah tentu, ini tidak dapat dilaksanakan. Untuk mematuhi spesifikasi kesopanan laman web, perayap perlu berhenti sejenak antara permintaan http berikutnya untuk mengelakkan beban yang terlalu tinggi pada pelayan http laman web tersebut. Akibatnya, menangkap laman web yang besar mungkin memakan waktu berjam-jam atau bahkan berhari-hari, yang meningkatkan risiko bahawa kandungan yang dikumpulkan sejauh ini tidak sesuai dengan bahagian-bahagian yang masih belum dirayapi. Makalah ini memperkenalkan model untuk mengenal pasti bahagian arkib yang koheren dan, dengan demikian, mengukur kualiti data dalam pengarkiban Web. Selain itu, kami menyajikan strategi merangkak yang bertujuan untuk memastikan kesesuaian arkib dengan meminimumkan penyebaran tangkapan laman web. Eksperimen awal menunjukkan kegunaan model dan keberkesanan strategi. [[EENNDD]] kesesuaian temporal; pengarkiban web; kualiti data; analisis kandungan dan pengindeksan"], [{"string": "Estimating required recall for successful knowledge acquisition from the web No contact information provided yet.", "keywords": ["redundancy", "quantitative performance measures", "information extraction", "information search and retrieval", "recall", "web metrics"], "combined": "Estimating required recall for successful knowledge acquisition from the web No contact information provided yet. [[EENNDD]] redundancy; quantitative performance measures; information extraction; information search and retrieval; recall; web metrics"}, "Menganggarkan penarikan yang diperlukan untuk pemerolehan pengetahuan yang berjaya dari web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] redundansi; ukuran prestasi kuantitatif; pengekstrakan maklumat; pencarian dan pengambilan maklumat; ingat semula; sukatan web"], [{"string": "Finding the search engine that works for you No contact information provided yet.", "keywords": ["search engines", "personalization", "performance evaluation"], "combined": "Finding the search engine that works for you No contact information provided yet. [[EENNDD]] search engines; personalization; performance evaluation"}, "Mencari enjin carian yang sesuai untuk anda Belum ada maklumat hubungan yang diberikan. [[EENNDD]] enjin carian; pemperibadian; penilaian prestasi"], [{"string": "Find me if you can: improving geographical prediction with social and spatial proximity Geography and social relationships are inextricably intertwined; the people we interact with on a daily basis almost always live near us. As people spend more time online, data regarding these two dimensions -- geography and social relationships -- are becoming increasingly precise, allowing us to build reliable models to describe their interaction. These models have important implications in the design of location-based services, security intrusion detection, and social media supporting local communities.", "keywords": ["social networks", "propagation", "geolocation"], "combined": "Find me if you can: improving geographical prediction with social and spatial proximity Geography and social relationships are inextricably intertwined; the people we interact with on a daily basis almost always live near us. As people spend more time online, data regarding these two dimensions -- geography and social relationships -- are becoming increasingly precise, allowing us to build reliable models to describe their interaction. These models have important implications in the design of location-based services, security intrusion detection, and social media supporting local communities. [[EENNDD]] social networks; propagation; geolocation"}, "Cari saya jika anda boleh: meningkatkan ramalan geografi dengan jarak sosial dan jarak Geografi dan hubungan sosial saling berkaitan; orang yang kita berinteraksi setiap hari hampir selalu tinggal berdekatan dengan kita. Apabila orang menghabiskan lebih banyak masa dalam talian, data mengenai dua dimensi ini - geografi dan hubungan sosial - menjadi semakin tepat, yang membolehkan kita membina model yang boleh dipercayai untuk menggambarkan interaksi mereka. Model-model ini mempunyai implikasi penting dalam reka bentuk perkhidmatan berdasarkan lokasi, pengesanan pencerobohan keselamatan, dan media sosial yang menyokong masyarakat setempat. [[EENNDD]] rangkaian sosial; penyebaran; geolokasi"], [{"string": "Learning and inferencing in user ontology for personalized semantic web services No contact information provided yet.", "keywords": ["spreading-activation theory", "information search and retrieval", "user ontology"], "combined": "Learning and inferencing in user ontology for personalized semantic web services No contact information provided yet. [[EENNDD]] spreading-activation theory; information search and retrieval; user ontology"}, "Belajar dan membuat kesimpulan dalam ontologi pengguna untuk perkhidmatan web semantik yang diperibadikan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] teori penyebaran-pengaktifan; pencarian dan pengambilan maklumat; ontologi pengguna"], [{"string": "Huskysim: a simulation toolkit for application scheduling in computational grids No contact information provided yet.", "keywords": ["performance prediction", "discrete event simulation", "computational grids", "adaptive scheduling"], "combined": "Huskysim: a simulation toolkit for application scheduling in computational grids No contact information provided yet. [[EENNDD]] performance prediction; discrete event simulation; computational grids; adaptive scheduling"}, "Huskysim: kit alat simulasi untuk penjadualan aplikasi dalam grid komputasi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ramalan prestasi; simulasi peristiwa diskrit; grid pengiraan; penjadualan adaptif"], [{"string": "Reappraising cognitive styles in adaptive web applications No contact information provided yet.", "keywords": ["web applications", "cognitive styles", "user trials", "user modelling", "adaptive hypermedia"], "combined": "Reappraising cognitive styles in adaptive web applications No contact information provided yet. [[EENNDD]] web applications; cognitive styles; user trials; user modelling; adaptive hypermedia"}, "Menilai semula gaya kognitif dalam aplikasi web adaptif Belum ada maklumat hubungan yang diberikan. [[EENNDD]] aplikasi web; gaya kognitif; percubaan pengguna; pemodelan pengguna; hipermedia adaptif"], [{"string": "Improving recommendation lists through topic diversification No contact information provided yet.", "keywords": ["collaborative filtering", "diversification", "metrics", "performance evaluation", "accuracy", "recommender systems"], "combined": "Improving recommendation lists through topic diversification No contact information provided yet. [[EENNDD]] collaborative filtering; diversification; metrics; performance evaluation; accuracy; recommender systems"}, "Memperbaiki senarai cadangan melalui kepelbagaian topik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penapisan kolaboratif; kepelbagaian; sukatan; penilaian prestasi; ketepatan; sistem cadangan"], [{"string": "CC-Buddy: an adaptive framework for maintaining cache coherency using peers No contact information provided yet.", "keywords": ["peer-to-peer", "dynamic data", "distributed systems", "cache coherency"], "combined": "CC-Buddy: an adaptive framework for maintaining cache coherency using peers No contact information provided yet. [[EENNDD]] peer-to-peer; dynamic data; distributed systems; cache coherency"}, "CC-Buddy: kerangka penyesuaian untuk mengekalkan koherensi cache menggunakan rakan sebaya Belum ada maklumat hubungan yang diberikan. [[EENNDD]] rakan sebaya; data dinamik; sistem yang diedarkan; koherensi cache"], [{"string": "Path summaries and path partitioning in modern XML databases An abstract is not available.", "keywords": ["path summaries", "xml", "xquery processing", "path partition"], "combined": "Path summaries and path partitioning in modern XML databases An abstract is not available. [[EENNDD]] path summaries; xml; xquery processing; path partition"}, "Ringkasan jalan dan partition path dalam pangkalan data XML moden Abstrak tidak tersedia. [[EENNDD]] ringkasan laluan; xml; pemprosesan xquery; partition laluan"], [{"string": "XML-based XML schema access XML Schema's abstract data model consists of components, which are the structures that eventually define a schema as a whole. XML Schema's XML syntax, on the other hand, is not a direct representation of the schema components, and it proves to be surprisingly hard to derive a schema's components from the XML syntax. The Schema Component XML Syntax (SCX) is a representation which attempts to map schema components as faithfully as possible to XML structures. SCX serves as the starting point for applications which need access to schema components and want to do so using standardized and widely available XML technologies.", "keywords": ["xml schema", "xml", "scx"], "combined": "XML-based XML schema access XML Schema's abstract data model consists of components, which are the structures that eventually define a schema as a whole. XML Schema's XML syntax, on the other hand, is not a direct representation of the schema components, and it proves to be surprisingly hard to derive a schema's components from the XML syntax. The Schema Component XML Syntax (SCX) is a representation which attempts to map schema components as faithfully as possible to XML structures. SCX serves as the starting point for applications which need access to schema components and want to do so using standardized and widely available XML technologies. [[EENNDD]] xml schema; xml; scx"}, "Skema XML berasaskan XML mengakses model data abstrak XML Schema terdiri daripada komponen, yang merupakan struktur yang akhirnya menentukan skema secara keseluruhan. Sintaks XML XML Schema, sebaliknya, bukan representasi langsung dari komponen skema, dan ternyata sangat sukar untuk memperoleh komponen skema dari sintaks XML. The Schema Component XML Syntax (SCX) adalah representasi yang cuba memetakan komponen skema seiman mungkin dengan struktur XML. SCX berfungsi sebagai titik awal untuk aplikasi yang memerlukan akses ke komponen skema dan ingin melakukannya menggunakan teknologi XML yang standard dan tersedia secara meluas. [[EENNDD]] skema xml; xml; scx"], [{"string": "Classifying web sites In this paper, we present a novel method for the classification of Web sites. This method exploits both structure and content of Web sites in order to discern their functionality. It allows for distinguishing between eight of the most relevant functional classes of Web sites. We show that a pre-classification of Web sites utilizing structural properties considerably improves a subsequent textual classification with standard techniques. We evaluate this approach on a dataset comprising more than 16,000 Web sites with about 20 million crawled and 100 million known Web pages. Our approach achieves an accuracy of 92% for the coarse-grained classification of these Web sites.", "keywords": ["web site classification", "web mining", "web measurement"], "combined": "Classifying web sites In this paper, we present a novel method for the classification of Web sites. This method exploits both structure and content of Web sites in order to discern their functionality. It allows for distinguishing between eight of the most relevant functional classes of Web sites. We show that a pre-classification of Web sites utilizing structural properties considerably improves a subsequent textual classification with standard techniques. We evaluate this approach on a dataset comprising more than 16,000 Web sites with about 20 million crawled and 100 million known Web pages. Our approach achieves an accuracy of 92% for the coarse-grained classification of these Web sites. [[EENNDD]] web site classification; web mining; web measurement"}, "Mengklasifikasikan laman web Dalam makalah ini, kami menyajikan kaedah baru untuk klasifikasi laman web. Kaedah ini memanfaatkan struktur dan kandungan laman web untuk mengetahui fungsi mereka. Ini memungkinkan untuk membezakan antara lapan kelas fungsi laman web yang paling relevan. Kami menunjukkan bahawa pra-klasifikasi laman web yang menggunakan sifat struktur jauh meningkatkan klasifikasi teks seterusnya dengan teknik standard. Kami menilai pendekatan ini pada set data yang merangkumi lebih daripada 16,000 laman web dengan kira-kira 20 juta laman web yang dirayapi dan 100 juta yang diketahui. Pendekatan kami mencapai ketepatan 92% untuk klasifikasi kasar laman web ini. [[EENNDD]] pengelasan laman web; perlombongan web; pengukuran web"], [{"string": "Latent space domain transfer between high dimensional overlapping distributions Transferring knowledge from one domain to another is challenging due to a number of reasons. Since both conditional and marginal distribution of the training data and test data are non-identical, model trained in one domain, when directly applied to a different domain, is usually low in accuracy. For many applications with large feature sets, such as text document, sequence data, medical data, image data of different resolutions, etc. two domains usually do not contain exactly the same features, thus introducing large numbers of \"missing values\" when considered over the union of features from both domains. In other words, its marginal distributions are at most overlapping. In the same time, these problems are usually high dimensional, such as, several thousands of features. Thus, the combination of high dimensionality and missing values make the relationship in conditional probabilities between two domains hard to measure and model. To address these challenges, we propose a framework that first brings the marginal distributions of two domains closer by \"filling up\" those missing values of disjoint features. Afterwards, it looks for those comparable sub-structures in the \"latent-space\" as mapped from the expanded feature vector, where both marginal and conditional distribution are similar. With these sub-structures in latent space, the proposed approach then find common concepts that are transferable across domains with high probability. During prediction, unlabeled instances are treated as \"queries\", the mostly related labeled instances from out-domain are retrieved, and the classification is made by weighted voting using retrieved out-domain examples. We formally show that importing feature values across domains and latent semantic index can jointly make the distributions of two related domains easier to measure than in original feature space, the nearest neighbor method employed to retrieve related out domain examples is bounded in error when predicting in-domain examples. Software and datasets are available for download.", "keywords": ["transfer learning", "missing value", "text mining", "information search and retrieval", "high dimensional", "latent"], "combined": "Latent space domain transfer between high dimensional overlapping distributions Transferring knowledge from one domain to another is challenging due to a number of reasons. Since both conditional and marginal distribution of the training data and test data are non-identical, model trained in one domain, when directly applied to a different domain, is usually low in accuracy. For many applications with large feature sets, such as text document, sequence data, medical data, image data of different resolutions, etc. two domains usually do not contain exactly the same features, thus introducing large numbers of \"missing values\" when considered over the union of features from both domains. In other words, its marginal distributions are at most overlapping. In the same time, these problems are usually high dimensional, such as, several thousands of features. Thus, the combination of high dimensionality and missing values make the relationship in conditional probabilities between two domains hard to measure and model. To address these challenges, we propose a framework that first brings the marginal distributions of two domains closer by \"filling up\" those missing values of disjoint features. Afterwards, it looks for those comparable sub-structures in the \"latent-space\" as mapped from the expanded feature vector, where both marginal and conditional distribution are similar. With these sub-structures in latent space, the proposed approach then find common concepts that are transferable across domains with high probability. During prediction, unlabeled instances are treated as \"queries\", the mostly related labeled instances from out-domain are retrieved, and the classification is made by weighted voting using retrieved out-domain examples. We formally show that importing feature values across domains and latent semantic index can jointly make the distributions of two related domains easier to measure than in original feature space, the nearest neighbor method employed to retrieve related out domain examples is bounded in error when predicting in-domain examples. Software and datasets are available for download. [[EENNDD]] transfer learning; missing value; text mining; information search and retrieval; high dimensional; latent"}, "Pemindahan domain ruang laten antara pengagihan tumpang tindih dimensi tinggi Memindahkan pengetahuan dari satu domain ke domain yang lain memang mencabar kerana beberapa sebab. Oleh kerana pengedaran bersyarat dan marginal data latihan dan data ujian tidak serupa, model yang dilatih dalam satu domain, ketika diterapkan secara langsung ke domain yang berbeda, biasanya rendahnya ketepatannya. Untuk banyak aplikasi dengan set fitur yang besar, seperti dokumen teks, data urutan, data perubatan, data gambar dengan resolusi yang berbeza, dll. Dua domain biasanya tidak mengandungi ciri yang sama, sehingga memperkenalkan sebilangan besar \"nilai yang hilang\" ketika dipertimbangkan lebih penyatuan ciri dari kedua-dua domain. Dengan kata lain, pengedaran marginalnya paling banyak bertindih. Pada masa yang sama, masalah ini biasanya berukuran tinggi, seperti beberapa ribu ciri. Oleh itu, gabungan dimensi tinggi dan nilai yang hilang menjadikan hubungan dalam kebarangkalian bersyarat antara dua domain sukar diukur dan dimodelkan. Untuk mengatasi cabaran ini, kami mencadangkan kerangka kerja yang pertama kali mendekatkan pembahagian marginal dua domain dengan \"mengisi\" nilai-nilai yang hilang dari ciri-ciri yang tidak disatukan. Selepas itu, ia mencari sub-struktur yang setanding di \"latent-space\" seperti yang dipetakan dari vektor ciri yang diperluas, di mana pengedaran marginal dan bersyarat serupa. Dengan sub-struktur ini di ruang pendam, pendekatan yang dicadangkan kemudian mencari konsep umum yang dapat dipindahkan di seluruh domain dengan kebarangkalian yang tinggi. Semasa ramalan, kejadian tidak berlabel diperlakukan sebagai \"pertanyaan\", contoh berlabel yang paling banyak berkaitan dari luar domain diambil, dan klasifikasi dibuat dengan pemberian wajaran dengan menggunakan contoh di luar domain yang diambil. Kami secara rasmi menunjukkan bahawa mengimport nilai ciri di seluruh domain dan indeks semantik laten secara bersama dapat menjadikan pengedaran dua domain yang berkaitan lebih mudah diukur daripada di ruang ciri asal, kaedah jiran terdekat yang digunakan untuk mengambil contoh domain yang berkaitan terikat dalam kesilapan ketika meramalkan dalam- contoh domain. Perisian dan set data tersedia untuk dimuat turun. [[EENNDD]] memindahkan pembelajaran; hilang nilai; perlombongan teks; carian dan pengambilan maklumat; dimensi tinggi; pendam"], [{"string": "On granting limited access to private information An abstract is not available.", "keywords": ["security and protection", "access control"], "combined": "On granting limited access to private information An abstract is not available. [[EENNDD]] security and protection; access control"}, "Mengenai akses terhad kepada maklumat peribadi Abstrak tidak tersedia. [[EENNDD]] keselamatan dan perlindungan; kawalan akses"], [{"string": "Xspect: bridging open hypermedia and XLink No contact information provided yet.", "keywords": ["xlink", "fohm", "open hypermedia", "xpointer", "hypertext/hypermedia", "svg", "annotations", "ohif", "xspect"], "combined": "Xspect: bridging open hypermedia and XLink No contact information provided yet. [[EENNDD]] xlink; fohm; open hypermedia; xpointer; hypertext/hypermedia; svg; annotations; ohif; xspect"}, "Xspect: merapatkan hipermedia terbuka dan XLink Belum ada maklumat hubungan yang diberikan. [[EENNDD]] xlink; fohm; hipermedia terbuka; xpointer; hiperteks / hipermedia; svg; anotasi; ohif; xspek"], [{"string": "Externalities in online advertising Most models for online advertising assume that an advertiser's value from winning an ad auction, which depends on the clickthrough rate or conversion rate of the advertisement, is independent of other advertisements served alongside it in the same session. This ignores an important 'externality effect': as the advertising audience has a limited attention span, a high-quality ad on a page can detract attention from other ads on the same page. That is, the utility to a winner in such an auction also depends on the set of other winners.", "keywords": ["general", "auctions", "advertising", "externalities", "approximation algorithms"], "combined": "Externalities in online advertising Most models for online advertising assume that an advertiser's value from winning an ad auction, which depends on the clickthrough rate or conversion rate of the advertisement, is independent of other advertisements served alongside it in the same session. This ignores an important 'externality effect': as the advertising audience has a limited attention span, a high-quality ad on a page can detract attention from other ads on the same page. That is, the utility to a winner in such an auction also depends on the set of other winners. [[EENNDD]] general; auctions; advertising; externalities; approximation algorithms"}, "Eksternaliti dalam pengiklanan dalam talian Sebilangan besar model untuk iklan dalam talian menganggap bahawa nilai pengiklan daripada memenangi lelongan iklan, yang bergantung pada kadar klik-tayang atau kadar penukaran iklan, adalah bebas daripada iklan lain yang disajikan bersama-sama di sesi yang sama. Ini mengabaikan 'kesan luaran' yang penting: kerana khalayak pengiklanan mempunyai jangkauan perhatian yang terbatas, iklan berkualiti tinggi di halaman dapat menarik perhatian dari iklan lain di halaman yang sama. Iaitu, kegunaan pemenang dalam lelongan seperti itu juga bergantung pada kumpulan pemenang lain. [[EENNDD]] umum; lelong; mengiklankan; luaran; algoritma penghampiran"], [{"string": "Antourage: mining distance-constrained trips from flickr We study how to automatically extract tourist trips from large volumes of geo-tagged photographs. Working with more than 8 million of these photographs that are publicly available via photo- sharing communities such as Flickr and Panoramio, our goal is to satisfy the needs of a tourist who specifies a starting location (typically a hotel) together with a bounded travel distance and demands a tour that visits the popular sites along the way. Our system, named ANTOURAGE, solves this intractable problem using a novel adaptation of the max-min ant system (MMAS) meta-heuristic. Experiments using GPS metadata crawled from Flickr show that ANTOURAGE can generate high-quality tours.", "keywords": ["photo collections", "trip planning", "graph mining", "flickr", "geolocation"], "combined": "Antourage: mining distance-constrained trips from flickr We study how to automatically extract tourist trips from large volumes of geo-tagged photographs. Working with more than 8 million of these photographs that are publicly available via photo- sharing communities such as Flickr and Panoramio, our goal is to satisfy the needs of a tourist who specifies a starting location (typically a hotel) together with a bounded travel distance and demands a tour that visits the popular sites along the way. Our system, named ANTOURAGE, solves this intractable problem using a novel adaptation of the max-min ant system (MMAS) meta-heuristic. Experiments using GPS metadata crawled from Flickr show that ANTOURAGE can generate high-quality tours. [[EENNDD]] photo collections; trip planning; graph mining; flickr; geolocation"}, "Antourage: perjalanan perlombongan jarak tempuh dari flickr Kami mengkaji cara mengekstrak perjalanan pelancong secara automatik dari sejumlah besar foto dengan tag geo. Bekerja dengan lebih dari 8 juta foto ini yang tersedia untuk umum melalui komuniti perkongsian foto seperti Flickr dan Panoramio, tujuan kami adalah untuk memenuhi keperluan pelancong yang menentukan lokasi permulaan (biasanya hotel) bersama dengan jarak perjalanan yang terbatas dan menuntut lawatan yang mengunjungi laman web popular di sepanjang jalan. Sistem kami, bernama ANTOURAGE, menyelesaikan masalah yang tidak dapat diselesaikan ini dengan menggunakan adaptasi baru dari sistem meta-heuristik sistem semut maksimum (MMAS). Eksperimen menggunakan metadata GPS yang dirangkak dari Flickr menunjukkan bahawa ANTOURAGE dapat menghasilkan lawatan berkualiti tinggi. [[EENNDD]] koleksi gambar; perancangan perjalanan; perlombongan grafik; flickr; geolokasi"], [{"string": "Composite event queries for reactivity on the web No contact information provided yet.", "keywords": ["web", "language constructs and features", "event-condition-action rules", "composite events", "reactive languages"], "combined": "Composite event queries for reactivity on the web No contact information provided yet. [[EENNDD]] web; language constructs and features; event-condition-action rules; composite events; reactive languages"}, "Pertanyaan peristiwa komposit untuk kereaktifan di web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] web; konstruk dan ciri bahasa; peraturan peristiwa-keadaan-tindakan; acara komposit; bahasa reaktif"], [{"string": "Evaluating similarity measures for emergent semantics of social tagging Social bookmarking systems are becoming increasingly important data sources for bootstrapping and maintaining Semantic Web applications. Their emergent information structures have become known as folksonomies. A key question for harvesting semantics from these systems is how to extend and adapt traditional notions of similarity to folksonomies, and which measures are best suited for applications such as community detection, navigation support, semantic search, user profiling and ontology learning. Here we build an evaluation framework to compare various general folksonomy-based similarity measures, which are derived from several established information-theoretic, statistical, and practical measures. Our framework deals generally and symmetrically with users, tags, and resources. For evaluation purposes we focus on similarity between tags and between resources and consider different methods to aggregate annotations across users. After comparing the ability of several tag similarity measures to predict user-created tag relations, we provide an external grounding by user-validated semantic proxies based on WordNet and the Open Directory Project. We also investigate the issue of scalability. We find that mutual information with distributional micro-aggregation across users yields the highest accuracy, but is not scalable; per-user projection with collaborative aggregation provides the best scalable approach via incremental computations. The results are consistent across resource and tag similarity.", "keywords": ["semantic grounding", "social similarity", "information search and retrieval", "web 2.0", "ontology learning"], "combined": "Evaluating similarity measures for emergent semantics of social tagging Social bookmarking systems are becoming increasingly important data sources for bootstrapping and maintaining Semantic Web applications. Their emergent information structures have become known as folksonomies. A key question for harvesting semantics from these systems is how to extend and adapt traditional notions of similarity to folksonomies, and which measures are best suited for applications such as community detection, navigation support, semantic search, user profiling and ontology learning. Here we build an evaluation framework to compare various general folksonomy-based similarity measures, which are derived from several established information-theoretic, statistical, and practical measures. Our framework deals generally and symmetrically with users, tags, and resources. For evaluation purposes we focus on similarity between tags and between resources and consider different methods to aggregate annotations across users. After comparing the ability of several tag similarity measures to predict user-created tag relations, we provide an external grounding by user-validated semantic proxies based on WordNet and the Open Directory Project. We also investigate the issue of scalability. We find that mutual information with distributional micro-aggregation across users yields the highest accuracy, but is not scalable; per-user projection with collaborative aggregation provides the best scalable approach via incremental computations. The results are consistent across resource and tag similarity. [[EENNDD]] semantic grounding; social similarity; information search and retrieval; web 2.0; ontology learning"}, "Menilai ukuran kesamaan untuk semantik baru penandaan sosial Sistem penanda buku sosial menjadi sumber data yang semakin penting untuk bootstrap dan mengekalkan aplikasi Web Semantik. Struktur maklumat mereka yang muncul telah dikenali sebagai folksonomies. Soalan utama untuk mengambil semantik dari sistem ini adalah bagaimana memperluas dan menyesuaikan konsep persamaan tradisional dengan folksonomies, dan langkah-langkah mana yang paling sesuai untuk aplikasi seperti pengesanan komuniti, sokongan navigasi, carian semantik, profil pengguna dan pembelajaran ontologi. Di sini kita membina kerangka penilaian untuk membandingkan berbagai ukuran kesamaan berdasarkan folksonomi umum, yang berasal dari beberapa langkah maklumat-teori, statistik, dan praktikal. Rangka kerja kami berurusan secara umum dan simetri dengan pengguna, tag dan sumber. Untuk tujuan penilaian, kami memfokuskan pada persamaan antara teg dan antara sumber dan mempertimbangkan kaedah yang berbeza untuk mengumpulkan anotasi di antara pengguna. Setelah membandingkan kemampuan beberapa ukuran kesamaan tag untuk meramalkan hubungan tag yang dibuat pengguna, kami memberikan landasan luaran oleh proksi semantik yang disahkan oleh pengguna berdasarkan WordNet dan Open Directory Project. Kami juga menyiasat masalah skalabiliti. Kami mendapati bahawa maklumat bersama dengan pengagregatan mikro pengedaran di seluruh pengguna menghasilkan ketepatan tertinggi, tetapi tidak dapat ditingkatkan; unjuran setiap pengguna dengan penggabungan kolaboratif memberikan pendekatan skala terbaik melalui pengiraan tambahan. Hasilnya konsisten di antara sumber dan persamaan teg. [[EENNDD]] landasan semantik; persamaan sosial; carian dan pengambilan maklumat; laman web 2.0; pembelajaran ontologi"], [{"string": "Three theses of representation in the semantic web No contact information provided yet.", "keywords": ["representation", "model-theoretic semantics", "semantic web"], "combined": "Three theses of representation in the semantic web No contact information provided yet. [[EENNDD]] representation; model-theoretic semantics; semantic web"}, "Tiga tesis perwakilan dalam web semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perwakilan; semantik model-teori; web semantik"], [{"string": "Optimal rare query suggestion with implicit user feedback Query suggestion has been an effective approach to help users narrow down to the information they need. However, most of existing studies focused on only popular/head queries. Since rare queries possess much less information (e.g., clicks) than popular queries in the query logs, it is much more difficult to efficiently suggest relevant queries to a rare query. In this paper, we propose an optimal rare query suggestion framework by leveraging implicit feedbacks from users in the query logs. Our model resembles the principle of pseudo-relevance feedback which assumes that top-returned results by search engines are relevant. However, we argue that the clicked URLs and skipped URLs contain different levels of information and thus should be treated differently. Hence, our framework optimally combines both the click and skip information from users and uses a random walk model to optimize the query correlation. Our model specifically optimizes two parameters: (1) the restarting (jumping) rate of random walk, and (2) the combination ratio of click and skip information. Unlike the Rocchio algorithm, our learning process does not involve the content of the URLs but simply leverages the click and skip counts in the query-URL bipartite graphs. Consequently, our model is capable of scaling up to the need of commercial search engines. Experimental results on one-month query logs from a large commercial search engine with over 40 million rare queries demonstrate the superiority of our framework, with statistical significance, over the traditional random walk models and pseudo-relevance feedback models.", "keywords": ["pseudo-relevance feedback", "query suggestion", "random walk"], "combined": "Optimal rare query suggestion with implicit user feedback Query suggestion has been an effective approach to help users narrow down to the information they need. However, most of existing studies focused on only popular/head queries. Since rare queries possess much less information (e.g., clicks) than popular queries in the query logs, it is much more difficult to efficiently suggest relevant queries to a rare query. In this paper, we propose an optimal rare query suggestion framework by leveraging implicit feedbacks from users in the query logs. Our model resembles the principle of pseudo-relevance feedback which assumes that top-returned results by search engines are relevant. However, we argue that the clicked URLs and skipped URLs contain different levels of information and thus should be treated differently. Hence, our framework optimally combines both the click and skip information from users and uses a random walk model to optimize the query correlation. Our model specifically optimizes two parameters: (1) the restarting (jumping) rate of random walk, and (2) the combination ratio of click and skip information. Unlike the Rocchio algorithm, our learning process does not involve the content of the URLs but simply leverages the click and skip counts in the query-URL bipartite graphs. Consequently, our model is capable of scaling up to the need of commercial search engines. Experimental results on one-month query logs from a large commercial search engine with over 40 million rare queries demonstrate the superiority of our framework, with statistical significance, over the traditional random walk models and pseudo-relevance feedback models. [[EENNDD]] pseudo-relevance feedback; query suggestion; random walk"}, "Cadangan pertanyaan jarang yang optimum dengan maklum balas pengguna yang tersirat Cadangan pertanyaan telah menjadi pendekatan yang berkesan untuk membantu pengguna menyempitkan maklumat yang mereka perlukan. Walau bagaimanapun, kebanyakan kajian yang ada hanya tertumpu pada pertanyaan popular / head. Oleh kerana pertanyaan yang jarang berlaku mempunyai maklumat yang jauh lebih sedikit (mis. Klik) daripada pertanyaan yang popular di log pertanyaan, adalah lebih sukar untuk mencadangkan pertanyaan yang relevan kepada pertanyaan yang jarang berlaku. Dalam makalah ini, kami mengusulkan kerangka cadangan pertanyaan langka yang optimum dengan memanfaatkan maklum balas tersirat dari pengguna dalam log pertanyaan. Model kami menyerupai prinsip maklum balas pseudo-relevansi yang menganggap bahawa hasil yang dikembalikan oleh enjin carian adalah relevan. Walau bagaimanapun, kami berpendapat bahawa URL yang diklik dan URL yang dilangkau mengandungi tahap maklumat yang berbeza dan dengan itu harus diperlakukan secara berbeza. Oleh itu, kerangka kerja kami menggabungkan maklumat klik dan langkau dari pengguna secara optimum dan menggunakan model jalan rawak untuk mengoptimumkan korelasi pertanyaan. Model kami secara khusus mengoptimumkan dua parameter: (1) kadar memulakan semula (melompat) berjalan secara rawak, dan (2) nisbah gabungan maklumat klik dan langkau. Tidak seperti algoritma Rocchio, proses pembelajaran kami tidak melibatkan kandungan URL tetapi hanya memanfaatkan jumlah klik dan langkau dalam grafik bipartit URL-pertanyaan. Oleh itu, model kami mampu memenuhi keperluan mesin carian komersial. Hasil eksperimen pada log pertanyaan satu bulan dari enjin carian komersial yang besar dengan lebih daripada 40 juta pertanyaan jarang menunjukkan keunggulan kerangka kerja kami, dengan kepentingan statistik, berbanding model jalan rawak tradisional dan model maklum balas pseudo-relevansi. [[EENNDD]] maklum balas berkaitan pseudo; cadangan pertanyaan; jalan rawak"], [{"string": "Towards content trust of web resources No contact information provided yet.", "keywords": ["trust", "information search and retrieval", "semantic web", "web of trust"], "combined": "Towards content trust of web resources No contact information provided yet. [[EENNDD]] trust; information search and retrieval; semantic web; web of trust"}, "Ke arah kepercayaan kandungan sumber web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kepercayaan; pencarian dan pengambilan maklumat; web semantik; laman web kepercayaan"], [{"string": "Architecture of a p2p distributed adaptive directory An abstract is not available.", "keywords": ["bookmark sharing", "peer to peer", "adaptivity"], "combined": "Architecture of a p2p distributed adaptive directory An abstract is not available. [[EENNDD]] bookmark sharing; peer to peer; adaptivity"}, "Senibina direktori adaptif p2p diedarkan Abstrak tidak tersedia. [[EENNDD]] perkongsian penanda buku; rakan sebaya; penyesuaian"], [{"string": "Tv2web: generating and browsing web with multiple lod from video streams and their metadata No contact information provided yet.", "keywords": ["web browser from video streams and their metadat", "generation of web content", "video stream", "level of detail", "metadata"], "combined": "Tv2web: generating and browsing web with multiple lod from video streams and their metadata No contact information provided yet. [[EENNDD]] web browser from video streams and their metadat; generation of web content; video stream; level of detail; metadata"}, "Tv2web: menjana dan melayari web dengan banyak tempat dari aliran video dan metadata mereka Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penyemak imbas web dari aliran video dan metadatnya; penjanaan kandungan web; aliran video; tahap perincian; metadata"], [{"string": "On business activity modeling using grammars No contact information provided yet.", "keywords": ["data mining", "time management", "web log analysis"], "combined": "On business activity modeling using grammars No contact information provided yet. [[EENNDD]] data mining; time management; web log analysis"}, "Pemodelan aktiviti perniagaan menggunakan tatabahasa Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlombongan data; pengurusan masa; analisis log web"], [{"string": "Interpreting distributed ontologies No contact information provided yet.", "keywords": ["owl", "commitment relationship", "vocabulary provenance", "distributed description logic"], "combined": "Interpreting distributed ontologies No contact information provided yet. [[EENNDD]] owl; commitment relationship; vocabulary provenance; distributed description logic"}, "Mentafsir ontologi yang diedarkan [[EENNDD]] burung hantu; hubungan komitmen; perbendaharaan kata perbendaharaan kata; logik penerangan yang diedarkan"], [{"string": "Distributed location aware web crawling No contact information provided yet.", "keywords": ["distributed web crawling", "location aware web crawling"], "combined": "Distributed location aware web crawling No contact information provided yet. [[EENNDD]] distributed web crawling; location aware web crawling"}, "Perayapan web yang diketahui mengenai lokasi yang diedarkan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] merangkak web yang diedarkan; merangkak web yang sedar lokasi"], [{"string": "Mining contiguous sequential patterns from web logs Finding Contiguous Sequential Patterns (CSP) is an important problem in Web usage mining. In this paper we propose a new data structure, UpDown Tree, for CSP mining. An UpDown Tree combines suffix tree and prefix tree for efficient storage of all the sequences that contain a given item. The special structure of UpDown Tree ensures efficient detection of CSPs. Experiments show that UpDown Tree improves CSP mining in terms of both time and memory usage comparing to previous approaches.", "keywords": ["sequential pattern", "web usage mining", "contiguous sequential pattern"], "combined": "Mining contiguous sequential patterns from web logs Finding Contiguous Sequential Patterns (CSP) is an important problem in Web usage mining. In this paper we propose a new data structure, UpDown Tree, for CSP mining. An UpDown Tree combines suffix tree and prefix tree for efficient storage of all the sequences that contain a given item. The special structure of UpDown Tree ensures efficient detection of CSPs. Experiments show that UpDown Tree improves CSP mining in terms of both time and memory usage comparing to previous approaches. [[EENNDD]] sequential pattern; web usage mining; contiguous sequential pattern"}, "Melombong corak urutan bersebelahan dari log web Mencari Corak Berurutan Berterusan (CSP) adalah masalah penting dalam perlombongan penggunaan Web. Dalam makalah ini kami mencadangkan struktur data baru, UpDown Tree, untuk perlombongan CSP. Pokok UpDown menggabungkan pokok akhiran dan pohon awalan untuk penyimpanan semua urutan yang cekap yang mengandungi item tertentu. Struktur khas UpDown Tree memastikan pengesanan CSP yang cekap. Eksperimen menunjukkan bahawa UpDown Tree meningkatkan perlombongan CSP dari segi penggunaan masa dan memori berbanding dengan pendekatan sebelumnya. [[EENNDD]] corak jujukan; perlombongan penggunaan web; corak jujukan bersebelahan"], [{"string": "Implementing optimal outcomes in social computing: a game-theoretic approach In many social computing applications such as online Q&amp;A forums, the best contribution for each task receives some high reward, while all remaining contributions receive an identical, lower reward irrespective of their actual qualities. Suppose a mechanism designer (site owner) wishes to optimize an objective that is some function of the number and qualities of received contributions. When potential contributors are {\\em strategic} agents, who decide whether to contribute or not to selfishly maximize their own utilities, is such a \"best contribution\" mechanism, Mb, adequate to implement an outcome that is optimal for the mechanism designer? We first show that in settings where a contribution's value is determined primarily by an agent's expertise, and agents only strategically choose whether to contribute or not, contests can implement optimal outcomes: for any reasonable objective, the rewards for the best and remaining contributions in Mb can always be chosen so that the outcome in the unique symmetric equilibrium of Mb maximizes the mechanism designer's utility. We also show how the mechanism designer can learn these optimal rewards when she does not know the parameters of the agents' utilities, as might be the case in practice. We next consider settings where a contribution's value depends on both the contributor's expertise as well as her effort, and agents endogenously choose how much effort to exert in addition to deciding whether to contribute. Here, we show that optimal outcomes can never be implemented by contests if the system can rank the qualities of contributions perfectly. However, if there is noise in the contributions' rankings, then the mechanism designer can again induce agents to follow strategies that maximize his utility. Thus imperfect rankings can actually help achieve implementability of optimal outcomes when effort is endogenous and influences quality.", "keywords": ["social computing", "game theory", "user generated content", "implementation", "online q&amp;a forums"], "combined": "Implementing optimal outcomes in social computing: a game-theoretic approach In many social computing applications such as online Q&amp;A forums, the best contribution for each task receives some high reward, while all remaining contributions receive an identical, lower reward irrespective of their actual qualities. Suppose a mechanism designer (site owner) wishes to optimize an objective that is some function of the number and qualities of received contributions. When potential contributors are {\\em strategic} agents, who decide whether to contribute or not to selfishly maximize their own utilities, is such a \"best contribution\" mechanism, Mb, adequate to implement an outcome that is optimal for the mechanism designer? We first show that in settings where a contribution's value is determined primarily by an agent's expertise, and agents only strategically choose whether to contribute or not, contests can implement optimal outcomes: for any reasonable objective, the rewards for the best and remaining contributions in Mb can always be chosen so that the outcome in the unique symmetric equilibrium of Mb maximizes the mechanism designer's utility. We also show how the mechanism designer can learn these optimal rewards when she does not know the parameters of the agents' utilities, as might be the case in practice. We next consider settings where a contribution's value depends on both the contributor's expertise as well as her effort, and agents endogenously choose how much effort to exert in addition to deciding whether to contribute. Here, we show that optimal outcomes can never be implemented by contests if the system can rank the qualities of contributions perfectly. However, if there is noise in the contributions' rankings, then the mechanism designer can again induce agents to follow strategies that maximize his utility. Thus imperfect rankings can actually help achieve implementability of optimal outcomes when effort is endogenous and influences quality. [[EENNDD]] social computing; game theory; user generated content; implementation; online q&amp;a forums"}, ""], [{"string": "Ranking community answers via analogical reasoning Due to the lexical gap between questions and answers, automatically detecting right answers becomes very challenging for community question-answering sites. In this paper, we propose an analogical reasoning-based method. It treats questions and answers as relational data and ranks an answer by measuring the analogy of its link to a query with the links embedded in previous relevant knowledge; the answer that links in the most analogous way to the new question is assumed to be the best answer. We based our experiments on 29.8 million Yahoo!Answer question-answer threads and showed the effectiveness of the approach.", "keywords": ["community question answering", "analogical reasoning"], "combined": "Ranking community answers via analogical reasoning Due to the lexical gap between questions and answers, automatically detecting right answers becomes very challenging for community question-answering sites. In this paper, we propose an analogical reasoning-based method. It treats questions and answers as relational data and ranks an answer by measuring the analogy of its link to a query with the links embedded in previous relevant knowledge; the answer that links in the most analogous way to the new question is assumed to be the best answer. We based our experiments on 29.8 million Yahoo!Answer question-answer threads and showed the effectiveness of the approach. [[EENNDD]] community question answering; analogical reasoning"}, "Menentukan jawapan komuniti melalui penaakulan analogis Oleh kerana jurang leksikal antara soalan dan jawapan, secara automatik mengesan jawapan yang tepat menjadi sangat mencabar bagi laman web menjawab soalan masyarakat. Dalam makalah ini, kami mencadangkan kaedah berdasarkan penaakulan analog. Ia memperlakukan soalan dan jawapan sebagai data hubungan dan memberi peringkat jawapan dengan mengukur analogi pautannya ke pertanyaan dengan pautan yang disertakan dalam pengetahuan berkaitan sebelumnya; jawapan yang menghubungkan dengan cara yang paling serupa dengan soalan baru dianggap sebagai jawapan terbaik. Kami berdasarkan eksperimen kami pada 29.8 juta benang soal jawab Yahoo! Dan menunjukkan keberkesanan pendekatannya. [[EENNDD]] menjawab soalan komuniti; penaakulan analog"], [{"string": "The indexable web is more than 11.5 billion pages No contact information provided yet.", "keywords": ["search engines", "index sizes", "size of the web", "information search and retrieval"], "combined": "The indexable web is more than 11.5 billion pages No contact information provided yet. [[EENNDD]] search engines; index sizes; size of the web; information search and retrieval"}, "Laman web yang dapat diindeks lebih dari 11.5 bilion halaman Belum ada maklumat hubungan yang diberikan. [[EENNDD]] enjin carian; saiz indeks; saiz web; pencarian dan pencarian maklumat"], [{"string": "Automatic detection of fragments in dynamically generated web pages No contact information provided yet.", "keywords": ["general", "fragment-based caching", "dynamic content caching", "l-p fragments", "fragment detection", "shared fragments"], "combined": "Automatic detection of fragments in dynamically generated web pages No contact information provided yet. [[EENNDD]] general; fragment-based caching; dynamic content caching; l-p fragments; fragment detection; shared fragments"}, "Pengesanan serpihan secara automatik di laman web yang dihasilkan secara dinamik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] umum; cache berasaskan serpihan; cache kandungan dinamik; serpihan l-p; pengesanan serpihan; serpihan bersama"], [{"string": "ATMEN: a triggered network measurement infrastructure No contact information provided yet.", "keywords": ["reuse", "performance-based ranking", "dns availability", "measurement infrastructure", "triggered measurements"], "combined": "ATMEN: a triggered network measurement infrastructure No contact information provided yet. [[EENNDD]] reuse; performance-based ranking; dns availability; measurement infrastructure; triggered measurements"}, "ATMEN: infrastruktur pengukuran rangkaian yang dicetuskan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penggunaan semula; peringkat berdasarkan prestasi; ketersediaan dns; infrastruktur pengukuran; pengukuran yang dicetuskan"], [{"string": "We know who you followed last summer: inferring social link creation times in twitter Understanding a network's temporal evolution appears to require multiple observations of the graph over time. These often expensive repeated crawls are only able to answer questions about what happened from observation to observation, and not what happened before or between network snapshots. Contrary to this picture, we propose a method for Twitter's social network that takes a single static snapshot of network edges and user account creation times to accurately infer when these edges were formed. This method can be exact in theory, and we demonstrate empirically for a large subset of Twitter relationships that it is accurate to within a few hours in practice.", "keywords": ["large-scale data collection", "miscellaneous", "network evolution", "graph analysis", "user behavior", "online social networks"], "combined": "We know who you followed last summer: inferring social link creation times in twitter Understanding a network's temporal evolution appears to require multiple observations of the graph over time. These often expensive repeated crawls are only able to answer questions about what happened from observation to observation, and not what happened before or between network snapshots. Contrary to this picture, we propose a method for Twitter's social network that takes a single static snapshot of network edges and user account creation times to accurately infer when these edges were formed. This method can be exact in theory, and we demonstrate empirically for a large subset of Twitter relationships that it is accurate to within a few hours in practice. [[EENNDD]] large-scale data collection; miscellaneous; network evolution; graph analysis; user behavior; online social networks"}, "Kami tahu siapa yang anda ikuti musim panas lalu: menyimpulkan masa pembuatan pautan sosial di twitter Memahami evolusi temporal rangkaian nampaknya memerlukan banyak pemerhatian grafik dari masa ke masa. Perayapan berulang yang sering kali mahal ini hanya dapat menjawab soalan mengenai apa yang berlaku dari pemerhatian ke pemerhatian, dan bukan apa yang berlaku sebelum atau antara tangkapan gambar rangkaian. Bertentangan dengan gambar ini, kami mencadangkan kaedah untuk rangkaian sosial Twitter yang mengambil satu gambaran statik tunggal dari tepi rangkaian dan masa pembuatan akaun pengguna untuk membuat kesimpulan secara tepat ketika bahagian ini terbentuk. Kaedah ini tepat dalam teori, dan kami menunjukkan secara empirik untuk sebahagian besar hubungan Twitter bahawa tepat dalam beberapa jam dalam praktiknya. [[EENNDD]] pengumpulan data berskala besar; pelbagai; evolusi rangkaian; analisis grafik; tingkah laku pengguna; rangkaian sosial dalam talian"], [{"string": "Hunter gatherer: interaction support for the creation and management of within-web-page collections No contact information provided yet.", "keywords": ["information gathering and management", "web-based interaction design", "collections", "transclusions", "attention"], "combined": "Hunter gatherer: interaction support for the creation and management of within-web-page collections No contact information provided yet. [[EENNDD]] information gathering and management; web-based interaction design; collections; transclusions; attention"}, "Hunter collecter: sokongan interaksi untuk pembuatan dan pengurusan koleksi halaman dalam laman web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengumpulan dan pengurusan maklumat; reka bentuk interaksi berasaskan web; koleksi; penyekat; perhatian"], [{"string": "Mining advertiser-specific user behavior using adfactors Consider an online ad campaign run by an advertiser. The ad serving companies that handle such campaigns record users' behavior that leads to impressions of campaign ads, as well as users' responses to such impressions. This is summarized and reported to the advertisers to help them evaluate the performance of their campaigns and make better budget allocation decisions.", "keywords": ["pagerank", "ad auctions", "miscellaneous", "sponsored search", "user behavior models", "conversion rate", "clickthrough rate", "online advertising"], "combined": "Mining advertiser-specific user behavior using adfactors Consider an online ad campaign run by an advertiser. The ad serving companies that handle such campaigns record users' behavior that leads to impressions of campaign ads, as well as users' responses to such impressions. This is summarized and reported to the advertisers to help them evaluate the performance of their campaigns and make better budget allocation decisions. [[EENNDD]] pagerank; ad auctions; miscellaneous; sponsored search; user behavior models; conversion rate; clickthrough rate; online advertising"}, "Menambang perilaku pengguna khusus pengiklan menggunakan adfactors Pertimbangkan kempen iklan dalam talian yang dijalankan oleh pengiklan. Syarikat penayangan iklan yang menangani kempen tersebut merekodkan perilaku pengguna yang mengarah ke tayangan iklan kempen, serta tanggapan pengguna terhadap tayangan tersebut. Ini diringkaskan dan dilaporkan kepada pengiklan untuk membantu mereka menilai prestasi kempen mereka dan membuat keputusan peruntukan anggaran yang lebih baik. [[EENNDD]] pagerank; lelongan iklan; pelbagai; carian tajaan; model tingkah laku pengguna; kadar penukaran; kadar klik lalu; iklan dalam talian"], [{"string": "Globetp: template-based database replication for scalable web applications Generic database replication algorithms do not scale linearly in throughput as all update, deletion and insertion (UDI) queries must be applied to every database replica. The throughput is therefore limited to the point where the number of UDI queries alone is sufficient to overload one server. In such scenarios, partial replication of a database can help, as UDI queries are executed only by a subset of all servers. In this paper we propose GlobeTP, a system that employs partial replication to improve database throughput. GlobeTP exploits the fact that a Web application's query workload is composed of a small set of read and write templates. Using knowledge of these templates and their respective execution costs, GlobeTP provides database table placements that produce significant improvements in database throughput. We demonstrate the efficiency of this technique using two different industry standard benchmarks. In our experiments, GlobeTP increases the throughput by 57% to 150% compared to full replication, while using identical hardware configuration. Furthermore, adding a single query cache improves the throughput by another 30% to 60%.", "keywords": ["web applications", "partial replication", "scalability", "database replication"], "combined": "Globetp: template-based database replication for scalable web applications Generic database replication algorithms do not scale linearly in throughput as all update, deletion and insertion (UDI) queries must be applied to every database replica. The throughput is therefore limited to the point where the number of UDI queries alone is sufficient to overload one server. In such scenarios, partial replication of a database can help, as UDI queries are executed only by a subset of all servers. In this paper we propose GlobeTP, a system that employs partial replication to improve database throughput. GlobeTP exploits the fact that a Web application's query workload is composed of a small set of read and write templates. Using knowledge of these templates and their respective execution costs, GlobeTP provides database table placements that produce significant improvements in database throughput. We demonstrate the efficiency of this technique using two different industry standard benchmarks. In our experiments, GlobeTP increases the throughput by 57% to 150% compared to full replication, while using identical hardware configuration. Furthermore, adding a single query cache improves the throughput by another 30% to 60%. [[EENNDD]] web applications; partial replication; scalability; database replication"}, "Globetp: replikasi pangkalan data berasaskan templat untuk aplikasi web berskala Algoritma replikasi pangkalan data generik tidak skala secara linear dalam throughput kerana semua pertanyaan kemas kini, penghapusan dan penyisipan (UDI) mesti diterapkan pada setiap replika pangkalan data. Oleh itu, throughput terhad pada tahap di mana bilangan pertanyaan UDI sahaja mencukupi untuk memuatkan satu pelayan. Dalam senario seperti itu, replikasi sebahagian pangkalan data dapat membantu, kerana pertanyaan UDI dijalankan hanya oleh subset dari semua pelayan. Dalam makalah ini kami mengusulkan GlobeTP, sistem yang menggunakan replikasi separa untuk meningkatkan throughput pangkalan data. GlobeTP memanfaatkan kenyataan bahawa beban kerja pertanyaan aplikasi Web terdiri daripada sekumpulan kecil templat baca dan tulis. Dengan menggunakan pengetahuan mengenai templat ini dan biaya pelaksanaan masing-masing, GlobeTP menyediakan penempatan jadual pangkalan data yang menghasilkan peningkatan yang signifikan dalam throughput pangkalan data. Kami menunjukkan kecekapan teknik ini menggunakan dua tanda aras standard industri yang berbeza. Dalam eksperimen kami, GlobeTP meningkatkan throughput sebanyak 57% hingga 150% berbanding replikasi penuh, sambil menggunakan konfigurasi perkakasan yang sama. Tambahan pula, menambahkan satu cache pertanyaan meningkatkan throughput sebanyak 30% hingga 60% lagi. [[EENNDD]] aplikasi web; replikasi separa; skalabiliti; replikasi pangkalan data"], [{"string": "Function-based object model towards website adaptation An abstract is not available.", "keywords": ["html/wml conversion", "document preparation", "website understanding", "content function", "content adaptation"], "combined": "Function-based object model towards website adaptation An abstract is not available. [[EENNDD]] html/wml conversion; document preparation; website understanding; content function; content adaptation"}, "Model objek berasaskan fungsi ke arah penyesuaian laman web Abstrak tidak tersedia. [[EENNDD]] penukaran html / wml; penyediaan dokumen; pemahaman laman web; fungsi kandungan; penyesuaian kandungan"], [{"string": "Contextual advertising by combining relevance with click feedback Contextual advertising supports much of the Web's ecosystem today. User experience and revenue (shared by the site publisher and the ad network) depend on the relevance of the displayed ads to the page content. As with other document retrieval systems, relevance is provided by scoring the match between individual ads (documents) and the content of the page where the ads are shown (query). In this paper we show how this match can be improved significantly by augmenting the ad-page scoring function with extra parameters from a logistic regression model on the words in the pages and ads. A key property of the proposed model is that it can be mapped to standard cosine similarity matching and is suitable for efficient and scalable implementation over inverted indexes. The model parameter values are learnt from logs containing ad impressions and clicks, with shrinkage estimators being used to combat sparsity. To scale our computations to train on an extremely large training corpus consisting of several gigabytes of data, we parallelize our fitting algorithm in a Hadoop framework [10]. Experimental evaluation is provided showing improved click prediction over a holdout set of impression and click events from a large scale real-world ad placement engine. Our best model achieves a 25% lift in precision relative to a traditional information retrieval model which is based on cosine similarity, for recalling 10% of the clicks in our test data.", "keywords": ["miscellaneous", "clickthrough rate", "modeling", "interaction effects"], "combined": "Contextual advertising by combining relevance with click feedback Contextual advertising supports much of the Web's ecosystem today. User experience and revenue (shared by the site publisher and the ad network) depend on the relevance of the displayed ads to the page content. As with other document retrieval systems, relevance is provided by scoring the match between individual ads (documents) and the content of the page where the ads are shown (query). In this paper we show how this match can be improved significantly by augmenting the ad-page scoring function with extra parameters from a logistic regression model on the words in the pages and ads. A key property of the proposed model is that it can be mapped to standard cosine similarity matching and is suitable for efficient and scalable implementation over inverted indexes. The model parameter values are learnt from logs containing ad impressions and clicks, with shrinkage estimators being used to combat sparsity. To scale our computations to train on an extremely large training corpus consisting of several gigabytes of data, we parallelize our fitting algorithm in a Hadoop framework [10]. Experimental evaluation is provided showing improved click prediction over a holdout set of impression and click events from a large scale real-world ad placement engine. Our best model achieves a 25% lift in precision relative to a traditional information retrieval model which is based on cosine similarity, for recalling 10% of the clicks in our test data. [[EENNDD]] miscellaneous; clickthrough rate; modeling; interaction effects"}, "Pengiklanan kontekstual dengan menggabungkan perkaitan dengan maklum balas klik Pengiklanan kontekstual menyokong sebahagian besar ekosistem Web hari ini. Pengalaman dan pendapatan pengguna (dikongsi oleh penerbit laman web dan jaringan iklan) bergantung pada relevansi iklan yang ditampilkan dengan konten halaman. Seperti sistem pengambilan dokumen lain, relevansi diberikan dengan mencocokkan pertandingan antara iklan individu (dokumen) dan isi halaman di mana iklan ditampilkan (pertanyaan). Dalam makalah ini kami menunjukkan bagaimana pertandingan ini dapat ditingkatkan dengan signifikan dengan menambah fungsi pemarkahan halaman iklan dengan parameter tambahan dari model regresi logistik pada kata-kata di halaman dan iklan. Satu sifat utama dari model yang dicadangkan adalah bahawa ia dapat dipetakan untuk pencocokan kesamaan kosinus standard dan sesuai untuk pelaksanaan yang efisien dan berskala berbanding indeks terbalik. Nilai parameter model dipelajari dari log yang mengandungi tayangan dan klik iklan, dengan penganggar pengecutan digunakan untuk memerangi sparsity. Untuk meningkatkan pengiraan kami untuk melatih korpus latihan yang sangat besar yang terdiri daripada beberapa gigabait data, kami menyelaraskan algoritma yang sesuai dalam kerangka Hadoop [10]. Penilaian eksperimental diberikan yang menunjukkan ramalan klik yang lebih baik berbanding sekumpulan kesan dan peristiwa klik yang ditangguhkan dari mesin penempatan iklan dunia nyata berskala besar. Model terbaik kami mencapai peningkatan ketepatan 25% berbanding model pengambilan maklumat tradisional yang berdasarkan kesamaan kosinus, kerana mengingat 10% klik dalam data ujian kami. [[EENNDD]] pelbagai; kadar klik lalu; pemodelan; kesan interaksi"], [{"string": "Scalable integration and processing of linked data The goal of this tutorial is to introduce, motivate and detail techniques for integrating heterogeneous structured data from across the Web. Inspired by the growth in Linked Data publishing, our tutorial aims at educating Web researchers and practitioners about this new publishing paradigm. The tutorial will show how Linked Data enables uniform access, parsing and interpretation of data, and how this novel wealth of structured data can potentially be exploited for creating new applications or enhancing existing ones.", "keywords": ["linked data"], "combined": "Scalable integration and processing of linked data The goal of this tutorial is to introduce, motivate and detail techniques for integrating heterogeneous structured data from across the Web. Inspired by the growth in Linked Data publishing, our tutorial aims at educating Web researchers and practitioners about this new publishing paradigm. The tutorial will show how Linked Data enables uniform access, parsing and interpretation of data, and how this novel wealth of structured data can potentially be exploited for creating new applications or enhancing existing ones. [[EENNDD]] linked data"}, "Penggabungan dan pemprosesan data terpaut yang berskala Matlamat tutorial ini adalah untuk memperkenalkan, memotivasi dan memperincikan teknik untuk mengintegrasikan data berstruktur heterogen dari seluruh Web. Terinspirasi oleh pertumbuhan penerbitan Data Terhubung, tutorial kami bertujuan untuk mendidik penyelidik dan pengamal Web mengenai paradigma penerbitan baru ini. Tutorial akan menunjukkan bagaimana Data Terhubung membolehkan akses, penghuraian dan penafsiran data yang seragam, dan bagaimana kekayaan data berstruktur baru ini berpotensi dimanfaatkan untuk membuat aplikasi baru atau meningkatkan aplikasi yang ada. [[EENNDD]] data terpaut"], [{"string": "A contextual-bandit approach to personalized news article recommendation Personalized web services strive to adapt their services (advertisements, news articles, etc.) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation.", "keywords": ["personalization", "on-line information services", "contextual bandit", "learning", "exploration/exploitation dilemma", "web service", "recommender systems"], "combined": "A contextual-bandit approach to personalized news article recommendation Personalized web services strive to adapt their services (advertisements, news articles, etc.) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation. [[EENNDD]] personalization; on-line information services; contextual bandit; learning; exploration/exploitation dilemma; web service; recommender systems"}, "Pendekatan kontekstual-bandit untuk cadangan artikel berita yang diperibadikan Perkhidmatan web yang diperibadikan berusaha untuk menyesuaikan perkhidmatan mereka (iklan, artikel berita, dll.) Kepada pengguna individu dengan memanfaatkan kedua-dua kandungan dan maklumat pengguna. Walaupun terdapat beberapa kemajuan baru-baru ini, masalah ini tetap mencabar sekurang-kurangnya dua sebab. Pertama, perkhidmatan web ditampilkan dengan kumpulan kandungan yang berubah secara dinamik, menjadikan kaedah penapisan kolaboratif tradisional tidak dapat diterapkan. Kedua, skala kebanyakan perkhidmatan web dengan minat praktikal memerlukan penyelesaian yang cepat dalam pembelajaran dan pengiraan. [[EENNDD]] pemperibadian; perkhidmatan maklumat dalam talian; penyamun kontekstual; belajar; dilema penerokaan / eksploitasi; perkhidmatan web; sistem cadangan"], [{"string": "Genealogical trees on the web: a search engine user perspective This paper presents an extensive study about the evolution of textual content on the Web, which shows how some new pages are created from scratch while others are created using already existing content. We show that a significant fraction of the Web is a byproduct of the latter case. We introduce the concept of Web genealogical tree, in which every page in a Web snapshot is classified into a component. We study in detail these components, characterizing the copies and identifying the relation between a source of content and a search engine, by comparing page relevance measures, documents returned by real queries performed in the past, and click-through data. We observe that sources of copies are more frequently returned by queries and more clicked than other documents.", "keywords": ["search engine", "web", "information search and retrieval", "content evolution", "web mining", "text"], "combined": "Genealogical trees on the web: a search engine user perspective This paper presents an extensive study about the evolution of textual content on the Web, which shows how some new pages are created from scratch while others are created using already existing content. We show that a significant fraction of the Web is a byproduct of the latter case. We introduce the concept of Web genealogical tree, in which every page in a Web snapshot is classified into a component. We study in detail these components, characterizing the copies and identifying the relation between a source of content and a search engine, by comparing page relevance measures, documents returned by real queries performed in the past, and click-through data. We observe that sources of copies are more frequently returned by queries and more clicked than other documents. [[EENNDD]] search engine; web; information search and retrieval; content evolution; web mining; text"}, "Salasilah silsilah di web: perspektif pengguna mesin pencari Makalah ini menyajikan kajian luas mengenai evolusi kandungan teks di Web, yang menunjukkan bagaimana beberapa halaman baru dibuat dari awal sementara yang lain dibuat menggunakan kandungan yang sudah ada. Kami menunjukkan bahawa sebahagian besar Web adalah hasil sampingan dari kes terakhir. Kami memperkenalkan konsep silsilah Web, di mana setiap halaman dalam snapshot Web diklasifikasikan menjadi komponen. Kami mengkaji secara terperinci komponen ini, mencirikan salinan dan mengenal pasti hubungan antara sumber kandungan dan mesin pencari, dengan membandingkan ukuran relevansi halaman, dokumen yang dikembalikan oleh pertanyaan sebenar yang dilakukan pada masa lalu, dan data klik-tayang. Kami melihat bahawa sumber salinan lebih kerap dikembalikan oleh pertanyaan dan lebih banyak diklik daripada dokumen lain. [[EENNDD]] enjin carian; laman web; pencarian dan pengambilan maklumat; evolusi kandungan; perlombongan web; teks"], [{"string": "Making RDF presentable: integrated global and local semantic Web browsing No contact information provided yet.", "keywords": ["document preparation", "hypertext/hypermedia", "browsing", "knowledge representation formalisms and methods", "multimedia information systems", "media archives", "hypermedia generation", "semantic web", "rdf", "search", "clustering"], "combined": "Making RDF presentable: integrated global and local semantic Web browsing No contact information provided yet. [[EENNDD]] document preparation; hypertext/hypermedia; browsing; knowledge representation formalisms and methods; multimedia information systems; media archives; hypermedia generation; semantic web; rdf; search; clustering"}, "Menjadikan RDF rapi: penyemakan imbas semantik global dan tempatan bersepadu Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penyediaan dokumen; hiperteks / hipermedia; melayari; formalisme dan kaedah perwakilan pengetahuan; sistem maklumat multimedia; arkib media; penjanaan hipermedia; web semantik; rdf; cari; pengelompokan"], [{"string": "A software framework for matchmaking based on semantic web technology No contact information provided yet.", "keywords": ["web services", "ontologies", "semantic web"], "combined": "A software framework for matchmaking based on semantic web technology No contact information provided yet. [[EENNDD]] web services; ontologies; semantic web"}, "Rangka kerja perisian untuk perjodohan berdasarkan teknologi web semantik Belum ada maklumat hubungan yang disediakan. [[EENNDD]] perkhidmatan web; ontologi; web semantik"], [{"string": "Random surfer with back step No contact information provided yet.", "keywords": ["ranking algorithms", "pagerank", "back step"], "combined": "Random surfer with back step No contact information provided yet. [[EENNDD]] ranking algorithms; pagerank; back step"}, "Surfer secara rawak dengan langkah belakang Belum ada maklumat hubungan yang diberikan. [[EENNDD]] algoritma ranking; pagerank; langkah belakang"], [{"string": "A quality model for multichannel adaptive information No contact information provided yet.", "keywords": ["quality of service", "model", "miscellaneous"], "combined": "A quality model for multichannel adaptive information No contact information provided yet. [[EENNDD]] quality of service; model; miscellaneous"}, "Model yang berkualiti untuk maklumat adaptif pelbagai saluran Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kualiti perkhidmatan; model; pelbagai"], [{"string": "A probabilistic approach to spatiotemporal theme pattern mining on weblogs No contact information provided yet.", "keywords": ["spatiotemporal text mining", "theme pattern", "information search and retrieval", "mixture model", "weblog"], "combined": "A probabilistic approach to spatiotemporal theme pattern mining on weblogs No contact information provided yet. [[EENNDD]] spatiotemporal text mining; theme pattern; information search and retrieval; mixture model; weblog"}, "Pendekatan probabilistik untuk perlombongan corak tema spatiotemporal di blog web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlombongan teks spatiotemporal; corak tema; pencarian dan pengambilan maklumat; model campuran; blog web"], [{"string": "Unparsing RDF/XML No contact information provided yet.", "keywords": ["unparsing", "xml", "generation", "rdf", "grammar", "universal algebra"], "combined": "Unparsing RDF/XML No contact information provided yet. [[EENNDD]] unparsing; xml; generation; rdf; grammar; universal algebra"}, "Tidak menguraikan RDF / XML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] menghuraikan; xml; generasi; rdf; tatabahasa; aljabar sejagat"], [{"string": "Liveclassifier: creating hierarchical text classifiers through web corpora No contact information provided yet.", "keywords": ["text classification", "web mining", "topic hierarchy", "information storage and retrieval"], "combined": "Liveclassifier: creating hierarchical text classifiers through web corpora No contact information provided yet. [[EENNDD]] text classification; web mining; topic hierarchy; information storage and retrieval"}, "Liveclassifier: membuat pengkelasan teks hierarki melalui korporat web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengelasan teks; perlombongan web; hierarki topik; penyimpanan dan pengambilan maklumat"], [{"string": "P-tree: a p2p index for resource discovery applications No contact information provided yet.", "keywords": ["range queries", "resource discovery", "indexing", "peer-to-peer"], "combined": "P-tree: a p2p index for resource discovery applications No contact information provided yet. [[EENNDD]] range queries; resource discovery; indexing; peer-to-peer"}, "P-tree: indeks p2p untuk aplikasi penemuan sumber Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] pertanyaan pelbagai; penemuan sumber; pengindeksan; rakan sebaya"], [{"string": "Modeling relationship strength in online social networks Previous work analyzing social networks has mainly focused on binary friendship relations. However, in online social networks the low cost of link formation can lead to networks with heterogeneous relationship strengths (e.g., acquaintances and best friends mixed together). In this case, the binary friendship indicator provides only a coarse representation of relationship information. In this work, we develop an unsupervised model to estimate relationship strength from interaction activity (e.g., communication, tagging) and user similarity. More specifically, we formulate a link-based latent variable model, along with a coordinate ascent optimization procedure for the inference. We evaluate our approach on real-world data from Facebook and LinkedIn, showing that the estimated link weights result in higher autocorrelation and lead to improved classification accuracy.", "keywords": ["homophily", "randomization", "miscellaneous", "social networks", "social influence"], "combined": "Modeling relationship strength in online social networks Previous work analyzing social networks has mainly focused on binary friendship relations. However, in online social networks the low cost of link formation can lead to networks with heterogeneous relationship strengths (e.g., acquaintances and best friends mixed together). In this case, the binary friendship indicator provides only a coarse representation of relationship information. In this work, we develop an unsupervised model to estimate relationship strength from interaction activity (e.g., communication, tagging) and user similarity. More specifically, we formulate a link-based latent variable model, along with a coordinate ascent optimization procedure for the inference. We evaluate our approach on real-world data from Facebook and LinkedIn, showing that the estimated link weights result in higher autocorrelation and lead to improved classification accuracy. [[EENNDD]] homophily; randomization; miscellaneous; social networks; social influence"}, "Memodelkan kekuatan hubungan dalam rangkaian sosial dalam talian Karya sebelumnya yang menganalisis rangkaian sosial terutamanya memfokuskan pada hubungan persahabatan binari. Walau bagaimanapun, dalam rangkaian sosial dalam talian, kos pembentukan pautan yang rendah dapat menyebabkan rangkaian dengan kekuatan hubungan yang heterogen (misalnya, kenalan dan sahabat baik bercampur). Dalam kes ini, petunjuk persahabatan binari hanya memberikan gambaran kasar mengenai maklumat hubungan. Dalam karya ini, kami mengembangkan model yang tidak diawasi untuk menganggarkan kekuatan hubungan dari aktiviti interaksi (mis., Komunikasi, penandaan) dan kesamaan pengguna. Lebih khusus lagi, kami merumuskan model pemboleh ubah laten berasaskan pautan, bersama dengan prosedur pengoptimuman pendakian koordinat untuk kesimpulan. Kami menilai pendekatan kami terhadap data dunia nyata dari Facebook dan LinkedIn, menunjukkan bahawa anggaran pautan yang dihasilkan menghasilkan korelasi autokorelasi yang lebih tinggi dan membawa kepada peningkatan ketepatan klasifikasi. [[EENNDD]] homofili; rawak; pelbagai; rangkaian sosial; pengaruh sosial"], [{"string": "On directly mapping relational databases to RDF and OWL Mapping relational databases to RDF is a fundamental problem for the development of the Semantic Web. We present a solution, inspired by draft methods defined by the W3C where relational databases are directly mapped to RDF and OWL. Given a relational database schema and its integrity constraints, this direct mapping produces an OWL ontology, which, provides the basis for generating RDF instances. The semantics of this mapping is defined using Datalog. Two fundamental properties are information preservation and query preservation. We prove that our mapping satisfies both conditions, even for relational databases that contain null values. We also consider two desirable properties: monotonicity and semantics preservation. We prove that our mapping is monotone and also prove that no monotone mapping, including ours, is semantic preserving. We realize that monotonicity is an obstacle for semantic preservation and thus present a non-monotone direct mapping that is semantics preserving.", "keywords": ["data translation", "direct mapping", "rdb2rdf", "sql", "semantic web", "sparql", "relational databases", "rdf", "owl"], "combined": "On directly mapping relational databases to RDF and OWL Mapping relational databases to RDF is a fundamental problem for the development of the Semantic Web. We present a solution, inspired by draft methods defined by the W3C where relational databases are directly mapped to RDF and OWL. Given a relational database schema and its integrity constraints, this direct mapping produces an OWL ontology, which, provides the basis for generating RDF instances. The semantics of this mapping is defined using Datalog. Two fundamental properties are information preservation and query preservation. We prove that our mapping satisfies both conditions, even for relational databases that contain null values. We also consider two desirable properties: monotonicity and semantics preservation. We prove that our mapping is monotone and also prove that no monotone mapping, including ours, is semantic preserving. We realize that monotonicity is an obstacle for semantic preservation and thus present a non-monotone direct mapping that is semantics preserving. [[EENNDD]] data translation; direct mapping; rdb2rdf; sql; semantic web; sparql; relational databases; rdf; owl"}, "Pada pemetaan pangkalan data hubungan secara langsung ke RDF dan OWL Pemetaan pangkalan data hubungan ke RDF adalah masalah asas untuk pengembangan Web Semantik. Kami menyajikan penyelesaian, yang diilhami oleh kaedah draf yang ditentukan oleh W3C di mana pangkalan data hubungan dipetakan secara langsung ke RDF dan OWL. Memandangkan skema pangkalan data relasional dan kekangan integriti, pemetaan langsung ini menghasilkan ontologi OWL, yang memberikan asas untuk menghasilkan kejadian RDF. Semantik pemetaan ini ditentukan menggunakan Datalog. Dua sifat asas adalah pemeliharaan maklumat dan pemeliharaan pertanyaan. Kami membuktikan bahawa pemetaan kami memenuhi kedua-dua syarat, bahkan untuk pangkalan data hubungan yang mengandungi nilai nol. Kami juga mempertimbangkan dua sifat yang diinginkan: monotonik dan pemeliharaan semantik. Kami membuktikan bahawa pemetaan kami adalah monoton dan juga membuktikan bahawa tidak ada pemetaan monoton, termasuk pemetaan kita, yang semantik. Kami menyedari bahawa monotonik adalah halangan untuk pemeliharaan semantik dan dengan demikian menyajikan pemetaan langsung bukan monoton yang memelihara semantik. [[EENNDD]] terjemahan data; pemetaan langsung; rdb2rdf; sql; web semantik; sparql; pangkalan data hubungan; rdf; burung hantu"], [{"string": "Forcehttps: protecting high-security web sites from network attacks As wireless networks proliferate, web browsers operate in an increasingly hostile network environment. The HTTPS protocol has the potential to protect web users from network attackers, but real-world deployments must cope with misconfigured servers, causing imperfect web sites and users to compromise browsing sessions inadvertently. ForceHTTPS is a simple browser security mechanism that web sites or users can use to opt in to stricter error processing, improving the security of HTTPS by preventing network attacks that leverage the browser's lax error processing. By augmenting the browser with a database of custom URL rewrite rules, ForceHTTPS allows sophisticated users to transparently retrofit security onto some insecure sites that support HTTPS. We provide a prototype implementation of ForceHTTPS as a Firefox browser extension.", "keywords": ["pharming", "unauthorized access", "same-origin policy", "eavesdropping", "https"], "combined": "Forcehttps: protecting high-security web sites from network attacks As wireless networks proliferate, web browsers operate in an increasingly hostile network environment. The HTTPS protocol has the potential to protect web users from network attackers, but real-world deployments must cope with misconfigured servers, causing imperfect web sites and users to compromise browsing sessions inadvertently. ForceHTTPS is a simple browser security mechanism that web sites or users can use to opt in to stricter error processing, improving the security of HTTPS by preventing network attacks that leverage the browser's lax error processing. By augmenting the browser with a database of custom URL rewrite rules, ForceHTTPS allows sophisticated users to transparently retrofit security onto some insecure sites that support HTTPS. We provide a prototype implementation of ForceHTTPS as a Firefox browser extension. [[EENNDD]] pharming; unauthorized access; same-origin policy; eavesdropping; https"}, "Forcehttps: melindungi laman web keselamatan tinggi dari serangan rangkaian Ketika rangkaian tanpa wayar berkembang, penyemak imbas web beroperasi dalam persekitaran rangkaian yang semakin bermusuhan. Protokol HTTPS berpotensi melindungi pengguna web dari penyerang rangkaian, tetapi penyebaran di dunia nyata harus mengatasi pelayan yang salah dikonfigurasi, menyebabkan laman web dan pengguna yang tidak sempurna berkompromi dengan sesi melayari secara tidak sengaja. ForceHTTPS adalah mekanisme keselamatan penyemak imbas sederhana yang boleh digunakan oleh laman web atau pengguna untuk memilih pemprosesan ralat yang lebih ketat, meningkatkan keselamatan HTTPS dengan mencegah serangan rangkaian yang memanfaatkan pemprosesan ralat penyemak imbas penyemak imbas. Dengan menambah penyemak imbas dengan pangkalan data peraturan penulisan semula URL tersuai, ForceHTTPS membolehkan pengguna yang canggih mengubah keselamatan secara telus ke beberapa laman web yang tidak selamat yang menyokong HTTPS. Kami menyediakan prototaip pelaksanaan ForceHTTPS sebagai pelanjutan penyemak imbas Firefox. [[EENNDD]] farmasi; akses tidak dibenarkan; dasar asal sama; menguping; https"], [{"string": "General auction mechanism for search advertising In sponsored search, a number of advertising slots is available on a search results page, and have to be allocated among a set of advertisers competing to display an ad on the page. This gives rise to a bipartite matching market that is typically cleared by the way of an automated auction. Several auction mechanisms have been proposed, with variants of the Generalized Second Price (GSP) being widely used in practice. There is a rich body of work on bipartite matching markets that builds upon the stable marriage model of Gale and Shapley and the assignment model of Shapley and Shubik. This line of research offers deep insights into the structure of stable outcomes in such markets and their incentive properties. In this paper, we model advertising auctions in terms of an assignment model with linear utilities, extended with bidder and item specific maximum and minimum prices. Auction mechanisms like the commonly used GSP or the well-known Vickrey-Clarke-Groves (VCG) can be interpreted as simply computing a bidder-optimal stable matching in this model, for a suitably defined set of bidder preferences, but our model includes much richer bidders and preferences. We prove that in our model the existence of a stable matching is guaranteed, and under a non-degeneracy assumption a bidder-optimal stable matching exists as well. We give an algorithm to find such matching in polynomial time, and use it to design truthful mechanism that generalizes GSP, is truthful for profit-maximizing bidders, correctly implements features like bidder-specific minimum prices and position-specific bids, and works for rich mixtures of bidders and preferences. Our main technical contributions are the existence of bidder-optimal matchings and strategyproofness of the resulting mechanism, and are proved by induction on the progress of the matching algorithm.", "keywords": ["stable matchings", "sponsored search auctions", "game theory"], "combined": "General auction mechanism for search advertising In sponsored search, a number of advertising slots is available on a search results page, and have to be allocated among a set of advertisers competing to display an ad on the page. This gives rise to a bipartite matching market that is typically cleared by the way of an automated auction. Several auction mechanisms have been proposed, with variants of the Generalized Second Price (GSP) being widely used in practice. There is a rich body of work on bipartite matching markets that builds upon the stable marriage model of Gale and Shapley and the assignment model of Shapley and Shubik. This line of research offers deep insights into the structure of stable outcomes in such markets and their incentive properties. In this paper, we model advertising auctions in terms of an assignment model with linear utilities, extended with bidder and item specific maximum and minimum prices. Auction mechanisms like the commonly used GSP or the well-known Vickrey-Clarke-Groves (VCG) can be interpreted as simply computing a bidder-optimal stable matching in this model, for a suitably defined set of bidder preferences, but our model includes much richer bidders and preferences. We prove that in our model the existence of a stable matching is guaranteed, and under a non-degeneracy assumption a bidder-optimal stable matching exists as well. We give an algorithm to find such matching in polynomial time, and use it to design truthful mechanism that generalizes GSP, is truthful for profit-maximizing bidders, correctly implements features like bidder-specific minimum prices and position-specific bids, and works for rich mixtures of bidders and preferences. Our main technical contributions are the existence of bidder-optimal matchings and strategyproofness of the resulting mechanism, and are proved by induction on the progress of the matching algorithm. [[EENNDD]] stable matchings; sponsored search auctions; game theory"}, "Mekanisme lelongan umum untuk iklan carian Dalam carian yang ditaja, sejumlah slot iklan tersedia di halaman hasil pencarian, dan harus dialokasikan di antara sekumpulan pengiklan yang bersaing untuk menampilkan iklan di halaman tersebut. Ini menimbulkan pasaran sepadan bipartit yang biasanya dibersihkan dengan cara lelong automatik. Beberapa mekanisme lelong telah diusulkan, dengan varian Harga Kedua Umum (GSP) digunakan secara meluas dalam praktik. Terdapat banyak pekerjaan di pasar sepadan bipartit yang dibangun berdasarkan model perkahwinan Gale dan Shapley yang stabil dan model penugasan Shapley dan Shubik. Bidang penyelidikan ini memberikan gambaran mendalam mengenai struktur hasil yang stabil di pasaran tersebut dan sifat insentif mereka. Dalam makalah ini, kami memodelkan lelongan iklan dari segi model tugas dengan utiliti linier, diperluas dengan penawar dan harga maksimum dan minimum spesifik item. Mekanisme lelong seperti GSP yang biasa digunakan atau Vickrey-Clarke-Groves (VCG) yang terkenal dapat ditafsirkan sebagai hanya mengira padanan stabil yang optimum penawar dalam model ini, untuk sekumpulan pilihan penawar yang ditentukan dengan tepat, tetapi model kami merangkumi banyak pembida dan pilihan yang lebih kaya. Kami membuktikan bahawa dalam model kami keberadaan pemadanan stabil dijamin, dan dengan anggapan non-degenerasi, terdapat juga pencocokan stabil yang optimum bagi penawar. Kami memberikan algoritma untuk mencari padanan sedemikian dalam waktu polinomial, dan menggunakannya untuk merancang mekanisme jujur yang menggeneralisasi GSP, jujur untuk pembida yang memaksimumkan keuntungan, menerapkan dengan betul ciri seperti harga minimum khusus penawar dan tawaran khusus kedudukan, dan berfungsi untuk kaya campuran pembida dan pilihan. Sumbangan teknikal utama kami adalah adanya pencocokan optimum penawar dan ketahanan strategi mekanisme yang dihasilkan, dan dibuktikan dengan induksi pada kemajuan algoritma pemadanan. [[EENNDD]] padanan yang stabil; lelongan carian yang ditaja; teori permainan"], [{"string": "A trust management framework for service-oriented environments Many reputation management systems have been developed under the assumption that each entity in the system will use a variant of the same scoring function. Much of the previous work in reputation management has focused on providing robustness and improving performance for a given reputation scheme. In this paper, we present a reputation-based trust management framework that supports the synthesis of trust-related feedback from many different entities while also providing each entity with the flexibility to apply different scoring functions over the same feedback data for customized trust evaluations. We also propose a novel scheme to cache trust values based on recent client activity. To evaluate our approach, we implemented our trust management service and tested it on a realistic application scenario in both LAN and WAN distributed environments. Our results indicate that our trust management service can effectively support multiple scoring functions with low overhead and high availability.", "keywords": ["service-oriented architectures", "security and protection", "trust management", "reputation", "distributed systems"], "combined": "A trust management framework for service-oriented environments Many reputation management systems have been developed under the assumption that each entity in the system will use a variant of the same scoring function. Much of the previous work in reputation management has focused on providing robustness and improving performance for a given reputation scheme. In this paper, we present a reputation-based trust management framework that supports the synthesis of trust-related feedback from many different entities while also providing each entity with the flexibility to apply different scoring functions over the same feedback data for customized trust evaluations. We also propose a novel scheme to cache trust values based on recent client activity. To evaluate our approach, we implemented our trust management service and tested it on a realistic application scenario in both LAN and WAN distributed environments. Our results indicate that our trust management service can effectively support multiple scoring functions with low overhead and high availability. [[EENNDD]] service-oriented architectures; security and protection; trust management; reputation; distributed systems"}, "Kerangka pengurusan kepercayaan untuk persekitaran berorientasikan perkhidmatan. Banyak sistem pengurusan reputasi telah dikembangkan dengan anggapan bahawa setiap entiti dalam sistem akan menggunakan varian fungsi pemarkahan yang sama. Sebilangan besar pekerjaan sebelumnya dalam pengurusan reputasi telah berfokus pada penyediaan ketahanan dan peningkatan prestasi untuk skema reputasi tertentu. Dalam makalah ini, kami menyajikan kerangka kerja pengurusan kepercayaan berdasarkan reputasi yang menyokong sintesis maklum balas yang berkaitan dengan kepercayaan dari banyak entiti yang berlainan sementara juga memberikan setiap entiti dengan fleksibilitas untuk menerapkan fungsi pemarkahan yang berlainan atas data maklum balas yang sama untuk penilaian kepercayaan yang disesuaikan. Kami juga mencadangkan skema baru untuk menyimpan nilai kepercayaan berdasarkan aktiviti pelanggan baru-baru ini. Untuk menilai pendekatan kami, kami melaksanakan perkhidmatan pengurusan kepercayaan dan mengujinya pada senario aplikasi yang realistik di persekitaran diedarkan LAN dan WAN. Hasil kami menunjukkan bahawa perkhidmatan pengurusan kepercayaan kami dapat menyokong pelbagai fungsi pemarkahan secara efektif dengan overhead rendah dan ketersediaan tinggi. [[EENNDD]] seni bina berorientasikan perkhidmatan; keselamatan dan perlindungan; pengurusan kepercayaan; reputasi; sistem yang diedarkan"], [{"string": "Towards automating regression test selection for web services This paper reports a safe regression test selection (RTS) approach that is designed for verifying Web services in an end-to-end manner. The Safe RTS technique has been integrated into a systematic method that monitors distributed code modifications and automates the RTS and RT processes.", "keywords": ["automation", "web services", "testing tools", "regression test selection", "control-flow graphs"], "combined": "Towards automating regression test selection for web services This paper reports a safe regression test selection (RTS) approach that is designed for verifying Web services in an end-to-end manner. The Safe RTS technique has been integrated into a systematic method that monitors distributed code modifications and automates the RTS and RT processes. [[EENNDD]] automation; web services; testing tools; regression test selection; control-flow graphs"}, "Menuju pemilihan ujian regresi automatik untuk perkhidmatan web Makalah ini melaporkan pendekatan pemilihan ujian regresi selamat (RTS) yang dirancang untuk mengesahkan perkhidmatan Web secara end-to-end. Teknik Safe RTS telah disatukan ke dalam kaedah sistematik yang memantau pengubahsuaian kod yang diedarkan dan mengotomatisasi proses RTS dan RT. [[EENNDD]] automasi; perkhidmatan web; alat ujian; pemilihan ujian regresi; grafik aliran-kawalan"], [{"string": "Automatic search engine performance evaluation with click-through data analysis Performance evaluation is an important issue in Web search engine researches. Traditional evaluation methods rely on much human efforts and are therefore quite time-consuming. With click-through data analysis, we proposed an automatic search engine performance evaluation method. This method generates navigational type query topics and answers automatically based on search users. querying and clicking behavior. Experimental results based on a commercial Chinese search engine's user logs show that the automatically method gets a similar evaluation result with traditional assessor-based ones.", "keywords": ["click-through data analysis", "performance evaluation"], "combined": "Automatic search engine performance evaluation with click-through data analysis Performance evaluation is an important issue in Web search engine researches. Traditional evaluation methods rely on much human efforts and are therefore quite time-consuming. With click-through data analysis, we proposed an automatic search engine performance evaluation method. This method generates navigational type query topics and answers automatically based on search users. querying and clicking behavior. Experimental results based on a commercial Chinese search engine's user logs show that the automatically method gets a similar evaluation result with traditional assessor-based ones. [[EENNDD]] click-through data analysis; performance evaluation"}, "Penilaian prestasi mesin pencari automatik dengan analisis data klik-tayang Penilaian prestasi adalah masalah penting dalam penyelidikan mesin carian Web. Kaedah penilaian tradisional bergantung pada banyak usaha manusia dan oleh itu memakan masa. Dengan analisis data klik-tayang, kami mencadangkan kaedah penilaian prestasi mesin pencari automatik. Kaedah ini menghasilkan topik pertanyaan jawapan dan jawapan secara automatik berdasarkan pengguna carian. pertanyaan dan tingkah laku mengklik. Hasil eksperimen berdasarkan log pengguna mesin carian Cina komersial menunjukkan bahawa kaedah secara automatik mendapat hasil penilaian yang serupa dengan hasil penilaian tradisional berdasarkan penilaian. [[EENNDD]] analisis data klik-tayang; penilaian prestasi"], [{"string": "RuralCafe: web search in the rural developing world The majority of people in rural developing regions do not have access to the World Wide Web. Traditional network connectivity technologies have proven to be prohibitively expensive in these areas. The emergence of new long-range wireless technologies provide hope for connecting these rural regions to the Internet. However, the network connectivity provided by these new solutions are by nature intermittent due to high network usage rates, frequent power-cuts and the use of delay tolerant links. Typical applications, especially interactive applications like web search, do not tolerate intermittent connectivity. In this paper, we present the design and implementation of RuralCafe, a system intended to support efficient web search over intermittent networks. RuralCafe enables users to perform web search asynchronously and find what they are looking for in one round of intermittency as opposed to multiple rounds of search/downloads. RuralCafe does this by providing an expanded search query interface which allows a user to specify additional query terms to maximize the utility of the results returned by a search query. Given knowledge of the limited available network resources, RuralCafe performs optimizations to prefetch pages to best satisfy a search query based on a user's search preferences. In addition, RuralCafe does not require modifications to the web browser, and can provide single round search results tailored to various types of networks and economic constraints. We have implemented and evaluated the effectiveness of RuralCafe using queries from logs made to a large search engine, queries made by users in an intermittent setting, and live queries from a small testbed deployment. We have also deployed a prototype of RuralCafe in Kerala, India.", "keywords": ["web search", "information search and retrieval", "low bandwidth", "intermittent network", "world wide web"], "combined": "RuralCafe: web search in the rural developing world The majority of people in rural developing regions do not have access to the World Wide Web. Traditional network connectivity technologies have proven to be prohibitively expensive in these areas. The emergence of new long-range wireless technologies provide hope for connecting these rural regions to the Internet. However, the network connectivity provided by these new solutions are by nature intermittent due to high network usage rates, frequent power-cuts and the use of delay tolerant links. Typical applications, especially interactive applications like web search, do not tolerate intermittent connectivity. In this paper, we present the design and implementation of RuralCafe, a system intended to support efficient web search over intermittent networks. RuralCafe enables users to perform web search asynchronously and find what they are looking for in one round of intermittency as opposed to multiple rounds of search/downloads. RuralCafe does this by providing an expanded search query interface which allows a user to specify additional query terms to maximize the utility of the results returned by a search query. Given knowledge of the limited available network resources, RuralCafe performs optimizations to prefetch pages to best satisfy a search query based on a user's search preferences. In addition, RuralCafe does not require modifications to the web browser, and can provide single round search results tailored to various types of networks and economic constraints. We have implemented and evaluated the effectiveness of RuralCafe using queries from logs made to a large search engine, queries made by users in an intermittent setting, and live queries from a small testbed deployment. We have also deployed a prototype of RuralCafe in Kerala, India. [[EENNDD]] web search; information search and retrieval; low bandwidth; intermittent network; world wide web"}, "RuralCafe: carian web di dunia membangun luar bandar Majoriti orang di kawasan membangun luar bandar tidak mempunyai akses ke World Wide Web. Teknologi penyambungan rangkaian tradisional terbukti sangat mahal di kawasan ini. Kemunculan teknologi tanpa wayar jarak jauh baru memberikan harapan untuk menghubungkan kawasan luar bandar ini ke Internet. Walau bagaimanapun, penyambungan rangkaian yang disediakan oleh penyelesaian baru ini secara semula jadi berselang-seling kerana kadar penggunaan rangkaian yang tinggi, pemadaman kuasa yang kerap dan penggunaan pautan toleransi kelewatan. Aplikasi khas, terutamanya aplikasi interaktif seperti carian web, tidak bertolak ansur dengan sambungan yang berselang. Dalam makalah ini, kami menyajikan reka bentuk dan pelaksanaan RuralCafe, sebuah sistem yang bertujuan untuk menyokong pencarian web yang efisien melalui rangkaian yang berselang. RuralCafe membolehkan pengguna melakukan carian web secara tidak serentak dan mencari apa yang mereka cari dalam satu putaran sekejap berbanding dengan beberapa pusingan carian / muat turun. RuralCafe melakukan ini dengan menyediakan antara muka pertanyaan carian yang diperluas yang membolehkan pengguna menentukan istilah pertanyaan tambahan untuk memaksimumkan utiliti hasil yang dikembalikan oleh pertanyaan carian. Memandangkan pengetahuan mengenai sumber rangkaian yang tersedia terhad, RuralCafe melakukan pengoptimuman untuk membuat halaman awal untuk memuaskan permintaan carian berdasarkan pilihan carian pengguna. Sebagai tambahan, RuralCafe tidak memerlukan pengubahsuaian pada penyemak imbas web, dan dapat memberikan hasil carian satu putaran yang disesuaikan dengan pelbagai jenis rangkaian dan kekangan ekonomi. Kami telah melaksanakan dan menilai keberkesanan RuralCafe menggunakan pertanyaan dari log yang dibuat ke mesin pencari yang besar, pertanyaan yang dibuat oleh pengguna dalam keadaan sekejap-sekejap, dan pertanyaan langsung dari penyebaran ujian kecil. Kami juga telah menggunakan prototaip RuralCafe di Kerala, India. [[EENNDD]] carian web; carian dan pengambilan maklumat; lebar jalur rendah; rangkaian sekejap; web seluruh dunia"], [{"string": "Dynamics of bid optimization in online advertisement auctions We consider the problem of online keyword advertising auctions among multiple bidders with limited budgets, and study a natural bidding heuristic in which advertisers attempt to optimize their utility by equalizing their return-on-investment across all keywords. We show that existing auction mechanisms combined with this heuristic can experience cycling (as has been observed in many current systems), and therefore propose a modified class of mechanisms with small random perturbations. This perturbation is reminiscent of the small time-dependent perturbations employed in the dynamical systems literature to convert many types of chaos into attracting motions. We show that the perturbed mechanism provably converges in the case of first-price auctions and experimentally converges in the case of second-price auctions. Moreover, the point of convergence has a natural economic interpretation as the unique market equilibrium in the case of first-price mechanisms. In the case of second-price auctions, we conjecture that it converges to the \"supply-aware\" market equilibrium. Thus, our results can be alternatively described as a t\u00e2tonnement process for convergence to market equilibriumin which prices are adjusted on the side of the buyers rather than the sellers. We also observe that perturbation in mechanism design is useful in a broader context: In general, it can allow bidders to \"share\" a particular item, leading to stable allocations and pricing for the bidders, and improved revenue for the auctioneer.", "keywords": ["advertisement auctions", "sponsored search", "analysis of algorithms and problem complexity", "bidding agent", "equilibrium analysis"], "combined": "Dynamics of bid optimization in online advertisement auctions We consider the problem of online keyword advertising auctions among multiple bidders with limited budgets, and study a natural bidding heuristic in which advertisers attempt to optimize their utility by equalizing their return-on-investment across all keywords. We show that existing auction mechanisms combined with this heuristic can experience cycling (as has been observed in many current systems), and therefore propose a modified class of mechanisms with small random perturbations. This perturbation is reminiscent of the small time-dependent perturbations employed in the dynamical systems literature to convert many types of chaos into attracting motions. We show that the perturbed mechanism provably converges in the case of first-price auctions and experimentally converges in the case of second-price auctions. Moreover, the point of convergence has a natural economic interpretation as the unique market equilibrium in the case of first-price mechanisms. In the case of second-price auctions, we conjecture that it converges to the \"supply-aware\" market equilibrium. Thus, our results can be alternatively described as a t\u00e2tonnement process for convergence to market equilibriumin which prices are adjusted on the side of the buyers rather than the sellers. We also observe that perturbation in mechanism design is useful in a broader context: In general, it can allow bidders to \"share\" a particular item, leading to stable allocations and pricing for the bidders, and improved revenue for the auctioneer. [[EENNDD]] advertisement auctions; sponsored search; analysis of algorithms and problem complexity; bidding agent; equilibrium analysis"}, "Dinamika pengoptimuman bid dalam lelongan iklan dalam talian Kami menganggap masalah lelongan iklan kata kunci dalam talian di antara beberapa pembida dengan anggaran terhad, dan mengkaji heuristik pembidaan semula jadi di mana pengiklan berusaha mengoptimumkan utiliti mereka dengan menyamakan pulangan pelaburan mereka di semua kata kunci. Kami menunjukkan bahawa mekanisme lelong yang ada digabungkan dengan heuristik ini dapat mengalami berbasikal (seperti yang telah diperhatikan dalam banyak sistem semasa), dan oleh itu mencadangkan kelas mekanisme yang diubah suai dengan gangguan rawak kecil. Gangguan ini mengingatkan pada gangguan kecil yang bergantung pada masa yang digunakan dalam literatur sistem dinamik untuk mengubah banyak jenis kekacauan menjadi gerakan menarik. Kami menunjukkan bahawa mekanisme yang terganggu terbukti berkumpul dalam hal pelelangan harga pertama dan percubaan berlaku dalam kes lelongan harga kedua. Lebih-lebih lagi, titik penumpuan mempunyai penafsiran ekonomi semula jadi sebagai keseimbangan pasaran yang unik dalam hal mekanisme harga pertama. Sekiranya terdapat lelongan harga kedua, kami menduga bahawa ia akan berubah menjadi keseimbangan pasaran \"menyedari bekalan\". Oleh itu, hasil kami boleh digambarkan sebagai proses penyesuaian untuk penumpuan ke keseimbangan pasaran yang mana harga disesuaikan di pihak pembeli dan bukannya penjual. Kami juga memperhatikan bahawa gangguan dalam reka bentuk mekanisme berguna dalam konteks yang lebih luas: Secara umum, ini dapat memungkinkan pembida untuk \"berkongsi\" item tertentu, yang menyebabkan peruntukan dan harga yang stabil untuk penawar, dan peningkatan pendapatan bagi pelelong. [[EENNDD]] lelongan iklan; carian tajaan; analisis algoritma dan kerumitan masalah; ejen pembida; analisis keseimbangan"], [{"string": "Analysis of interacting BPEL web services No contact information provided yet.", "keywords": ["bpel", "synchronizability", "conversation", "software/program verification", "systems and information theory", "xpath", "web service", "asynchronous communication", "spin", "model checking"], "combined": "Analysis of interacting BPEL web services No contact information provided yet. [[EENNDD]] bpel; synchronizability; conversation; software/program verification; systems and information theory; xpath; web service; asynchronous communication; spin; model checking"}, "Analisis perkhidmatan web BPEL yang berinteraksi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] bpel; penyegerakan; perbualan; pengesahan perisian / program; sistem dan teori maklumat; xpath; perkhidmatan web; komunikasi tak segerak; putaran; pemeriksaan model"], [{"string": "Learning from the past: answering new questions with past answers Community-based Question Answering sites, such as Yahoo! Answers or Baidu Zhidao, allow users to get answers to complex, detailed and personal questions from other users. However, since answering a question depends on the ability and willingness of users to address the asker's needs, a significant fraction of the questions remain unanswered. We measured that in Yahoo! Answers, this fraction represents 15% of all incoming English questions. At the same time, we discovered that around 25% of questions in certain categories are recurrent, at least at the question-title level, over a period of one year.", "keywords": ["automatic question answering", "community-based question answering", "question-answering systems"], "combined": "Learning from the past: answering new questions with past answers Community-based Question Answering sites, such as Yahoo! Answers or Baidu Zhidao, allow users to get answers to complex, detailed and personal questions from other users. However, since answering a question depends on the ability and willingness of users to address the asker's needs, a significant fraction of the questions remain unanswered. We measured that in Yahoo! Answers, this fraction represents 15% of all incoming English questions. At the same time, we discovered that around 25% of questions in certain categories are recurrent, at least at the question-title level, over a period of one year. [[EENNDD]] automatic question answering; community-based question answering; question-answering systems"}, "Belajar dari masa lalu: menjawab soalan baru dengan jawapan masa lalu Laman menjawab soalan berasaskan Komuniti, seperti Yahoo! Jawapan atau Baidu Zhidao, membolehkan pengguna mendapatkan jawapan untuk soalan yang rumit, terperinci dan peribadi dari pengguna lain. Namun, kerana menjawab soalan bergantung pada kemampuan dan kesediaan pengguna untuk memenuhi keperluan penanya, sebilangan besar soalan tetap tidak terjawab. Kami mengukurnya di Yahoo! Jawapannya, pecahan ini mewakili 15% daripada semua soalan Bahasa Inggeris yang masuk. Pada masa yang sama, kami mendapati bahawa sekitar 25% soalan dalam kategori tertentu berulang, sekurang-kurangnya pada tingkat judul soalan, selama satu tahun. [[EENNDD]] menjawab soalan automatik; menjawab soalan berasaskan komuniti; sistem menjawab soalan"], [{"string": "Semantic analytics on social networks: experiences in addressing the problem of conflict of interest detection No contact information provided yet.", "keywords": ["ontologies", "entity disambiguation", "semantic associations", "semantic analytics", "data fusion", "miscellaneous", "social networks", "semantic web", "conflict of interest", "rdf", "peer review process"], "combined": "Semantic analytics on social networks: experiences in addressing the problem of conflict of interest detection No contact information provided yet. [[EENNDD]] ontologies; entity disambiguation; semantic associations; semantic analytics; data fusion; miscellaneous; social networks; semantic web; conflict of interest; rdf; peer review process"}, "Analisis semantik di rangkaian sosial: pengalaman dalam menangani masalah pengesanan konflik kepentingan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ontologi; disambiguasi entiti; persatuan semantik; analisis semantik; gabungan data; pelbagai; rangkaian sosial; web semantik; konflik kepentingan; rdf; proses tinjauan rakan sebaya"], [{"string": "A measurement-driven analysis of information propagation in the flickr social network Online social networking sites like MySpace, Facebook, and Flickr have become a popular way to share and disseminate content. Their massive popularity has led to viral marketing techniques that attempt to spread content, products, and ideas on these sites. However, there is little data publicly available on viral propagation in the real world and few studies have characterized how information spreads over current online social networks.", "keywords": ["cascades", "viral marketing", "social networks", "information dissemination", "flickr"], "combined": "A measurement-driven analysis of information propagation in the flickr social network Online social networking sites like MySpace, Facebook, and Flickr have become a popular way to share and disseminate content. Their massive popularity has led to viral marketing techniques that attempt to spread content, products, and ideas on these sites. However, there is little data publicly available on viral propagation in the real world and few studies have characterized how information spreads over current online social networks. [[EENNDD]] cascades; viral marketing; social networks; information dissemination; flickr"}, "Analisis berdasarkan pengukuran penyebaran maklumat di rangkaian sosial flickr Laman rangkaian sosial dalam talian seperti MySpace, Facebook, dan Flickr telah menjadi kaedah yang popular untuk berkongsi dan menyebarkan kandungan. Populariti mereka yang besar telah menyebabkan teknik pemasaran viral yang berusaha menyebarkan kandungan, produk, dan idea di laman web ini. Walau bagaimanapun, terdapat sedikit data yang tersedia untuk umum mengenai penyebaran virus di dunia nyata dan beberapa kajian menunjukkan bagaimana maklumat tersebar di rangkaian sosial dalam talian semasa. [[EENNDD]] lata; pemasaran viral; rangkaian sosial; penyebaran maklumat; flickr"], [{"string": "Towards context-aware adaptable web services No contact information provided yet.", "keywords": ["information services", "context", "extensible framework", "web services", "automatic context processing", "service platform", "extensibility"], "combined": "Towards context-aware adaptable web services No contact information provided yet. [[EENNDD]] information services; context; extensible framework; web services; automatic context processing; service platform; extensibility"}, "Ke arah perkhidmatan web yang disesuaikan dengan konteks Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] perkhidmatan maklumat; konteks; rangka kerja yang boleh diperluas; perkhidmatan web; pemprosesan konteks automatik; platform perkhidmatan; kepanjangan"], [{"string": "OREL: an ontology-based rights expression language No contact information provided yet.", "keywords": ["rights expression language", "owl", "orel", "xrml"], "combined": "OREL: an ontology-based rights expression language No contact information provided yet. [[EENNDD]] rights expression language; owl; orel; xrml"}, "OREL: bahasa ekspresi hak berasaskan ontologi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] bahasa ekspresi hak; burung hantu; orel; xrml"], [{"string": "Online mining of frequent query trees over XML data streams No contact information provided yet.", "keywords": ["frequent query trees", "data streams", "xml", "online mining", "web mining"], "combined": "Online mining of frequent query trees over XML data streams No contact information provided yet. [[EENNDD]] frequent query trees; data streams; xml; online mining; web mining"}, "Perlombongan dalam talian pokok pertanyaan yang kerap melalui aliran data XML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pokok pertanyaan yang kerap; aliran data; xml; perlombongan dalam talian; perlombongan web"], [{"string": "Predictive ranking: a novel page ranking approach by estimating the web structure No contact information provided yet.", "keywords": ["predictive ranking", "pagerank", "information search and retrieval", "link analysis"], "combined": "Predictive ranking: a novel page ranking approach by estimating the web structure No contact information provided yet. [[EENNDD]] predictive ranking; pagerank; information search and retrieval; link analysis"}, "Peringkat ramalan: pendekatan peringkat halaman baru dengan menganggarkan struktur web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kedudukan ramalan; pagerank; pencarian dan pengambilan maklumat; analisis pautan"], [{"string": "A content-driven reputation system for the wikipedia We present a content-driven reputation system for Wikipedia authors. In our system, authors gain reputation when the edits they perform to Wikipedia articles are preserved by subsequent authors, and they lose reputation when their edits are rolled back or undone in short order. Thus, author reputation is computed solely on the basis of content evolution; user-to-user comments or ratings are not used. The author reputation we compute could be used to flag new contributions from low-reputation authors, or it could be used to allow only authors with high reputation to contribute to controversialor critical pages. A reputation system for the Wikipedia could also provide an incentive for high-quality contributions. We have implemented the proposed system, and we have used it to analyze the entire Italian and French Wikipedias, consisting of a total of 691, 551 pages and 5, 587, 523 revisions. Our results show that our notion of reputation has good predictive value: changes performed by low-reputation authors have a significantly larger than average probability of having poor quality, as judged by human observers, and of being later undone, as measured by our algorithms.", "keywords": ["user-generated content", "reputation", "wikipedia"], "combined": "A content-driven reputation system for the wikipedia We present a content-driven reputation system for Wikipedia authors. In our system, authors gain reputation when the edits they perform to Wikipedia articles are preserved by subsequent authors, and they lose reputation when their edits are rolled back or undone in short order. Thus, author reputation is computed solely on the basis of content evolution; user-to-user comments or ratings are not used. The author reputation we compute could be used to flag new contributions from low-reputation authors, or it could be used to allow only authors with high reputation to contribute to controversialor critical pages. A reputation system for the Wikipedia could also provide an incentive for high-quality contributions. We have implemented the proposed system, and we have used it to analyze the entire Italian and French Wikipedias, consisting of a total of 691, 551 pages and 5, 587, 523 revisions. Our results show that our notion of reputation has good predictive value: changes performed by low-reputation authors have a significantly larger than average probability of having poor quality, as judged by human observers, and of being later undone, as measured by our algorithms. [[EENNDD]] user-generated content; reputation; wikipedia"}, "Sistem reputasi berdasarkan kandungan untuk wikipedia Kami menyajikan sistem reputasi berdasarkan kandungan untuk penulis Wikipedia. Dalam sistem kami, pengarang mendapat reputasi apabila suntingan yang mereka lakukan ke artikel Wikipedia disimpan oleh pengarang berikutnya, dan mereka kehilangan reputasi apabila pengeditannya dilancarkan kembali atau dibuat dalam jangka pendek. Oleh itu, reputasi pengarang dihitung berdasarkan evolusi kandungan; komen atau penilaian pengguna-ke-pengguna tidak digunakan. Reputasi pengarang yang kami hitung dapat digunakan untuk menandakan sumbangan baru dari pengarang yang memiliki reputasi rendah, atau dapat digunakan untuk membenarkan hanya pengarang yang mempunyai reputasi tinggi untuk menyumbang pada halaman kritikal kontroversial. Sistem reputasi untuk Wikipedia juga dapat memberikan insentif untuk sumbangan berkualiti tinggi. Kami telah melaksanakan sistem yang dicadangkan, dan kami menggunakannya untuk menganalisis keseluruhan Wikipedia Itali dan Perancis, yang terdiri daripada sejumlah 691, 551 halaman dan 5, 587, 523 semakan. Hasil kajian kami menunjukkan bahawa tanggapan reputasi kami mempunyai nilai ramalan yang baik: perubahan yang dilakukan oleh pengarang reputasi rendah mempunyai kebarangkalian yang jauh lebih besar daripada rata-rata kemungkinan mempunyai kualiti yang buruk, seperti yang dinilai oleh pemerhati manusia, dan kemudian dibatalkan, seperti yang diukur oleh algoritma kami. [[EENNDD]] kandungan yang dihasilkan pengguna; reputasi; wikipedia"], [{"string": "Mining anchor text for query refinement No contact information provided yet.", "keywords": ["web search", "query refinement", "rank", "anchor text"], "combined": "Mining anchor text for query refinement No contact information provided yet. [[EENNDD]] web search; query refinement; rank; anchor text"}, "Perlombongan teks penambangan untuk penyempurnaan pertanyaan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] carian web; penyempurnaan pertanyaan; pangkat; teks sauh"], [{"string": "Probabilistic query expansion using query logs No contact information provided yet.", "keywords": ["search engine", "query expansion", "information retrieval", "probabilistic model", "information search and retrieval", "log mining"], "combined": "Probabilistic query expansion using query logs No contact information provided yet. [[EENNDD]] search engine; query expansion; information retrieval; probabilistic model; information search and retrieval; log mining"}, "Perluasan pertanyaan probabilistik menggunakan log pertanyaan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] enjin carian; pengembangan pertanyaan; pengambilan maklumat; model kebarangkalian; pencarian dan pengambilan maklumat; perlombongan balak"], [{"string": "Debugging OWL ontologies No contact information provided yet.", "keywords": ["explanation", "semantic web", "owl", "testing and debugging", "ontology engineering"], "combined": "Debugging OWL ontologies No contact information provided yet. [[EENNDD]] explanation; semantic web; owl; testing and debugging; ontology engineering"}, "Menyahpastikan ontologi OWL Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penjelasan; web semantik; burung hantu; ujian dan penyahpepijatan; kejuruteraan ontologi"], [{"string": "Value-based web caching No contact information provided yet.", "keywords": ["duplicate suppression", "proxy", "privacy", "applications", "hypertext transfer protocol", "http", "resource modification", "caching", "scalability", "redundant transfers", "www", "dynamic content", "aliasing", "world wide web"], "combined": "Value-based web caching No contact information provided yet. [[EENNDD]] duplicate suppression; proxy; privacy; applications; hypertext transfer protocol; http; resource modification; caching; scalability; redundant transfers; www; dynamic content; aliasing; world wide web"}, "Caching web berasaskan nilai Belum ada maklumat hubungan yang diberikan. [[EENNDD]] penindasan pendua; proksi; privasi; permohonan; Protokol Pemindahan Hiperteks; http; pengubahsuaian sumber; caching; skalabiliti; pemindahan berlebihan; www; kandungan dinamik; mengasingkan; web seluruh dunia"], [{"string": "Automatically refining the wikipedia infobox ontology The combined efforts of human volunteers have recently extracted numerous facts from Wikipedia, storing them as machine-harvestable object-attribute-value triples in Wikipedia infoboxes. Machine learning systems, such as Kylin, use these infoboxes as training data, accurately extracting even more semantic knowledge from natural language text. But in order to realize the full power of this information, it must be situated in a cleanly-structured ontology. This paper introduces KOG, an autonomous system for refining Wikipedia's infobox-class ontology towards this end. We cast the problem of ontology refinement as a machine learning problem and solve it using both SVMs and a more powerful joint-inference approach expressed in Markov Logic Networks. We present experiments demonstrating the superiority of the joint-inference approach and evaluating other aspects of our system. Using these techniques, we build a rich ontology, integrating Wikipedia's infobox-class schemata with WordNet. We demonstrate how the resulting ontology may be used to enhance Wikipedia with improved query processing and other features.", "keywords": ["content analysis and indexing", "ontology", "markov logic networks", "wikipedia", "semantic web"], "combined": "Automatically refining the wikipedia infobox ontology The combined efforts of human volunteers have recently extracted numerous facts from Wikipedia, storing them as machine-harvestable object-attribute-value triples in Wikipedia infoboxes. Machine learning systems, such as Kylin, use these infoboxes as training data, accurately extracting even more semantic knowledge from natural language text. But in order to realize the full power of this information, it must be situated in a cleanly-structured ontology. This paper introduces KOG, an autonomous system for refining Wikipedia's infobox-class ontology towards this end. We cast the problem of ontology refinement as a machine learning problem and solve it using both SVMs and a more powerful joint-inference approach expressed in Markov Logic Networks. We present experiments demonstrating the superiority of the joint-inference approach and evaluating other aspects of our system. Using these techniques, we build a rich ontology, integrating Wikipedia's infobox-class schemata with WordNet. We demonstrate how the resulting ontology may be used to enhance Wikipedia with improved query processing and other features. [[EENNDD]] content analysis and indexing; ontology; markov logic networks; wikipedia; semantic web"}, "Menyempurnakan secara ontologi wobipedia infobox secara automatik Gabungan usaha sukarelawan manusia baru-baru ini mengekstrak banyak fakta dari Wikipedia, menyimpannya sebagai tiga kali ganda objek-atribut-nilai yang dapat diusahakan dalam infobox Wikipedia. Sistem pembelajaran mesin, seperti Kylin, menggunakan infobox ini sebagai data latihan, dengan tepat mengambil pengetahuan semantik dari teks bahasa semula jadi. Tetapi untuk merealisasikan kekuatan penuh maklumat ini, maklumat tersebut mesti diletakkan dalam ontologi yang tersusun dengan bersih. Makalah ini memperkenalkan KOG, sistem autonomi untuk menyempurnakan ontologi kelas infobox Wikipedia ke arah tujuan ini. Kami meletakkan masalah penyempurnaan ontologi sebagai masalah pembelajaran mesin dan menyelesaikannya menggunakan kedua SVM dan pendekatan inferens bersama yang lebih kuat yang dinyatakan dalam Markov Logic Networks. Kami mengemukakan eksperimen yang menunjukkan keunggulan pendekatan inferensi bersama dan menilai aspek lain dari sistem kami. Dengan menggunakan teknik ini, kami membina ontologi yang kaya, mengintegrasikan skema kelas infobox Wikipedia dengan WordNet. Kami menunjukkan bagaimana ontologi yang dihasilkan dapat digunakan untuk meningkatkan Wikipedia dengan pemprosesan pertanyaan yang lebih baik dan ciri-ciri lain. [[EENNDD]] analisis kandungan dan pengindeksan; ontologi; rangkaian logik markov; wikipedia; web semantik"], [{"string": "Actions speak as loud as words: predicting relationships from social behavior data In recent years, new studies concentrating on analyzing user personality and finding credible content in social media have become quite popular. Most such work augments features from textual content with features representing the user's social ties and the tie strength. Social ties are crucial in understanding the network the people are a part of. However, textual content is extremely useful in understanding topics discussed and the personality of the individual. We bring a new dimension to this type of analysis with methods to compute the type of ties individuals have and the strength of the ties in each dimension. We present a new genre of behavioral features that are able to capture the \"function\" of a specific relationship without the help of textual features. Our novel features are based on the statistical properties of communication patterns between individuals such as reciprocity, assortativity, attention and latency. We introduce a new methodology for determining how such features can be compared to textual features, and show, using Twitter data, that our features can be used to capture contextual information present in textual features very accurately. Conversely, we also demonstrate how textual features can be used to determine social attributes related to an individual.", "keywords": ["social networks", "social ties", "social signals", "behavior analysis"], "combined": "Actions speak as loud as words: predicting relationships from social behavior data In recent years, new studies concentrating on analyzing user personality and finding credible content in social media have become quite popular. Most such work augments features from textual content with features representing the user's social ties and the tie strength. Social ties are crucial in understanding the network the people are a part of. However, textual content is extremely useful in understanding topics discussed and the personality of the individual. We bring a new dimension to this type of analysis with methods to compute the type of ties individuals have and the strength of the ties in each dimension. We present a new genre of behavioral features that are able to capture the \"function\" of a specific relationship without the help of textual features. Our novel features are based on the statistical properties of communication patterns between individuals such as reciprocity, assortativity, attention and latency. We introduce a new methodology for determining how such features can be compared to textual features, and show, using Twitter data, that our features can be used to capture contextual information present in textual features very accurately. Conversely, we also demonstrate how textual features can be used to determine social attributes related to an individual. [[EENNDD]] social networks; social ties; social signals; behavior analysis"}, "Tindakan bercakap sekuat kata-kata: meramalkan hubungan dari data tingkah laku sosial Dalam beberapa tahun kebelakangan ini, kajian baru yang menumpukan perhatian untuk menganalisis keperibadian pengguna dan mencari kandungan yang boleh dipercayai di media sosial menjadi sangat popular. Sebilangan besar karya seperti itu menambah ciri dari kandungan teks dengan ciri-ciri yang mewakili hubungan sosial pengguna dan kekuatan ikatan. Hubungan sosial sangat penting dalam memahami jaringan yang menjadi sebahagian daripada orang itu. Walau bagaimanapun, kandungan teks sangat berguna dalam memahami topik yang dibincangkan dan keperibadian individu. Kami membawa dimensi baru untuk jenis analisis ini dengan kaedah untuk menghitung jenis hubungan yang dimiliki individu dan kekuatan ikatan dalam setiap dimensi. Kami menyajikan genre ciri tingkah laku baru yang mampu menangkap \"fungsi\" hubungan tertentu tanpa bantuan ciri teks. Ciri-ciri novel kami didasarkan pada sifat statistik corak komunikasi antara individu seperti timbal balik, kepelbagaian, perhatian dan kependaman. Kami memperkenalkan metodologi baru untuk menentukan bagaimana ciri-ciri tersebut dapat dibandingkan dengan fitur teks, dan menunjukkan, dengan menggunakan data Twitter, bahawa fitur kami dapat digunakan untuk menangkap maklumat kontekstual yang terdapat dalam fitur teks dengan sangat tepat. Sebaliknya, kami juga menunjukkan bagaimana ciri teks dapat digunakan untuk menentukan atribut sosial yang berkaitan dengan seseorang. [[EENNDD]] rangkaian sosial; hubungan sosial; isyarat sosial; analisis tingkah laku"], [{"string": "Alhambra: a system for creating, enforcing, and testing browser security policies Alhambra is a browser-based system designed to enforce and test web browser security policies. At the core of Alhambra is a policy-enhanced browser supporting fine-grain security policies that restrict web page contents and execution. Alhambra requires no server-side modifications or additions to the web application. Policies can restrict the construction of the document as well as the execution of JavaScript using access control rules and a taint-tracking engine. Using the Alhambra browser, we present two security policies that we have built using our architecture, both designed to prevent cross-site scripting. The first policy uses a taint-tracking engine to prevent cross-site scripting attacks that exploit bugs in the client-side of the web applications. The second one uses browsing history to create policies that restrict the contents of documents and prevent the inclusion of malicious content.", "keywords": ["security and protection", "cross-site scripting", "web security", "web browser"], "combined": "Alhambra: a system for creating, enforcing, and testing browser security policies Alhambra is a browser-based system designed to enforce and test web browser security policies. At the core of Alhambra is a policy-enhanced browser supporting fine-grain security policies that restrict web page contents and execution. Alhambra requires no server-side modifications or additions to the web application. Policies can restrict the construction of the document as well as the execution of JavaScript using access control rules and a taint-tracking engine. Using the Alhambra browser, we present two security policies that we have built using our architecture, both designed to prevent cross-site scripting. The first policy uses a taint-tracking engine to prevent cross-site scripting attacks that exploit bugs in the client-side of the web applications. The second one uses browsing history to create policies that restrict the contents of documents and prevent the inclusion of malicious content. [[EENNDD]] security and protection; cross-site scripting; web security; web browser"}, "Alhambra: sistem untuk membuat, menegakkan, dan menguji dasar keselamatan penyemak imbas Alhambra adalah sistem berasaskan penyemak imbas yang dirancang untuk menguatkuasakan dan menguji dasar keselamatan penyemak imbas web. Inti dari Alhambra adalah penyemak imbas yang disempurnakan oleh polisi yang menyokong dasar keselamatan yang membatasi kandungan dan pelaksanaan halaman web. Alhambra tidak memerlukan pengubahsuaian atau penambahan dari pelayan ke aplikasi web. Polisi boleh menyekat pembinaan dokumen serta pelaksanaan JavaScript menggunakan peraturan kawalan akses dan mesin pengesan taint. Dengan menggunakan penyemak imbas Alhambra, kami menyajikan dua dasar keselamatan yang kami bina menggunakan seni bina kami, yang kedua-duanya dirancang untuk mencegah skrip lintas-laman web. Dasar pertama menggunakan enjin pengesanan taint untuk mencegah serangan skrip lintas-laman web yang mengeksploitasi bug di sisi klien aplikasi web. Yang kedua menggunakan sejarah penyemakan imbas untuk membuat dasar yang menyekat kandungan dokumen dan mencegah kemasukan kandungan berniat jahat. [[EENNDD]] keselamatan dan perlindungan; skrip merentas laman web; keselamatan web; pelayar web"], [{"string": "Personalized interactive faceted search Faceted search is becoming a popular method to allow users to interactively search and navigate complex information spaces. A faceted search system presents users with key-value metadata that is used for query refinement. While popular in e-commerce and digital libraries, not much research has been conducted on which metadata to present to a user in order to improve the search experience. Nor are there repeatable benchmarks for evaluating a faceted search engine. This paper proposes the use of collaborative filtering and personalization to customize the search interface to each user's behavior. This paper also proposes a utility based framework to evaluate the faceted interface. In order to demonstrate these ideas and better understand personalized faceted search, several faceted search algorithms are proposed and evaluated using the novel evaluation methodology.", "keywords": ["faceted search", "collaborative recommendation", "personalization", "user modeling", "interactive search", "evaluation"], "combined": "Personalized interactive faceted search Faceted search is becoming a popular method to allow users to interactively search and navigate complex information spaces. A faceted search system presents users with key-value metadata that is used for query refinement. While popular in e-commerce and digital libraries, not much research has been conducted on which metadata to present to a user in order to improve the search experience. Nor are there repeatable benchmarks for evaluating a faceted search engine. This paper proposes the use of collaborative filtering and personalization to customize the search interface to each user's behavior. This paper also proposes a utility based framework to evaluate the faceted interface. In order to demonstrate these ideas and better understand personalized faceted search, several faceted search algorithms are proposed and evaluated using the novel evaluation methodology. [[EENNDD]] faceted search; collaborative recommendation; personalization; user modeling; interactive search; evaluation"}, "Pencarian pelbagai aspek interaktif yang diperibadikan Carian faset menjadi kaedah yang popular untuk membolehkan pengguna mencari dan menavigasi ruang maklumat yang kompleks. Sistem carian faset menghadirkan pengguna dengan metadata nilai-kunci yang digunakan untuk penyempurnaan pertanyaan. Walaupun popular di e-commerce dan perpustakaan digital, tidak banyak kajian dilakukan mengenai metadata mana yang harus disampaikan kepada pengguna untuk meningkatkan pengalaman pencarian. Tidak ada penanda aras berulang untuk menilai enjin carian yang berwajah. Makalah ini mencadangkan penggunaan penapisan kolaboratif dan pemperibadian untuk menyesuaikan antara muka carian dengan tingkah laku setiap pengguna. Makalah ini juga mencadangkan kerangka kerja berasaskan utiliti untuk menilai antara muka. Untuk menunjukkan idea-idea ini dan lebih memahami pencarian aspek yang diperibadikan, beberapa algoritma carian pelbagai dicadangkan dan dinilai menggunakan metodologi penilaian novel. [[EENNDD]] carian segi; cadangan kolaboratif; pemperibadian; pemodelan pengguna; carian interaktif; penilaian"], [{"string": "Dynamic maintenance of web indexes using landmarks No contact information provided yet.", "keywords": ["inverted files", "information search and retrieval", "miscellaneous", "update processing"], "combined": "Dynamic maintenance of web indexes using landmarks No contact information provided yet. [[EENNDD]] inverted files; information search and retrieval; miscellaneous; update processing"}, "Penyelenggaraan indeks web yang dinamik menggunakan mercu tanda Belum ada maklumat hubungan yang diberikan. [[EENNDD]] fail terbalik; pencarian dan pengambilan maklumat; pelbagai; pemprosesan kemas kini"], [{"string": "Efficient search engine measurements We address the problem of measuring global quality met-rics of search engines, like corpus size, index freshness, anddensity of duplicates in the corpus. The recently proposedestimators for such metrics [2, 6] suffer from significant biasand/or poor performance, due to inaccurate approximationof the so called .document degrees..We present two new estimators that are able to overcomethe bias introduced by approximate degrees. Our estimatorsare based on a careful implementation of an approximateimportance sampling procedure. Comprehensive theoreti-cal and empirical analysis of the estimators demonstratesthat they have essentially no bias even in situations wheredocument degrees are poorly approximated.Building on an idea from [6], we discuss Rao Blackwelliza-tion as a generic method for reducing variance in searchengine estimators. We show that Rao-Blackwellizing ourestimators results in significant performance improvements,while not compromising accuracy.", "keywords": ["search engines", "information search and retrieval", "evaluation", "corpus size estimation"], "combined": "Efficient search engine measurements We address the problem of measuring global quality met-rics of search engines, like corpus size, index freshness, anddensity of duplicates in the corpus. The recently proposedestimators for such metrics [2, 6] suffer from significant biasand/or poor performance, due to inaccurate approximationof the so called .document degrees..We present two new estimators that are able to overcomethe bias introduced by approximate degrees. Our estimatorsare based on a careful implementation of an approximateimportance sampling procedure. Comprehensive theoreti-cal and empirical analysis of the estimators demonstratesthat they have essentially no bias even in situations wheredocument degrees are poorly approximated.Building on an idea from [6], we discuss Rao Blackwelliza-tion as a generic method for reducing variance in searchengine estimators. We show that Rao-Blackwellizing ourestimators results in significant performance improvements,while not compromising accuracy. [[EENNDD]] search engines; information search and retrieval; evaluation; corpus size estimation"}, "Pengukuran mesin pencari yang cekap Kami menangani masalah pengukuran metrik kualiti global mesin pencari, seperti ukuran korpus, kesegaran indeks, dan kepadatan pendua di korpus. Estimator yang baru-baru ini dicadangkan untuk metrik tersebut [2, 6] menderita prestasi berat sebelah yang signifikan / atau buruk, kerana penghampiran yang tidak tepat dari apa yang disebut .dokumen darjah .. Kami menghadirkan dua penganggar baru yang dapat mengatasi bias yang diperkenalkan dengan darjah anggaran. Penganggar kami berdasarkan pada pelaksanaan prosedur persampelan kepentingan yang tepat. Analisis teori dan empirikal komprehensif penganggar menunjukkan bahawa mereka pada dasarnya tidak mempunyai berat sebelah walaupun dalam keadaan derajat dokumen kurang didekati. Berdasarkan idea dari [6], kita membincangkan Rao Blackwelliza-tion sebagai kaedah generik untuk mengurangkan varians dalam penganggar mesin pencari . Kami menunjukkan bahawa Rao-Blackwellizing ourestimators menghasilkan peningkatan prestasi yang ketara, dan tidak menjejaskan ketepatan. [[EENNDD]] enjin carian; carian dan pengambilan maklumat; penilaian; anggaran ukuran korpus"], [{"string": "Inferring private information using social network data On-line social networks, such as Facebook, are increasingly utilized by many users. These networks allow people to publish details about themselves and connect to their friends. Some of the information revealed inside these networks is private and it is possible that corporations could use learning algorithms on the released data to predict undisclosed private information. In this paper, we explore how to launch inference attacks using released social networking data to predict undisclosed private information about individuals. We then explore the effectiveness of possible sanitization techniques that can be used to combat such inference attacks under different scenarios.", "keywords": ["privacy", "inference", "learning", "social networks", "models"], "combined": "Inferring private information using social network data On-line social networks, such as Facebook, are increasingly utilized by many users. These networks allow people to publish details about themselves and connect to their friends. Some of the information revealed inside these networks is private and it is possible that corporations could use learning algorithms on the released data to predict undisclosed private information. In this paper, we explore how to launch inference attacks using released social networking data to predict undisclosed private information about individuals. We then explore the effectiveness of possible sanitization techniques that can be used to combat such inference attacks under different scenarios. [[EENNDD]] privacy; inference; learning; social networks; models"}, "Menyimpulkan maklumat peribadi menggunakan data rangkaian sosial Rangkaian sosial dalam talian, seperti Facebook, semakin banyak digunakan oleh banyak pengguna. Rangkaian ini membolehkan orang menerbitkan maklumat mengenai diri mereka dan berhubung dengan rakan mereka. Sebilangan maklumat yang terungkap di dalam jaringan ini bersifat pribadi dan ada kemungkinan syarikat dapat menggunakan algoritma pembelajaran pada data yang dikeluarkan untuk meramalkan maklumat peribadi yang tidak diungkapkan. Dalam makalah ini, kami meneroka cara melancarkan serangan inferensi menggunakan data rangkaian sosial yang dilepaskan untuk meramalkan maklumat peribadi yang tidak didedahkan mengenai individu. Kami kemudian meneroka keberkesanan teknik pembersihan yang mungkin dapat digunakan untuk memerangi serangan inferensi seperti di bawah senario yang berbeza. [[EENNDD]] privasi; kesimpulan; belajar; rangkaian sosial; model"], [{"string": "XQuery in the browser Since the invention of the Web, the browser has become more and more powerful. By now, it is a programming and execution environment in itself. The predominant language to program applications in the browser today is JavaScript. With browsers becoming more powerful, JavaScript has been extended and new layers have been added (e.g., DOM-Support and XPath). Today, JavaScript is very successful and applications and GUI features implemented in the browser have become increasingly complex. The purpose of this paper is to improve the programmability of Web browsers by enabling the execution of XQuery programs in the browser. Although it has the potential to ideally replace JavaScript, it is possible to run it in addition to JavaScript for more flexibility. Furthermore, it allows instant code migration from the server to the client and vice-versa. This enables a significant simplification of the technology stack. The intuition is that programming the browser involves mostly XML (i.e., DOM) navigation and manipulation, and the XQuery family of W3C standards were designed exactly for that purpose. The paper proposes extensions to XQuery for Web browsers and gives a number of examples that demonstrate the usefulness of XQuery for the development of AJAX-style applications. Furthermore, the paper presents the design of an XQuery plug-in for Microsoft's Internet Explorer. The paper also gives examples of applications which were developed with the help of this plug-in.", "keywords": ["scripting", "events", "general", "xquery", "script", "dom", "html", "css", "javascript", "client-side programming", "xml", "xhtml", "browser", "language classifications", "stylesheets", "mash-up"], "combined": "XQuery in the browser Since the invention of the Web, the browser has become more and more powerful. By now, it is a programming and execution environment in itself. The predominant language to program applications in the browser today is JavaScript. With browsers becoming more powerful, JavaScript has been extended and new layers have been added (e.g., DOM-Support and XPath). Today, JavaScript is very successful and applications and GUI features implemented in the browser have become increasingly complex. The purpose of this paper is to improve the programmability of Web browsers by enabling the execution of XQuery programs in the browser. Although it has the potential to ideally replace JavaScript, it is possible to run it in addition to JavaScript for more flexibility. Furthermore, it allows instant code migration from the server to the client and vice-versa. This enables a significant simplification of the technology stack. The intuition is that programming the browser involves mostly XML (i.e., DOM) navigation and manipulation, and the XQuery family of W3C standards were designed exactly for that purpose. The paper proposes extensions to XQuery for Web browsers and gives a number of examples that demonstrate the usefulness of XQuery for the development of AJAX-style applications. Furthermore, the paper presents the design of an XQuery plug-in for Microsoft's Internet Explorer. The paper also gives examples of applications which were developed with the help of this plug-in. [[EENNDD]] scripting; events; general; xquery; script; dom; html; css; javascript; client-side programming; xml; xhtml; browser; language classifications; stylesheets; mash-up"}, "XQuery dalam penyemak imbas Sejak penemuan Web, penyemak imbas menjadi semakin kuat. Sekarang, ini adalah lingkungan pengaturcaraan dan pelaksanaannya sendiri. Bahasa utama untuk memprogram aplikasi dalam penyemak imbas hari ini adalah JavaScript. Dengan penyemak imbas menjadi lebih kuat, JavaScript telah diperluas dan lapisan baru telah ditambahkan (mis., DOM-Support dan XPath). Hari ini, JavaScript sangat berjaya dan aplikasi dan ciri GUI yang dilaksanakan di penyemak imbas menjadi semakin rumit. Tujuan makalah ini adalah untuk meningkatkan pengaturcaraan penyemak imbas Web dengan memungkinkan pelaksanaan program XQuery di penyemak imbas. Walaupun berpotensi mengganti JavaScript dengan ideal, mungkin untuk menjalankannya selain JavaScript untuk lebih fleksibel. Selain itu, ia membolehkan pemindahan kod segera dari pelayan ke pelanggan dan sebaliknya. Ini memungkinkan penyederhanaan yang signifikan dari timbunan teknologi. Intuisi adalah bahawa pengaturcaraan penyemak imbas melibatkan kebanyakan navigasi dan manipulasi XML (iaitu, DOM), dan keluarga XQuery standard W3C dirancang tepat untuk tujuan itu. Makalah ini mencadangkan peluasan ke XQuery untuk penyemak imbas Web dan memberikan sejumlah contoh yang menunjukkan kegunaan XQuery untuk pengembangan aplikasi gaya AJAX. Selanjutnya, makalah ini menyajikan reka bentuk pemalam XQuery untuk Microsoft Internet Explorer. Makalah ini juga memberikan contoh aplikasi yang dikembangkan dengan bantuan plug-in ini. [[EENNDD]] skrip; acara; umum; xquery; skrip; dom; html; css; javascript; pengaturcaraan pihak pelanggan; xml; xhtml; penyemak imbas; pengelasan bahasa; helaian gaya; tumbuk"], [{"string": "Ranking related entities for web search queries Entity ranking is a recent paradigm that refers to retrieving and ranking related objects and entities from different structured sources in various scenarios. Entities typically have associated categories and relationships with other entities. In this work, we present an extensive analysis of Web-scale entity ranking, based on machine learned ranking models using an ensemble of pairwise preference models. Our proposed system for entity ranking uses structured knowledge bases, entity relationship graphs and user data to derive useful features to facilitate semantic search with entities directly within the learning to rank framework. The experimental results are validated on a large-scale graph containing millions of entities and hundreds of millions of entity relationships. We show that our proposed ranking solution clearly improves a simple user behavior based ranking model.", "keywords": ["structured data", "semantic search", "object ranking", "entity ranking"], "combined": "Ranking related entities for web search queries Entity ranking is a recent paradigm that refers to retrieving and ranking related objects and entities from different structured sources in various scenarios. Entities typically have associated categories and relationships with other entities. In this work, we present an extensive analysis of Web-scale entity ranking, based on machine learned ranking models using an ensemble of pairwise preference models. Our proposed system for entity ranking uses structured knowledge bases, entity relationship graphs and user data to derive useful features to facilitate semantic search with entities directly within the learning to rank framework. The experimental results are validated on a large-scale graph containing millions of entities and hundreds of millions of entity relationships. We show that our proposed ranking solution clearly improves a simple user behavior based ranking model. [[EENNDD]] structured data; semantic search; object ranking; entity ranking"}, "Peringkat entiti yang berkaitan untuk pertanyaan carian web Peringkat entiti adalah paradigma baru-baru ini yang merujuk kepada pengambilan dan pemeringkatan objek dan entiti yang berkaitan dari sumber berstruktur yang berbeza dalam pelbagai senario. Entiti biasanya mempunyai kategori dan hubungan yang berkaitan dengan entiti lain. Dalam karya ini, kami menyajikan analisis luas peringkat entiti skala Web, berdasarkan model peringkat mesin yang dipelajari menggunakan kumpulan model preferensi berpasangan. Sistem yang dicadangkan kami untuk pemeringkatan entiti menggunakan pangkalan pengetahuan berstruktur, grafik hubungan entiti dan data pengguna untuk memperoleh ciri berguna untuk memudahkan pencarian semantik dengan entiti secara langsung dalam rangka pembelajaran peringkat. Hasil eksperimen disahkan pada graf berskala besar yang mengandungi berjuta-juta entiti dan ratusan juta hubungan entiti. Kami menunjukkan bahawa penyelesaian pemeringkatan yang kami cadangkan dengan jelas meningkatkan model peringkat berdasarkan tingkah laku pengguna yang sederhana. [[EENNDD]] data berstruktur; carian semantik; kedudukan objek; kedudukan entiti"], [{"string": "Fractal summarization for mobile devices to access large documents on the web No contact information provided yet.", "keywords": ["document summarization", "fractal summarization", "mobile commerce", "handheld devices"], "combined": "Fractal summarization for mobile devices to access large documents on the web No contact information provided yet. [[EENNDD]] document summarization; fractal summarization; mobile commerce; handheld devices"}, "Ringkasan fraktal untuk peranti mudah alih untuk mengakses dokumen besar di web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ringkasan dokumen; ringkasan fraktal; perdagangan mudah alih; peranti pegang tangan"], [{"string": "Network-aware forward caching This paper proposes and evaluates a Network Aware Forward Caching approach for determining the optimal deployment strategy of forward caches to a network. A key advantage of this approach is that we can reduce the network costs associated with forward caching to maximize the benefit obtained from their deployment. We show in our simulation that a 37% increase to net benefits could be achieved over the standard method of full cache deployment to cache all POPs traffic. In addition, we show that this maximal point occurs when only 68% of the total traffic is cached.", "keywords": ["web caching"], "combined": "Network-aware forward caching This paper proposes and evaluates a Network Aware Forward Caching approach for determining the optimal deployment strategy of forward caches to a network. A key advantage of this approach is that we can reduce the network costs associated with forward caching to maximize the benefit obtained from their deployment. We show in our simulation that a 37% increase to net benefits could be achieved over the standard method of full cache deployment to cache all POPs traffic. In addition, we show that this maximal point occurs when only 68% of the total traffic is cached. [[EENNDD]] web caching"}, "Cache ke depan yang sedar rangkaian Ini makalah ini mencadangkan dan menilai pendekatan Network Aware Forward Caching untuk menentukan strategi penyebaran cache maju ke rangkaian yang optimum. Kelebihan utama dari pendekatan ini ialah kita dapat mengurangkan biaya jaringan yang berkaitan dengan cache ke depan untuk memaksimumkan manfaat yang diperoleh dari penggunaannya. Kami menunjukkan dalam simulasi kami bahawa peningkatan 37% ke keuntungan bersih dapat dicapai melalui kaedah standard penyebaran cache penuh untuk menyimpan semua lalu lintas POP. Sebagai tambahan, kami menunjukkan bahawa titik maksimum ini berlaku apabila hanya 68% dari jumlah lalu lintas yang di-cache. [[EENNDD]] cache web"], [{"string": "Performance enhancement of scheduling algorithms in clusters and grids using improved dynamic load balancing techniques This paper describes the research work done for during PhD study. Cluster computing, grid computing and cloud computing are distributed computing environments (DCEs) widely accepted for the next generation Web based commercial and scientific applications. These applications work around the globally distributed data of petabyte scale that can only be processed by the aggregating the capability of globally distributed resources. The resource management and process scheduling in large scale distributed computing environment are a challenging task. In this research work we have devised new scheduling algorithms and resource management strategies specially designed for the cluster and grid cloud and peer-to-peer computing. The research work finally presented the distributed computing solutions to one scientific and one commercial application viz. e-Learning and data mining.", "keywords": ["grid service", "resource management", "grid", "trust management", "cluster", "cloud"], "combined": "Performance enhancement of scheduling algorithms in clusters and grids using improved dynamic load balancing techniques This paper describes the research work done for during PhD study. Cluster computing, grid computing and cloud computing are distributed computing environments (DCEs) widely accepted for the next generation Web based commercial and scientific applications. These applications work around the globally distributed data of petabyte scale that can only be processed by the aggregating the capability of globally distributed resources. The resource management and process scheduling in large scale distributed computing environment are a challenging task. In this research work we have devised new scheduling algorithms and resource management strategies specially designed for the cluster and grid cloud and peer-to-peer computing. The research work finally presented the distributed computing solutions to one scientific and one commercial application viz. e-Learning and data mining. [[EENNDD]] grid service; resource management; grid; trust management; cluster; cloud"}, "Peningkatan prestasi algoritma penjadualan dalam kelompok dan grid menggunakan teknik pengimbangan beban dinamik yang ditingkatkan Makalah ini menerangkan kerja penyelidikan yang dilakukan selama kajian PhD. Pengkomputeran kluster, pengkomputeran grid dan pengkomputeran awan adalah persekitaran pengkomputeran terdistribusi (DCE) yang diterima secara meluas untuk aplikasi komersial dan saintifik berasaskan Web generasi akan datang. Aplikasi ini berfungsi berdasarkan data skala petabyte yang diedarkan secara global yang hanya dapat diproses dengan mengagregasi kemampuan sumber yang diedarkan secara global. Pengurusan sumber dan penjadualan proses dalam persekitaran pengkomputeran yang diedarkan secara besar-besaran adalah tugas yang mencabar. Dalam karya penyelidikan ini, kami telah merancang algoritma penjadualan baru dan strategi pengurusan sumber yang direka khas untuk pengkomputeran kluster dan grid dan peer-to-peer. Hasil penyelidikan akhirnya memberikan penyelesaian pengkomputeran yang diedarkan kepada satu aplikasi saintifik dan satu komersial. e-Pembelajaran dan perlombongan data. [[EENNDD]] perkhidmatan grid; pengurusan sumber; grid; pengurusan kepercayaan; gugusan; awan"], [{"string": "Exploit sequencing views in semantic cache to accelerate xpath query evaluation In XML databases, materializing queries and their results into views in a semantic cache can improve the performance of query evaluation by reducing computational complexity and I/O cost. Although there are a number of proposals of semantic cache for XML queries, the issues of fast cache lookup and compensation query construction could be further studied. In this paper, based on sequential XPath queries, we propose fastCLU, a fast Cache LookUp algorithm and effiCQ, an efficient Compensation Query constructing algorithm to solve these two problems. Experimental results show that our algorithms outperform previous algorithms and can achieve good performance of query evaluation.", "keywords": ["xpath", "xml", "query evaluation", "semantic cache"], "combined": "Exploit sequencing views in semantic cache to accelerate xpath query evaluation In XML databases, materializing queries and their results into views in a semantic cache can improve the performance of query evaluation by reducing computational complexity and I/O cost. Although there are a number of proposals of semantic cache for XML queries, the issues of fast cache lookup and compensation query construction could be further studied. In this paper, based on sequential XPath queries, we propose fastCLU, a fast Cache LookUp algorithm and effiCQ, an efficient Compensation Query constructing algorithm to solve these two problems. Experimental results show that our algorithms outperform previous algorithms and can achieve good performance of query evaluation. [[EENNDD]] xpath; xml; query evaluation; semantic cache"}, "Eksploitasi penjujukan urutan dalam semantik cache untuk mempercepat penilaian pertanyaan xpath Dalam pangkalan data XML, mewujudkan pertanyaan dan hasilnya menjadi pandangan dalam semantik cache dapat meningkatkan prestasi penilaian pertanyaan dengan mengurangkan kerumitan komputasi dan kos I / O. Walaupun terdapat sejumlah cadangan semantik cache untuk pertanyaan XML, masalah pencarian cache cepat dan pembinaan pertanyaan pampasan dapat dikaji lebih lanjut. Dalam makalah ini, berdasarkan pertanyaan XPath yang berurutan, kami mencadangkan fastCLU, algoritma Cache LookUp cepat dan effiCQ, algoritma pembinaan Kompensasi Permintaan yang cekap untuk menyelesaikan dua masalah ini. Hasil eksperimen menunjukkan bahawa algoritma kami mengatasi algoritma sebelumnya dan dapat mencapai prestasi penilaian pertanyaan yang baik. [[EENNDD]] xpath; xml; penilaian pertanyaan; cache semantik"], [{"string": "Content hole search in community-type content In community-type content such as blogs and SNSs, we call the user's unawareness of information as a \"content hole\" and the search for this information as a \"content hole search.\" A content hole search differs from similarity searching and has a variety of types. In this paper, we propose different types of content holes and define each type. We also propose an analysis of dialogue related to community-type content and introduce content hole search by using Wikipedia as an example.", "keywords": ["content hole search", "sns", "miscellaneous", "community", "blog"], "combined": "Content hole search in community-type content In community-type content such as blogs and SNSs, we call the user's unawareness of information as a \"content hole\" and the search for this information as a \"content hole search.\" A content hole search differs from similarity searching and has a variety of types. In this paper, we propose different types of content holes and define each type. We also propose an analysis of dialogue related to community-type content and introduce content hole search by using Wikipedia as an example. [[EENNDD]] content hole search; sns; miscellaneous; community; blog"}, "Pencarian lubang kandungan dalam kandungan jenis komuniti Dalam kandungan jenis komuniti seperti blog dan SNS, kami memanggil maklumat pengguna yang tidak disedari sebagai \"lubang isi\" dan pencarian maklumat ini sebagai \"carian lubang isi.\" Pencarian lubang kandungan berbeza dengan pencarian kesamaan dan mempunyai pelbagai jenis. Dalam makalah ini, kami mencadangkan pelbagai jenis lubang isi dan menentukan setiap jenisnya. Kami juga mengusulkan analisis dialog yang berkaitan dengan kandungan jenis komuniti dan memperkenalkan pencarian lubang isi dengan menggunakan Wikipedia sebagai contoh. [[EENNDD]] carian lubang kandungan; sns; pelbagai; komuniti; blog"], [{"string": "Text joins in an RDBMS for web data integration No contact information provided yet.", "keywords": ["heterogeneous databases", "approximate text matching", "data cleaning", "text indexing"], "combined": "Text joins in an RDBMS for web data integration No contact information provided yet. [[EENNDD]] heterogeneous databases; approximate text matching; data cleaning; text indexing"}, "Teks bergabung dalam RDBMS untuk penyatuan data web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pangkalan data heterogen; padanan teks anggaran; pembersihan data; pengindeksan teks"], [{"string": "Post-processing inkml for random-access navigation of voluminous handwritten ink documents No contact information provided yet.", "keywords": ["freehand writing", "random access", "inkml", "digital ink"], "combined": "Post-processing inkml for random-access navigation of voluminous handwritten ink documents No contact information provided yet. [[EENNDD]] freehand writing; random access; inkml; digital ink"}, "Tinta pasca pemprosesan untuk navigasi akses rawak dokumen dakwat tulisan tangan yang banyak Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] tulisan bebas; akses rawak; dakwatml; dakwat digital"], [{"string": "How much is your personal recommendation worth? Suppose you buy a new laptop and, simply because you like it so much, you recommend it to friends, encouraging them to purchase it as well. What would be an adequate price for the vendor of the laptop to pay for your recommendation?", "keywords": ["recommendations", "general", "shapley value", "pricing mechanisms"], "combined": "How much is your personal recommendation worth? Suppose you buy a new laptop and, simply because you like it so much, you recommend it to friends, encouraging them to purchase it as well. What would be an adequate price for the vendor of the laptop to pay for your recommendation? [[EENNDD]] recommendations; general; shapley value; pricing mechanisms"}, "Berapa nilai cadangan peribadi anda? Katakan anda membeli komputer riba baru dan, kerana anda sangat menyukainya, anda mengesyorkannya kepada rakan, mendorong mereka untuk membelinya juga. Berapa harga yang mencukupi untuk penjual komputer riba untuk membayar cadangan anda? [[EENNDD]] cadangan; umum; nilai shapley; mekanisme penetapan harga"], [{"string": "Google news personalization: scalable online collaborative filtering Several approaches to collaborative filtering have been studied but seldom have studies been reported for large (several millionusers and items) and dynamic (the underlying item set is continually changing) settings. In this paper we describe our approach to collaborative filtering for generating personalized recommendations for users of Google News. We generate recommendations using three approaches: collaborative filtering using MinHash clustering, Probabilistic Latent Semantic Indexing (PLSI), and covisitation counts. We combine recommendations from different algorithms using a linear model. Our approach is content agnostic and consequently domain independent, making it easily adaptable for other applications and languages with minimal effort. This paper will describe our algorithms and system setup in detail, and report results of running the recommendations engine on Google News.", "keywords": ["oneline recommendation system", "personalization", "plsi", "google news", "miscellaneous", "scalable collaborative filtering", "minhash", "mapreduce"], "combined": "Google news personalization: scalable online collaborative filtering Several approaches to collaborative filtering have been studied but seldom have studies been reported for large (several millionusers and items) and dynamic (the underlying item set is continually changing) settings. In this paper we describe our approach to collaborative filtering for generating personalized recommendations for users of Google News. We generate recommendations using three approaches: collaborative filtering using MinHash clustering, Probabilistic Latent Semantic Indexing (PLSI), and covisitation counts. We combine recommendations from different algorithms using a linear model. Our approach is content agnostic and consequently domain independent, making it easily adaptable for other applications and languages with minimal effort. This paper will describe our algorithms and system setup in detail, and report results of running the recommendations engine on Google News. [[EENNDD]] oneline recommendation system; personalization; plsi; google news; miscellaneous; scalable collaborative filtering; minhash; mapreduce"}, "Pemperibadian berita Google: penapisan kolaboratif dalam talian yang boleh diskalakan Beberapa pendekatan untuk penapisan kolaboratif telah dikaji tetapi jarang terdapat kajian yang dilaporkan untuk tetapan yang besar (beberapa juta pengguna dan item) dan dinamik (set item yang mendasari terus berubah). Dalam makalah ini kami menerangkan pendekatan kami untuk menyaring kolaboratif untuk menghasilkan cadangan yang diperibadikan untuk pengguna Berita Google. Kami menghasilkan cadangan menggunakan tiga pendekatan: penapisan kolaboratif menggunakan pengelompokan MinHash, Probabilistic Latent Semantic Indexing (PLSI), dan jumlah kovitasi. Kami menggabungkan cadangan dari algoritma yang berbeza menggunakan model linear. Pendekatan kami adalah kandungan agnostik dan akibatnya domain bebas, menjadikannya mudah disesuaikan untuk aplikasi dan bahasa lain dengan usaha yang minimum. Makalah ini akan menerangkan algoritma dan penyediaan sistem kami secara terperinci, dan melaporkan hasil menjalankan mesin cadangan di Google News. [[EENNDD]] sistem cadangan dalam talian; pemperibadian; plsi; berita Google; pelbagai; penapisan kolaboratif berskala; minhash; pengurangan peta"], [{"string": "A framework for the server-side management of conversations with web services No contact information provided yet.", "keywords": ["tools and technologies for web services development", "software architectures", "service oriented architectures", "interoperability"], "combined": "A framework for the server-side management of conversations with web services No contact information provided yet. [[EENNDD]] tools and technologies for web services development; software architectures; service oriented architectures; interoperability"}, "Rangka kerja untuk pengurusan perbualan dari pihak server dengan perkhidmatan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] alat dan teknologi untuk pembangunan perkhidmatan web; seni bina perisian; seni bina berorientasikan perkhidmatan; saling kendali"], [{"string": "Deducing trip related information from flickr Uploading tourist photos is a popular activity on photo sharing platforms. These photographs and their associated metadata (tags, geo-tags, and temporal information) should be useful for mining information about the sites visited. However, user-supplied metadata are often noisy and efficient filtering methods are needed before extracting useful knowledge. We focus here on exploiting temporal information, associated with tourist sites that appear in Flickr. From automatically filtered sets of geo-tagged photos, we deduce answers to questions like \"how long does it take to visit a tourist attraction?\" or \"what can I visit in one day in this city?\" Our method is evaluated and validated by comparing the automatically obtained visit duration times to manual estimations.", "keywords": ["text mining", "image mining", "visit times", "tourist sites", "georeferencing", "geographical gazetteer", "flickr"], "combined": "Deducing trip related information from flickr Uploading tourist photos is a popular activity on photo sharing platforms. These photographs and their associated metadata (tags, geo-tags, and temporal information) should be useful for mining information about the sites visited. However, user-supplied metadata are often noisy and efficient filtering methods are needed before extracting useful knowledge. We focus here on exploiting temporal information, associated with tourist sites that appear in Flickr. From automatically filtered sets of geo-tagged photos, we deduce answers to questions like \"how long does it take to visit a tourist attraction?\" or \"what can I visit in one day in this city?\" Our method is evaluated and validated by comparing the automatically obtained visit duration times to manual estimations. [[EENNDD]] text mining; image mining; visit times; tourist sites; georeferencing; geographical gazetteer; flickr"}, "Mengurangkan maklumat berkaitan perjalanan dari flickr Memuat naik foto pelancong adalah aktiviti popular di platform perkongsian foto. Foto-foto ini dan metadata yang berkaitan (tag, geo-tag, dan maklumat temporal) semestinya berguna untuk melombong maklumat mengenai laman web yang dikunjungi. Walau bagaimanapun, metadata yang dibekalkan pengguna sering kali bising dan kaedah penyaringan yang cekap diperlukan sebelum mengambil pengetahuan yang berguna. Kami memberi tumpuan di sini untuk mengeksploitasi maklumat sementara, yang berkaitan dengan laman web pelancongan yang muncul di Flickr. Dari sekumpulan foto yang ditandai secara geografis secara automatik, kami menyimpulkan jawapan kepada soalan seperti \"berapa lama masa yang diperlukan untuk melawat tempat pelancongan?\" atau \"apa yang boleh saya lawati dalam satu hari di bandar ini?\" Kaedah kami dinilai dan disahkan dengan membandingkan jangka masa lawatan yang diperoleh secara automatik dengan anggaran manual. [[EENNDD]] perlombongan teks; perlombongan imej; masa lawatan; destinasi pelancongan; rujukan geografi; pewartaan geografi; flickr"], [{"string": "Personalized recommendation on dynamic content using predictive bilinear models In Web-based services of dynamic content (such as news articles), recommender systems face the difficulty of timely identifying new items of high-quality and providing recommendations for new users. We propose a feature-based machine learning approach to personalized recommendation that is capable of handling the cold-start issue effectively. We maintain profiles of content of interest, in which temporal characteristics of the content, e.g. popularity and freshness, are updated in real-time manner. We also maintain profiles of users including demographic information and a summary of user activities within Yahoo! properties. Based on all features in user and content profiles, we develop predictive bilinear regression models to provide accurate personalized recommendations of new items for both existing and new users. This approach results in an offline model with light computational overhead compared with other recommender systems that require online re-training. The proposed framework is general and flexible for other personalized tasks. The superior performance of our approach is verified on a large-scale data set collected from the Today-Module on Yahoo! Front Page, with comparison against six competitive approaches.", "keywords": ["general", "ranking", "personalization", "user and content profile", "dynamic features", "regression", "bilinear models", "recommender systems"], "combined": "Personalized recommendation on dynamic content using predictive bilinear models In Web-based services of dynamic content (such as news articles), recommender systems face the difficulty of timely identifying new items of high-quality and providing recommendations for new users. We propose a feature-based machine learning approach to personalized recommendation that is capable of handling the cold-start issue effectively. We maintain profiles of content of interest, in which temporal characteristics of the content, e.g. popularity and freshness, are updated in real-time manner. We also maintain profiles of users including demographic information and a summary of user activities within Yahoo! properties. Based on all features in user and content profiles, we develop predictive bilinear regression models to provide accurate personalized recommendations of new items for both existing and new users. This approach results in an offline model with light computational overhead compared with other recommender systems that require online re-training. The proposed framework is general and flexible for other personalized tasks. The superior performance of our approach is verified on a large-scale data set collected from the Today-Module on Yahoo! Front Page, with comparison against six competitive approaches. [[EENNDD]] general; ranking; personalization; user and content profile; dynamic features; regression; bilinear models; recommender systems"}, "Saranan khusus mengenai kandungan dinamik menggunakan model bilinear ramalan Dalam perkhidmatan berasaskan dinamik kandungan Web (seperti artikel berita), sistem pengesyoran menghadapi kesukaran untuk mengenal pasti item baru yang berkualiti tinggi dan memberikan cadangan untuk pengguna baru tepat pada masanya. Kami mencadangkan pendekatan pembelajaran mesin berasaskan ciri untuk cadangan yang diperibadikan yang mampu menangani masalah permulaan dengan berkesan. Kami mengekalkan profil kandungan yang menarik, di mana ciri-ciri temporal kandungan, mis. populariti dan kesegaran, dikemas kini dalam masa nyata. Kami juga mengekalkan profil pengguna termasuk maklumat demografi dan ringkasan aktiviti pengguna di dalam Yahoo! harta benda. Berdasarkan semua ciri dalam profil pengguna dan kandungan, kami mengembangkan model regresi bilinear prediktif untuk memberikan cadangan yang tepat untuk item baru untuk pengguna yang ada dan yang baru. Pendekatan ini menghasilkan model luar talian dengan overhead komputasi ringan dibandingkan dengan sistem pengesyoran lain yang memerlukan latihan semula dalam talian. Rangka kerja yang dicadangkan adalah umum dan fleksibel untuk tugas-tugas lain yang diperibadikan. Prestasi unggul pendekatan kami disahkan pada kumpulan data berskala besar yang dikumpulkan dari Modul Hari Ini di Yahoo! Halaman Depan, dengan perbandingan terhadap enam pendekatan kompetitif. [[EENNDD]] umum; peringkat; pemperibadian; profil pengguna dan kandungan; ciri dinamik; regresi; model bilinear; sistem cadangan"], [{"string": "An event-condition-action language for XML No contact information provided yet.", "keywords": ["reactive functionality", "xml repositories", "event-condition-action rules", "xml", "rule analysis"], "combined": "An event-condition-action language for XML No contact information provided yet. [[EENNDD]] reactive functionality; xml repositories; event-condition-action rules; xml; rule analysis"}, "Bahasa peristiwa-keadaan-tindakan untuk XML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] fungsi reaktif; repositori xml; peraturan peristiwa-keadaan-tindakan; xml; analisis peraturan"], [{"string": "An evaluation of binary xml encoding optimizations for fast stream based xml processing No contact information provided yet.", "keywords": ["xml binary formats", "xpath processing", "performance evaluation"], "combined": "An evaluation of binary xml encoding optimizations for fast stream based xml processing No contact information provided yet. [[EENNDD]] xml binary formats; xpath processing; performance evaluation"}, "Penilaian pengoptimuman pengekodan xml binari untuk pemprosesan xml berasaskan aliran pantas Belum ada maklumat hubungan yang diberikan. [[EENNDD]] format binari xml; pemprosesan xpath; penilaian prestasi"], [{"string": "On the analysis of cascading style sheets Developing and maintaining cascading style sheets (CSS) is an important issue to web developers as they suffer from the lack of rigorous methods. Most existing means rely on validators that check syntactic rules, and on runtime debuggers that check the behavior of a CSS style sheet on a particular document instance. However, the aim of most style sheets is to be applied to an entire set of documents, usually defined by some schema. To this end, a CSS style sheet is usually written w.r.t. a given schema. While usual debugging tools help reducing the number of bugs, they do not ultimately allow to prove properties over the whole set of documents to which the style sheet is intended to be applied. We propose a novel approach to fill this lack. We introduce ideas borrowed from the fields of logic and compile-time verification for the analysis of CSS style sheets. We present an original tool based on recent advances in tree logics. The tool is capable of statically detecting a wide range of errors (such as empty CSS selectors and semantically equivalent selectors), as well as proving properties related to sets of documents (such as coverage of styling information), in the presence or absence of schema information. This new tool can be used in addition to existing runtime debuggers to ensure a higher level of quality of CSS style sheets.", "keywords": ["style sheets", "css", "web development", "debugging"], "combined": "On the analysis of cascading style sheets Developing and maintaining cascading style sheets (CSS) is an important issue to web developers as they suffer from the lack of rigorous methods. Most existing means rely on validators that check syntactic rules, and on runtime debuggers that check the behavior of a CSS style sheet on a particular document instance. However, the aim of most style sheets is to be applied to an entire set of documents, usually defined by some schema. To this end, a CSS style sheet is usually written w.r.t. a given schema. While usual debugging tools help reducing the number of bugs, they do not ultimately allow to prove properties over the whole set of documents to which the style sheet is intended to be applied. We propose a novel approach to fill this lack. We introduce ideas borrowed from the fields of logic and compile-time verification for the analysis of CSS style sheets. We present an original tool based on recent advances in tree logics. The tool is capable of statically detecting a wide range of errors (such as empty CSS selectors and semantically equivalent selectors), as well as proving properties related to sets of documents (such as coverage of styling information), in the presence or absence of schema information. This new tool can be used in addition to existing runtime debuggers to ensure a higher level of quality of CSS style sheets. [[EENNDD]] style sheets; css; web development; debugging"}, "Mengenai analisis cascading style sheet Membangunkan dan mengekalkan cascading style sheet (CSS) adalah masalah penting bagi pembangun web kerana mereka mengalami kekurangan kaedah yang ketat. Sebilangan besar kaedah yang ada bergantung pada validator yang memeriksa peraturan sintaksis, dan pada debug runtime yang memeriksa tingkah laku helaian gaya CSS pada contoh dokumen tertentu. Walau bagaimanapun, tujuan kebanyakan helaian gaya adalah untuk diterapkan ke seluruh rangkaian dokumen, biasanya ditentukan oleh beberapa skema. Untuk tujuan ini, lembaran gaya CSS biasanya ditulis w.r.t. skema yang diberikan. Walaupun alat penyahpepijatan yang biasa membantu mengurangkan bilangan pepijat, mereka akhirnya tidak membenarkan untuk membuktikan sifat pada keseluruhan dokumen yang mana helaian gaya dimaksudkan untuk digunakan. Kami mencadangkan pendekatan baru untuk mengisi kekurangan ini. Kami memperkenalkan idea yang dipinjam dari bidang logik dan pengesahan waktu kompilasi untuk analisis helaian gaya CSS. Kami membentangkan alat asli berdasarkan kemajuan terkini dalam logik pokok. Alat ini mampu secara statistik mengesan pelbagai ralat (seperti pemilih CSS kosong dan pemilih setara semantik), serta membuktikan sifat yang berkaitan dengan set dokumen (seperti liputan maklumat gaya), sekiranya terdapat atau tidak adanya skema maklumat. Alat baru ini dapat digunakan sebagai tambahan untuk debuger runtime yang ada untuk memastikan tahap kualiti helaian gaya CSS yang lebih tinggi. [[EENNDD]] helaian gaya; css; pembangunan web; penyahpepijatan"], [{"string": "Multi-step media adaptation: implementation of a knowledge-based engine No contact information provided yet.", "keywords": ["content adaptation", "standards", "services", "semantic web", "device independence", "owl", "multimedia"], "combined": "Multi-step media adaptation: implementation of a knowledge-based engine No contact information provided yet. [[EENNDD]] content adaptation; standards; services; semantic web; device independence; owl; multimedia"}, "Penyesuaian media pelbagai langkah: pelaksanaan enjin berasaskan pengetahuan Belum ada maklumat hubungan yang disediakan. [[EENNDD]] penyesuaian kandungan; standard; perkhidmatan; web semantik; kebebasan peranti; burung hantu; multimedia"], [{"string": "Logical structure based semantic relationship extraction from semi-structured documents No contact information provided yet.", "keywords": ["ontology", "relationship extraction", "logical structure", "semi-structured document"], "combined": "Logical structure based semantic relationship extraction from semi-structured documents No contact information provided yet. [[EENNDD]] ontology; relationship extraction; logical structure; semi-structured document"}, "Pengekstrakan hubungan semantik berdasarkan struktur logik dari dokumen separa berstruktur Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ontologi; pengekstrakan hubungan; struktur logik; dokumen separa berstruktur"], [{"string": "Trust analysis with clustering Web provides rich information about a variety of objects. Trustability is a major concern on the web. Truth establishment is an important task so as to provide the right information to the user from the most trustworthy source. Trustworthiness of information provider and the confidence of the facts it provides are inter-dependent on each other and hence can be expressed iteratively in terms of each other. However, a single information provider may not be the most trustworthy for all kinds of information. Every information provider has its own area of competence where it can perform better than others. We derive a model that can evaluate trustability on objects and information providers based on clusters (groups). We propose a method which groups the set of objects for which similar set of providers provide \"good\" facts, and provides better accuracy in addition to high quality object clusters.", "keywords": ["trust", "fact finding", "clustering"], "combined": "Trust analysis with clustering Web provides rich information about a variety of objects. Trustability is a major concern on the web. Truth establishment is an important task so as to provide the right information to the user from the most trustworthy source. Trustworthiness of information provider and the confidence of the facts it provides are inter-dependent on each other and hence can be expressed iteratively in terms of each other. However, a single information provider may not be the most trustworthy for all kinds of information. Every information provider has its own area of competence where it can perform better than others. We derive a model that can evaluate trustability on objects and information providers based on clusters (groups). We propose a method which groups the set of objects for which similar set of providers provide \"good\" facts, and provides better accuracy in addition to high quality object clusters. [[EENNDD]] trust; fact finding; clustering"}, "Analisis kepercayaan dengan pengelompokan Web memberikan banyak maklumat mengenai pelbagai objek. Kebolehpercayaan adalah perhatian utama di web. Pembentukan kebenaran adalah tugas penting untuk memberikan maklumat yang tepat kepada pengguna dari sumber yang paling dipercayai. Kebolehpercayaan penyedia maklumat dan keyakinan fakta yang diberikannya saling bergantung antara satu sama lain dan oleh itu dapat dinyatakan secara berulang-ulang dari segi satu sama lain. Walau bagaimanapun, satu penyedia maklumat mungkin bukan yang paling dipercayai untuk semua jenis maklumat. Setiap penyedia maklumat mempunyai bidang kompetensinya sendiri di mana ia boleh menunjukkan prestasi yang lebih baik daripada yang lain. Kami memperoleh model yang dapat menilai kebolehpercayaan pada objek dan penyedia maklumat berdasarkan kelompok (kumpulan). Kami mencadangkan kaedah yang mengelompokkan kumpulan objek yang kumpulan penyedia serupa memberikan fakta \"baik\", dan memberikan ketepatan yang lebih baik di samping kelompok objek berkualiti tinggi. [[EENNDD]] kepercayaan; pencarian fakta; pengelompokan"], [{"string": "Unsupervised extraction of template structure in web search queries Web search queries are an encoding of the user's search intent and extracting structured information from them can facilitate central search engine operations like improving the ranking of search results and advertisements. Not surprisingly, this area has attracted a lot of attention in the research community in the last few years. The problem is, however, made challenging by the fact that search queries tend to be extremely succinct; a condensation of user search needs to the bare-minimum set of keywords. In this paper we consider the problem of extracting, with no manual intervention, the hidden structure behind the observed search queries in a domain: the origins of the constituent keywords as well as the manner the individual keywords are assembled together. We formalize important properties of the problem and then give a principled solution based on generative models that satisfies these properties. Using manually labeled data we show that the query templates extracted by our solution are superior to those discovered by strong baseline methods.", "keywords": ["graphical models", "query templates", "intent analysis"], "combined": "Unsupervised extraction of template structure in web search queries Web search queries are an encoding of the user's search intent and extracting structured information from them can facilitate central search engine operations like improving the ranking of search results and advertisements. Not surprisingly, this area has attracted a lot of attention in the research community in the last few years. The problem is, however, made challenging by the fact that search queries tend to be extremely succinct; a condensation of user search needs to the bare-minimum set of keywords. In this paper we consider the problem of extracting, with no manual intervention, the hidden structure behind the observed search queries in a domain: the origins of the constituent keywords as well as the manner the individual keywords are assembled together. We formalize important properties of the problem and then give a principled solution based on generative models that satisfies these properties. Using manually labeled data we show that the query templates extracted by our solution are superior to those discovered by strong baseline methods. [[EENNDD]] graphical models; query templates; intent analysis"}, "Pengekstrakan struktur templat yang tidak diawasi dalam pertanyaan carian web Pertanyaan carian web adalah pengekodan maksud carian pengguna dan pengekstrakan maklumat berstruktur dari mereka dapat memudahkan operasi mesin pencari seperti meningkatkan peringkat hasil carian dan iklan. Tidak menghairankan bahawa kawasan ini telah menarik banyak perhatian dalam komuniti penyelidikan dalam beberapa tahun terakhir. Masalahnya, bagaimanapun, ditantang oleh kenyataan bahawa pertanyaan carian cenderung sangat ringkas; pemeluasan carian pengguna kepada set kata kunci minimum. Dalam makalah ini kami mempertimbangkan masalah mengekstrak, tanpa campur tangan manual, struktur tersembunyi di sebalik pertanyaan carian yang diperhatikan dalam domain: asal-usul kata kunci penyusun dan cara kata kunci individu digabungkan. Kami memformalkan sifat penting masalah dan kemudian memberikan penyelesaian berprinsip berdasarkan model generatif yang memenuhi sifat ini. Dengan menggunakan data berlabel secara manual, kami menunjukkan bahawa templat pertanyaan yang diekstrak oleh penyelesaian kami lebih tinggi daripada templat yang dijumpai dengan kaedah asas yang kuat. [[EENNDD]] model grafik; templat pertanyaan; analisis maksud"], [{"string": "RDF triples in XML No contact information provided yet.", "keywords": ["rdf", "xml", "semantic web"], "combined": "RDF triples in XML No contact information provided yet. [[EENNDD]] rdf; xml; semantic web"}, "RDF meningkat tiga kali ganda dalam XML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] rdf; xml; web semantik"], [{"string": "The volume and evolution of web page templates No contact information provided yet.", "keywords": ["boilerplate", "data mining", "information search and retrieval", "data cleaning", "templates", "web mining"], "combined": "The volume and evolution of web page templates No contact information provided yet. [[EENNDD]] boilerplate; data mining; information search and retrieval; data cleaning; templates; web mining"}, "Jumlah dan evolusi templat halaman web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] plat dandang; perlombongan data; pencarian dan pengambilan maklumat; pembersihan data; templat; perlombongan web"], [{"string": "U-REST: an unsupervised record extraction system In this paper, we describe a system that can extract recordstructures from web pages with no direct human supervision.Records are commonly occurring HTML-embedded data tuples that describe people, offered courses, products,company profiles, etc. We present a simplified frameworkfor studying the problem of unsupervised record extraction. one which separates the algorithms from the feature engineering.Our system, U-REST formalizes an approach tothe problem of unsupervised record extraction using a simple two-stage machine learning framework. The first stage involves clustering, where structurally similar regions are discovered, and the second stage involves classification, where discovered groupings (clusters of regions) are ranked by their likelihood of being records. In our work, we describe, and summarize the results of an extensive survey of features for both stages. We conclude by comparing U-REST to related systems. The results of our empirical evaluation show encouraging improvements in extraction accuracy.", "keywords": ["miscellaneous", "record extraction", "clustering"], "combined": "U-REST: an unsupervised record extraction system In this paper, we describe a system that can extract recordstructures from web pages with no direct human supervision.Records are commonly occurring HTML-embedded data tuples that describe people, offered courses, products,company profiles, etc. We present a simplified frameworkfor studying the problem of unsupervised record extraction. one which separates the algorithms from the feature engineering.Our system, U-REST formalizes an approach tothe problem of unsupervised record extraction using a simple two-stage machine learning framework. The first stage involves clustering, where structurally similar regions are discovered, and the second stage involves classification, where discovered groupings (clusters of regions) are ranked by their likelihood of being records. In our work, we describe, and summarize the results of an extensive survey of features for both stages. We conclude by comparing U-REST to related systems. The results of our empirical evaluation show encouraging improvements in extraction accuracy. [[EENNDD]] miscellaneous; record extraction; clustering"}, "U-REST: sistem pengekstrakan rekod yang tidak diawasi Dalam makalah ini, kami menerangkan sistem yang dapat mengekstrak struktur rekod dari laman web tanpa pengawasan manusia langsung. Rekod biasanya terdapat pada tupel data yang disisipkan HTML yang menggambarkan orang, kursus yang ditawarkan, produk, profil syarikat , dll. Kami menyajikan kerangka kerja yang dipermudahkan untuk mengkaji masalah pengekstrakan rekod yang tidak diawasi. yang memisahkan algoritma dari kejuruteraan ciri. Sistem kami, U-REST merumuskan pendekatan untuk mengatasi masalah pengekstrakan rekod tanpa pengawasan menggunakan kerangka pembelajaran mesin dua peringkat yang sederhana. Tahap pertama melibatkan pengelompokan, di mana wilayah serupa secara struktural ditemukan, dan tahap kedua melibatkan klasifikasi, di mana pengelompokan yang ditemukan (kelompok wilayah) diperingkat berdasarkan kemungkinannya menjadi catatan. Dalam kerja kami, kami menerangkan, dan meringkaskan hasil tinjauan luas mengenai ciri-ciri untuk kedua tahap. Kami membuat kesimpulan dengan membandingkan U-REST dengan sistem yang berkaitan. Hasil penilaian empirikal kami menunjukkan peningkatan yang menggalakkan dalam ketepatan pengekstrakan. [[EENNDD]] pelbagai; pengekstrakan rekod; pengelompokan"], [{"string": "Learning domain ontologies for Web service descriptions: an experiment in bioinformatics No contact information provided yet.", "keywords": ["bioinformatics", "domain ontology", "ontology learning", "web services", "owl-s", "semantic web", "ontology evaluation"], "combined": "Learning domain ontologies for Web service descriptions: an experiment in bioinformatics No contact information provided yet. [[EENNDD]] bioinformatics; domain ontology; ontology learning; web services; owl-s; semantic web; ontology evaluation"}, "Pembelajaran ontologi domain untuk perihalan perkhidmatan Web: percubaan dalam bioinformatik Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] bioinformatik; ontologi domain; pembelajaran ontologi; perkhidmatan web; burung hantu-s; web semantik; penilaian ontologi"], [{"string": "Segment-based proxy caching of multimedia streams An abstract is not available.", "keywords": ["variable-sized segmentation", "multimedia information systems", "proxy caching", "video caching", "multimedia streaming", "segment-based caching"], "combined": "Segment-based proxy caching of multimedia streams An abstract is not available. [[EENNDD]] variable-sized segmentation; multimedia information systems; proxy caching; video caching; multimedia streaming; segment-based caching"}, "Cache proxy berasaskan segmen aliran multimedia Abstrak tidak tersedia. [[EENNDD]] pembahagian bersaiz berubah; sistem maklumat multimedia; cache proksi; cache video; penstriman multimedia; cache berasaskan segmen"], [{"string": "Web-based personalization and management of interactive video An abstract is not available.", "keywords": ["smil", "dynamic content generation", "j2ee", "video", "micro-payment", "video personalization", "media asset management", "interactive video", "interaction styles"], "combined": "Web-based personalization and management of interactive video An abstract is not available. [[EENNDD]] smil; dynamic content generation; j2ee; video; micro-payment; video personalization; media asset management; interactive video; interaction styles"}, "Pemperibadian dan pengurusan video interaktif berasaskan web Abstrak tidak tersedia. [[EENNDD]] senyum; penjanaan kandungan dinamik; j2ee; video; pembayaran mikro; pemperibadian video; pengurusan aset media; video interaktif; gaya interaksi"], [{"string": "Sampling search-engine results No contact information provided yet.", "keywords": ["search engines", "information search and retrieval", "weighted and", "sampling", "wand"], "combined": "Sampling search-engine results No contact information provided yet. [[EENNDD]] search engines; information search and retrieval; weighted and; sampling; wand"}, "Sampel hasil carian enjin Belum ada maklumat hubungan yang diberikan. [[EENNDD]] enjin carian; pencarian dan pengambilan maklumat; berwajaran dan; persampelan; tongkat"], [{"string": "Enforcing strict model-view separation in template engines Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.", "keywords": ["web application", "patterns", "model-view-controller", "template engine"], "combined": "Enforcing strict model-view separation in template engines Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references. [[EENNDD]] web application; patterns; model-view-controller; template engine"}, "Menerapkan pemisahan pandangan model yang ketat dalam mesin templat Catatan: Kesalahan OCR mungkin terdapat dalam Senarai Rujukan ini yang diekstrak dari artikel teks lengkap. ACM memilih untuk mendedahkan Senarai lengkap dan bukan hanya rujukan yang betul dan berkaitan. [[EENNDD]] aplikasi web; corak; model-pandangan-pengawal; mesin templat"], [{"string": "PicASHOW: pictorial authority search by hyperlinks on the Web An abstract is not available.", "keywords": ["image retrieval", "hubs and authorities", "hypertext/hypermedia", "link structure analysis", "image hubs"], "combined": "PicASHOW: pictorial authority search by hyperlinks on the Web An abstract is not available. [[EENNDD]] image retrieval; hubs and authorities; hypertext/hypermedia; link structure analysis; image hubs"}, "PicASHOW: carian pihak berkuasa bergambar dengan pautan hiper di Web Abstrak tidak tersedia. [[EENNDD]] pengambilan gambar; hab dan pihak berkuasa; hiperteks / hipermedia; analisis struktur pautan; hab gambar"], [{"string": "LCA-based selection for XML document collections In this paper, we address the problem of database selection for XML document collections, that is, given a set of collections and a user query, how to rank the collections based on their goodness to the query. Goodness is determined by the relevance of the documents in the collection to the query. We consider keyword queries and support Lowest Common Ancestor (LCA) semantics for defining query results, where the relevance of each document to a query is determined by properties of the LCA of those nodes in the XML document that contain the query keywords. To avoid evaluating queries against each document in a collection, we propose maintaining in a preprocessing phase, information about the LCAs of all pairs of keywords in a document and use it to approximate the properties of the LCA-based results of a query. To improve storage and processing efficiency, we use appropriate summaries of the LCA information based on Bloom filters. We address both a boolean and a weighted version of the database selection problem. Our experimental results show that our approach incurs low errors in the estimation of the goodness of a collection and provides rankings that are very close to the actual ones.", "keywords": ["lowest common ancestor", "information search and retrieval", "database selection", "xml"], "combined": "LCA-based selection for XML document collections In this paper, we address the problem of database selection for XML document collections, that is, given a set of collections and a user query, how to rank the collections based on their goodness to the query. Goodness is determined by the relevance of the documents in the collection to the query. We consider keyword queries and support Lowest Common Ancestor (LCA) semantics for defining query results, where the relevance of each document to a query is determined by properties of the LCA of those nodes in the XML document that contain the query keywords. To avoid evaluating queries against each document in a collection, we propose maintaining in a preprocessing phase, information about the LCAs of all pairs of keywords in a document and use it to approximate the properties of the LCA-based results of a query. To improve storage and processing efficiency, we use appropriate summaries of the LCA information based on Bloom filters. We address both a boolean and a weighted version of the database selection problem. Our experimental results show that our approach incurs low errors in the estimation of the goodness of a collection and provides rankings that are very close to the actual ones. [[EENNDD]] lowest common ancestor; information search and retrieval; database selection; xml"}, "Pemilihan berdasarkan LCA untuk koleksi dokumen XML Dalam makalah ini, kami mengatasi masalah pemilihan pangkalan data untuk koleksi dokumen XML, yaitu, diberikan sekumpulan koleksi dan pertanyaan pengguna, bagaimana memberi peringkat koleksi berdasarkan kebaikannya kepada pertanyaan. Kebaikan ditentukan oleh kesesuaian dokumen dalam koleksi dengan pertanyaan. Kami mempertimbangkan pertanyaan kata kunci dan menyokong semantik Lowest Common Ancestor (LCA) untuk menentukan hasil pertanyaan, di mana perkaitan setiap dokumen dengan pertanyaan ditentukan oleh sifat-sifat LCA simpul tersebut dalam dokumen XML yang mengandungi kata kunci pertanyaan. Untuk mengelakkan menilai pertanyaan terhadap setiap dokumen dalam koleksi, kami mencadangkan agar tetap dalam fasa praprosesan, maklumat mengenai LCA semua pasangan kata kunci dalam dokumen dan menggunakannya untuk menghampiri sifat hasil pertanyaan berdasarkan LCA. Untuk meningkatkan kecekapan penyimpanan dan pemprosesan, kami menggunakan ringkasan maklumat LCA yang sesuai berdasarkan penapis Bloom. Kami menangani masalah pemilihan pangkalan data versi boolean dan berwajaran. Hasil eksperimen kami menunjukkan bahawa pendekatan kami melakukan kesalahan yang rendah dalam perkiraan kebaikan koleksi dan memberikan peringkat yang sangat dekat dengan yang sebenarnya. [[EENNDD]] nenek moyang yang paling rendah; carian dan pengambilan maklumat; pemilihan pangkalan data; xml"], [{"string": "A community-aware search engine No contact information provided yet.", "keywords": ["searching and ranking", "data mining"], "combined": "A community-aware search engine No contact information provided yet. [[EENNDD]] searching and ranking; data mining"}, "Enjin carian yang menyedari masyarakat Belum ada maklumat hubungan yang diberikan. [[EENNDD]] mencari dan peringkat; perlombongan data"], [{"string": "Toward expressive syndication on the web Syndication systems on the Web have attracted vast amounts of attention in recent years. As technologies have emerged and matured, there has been a transition to more expressive syndication approaches; that is, subscribers and publishers are provided with more expressive means of describing their interests and published content, enabling more accurate information filtering. In this paper, we formalize a syndication architecture that utilizes expressive Web ontologies and logic-based reasoning for selective content dissemination. This provides finer grained control for filtering and automated reasoning for discovering implicit subscription matches, both of which are not achievable in less expressive approaches. We then address one of the main limitations with such a syndication approach, namely matching newly published information with subscription requests in an efficient and practical manner. To this end, we investigate continuous query answering for a large subset of the Web Ontology Language (OWL); specifically, we formally define continuous queries for OWL knowledge bases and present a novel algorithm for continuous query answering in a large subset of this language. Lastly, an evaluation of the query approach is shown, demonstrating its effectiveness for syndication purposes.", "keywords": ["publish/subscribe", "description logics", "syndication", "continuous query answering"], "combined": "Toward expressive syndication on the web Syndication systems on the Web have attracted vast amounts of attention in recent years. As technologies have emerged and matured, there has been a transition to more expressive syndication approaches; that is, subscribers and publishers are provided with more expressive means of describing their interests and published content, enabling more accurate information filtering. In this paper, we formalize a syndication architecture that utilizes expressive Web ontologies and logic-based reasoning for selective content dissemination. This provides finer grained control for filtering and automated reasoning for discovering implicit subscription matches, both of which are not achievable in less expressive approaches. We then address one of the main limitations with such a syndication approach, namely matching newly published information with subscription requests in an efficient and practical manner. To this end, we investigate continuous query answering for a large subset of the Web Ontology Language (OWL); specifically, we formally define continuous queries for OWL knowledge bases and present a novel algorithm for continuous query answering in a large subset of this language. Lastly, an evaluation of the query approach is shown, demonstrating its effectiveness for syndication purposes. [[EENNDD]] publish/subscribe; description logics; syndication; continuous query answering"}, "Menjelang sindikasi ekspresif di web Sistem sindikasi di Web telah menarik banyak perhatian dalam beberapa tahun terakhir. Ketika teknologi telah muncul dan matang, telah terjadi peralihan ke pendekatan sindikasi yang lebih ekspresif; iaitu, pelanggan dan penerbit disediakan dengan cara yang lebih ekspresif untuk menggambarkan minat mereka dan kandungan yang diterbitkan, yang memungkinkan penyaringan maklumat yang lebih tepat. Dalam makalah ini, kami memformalkan seni bina sindikasi yang menggunakan ontologi Web ekspresif dan penaakulan berdasarkan logik untuk penyebaran isi selektif. Ini memberikan kawalan yang lebih rapi untuk menyaring dan penaakulan automatik untuk menemui padanan langganan tersirat, yang keduanya tidak dapat dicapai dalam pendekatan yang kurang ekspresif. Kami kemudian menangani salah satu batasan utama dengan pendekatan sindikasi seperti itu, iaitu memadankan maklumat yang baru diterbitkan dengan permintaan langganan dengan cara yang efisien dan praktikal. Untuk tujuan ini, kami menyiasat menjawab pertanyaan berterusan untuk sebahagian besar Bahasa Ontologi Web (OWL); secara khusus, kami secara formal menentukan pertanyaan berterusan untuk pangkalan pengetahuan OWL dan membentangkan algoritma baru untuk menjawab pertanyaan berterusan dalam subset besar bahasa ini. Terakhir, penilaian terhadap pendekatan pertanyaan ditunjukkan, menunjukkan keberkesanannya untuk tujuan sindikasi. [[EENNDD]] terbitkan / melanggan; logik keterangan; sindiket; menjawab pertanyaan berterusan"], [{"string": "Background knowledge for ontology construction No contact information provided yet.", "keywords": ["background knowledge", "semi-automatic ontology construction"], "combined": "Background knowledge for ontology construction No contact information provided yet. [[EENNDD]] background knowledge; semi-automatic ontology construction"}, "Latar belakang pengetahuan untuk pembinaan ontologi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengetahuan latar belakang; pembinaan ontologi separa automatik"], [{"string": "Shared lexicon for distributed annotations on the Web No contact information provided yet.", "keywords": ["emergent semantics", "interoperability", "language games", "distributed annotations"], "combined": "Shared lexicon for distributed annotations on the Web No contact information provided yet. [[EENNDD]] emergent semantics; interoperability; language games; distributed annotations"}, "Leksikon yang dikongsi untuk anotasi yang diedarkan di Web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] semantik baru muncul; saling kendali; permainan bahasa; diedarkan anotasi"], [{"string": "SPARQ2L: towards support for subgraph extraction queries in rdf databases Many applications in analytical domains often have the need to \"connect the dots\" i.e., query about the structure of data. In bioinformatics for example, it is typical to want to query about interactions between proteins. The aim of such queries is to \"extract\" relationships between entities i.e. paths from a data graph. Often, such queries will specify certain constraints that qualifying results must satisfy e.g. paths involving a set of mandatory nodes. Unfortunately, most present day Semantic Web query languages including the current draft of the anticipated recommendation SPARQL, lack the ability to express queries about arbitrary path structures in data. In addition, many systems that support some limited form of path queries rely on main memory graph algorithms limiting their applicability to very large scale graphs.", "keywords": ["rdf", "querying semantic web databases", "sparql extensions"], "combined": "SPARQ2L: towards support for subgraph extraction queries in rdf databases Many applications in analytical domains often have the need to \"connect the dots\" i.e., query about the structure of data. In bioinformatics for example, it is typical to want to query about interactions between proteins. The aim of such queries is to \"extract\" relationships between entities i.e. paths from a data graph. Often, such queries will specify certain constraints that qualifying results must satisfy e.g. paths involving a set of mandatory nodes. Unfortunately, most present day Semantic Web query languages including the current draft of the anticipated recommendation SPARQL, lack the ability to express queries about arbitrary path structures in data. In addition, many systems that support some limited form of path queries rely on main memory graph algorithms limiting their applicability to very large scale graphs. [[EENNDD]] rdf; querying semantic web databases; sparql extensions"}, "SPARQ2L: ke arah sokongan untuk pertanyaan ekstraksi subgraf dalam pangkalan data rdf. Banyak aplikasi dalam domain analitik sering kali memerlukan \"menghubungkan titik\", iaitu pertanyaan mengenai struktur data. Dalam bioinformatik misalnya, biasanya ingin bertanya tentang interaksi antara protein. Tujuan pertanyaan sedemikian adalah untuk \"mengekstrak\" hubungan antara entiti iaitu jalan dari grafik data. Selalunya, pertanyaan seperti itu akan menentukan kekangan tertentu yang mesti dipenuhi oleh keputusan kelayakan, mis. laluan yang melibatkan sekumpulan nod wajib. Sayangnya, kebanyakan bahasa pertanyaan Web Semantik sekarang termasuk draf SPARQL cadangan yang dinanti-nantikan, tidak mempunyai kemampuan untuk menyatakan pertanyaan mengenai struktur jalan sewenang-wenang dalam data. Di samping itu, banyak sistem yang menyokong beberapa bentuk pertanyaan jalan yang terhad bergantung pada algoritma grafik memori utama yang mengehadkan penerapannya kepada grafik skala yang sangat besar. [[EENNDD]] rdf; meminta pangkalan data web semantik; sambungan sparql"], [{"string": "How opinions are received by online communities: a case study on amazon.com helpfulness votes There are many on-line settings in which users publicly express opinions. A number of these offer mechanisms for other users to evaluate these opinions; a canonical example is Amazon.com, where reviews come with annotations like \"26 of 32 people found the following review helpful.\" Opinion evaluation appears in many off-line settings as well, including market research and political campaigns. Reasoning about the evaluation of an opinion is fundamentally different from reasoning about the opinion itself: rather than asking, \"What did Y think of X?\", we are asking, \"What did Z think of Y's opinion of X?\" Here we develop a framework for analyzing and modeling opinion evaluation, using a large-scale collection of Amazon book reviews as a dataset. We find that the perceived helpfulness of a review depends not just on its content but also but also in subtle ways on how the expressed evaluation relates to other evaluations of the same product. As part of our approach, we develop novel methods that take advantage of the phenomenon of review \"plagiarism\" to control for the effects of text in opinion evaluation, and we provide a simple and natural mathematical model consistent with our findings. Our analysis also allows us to distinguish among the predictions of competing theories from sociology and social psychology, and to discover unexpected differences in the collective opinion-evaluation behavior of user populations from ifferent countries.", "keywords": ["review helpfulness", "sentiment analysis", "review utility", "plagiarism", "opinion mining", "social influence", "online communities"], "combined": "How opinions are received by online communities: a case study on amazon.com helpfulness votes There are many on-line settings in which users publicly express opinions. A number of these offer mechanisms for other users to evaluate these opinions; a canonical example is Amazon.com, where reviews come with annotations like \"26 of 32 people found the following review helpful.\" Opinion evaluation appears in many off-line settings as well, including market research and political campaigns. Reasoning about the evaluation of an opinion is fundamentally different from reasoning about the opinion itself: rather than asking, \"What did Y think of X?\", we are asking, \"What did Z think of Y's opinion of X?\" Here we develop a framework for analyzing and modeling opinion evaluation, using a large-scale collection of Amazon book reviews as a dataset. We find that the perceived helpfulness of a review depends not just on its content but also but also in subtle ways on how the expressed evaluation relates to other evaluations of the same product. As part of our approach, we develop novel methods that take advantage of the phenomenon of review \"plagiarism\" to control for the effects of text in opinion evaluation, and we provide a simple and natural mathematical model consistent with our findings. Our analysis also allows us to distinguish among the predictions of competing theories from sociology and social psychology, and to discover unexpected differences in the collective opinion-evaluation behavior of user populations from ifferent countries. [[EENNDD]] review helpfulness; sentiment analysis; review utility; plagiarism; opinion mining; social influence; online communities"}, "Bagaimana pendapat diterima oleh komuniti dalam talian: kajian kes di amazon.com suara bermanfaat Terdapat banyak tetapan dalam talian di mana pengguna menyatakan pendapat secara terbuka. Sejumlah mekanisme tawaran ini untuk pengguna lain menilai pendapat ini; contoh kanonik adalah Amazon.com, di mana ulasan disertakan dengan anotasi seperti \"26 daripada 32 orang menganggap ulasan berikut bermanfaat.\" Penilaian pendapat muncul dalam banyak tetapan luar talian juga, termasuk penyelidikan pasaran dan kempen politik. Beralasan mengenai penilaian pendapat pada asasnya berbeza dari penaakulan tentang pendapat itu sendiri: daripada bertanya, \"Apa pendapat Y tentang X?\", Kita bertanya, \"Apa pendapat Z mengenai pendapat Y tentang X?\" Di sini kami mengembangkan kerangka kerja untuk menganalisis dan memodelkan penilaian pendapat, dengan menggunakan koleksi besar ulasan buku Amazon sebagai set data. Kami mendapati bahawa tinjauan yang dirasakan tidak hanya bergantung pada kandungannya tetapi juga tetapi dengan cara yang halus mengenai bagaimana penilaian yang dinyatakan berkaitan dengan penilaian lain terhadap produk yang sama. Sebagai sebahagian daripada pendekatan kami, kami mengembangkan kaedah baru yang memanfaatkan fenomena tinjauan \"plagiarisme\" untuk mengawal kesan teks dalam penilaian pendapat, dan kami menyediakan model matematik sederhana dan semula jadi yang sesuai dengan penemuan kami. Analisis kami juga membolehkan kami membezakan antara ramalan teori bersaing dari sosiologi dan psikologi sosial, dan untuk mengetahui perbezaan yang tidak dijangka dalam tingkah laku penilaian pendapat kolektif populasi pengguna dari negara-negara yang berbeza. [[EENNDD]] meninjau kebaikan; analisis sentimen; mengkaji semula utiliti; penipuan; perlombongan pendapat; pengaruh sosial; komuniti dalam talian"], [{"string": "Knowledge modeling and its application in life sciences: a tale of two ontologies No contact information provided yet.", "keywords": ["bioinformatics ontology", "information search and retrieval", "miscellaneous", "biological ontology development", "propreo", "glyco", "semantic bioinformatics", "ontology structural metrics", "ontology population", "glycoproteomics"], "combined": "Knowledge modeling and its application in life sciences: a tale of two ontologies No contact information provided yet. [[EENNDD]] bioinformatics ontology; information search and retrieval; miscellaneous; biological ontology development; propreo; glyco; semantic bioinformatics; ontology structural metrics; ontology population; glycoproteomics"}, "Pemodelan pengetahuan dan penerapannya dalam sains kehidupan: kisah dua ontologi Belum ada maklumat hubungan. [[EENNDD]] ontologi bioinformatik; pencarian dan pengambilan maklumat; pelbagai; pengembangan ontologi biologi; propreo; glyco; bioinformatik semantik; metrik struktur ontologi; populasi ontologi; glikoproteomik"], [{"string": "An infrastructure for searching, reusing and evolving distributed ontologies No contact information provided yet.", "keywords": ["ontology registry", "ontology reuse", "ontology evolution"], "combined": "An infrastructure for searching, reusing and evolving distributed ontologies No contact information provided yet. [[EENNDD]] ontology registry; ontology reuse; ontology evolution"}, "Infrastruktur untuk mencari, menggunakan kembali dan mengembangkan ontologi yang diedarkan Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] pendaftaran ontologi; penggunaan semula ontologi; evolusi ontologi"], [{"string": "Adaptive faceted browser for navigation in open information spaces Open information spaces have several unique characteristics such as their changeability, large size, complexity and diverse user base. These result in novel challenges during user navigation, information retrieval and data visualization in open information spaces.We propose a method of navigation in open information spaces based on an enhanced faceted browser with support for dynamic facet generation and adaptation based on user characteristics.", "keywords": ["information search and retrieval", "adaptive faceted browser", "open information space"], "combined": "Adaptive faceted browser for navigation in open information spaces Open information spaces have several unique characteristics such as their changeability, large size, complexity and diverse user base. These result in novel challenges during user navigation, information retrieval and data visualization in open information spaces.We propose a method of navigation in open information spaces based on an enhanced faceted browser with support for dynamic facet generation and adaptation based on user characteristics. [[EENNDD]] information search and retrieval; adaptive faceted browser; open information space"}, "Penyemak imbas bersesuaian untuk navigasi di ruang maklumat terbuka Ruang maklumat terbuka mempunyai beberapa ciri unik seperti kebolehubahannya, ukuran besar, kerumitan dan asas pengguna yang pelbagai. Ini menghasilkan cabaran baru semasa navigasi pengguna, pengambilan maklumat dan visualisasi data di ruang maklumat terbuka. Kami mencadangkan kaedah navigasi di ruang maklumat terbuka berdasarkan penyemak imbas yang disempurnakan dengan sokongan untuk penjanaan dan penyesuaian aspek dinamik berdasarkan ciri pengguna. [[EENNDD]] carian dan pengambilan maklumat; penyemak imbas beradaptasi; ruang maklumat terbuka"], [{"string": "GeoTV: navigating geocoded rss to create an iptv experience The Web is rapidly moving towards a platform for mass collaboration in content production and consumption from three screens: computers, mobile phones, and TVs. While there has been a surge of interests in making Web content accessible from mobile devices, there is a significant lack of progress when it comes to making the web experience suitable for viewing on a television. Towards this end, we describe a novel concept, namely GeoTV, where we explore a framework by which web content can be presented or pushed in a meaningful manner to create an entertainment experience for the TV audience. Fresh content on a variety of topics, people, and places is being created and made available on the Web at breathtaking speed. Navigating fresh content effectively on TV demands a new browsing paradigm that requires fewer mouse clicks or user interactions from the remote control. Novel geospatial and temporal browsing techniques are provided in GeoTV that allow users the capability of aggregating and navigating RSS-enabled content in a timely, personalized and automatic manner for viewing in an IPTV environment. This poster is an extension of our previous work on GeoTracker that utilizes both a geospatial representation and a temporal (chronological) presentation to help users spot the most relevant updates quickly within the context of a Web-enabled environment. We demonstrate 1) the usability of such a tool that greatly enhances a user.s ability in locating and browsing videos based on his or her geographical interests and 2) various innovative interface designs for showing RSS-enabled information in an IPTV environment.", "keywords": ["iptv", "geospatial tagging", "rss", "blog", "multimedia"], "combined": "GeoTV: navigating geocoded rss to create an iptv experience The Web is rapidly moving towards a platform for mass collaboration in content production and consumption from three screens: computers, mobile phones, and TVs. While there has been a surge of interests in making Web content accessible from mobile devices, there is a significant lack of progress when it comes to making the web experience suitable for viewing on a television. Towards this end, we describe a novel concept, namely GeoTV, where we explore a framework by which web content can be presented or pushed in a meaningful manner to create an entertainment experience for the TV audience. Fresh content on a variety of topics, people, and places is being created and made available on the Web at breathtaking speed. Navigating fresh content effectively on TV demands a new browsing paradigm that requires fewer mouse clicks or user interactions from the remote control. Novel geospatial and temporal browsing techniques are provided in GeoTV that allow users the capability of aggregating and navigating RSS-enabled content in a timely, personalized and automatic manner for viewing in an IPTV environment. This poster is an extension of our previous work on GeoTracker that utilizes both a geospatial representation and a temporal (chronological) presentation to help users spot the most relevant updates quickly within the context of a Web-enabled environment. We demonstrate 1) the usability of such a tool that greatly enhances a user.s ability in locating and browsing videos based on his or her geographical interests and 2) various innovative interface designs for showing RSS-enabled information in an IPTV environment. [[EENNDD]] iptv; geospatial tagging; rss; blog; multimedia"}, "GeoTV: menavigasi rss geocoded untuk mencipta pengalaman iptv Web bergerak pantas menuju platform untuk kolaborasi besar-besaran dalam pengeluaran dan penggunaan kandungan dari tiga skrin: komputer, telefon bimbit, dan TV. Walaupun ada lonjakan minat untuk membuat konten Web dapat diakses dari perangkat mudah alih, ada kemajuan yang signifikan ketika membuat pengalaman web sesuai untuk dilihat di televisi. Untuk mencapai tujuan ini, kami menerangkan konsep baru, iaitu GeoTV, di mana kami meneroka kerangka di mana kandungan web dapat disajikan atau didorong dengan cara yang bermakna untuk mewujudkan pengalaman hiburan bagi penonton TV. Isi segar mengenai berbagai topik, orang, dan tempat sedang dibuat dan tersedia di Web dengan kecepatan yang menakjubkan. Menavigasi kandungan segar dengan berkesan di TV menuntut paradigma penyemakan imbas baru yang memerlukan lebih sedikit klik tetikus atau interaksi pengguna dari alat kawalan jauh. Teknik penjelajahan geospasial dan temporal baru disediakan di GeoTV yang memungkinkan pengguna mengagregasi dan menavigasi kandungan berkemampuan RSS secara tepat pada masanya, diperibadikan dan automatik untuk dilihat dalam lingkungan IPTV. Poster ini adalah lanjutan dari karya kami sebelumnya di GeoTracker yang menggunakan representasi geospasial dan persembahan temporal (kronologi) untuk membantu pengguna melihat kemas kini yang paling relevan dengan cepat dalam konteks persekitaran yang diaktifkan oleh Web. Kami menunjukkan 1) kebolehgunaan alat seperti itu yang sangat meningkatkan kemampuan pengguna dalam mencari dan melayari video berdasarkan minat geografinya dan 2) pelbagai reka bentuk antara muka inovatif untuk menunjukkan maklumat yang membolehkan RSS dalam persekitaran IPTV. [[EENNDD]] iptv; penandaan geospatial; rss; blog; multimedia"], [{"string": "Estimating the impressionrank of web pages The ImpressionRank of a web page (or, more generally, of a web site) is the number of times users viewed the page while browsing search results. ImpressionRank captures the visibility of pages and sites in search engines and is thus an important measure, which is of interest to web site owners, competitors, market analysts, and end users.", "keywords": ["data mining", "impressionrank", "auto-completions", "search engines", "information search and retrieval", "suggestions", "estimation,", "popular keyword extraction"], "combined": "Estimating the impressionrank of web pages The ImpressionRank of a web page (or, more generally, of a web site) is the number of times users viewed the page while browsing search results. ImpressionRank captures the visibility of pages and sites in search engines and is thus an important measure, which is of interest to web site owners, competitors, market analysts, and end users. [[EENNDD]] data mining; impressionrank; auto-completions; search engines; information search and retrieval; suggestions; estimation,; popular keyword extraction"}, "Menganggar kedudukan jejak halaman web Peringkat Impression laman web (atau, lebih umum, laman web) adalah frekuensi pengguna melihat halaman semasa melihat hasil carian. ImpressionRank menangkap keterlihatan halaman dan laman web di enjin carian dan dengan demikian merupakan langkah penting, yang menarik bagi pemilik laman web, pesaing, penganalisis pasaran, dan pengguna akhir. [[EENNDD]] perlombongan data; peringkat kesan; penyelesaian automatik; enjin carian; pencarian dan pengambilan maklumat; cadangan; anggaran,; pengekstrakan kata kunci yang popular"], [{"string": "GlobeDB: autonomic data replication for web applications No contact information provided yet.", "keywords": ["data replication", "rerformance", "autonomic replication", "distributed systems", "edge services"], "combined": "GlobeDB: autonomic data replication for web applications No contact information provided yet. [[EENNDD]] data replication; rerformance; autonomic replication; distributed systems; edge services"}, "GlobeDB: replikasi data autonomi untuk aplikasi web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] replikasi data; prestasi semula; replikasi autonomi; sistem yang diedarkan; perkhidmatan tepi"], [{"string": "Estimating the cardinality of RDF graph patterns Most RDF query languages allow for graph structure search through a conjunction of triples which is typically processed using join operations. A key factor in optimizing joins is determining the join order which depends on the expected cardinality of intermediate results. This work proposes a pattern-based summarization framework for estimating the cardinality of RDF graph patterns. We present experiments on real world and synthetic datasets which confirm the feasibility of our approach.", "keywords": ["statistical summaries", "pattern cardinality estimation"], "combined": "Estimating the cardinality of RDF graph patterns Most RDF query languages allow for graph structure search through a conjunction of triples which is typically processed using join operations. A key factor in optimizing joins is determining the join order which depends on the expected cardinality of intermediate results. This work proposes a pattern-based summarization framework for estimating the cardinality of RDF graph patterns. We present experiments on real world and synthetic datasets which confirm the feasibility of our approach. [[EENNDD]] statistical summaries; pattern cardinality estimation"}, "Menganggar kardinaliti corak grafik RDF Sebilangan besar bahasa pertanyaan RDF memungkinkan pencarian struktur grafik melalui gabungan tiga kali ganda yang biasanya diproses menggunakan operasi bergabung. Faktor utama dalam mengoptimumkan penyertaan adalah menentukan urutan bergabung yang bergantung pada jangkaan keputusan hasil pertengahan. Karya ini mencadangkan kerangka ringkasan berasaskan corak untuk menganggar kardinaliti corak grafik RDF. Kami membentangkan eksperimen di dunia nyata dan kumpulan data sintetik yang mengesahkan kemungkinan pendekatan kami. [[EENNDD]] ringkasan statistik; anggaran kardinaliti corak"], [{"string": "On a web browsing support system with 3d visualization No contact information provided yet.", "keywords": ["3d technology", "visualization", "graphical user interfaces", "web browser"], "combined": "On a web browsing support system with 3d visualization No contact information provided yet. [[EENNDD]] 3d technology; visualization; graphical user interfaces; web browser"}, "Pada sistem sokongan penyemakan imbas web dengan visualisasi 3d Belum ada maklumat hubungan yang diberikan. [[EENNDD]] teknologi 3d; visualisasi; antara muka pengguna grafik; pelayar web"], [{"string": "Understanding the function of web elements for mobile content delivery using random walk models No contact information provided yet.", "keywords": ["html", "www", "graphical user interfaces", "classification"], "combined": "Understanding the function of web elements for mobile content delivery using random walk models No contact information provided yet. [[EENNDD]] html; www; graphical user interfaces; classification"}, "Memahami fungsi elemen web untuk penyampaian kandungan mudah alih menggunakan model jalan rawak Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] html; www; antara muka pengguna grafik; pengelasan"], [{"string": "RDFPeers: a scalable distributed RDF repository based on a structured peer-to-peer network No contact information provided yet.", "keywords": ["distributed rdf repositories", "semantic web", "peer-to-peer"], "combined": "RDFPeers: a scalable distributed RDF repository based on a structured peer-to-peer network No contact information provided yet. [[EENNDD]] distributed rdf repositories; semantic web; peer-to-peer"}, "RDFPeers: repositori RDF diedarkan yang berskala berdasarkan rangkaian peer-to-peer berstruktur Belum ada maklumat hubungan yang diberikan. [[EENNDD]] diedarkan repositori rdf; web semantik; rakan sebaya"], [{"string": "Profiles for the situated web No contact information provided yet.", "keywords": ["representations", "cc/pp", "vocabulary", "xml", "profiles", "web architecture", "situated-aware applications"], "combined": "Profiles for the situated web No contact information provided yet. [[EENNDD]] representations; cc/pp; vocabulary; xml; profiles; web architecture; situated-aware applications"}, "Profil untuk laman web yang ada Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perwakilan; cc / pp; perbendaharaan kata; xml; profil; seni bina laman web; aplikasi sedar-sedar"], [{"string": "Modeling the author bias between two on-line computer science citation databases No contact information provided yet.", "keywords": ["acquisition bias", "bibliometrics", "dblp", "miscellaneous", "citeseer"], "combined": "Modeling the author bias between two on-line computer science citation databases No contact information provided yet. [[EENNDD]] acquisition bias; bibliometrics; dblp; miscellaneous; citeseer"}, "Memodelkan kecenderungan penulis antara dua pangkalan data petikan sains komputer dalam talian Belum ada maklumat hubungan yang diberikan. [[EENNDD]] berat sebelah pemerolehan; bibliometrik; dblp; pelbagai; citeseer"], [{"string": "LiteMinutes: an Internet-based system for multimedia meeting minutes An abstract is not available.", "keywords": ["meeting capture", "meeting support systems", "note taking", "html", "computer conferencing, teleconferencing, and videoconferencing", "hypermedia systems", "internet", "video", "video applications", "multimedia applications"], "combined": "LiteMinutes: an Internet-based system for multimedia meeting minutes An abstract is not available. [[EENNDD]] meeting capture; meeting support systems; note taking; html; computer conferencing, teleconferencing, and videoconferencing; hypermedia systems; internet; video; video applications; multimedia applications"}, "LiteMinutes: sistem berasaskan Internet untuk minit mesyuarat multimedia Abstrak tidak tersedia. [[EENNDD]] tangkapan mesyuarat; sistem sokongan mesyuarat; pengambilan nota; html; persidangan komputer, telekonferensi, dan persidangan video; sistem hipermedia; internet; video; aplikasi video; aplikasi multimedia"], [{"string": "Communities from seed sets No contact information provided yet.", "keywords": ["graph conductance", "random walks", "community finding", "link analysis", "seed sets"], "combined": "Communities from seed sets No contact information provided yet. [[EENNDD]] graph conductance; random walks; community finding; link analysis; seed sets"}, "Komuniti dari set benih Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kekonduksian grafik; jalan rawak; penemuan masyarakat; analisis pautan; set biji"], [{"string": "Anycast CDNS revisited Because it is an integral part of the Internet routing apparatus, and because it allows multiple instances of the same service to be \"naturally\" discovered, IP Anycast has many attractive features for any service that involve the replication of multiple instances across the Internet. While briefly considered as an enabler when content distribution networks (CDNs) first emerged, the use of IP Anycast was deemed infeasible in that environment. The main reasons for this decision were the lack of load awareness of IP Anycast and unwanted side effects of Internet routing changes on the IP Anycast mechanism. Prompted by recent developments in route control technology, as well as a better understanding of the behavior of IP Anycast in operational settings, we revisit this decision and propose a load-aware IP Anycast CDN architecture that addresses these concerns while benefiting from inherent IP Anycast features. Our architecture makes use of route control mechanisms to take server and network load into account to realize load-aware Anycast. We show that the resulting redirection requirements can be formulated as a Generalized Assignment Problem and present practical algorithms that address these requirements while at the same time limiting session disruptions that plague regular IP Anycast. We evaluate our algorithms through trace based simulation using traces obtained from an operation CDN network.", "keywords": ["cdn", "routing", "anycast", "miscellaneous", "load balancing", "autonomous system"], "combined": "Anycast CDNS revisited Because it is an integral part of the Internet routing apparatus, and because it allows multiple instances of the same service to be \"naturally\" discovered, IP Anycast has many attractive features for any service that involve the replication of multiple instances across the Internet. While briefly considered as an enabler when content distribution networks (CDNs) first emerged, the use of IP Anycast was deemed infeasible in that environment. The main reasons for this decision were the lack of load awareness of IP Anycast and unwanted side effects of Internet routing changes on the IP Anycast mechanism. Prompted by recent developments in route control technology, as well as a better understanding of the behavior of IP Anycast in operational settings, we revisit this decision and propose a load-aware IP Anycast CDN architecture that addresses these concerns while benefiting from inherent IP Anycast features. Our architecture makes use of route control mechanisms to take server and network load into account to realize load-aware Anycast. We show that the resulting redirection requirements can be formulated as a Generalized Assignment Problem and present practical algorithms that address these requirements while at the same time limiting session disruptions that plague regular IP Anycast. We evaluate our algorithms through trace based simulation using traces obtained from an operation CDN network. [[EENNDD]] cdn; routing; anycast; miscellaneous; load balancing; autonomous system"}, "Anycast CDNS dikaji semula Oleh kerana ia merupakan bahagian yang tidak terpisahkan dari peralatan penghalaan Internet, dan kerana ia membolehkan banyak kejadian perkhidmatan yang sama \"secara semula jadi\" ditemui, IP Anycast mempunyai banyak ciri menarik untuk perkhidmatan apa pun yang melibatkan peniruan beberapa contoh di seluruh Internet. Walaupun secara ringkas dianggap sebagai pemboleh ketika jaringan pengedaran kandungan (CDN) pertama kali muncul, penggunaan IP Anycast dianggap tidak dapat dilaksanakan di lingkungan itu. Sebab utama keputusan ini adalah kurangnya kesedaran beban terhadap Anycast IP dan kesan sampingan perubahan routing Internet yang tidak diingini pada mekanisme IP Anycast. Didorong oleh perkembangan terkini dalam teknologi kawalan laluan, serta pemahaman yang lebih baik mengenai tingkah laku IP Anycast dalam tetapan operasi, kami mengkaji semula keputusan ini dan mencadangkan senibina CD Anycast IPN yang menyedari beban yang menangani kebimbangan ini sambil memanfaatkan ciri-ciri IP Anycast yang wujud . Senibina kami menggunakan mekanisme kawalan laluan untuk mempertimbangkan beban pelayan dan rangkaian untuk mewujudkan Anycast yang menyedari beban. Kami menunjukkan bahawa keperluan pengalihan yang dihasilkan dapat dirumuskan sebagai Masalah Penugasan Umum dan menghadirkan algoritma praktikal yang menangani keperluan ini dan pada masa yang sama membatasi gangguan sesi yang menimpa IP Anycast biasa. Kami menilai algoritma kami melalui simulasi berdasarkan jejak menggunakan jejak yang diperoleh dari rangkaian CDN operasi. [[EENNDD]] cdn; penghalaan; anycast; pelbagai; pengimbangan beban; sistem autonomi"], [{"string": "Semantic Wikipedia No contact information provided yet.", "keywords": ["wiki", "on-line information services", "wikipedia", "semantic web", "rdf"], "combined": "Semantic Wikipedia No contact information provided yet. [[EENNDD]] wiki; on-line information services; wikipedia; semantic web; rdf"}, "Wikipedia Semantik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] wiki; perkhidmatan maklumat dalam talian; wikipedia; web semantik; rdf"], [{"string": "Performing grouping and aggregate functions in XML queries Since more and more business data are represented in XML format, there is a compelling need of supporting analytical operations in XML queries. Particularly, the latest version of XQuery proposed by W3C, XQuery 1.1, introduces a new construct to explicitly express grouping operation in FLWOR expression. Existing works in XML query processing mainly focus on physically matching query structure over XML document. Given the explicit grouping operation in a query, how to efficiently compute grouping and aggregate functions over XML document is not well studied yet. In this paper, we extend our previous XML query processing algorithm, VERT, to efficiently perform grouping and aggregate function in queries. The main technique of our approach is introducing relational tables to index values. Query pattern matching and aggregation computing are both conducted with table indices. We also propose two semantic optimizations to further improve the query performance. Finally we present experimental results to validate the efficiency of our approach, over other existing approaches.", "keywords": ["xml", "grouping", "aggregate function"], "combined": "Performing grouping and aggregate functions in XML queries Since more and more business data are represented in XML format, there is a compelling need of supporting analytical operations in XML queries. Particularly, the latest version of XQuery proposed by W3C, XQuery 1.1, introduces a new construct to explicitly express grouping operation in FLWOR expression. Existing works in XML query processing mainly focus on physically matching query structure over XML document. Given the explicit grouping operation in a query, how to efficiently compute grouping and aggregate functions over XML document is not well studied yet. In this paper, we extend our previous XML query processing algorithm, VERT, to efficiently perform grouping and aggregate function in queries. The main technique of our approach is introducing relational tables to index values. Query pattern matching and aggregation computing are both conducted with table indices. We also propose two semantic optimizations to further improve the query performance. Finally we present experimental results to validate the efficiency of our approach, over other existing approaches. [[EENNDD]] xml; grouping; aggregate function"}, "Melakukan fungsi pengelompokan dan agregat dalam pertanyaan XML Oleh kerana semakin banyak data perniagaan diwakili dalam format XML, ada keperluan yang kuat untuk menyokong operasi analisis dalam pertanyaan XML. Terutama, versi terbaru XQuery yang dicadangkan oleh W3C, XQuery 1.1, memperkenalkan konstruk baru untuk mengekspresikan operasi pengelompokan secara eksplisit dalam ekspresi FLWOR. Kerja-kerja yang ada dalam pemprosesan pertanyaan XML terutamanya memfokuskan pada struktur pertanyaan yang sesuai secara fizikal berbanding dokumen XML. Memandangkan operasi pengelompokan eksplisit dalam pertanyaan, bagaimana cara mengira fungsi pengelompokan dan agregat yang lebih berkesan daripada dokumen XML belum dipelajari dengan baik. Dalam makalah ini, kami memperluas algoritma pemprosesan permintaan XML kami sebelumnya, VERT, untuk melakukan fungsi pengelompokan dan agregat secara berkesan dalam pertanyaan. Teknik utama pendekatan kami adalah memperkenalkan jadual hubungan ke nilai indeks. Pemadanan corak pertanyaan dan pengkomputeran agregasi dilakukan dengan indeks jadual. Kami juga mencadangkan dua pengoptimuman semantik untuk meningkatkan lagi prestasi pertanyaan. Akhirnya kami membentangkan hasil eksperimen untuk mengesahkan kecekapan pendekatan kami, berbanding pendekatan lain yang ada. [[EENNDD]] xml; pengelompokan; fungsi agregat"], [{"string": "Towards practical genre classification of web documents No contact information provided yet.", "keywords": ["genre classification", "term frequency", "linguistic"], "combined": "Towards practical genre classification of web documents No contact information provided yet. [[EENNDD]] genre classification; term frequency; linguistic"}, "Menuju penggolongan genre dokumen web praktikal Belum ada maklumat hubungan yang diberikan. [[EENNDD]] klasifikasi genre; kekerapan istilah; linguistik"], [{"string": "Max algorithms in crowdsourcing environments Our work investigates the problem of retrieving the maximum item from a set in crowdsourcing environments. We first develop parameterized families of max algorithms, that take as input a set of items and output an item from the set that is believed to be the maximum. Such max algorithms could, for instance, select the best Facebook profile that matches a given person or the best photo that describes a given restaurant. Then, we propose strategies that select appropriate max algorithm parameters. Our framework supports various human error and cost models and we consider many of them for our experiments. We evaluate under many metrics, both analytically and via simulations, the tradeoff between three quantities: (1) quality, (2) monetary cost, and (3) execution time. Also, we provide insights on the effectiveness of the strategies in selecting appropriate max algorithm parameters and guidelines for choosing max algorithms and strategies for each application.", "keywords": ["crowdsourcing", "vote aggregation", "plurality voting", "miscellaneous", "max algorithms", "human computation", "worker models"], "combined": "Max algorithms in crowdsourcing environments Our work investigates the problem of retrieving the maximum item from a set in crowdsourcing environments. We first develop parameterized families of max algorithms, that take as input a set of items and output an item from the set that is believed to be the maximum. Such max algorithms could, for instance, select the best Facebook profile that matches a given person or the best photo that describes a given restaurant. Then, we propose strategies that select appropriate max algorithm parameters. Our framework supports various human error and cost models and we consider many of them for our experiments. We evaluate under many metrics, both analytically and via simulations, the tradeoff between three quantities: (1) quality, (2) monetary cost, and (3) execution time. Also, we provide insights on the effectiveness of the strategies in selecting appropriate max algorithm parameters and guidelines for choosing max algorithms and strategies for each application. [[EENNDD]] crowdsourcing; vote aggregation; plurality voting; miscellaneous; max algorithms; human computation; worker models"}, "Algoritma maksimum dalam persekitaran sumber ramai Kerja kami menyiasat masalah pengambilan item maksimum dari satu set di persekitaran sumber ramai. Kami pertama kali mengembangkan keluarga parameter algoritma maksimum, yang mengambil sebagai input sekumpulan item dan mengeluarkan item dari set yang diyakini maksimum. Algoritma maksimum seperti itu, misalnya, dapat memilih profil Facebook terbaik yang sesuai dengan orang tertentu atau foto terbaik yang menggambarkan restoran tertentu. Kemudian, kami mencadangkan strategi yang memilih parameter algoritma maksimum yang sesuai. Rangka kerja kami menyokong pelbagai model kesalahan dan kos manusia dan kami menganggap banyak daripadanya untuk eksperimen kami. Kami menilai di bawah banyak metrik, baik secara analitis maupun melalui simulasi, pertukaran antara tiga kuantiti: (1) kualiti, (2) kos wang, dan (3) masa pelaksanaan. Juga, kami memberikan pandangan mengenai keberkesanan strategi dalam memilih parameter dan garis panduan maksimum algoritma yang sesuai untuk memilih algoritma dan strategi maksimum untuk setiap aplikasi. [[EENNDD]] sumber ramai; pengumpulan undi; pengundian jamak; pelbagai; algoritma maksimum; pengiraan manusia; model pekerja"], [{"string": "A web personalization system based on web usage mining techniques No contact information provided yet.", "keywords": ["web usage mining", "clustering", "web personalization"], "combined": "A web personalization system based on web usage mining techniques No contact information provided yet. [[EENNDD]] web usage mining; clustering; web personalization"}, "Sistem personalisasi web berdasarkan teknik perlombongan penggunaan web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlombongan penggunaan web; pengelompokan; pemperibadian web"], [{"string": "A personalized search engine based on web-snippet hierarchical clustering No contact information provided yet.", "keywords": ["content analysis and indexing", "on-line information services", "information extraction", "search engines", "information search and retrieval", "web snippets clustering", "new search applications and interfaces", "personalized web ranking", "clustering"], "combined": "A personalized search engine based on web-snippet hierarchical clustering No contact information provided yet. [[EENNDD]] content analysis and indexing; on-line information services; information extraction; search engines; information search and retrieval; web snippets clustering; new search applications and interfaces; personalized web ranking; clustering"}, "Enjin carian yang diperibadikan berdasarkan pengelompokan hierarki potongan web Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] analisis dan pengindeksan kandungan; perkhidmatan maklumat dalam talian; pengekstrakan maklumat; enjin carian; pencarian dan pengambilan maklumat; pengelompokan coretan web; aplikasi dan antara muka carian baru; kedudukan laman web yang diperibadikan; pengelompokan"], [{"string": "Answering approximate queries over autonomous web databases To deal with the problem of empty or too little answers returned from a Web database in response to a user query, this paper proposes a novel approach to provide relevant and ranked query results. Based on the user original query, we speculate how much the user cares about each specified attribute and assign a corresponding weight to it. This original query is then rewritten as an approximate query by relaxing the query criteria range. The relaxation order of all specified attributes and the relaxed degree on each specified attribute are varied with the attribute weights. For the approximate query results, we generate users' contextual preferences from database workload and use them to create a priori orders of tuples in an off-line preprocessing step. Only a few representative orders are saved, each corresponding to a set of contexts. Then, these orders and associated contexts are used at query time to expeditiously provide ranked answers. Results of a preliminary user study demonstrate that our query relaxation and results ranking methods can capture the user's preferences effectively. The efficiency and effectiveness of our approach is also demonstrated by experimental result.", "keywords": ["top-k.", "web database", "query relaxation", "query results ranking"], "combined": "Answering approximate queries over autonomous web databases To deal with the problem of empty or too little answers returned from a Web database in response to a user query, this paper proposes a novel approach to provide relevant and ranked query results. Based on the user original query, we speculate how much the user cares about each specified attribute and assign a corresponding weight to it. This original query is then rewritten as an approximate query by relaxing the query criteria range. The relaxation order of all specified attributes and the relaxed degree on each specified attribute are varied with the attribute weights. For the approximate query results, we generate users' contextual preferences from database workload and use them to create a priori orders of tuples in an off-line preprocessing step. Only a few representative orders are saved, each corresponding to a set of contexts. Then, these orders and associated contexts are used at query time to expeditiously provide ranked answers. Results of a preliminary user study demonstrate that our query relaxation and results ranking methods can capture the user's preferences effectively. The efficiency and effectiveness of our approach is also demonstrated by experimental result. [[EENNDD]] top-k.; web database; query relaxation; query results ranking"}, "Menjawab kira-kira pertanyaan melalui pangkalan data web autonomi Untuk menangani masalah jawapan kosong atau terlalu sedikit yang dikembalikan dari pangkalan data Web sebagai tindak balas kepada pertanyaan pengguna, makalah ini mencadangkan pendekatan baru untuk memberikan hasil pertanyaan yang relevan dan diperingkat. Berdasarkan pertanyaan asal pengguna, kami membuat spekulasi tentang bagaimana pengguna mengambil berat setiap atribut yang ditentukan dan memberikan bobot yang sesuai dengannya. Pertanyaan asal ini kemudian ditulis semula sebagai pertanyaan anggaran dengan melonggarkan julat kriteria pertanyaan. Urutan relaksasi semua atribut yang ditentukan dan tahap santai pada setiap atribut yang ditentukan berbeza dengan berat atribut. Untuk hasil carian anggaran, kami menjana preferensi kontekstual pengguna dari beban kerja pangkalan data dan menggunakannya untuk membuat pesanan apriori tupel dalam langkah praprosesan luar talian. Hanya beberapa pesanan wakil yang disimpan, masing-masing sesuai dengan sekumpulan konteks. Kemudian, pesanan dan konteks yang berkaitan ini digunakan pada waktu pertanyaan untuk memberikan jawapan yang diperingkat. Hasil kajian pengguna awal menunjukkan bahawa kelonggaran pertanyaan kami dan kaedah pemeringkatan keputusan dapat menangkap pilihan pengguna dengan berkesan. Kecekapan dan keberkesanan pendekatan kami juga ditunjukkan oleh hasil eksperimen. [[EENNDD]] top-k .; pangkalan data web; pertanyaan kelonggaran; kedudukan keputusan pertanyaan"], [{"string": "Race: finding and ranking compact connected trees for keyword proximity search over xml documents In this paper, we study the problem of keyword proximity search over XML documents and leverage the efficiency and effectiveness. We take the disjunctive semantics among input keywords into consideration and identify meaningful compact connected trees as the answers of keyword proximity queries. We introduce the notions of Compact Lowest Common Ancestor (CLCA) and Maximal CLCA (MCLCA) and propose Compact Connected Trees (CCTrees) and Maximal CCTrees (MCCTrees) to efficiently and effectively answer keyword queries. We propose a novel ranking mechanism, RACE, to Rank compAct Connected trEes, by taking into consideration both the structural similarity and the textual similarity. Our extensive experimental study shows that our method achieves both high search efficiency and effectiveness, and outperforms existing approaches significantly.", "keywords": ["compact lca", "lowest common ancestor", "maximal clca"], "combined": "Race: finding and ranking compact connected trees for keyword proximity search over xml documents In this paper, we study the problem of keyword proximity search over XML documents and leverage the efficiency and effectiveness. We take the disjunctive semantics among input keywords into consideration and identify meaningful compact connected trees as the answers of keyword proximity queries. We introduce the notions of Compact Lowest Common Ancestor (CLCA) and Maximal CLCA (MCLCA) and propose Compact Connected Trees (CCTrees) and Maximal CCTrees (MCCTrees) to efficiently and effectively answer keyword queries. We propose a novel ranking mechanism, RACE, to Rank compAct Connected trEes, by taking into consideration both the structural similarity and the textual similarity. Our extensive experimental study shows that our method achieves both high search efficiency and effectiveness, and outperforms existing approaches significantly. [[EENNDD]] compact lca; lowest common ancestor; maximal clca"}, "Perlumbaan: mencari dan menyusun pepohonan yang bersambung padat untuk carian kedekatan kata kunci berbanding dokumen xml Dalam makalah ini, kami mengkaji masalah pencarian jarak dekat kata kunci berbanding dokumen XML dan memanfaatkan kecekapan dan keberkesanannya. Kami mengambil pertimbangan semantik di antara kata kunci input dan mengenal pasti pokok yang bersambung padat yang bermakna sebagai jawapan bagi pertanyaan jarak dekat kata kunci. Kami memperkenalkan konsep Compact Lowest Common Ancestor (CLCA) dan Maximal CLCA (MCLCA) dan mencadangkan Compact Connected Pohon (CCTrees) dan Maximal CCTrees (MCCTrees) untuk menjawab pertanyaan kata kunci dengan cekap dan berkesan. Kami mencadangkan mekanisme pemeringkatan baru, RACE, kepada Rank compAct Connected trEes, dengan mempertimbangkan persamaan struktur dan persamaan teks. Kajian eksperimental kami yang luas menunjukkan bahawa kaedah kami mencapai kecekapan dan keberkesanan carian yang tinggi, dan mengatasi pendekatan yang ada dengan ketara. [[EENNDD]] lca padat; nenek moyang paling rendah; maksimum clca"], [{"string": "k-Centralities: local approximations of global measures based on shortest paths A lot of centrality measures have been developed to analyze different aspects of importance. Some of the most popular centrality measures (e.g. betweenness centrality, closeness centrality) are based on the calculation of shortest paths. This characteristic limits the applicability of these measures for larger networks. In this article we elaborate on the idea of bounded-distance shortest paths calculations. We claim criteria for k-centrality measures and we introduce one algorithm for calculating both betweenness and closeness based centralities. We also present normalizations for these measures. We show that k-centrality measures are good approximations for the corresponding centrality measures by achieving a tremendous gain of calculation time and also having linear calculation complexity O(n) for networks with constant average degree. This allows researchers to approximate centrality measures based on shortest paths for networks with millions of nodes or with high frequency in dynamically changing networks.", "keywords": ["graph theory", "closeness centrality", "centrality approximation", "large networks", "betweenness centrality", "shortest paths"], "combined": "k-Centralities: local approximations of global measures based on shortest paths A lot of centrality measures have been developed to analyze different aspects of importance. Some of the most popular centrality measures (e.g. betweenness centrality, closeness centrality) are based on the calculation of shortest paths. This characteristic limits the applicability of these measures for larger networks. In this article we elaborate on the idea of bounded-distance shortest paths calculations. We claim criteria for k-centrality measures and we introduce one algorithm for calculating both betweenness and closeness based centralities. We also present normalizations for these measures. We show that k-centrality measures are good approximations for the corresponding centrality measures by achieving a tremendous gain of calculation time and also having linear calculation complexity O(n) for networks with constant average degree. This allows researchers to approximate centrality measures based on shortest paths for networks with millions of nodes or with high frequency in dynamically changing networks. [[EENNDD]] graph theory; closeness centrality; centrality approximation; large networks; betweenness centrality; shortest paths"}, "k-Pusat: pendekatan tempatan mengenai langkah global berdasarkan jalan terpendek Banyak langkah sentral telah dikembangkan untuk menganalisis aspek kepentingan yang berbeza. Beberapa ukuran sentraliti yang paling popular (mis. Sentralitas antara, sentraliti jarak dekat) didasarkan pada pengiraan jalan terpendek. Karakteristik ini membatasi penerapan langkah-langkah ini untuk rangkaian yang lebih besar. Dalam artikel ini kami menghuraikan idea pengiraan jalan terpendek jarak terikat. Kami menuntut kriteria untuk ukuran k-sentraliti dan kami memperkenalkan satu algoritma untuk mengira kedua-dua pusat antara kedekatan dan kedekatan. Kami juga menunjukkan normalisasi untuk langkah-langkah ini. Kami menunjukkan bahawa ukuran k-sentraliti adalah perkiraan yang baik untuk langkah-langkah sentraliti yang sesuai dengan mencapai keuntungan waktu pengiraan yang luar biasa dan juga mempunyai kerumitan pengiraan linear O (n) untuk rangkaian dengan darjah purata tetap. Ini membolehkan para penyelidik menghitung ukuran sentral berdasarkan jalan terpendek untuk rangkaian dengan berjuta-juta nod atau dengan frekuensi tinggi dalam rangkaian yang berubah secara dinamik. [[EENNDD]] teori grafik; pusat jarak dekat; penghampiran pusat; rangkaian besar; sentraliti antara; jalan terpendek"], [{"string": "Bringing communities to the semantic web and the semantic web to communities No contact information provided yet.", "keywords": ["case study", "communities", "group and organization interfaces", "semantic web", "e-applications"], "combined": "Bringing communities to the semantic web and the semantic web to communities No contact information provided yet. [[EENNDD]] case study; communities; group and organization interfaces; semantic web; e-applications"}, "Membawa komuniti ke web semantik dan web semantik ke komuniti Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kajian kes; komuniti; antara muka kumpulan dan organisasi; web semantik; e-aplikasi"], [{"string": "Using visual cues for extraction of tabular data from arbitrary HTML documents No contact information provided yet.", "keywords": ["systems and software", "web information extraction", "document capture", "visual analysis", "table detection"], "combined": "Using visual cues for extraction of tabular data from arbitrary HTML documents No contact information provided yet. [[EENNDD]] systems and software; web information extraction; document capture; visual analysis; table detection"}, "Menggunakan petunjuk visual untuk pengekstrakan data tabular dari dokumen HTML sewenang-wenangnya Belum ada maklumat hubungan yang diberikan. [[EENNDD]] sistem dan perisian; pengekstrakan maklumat web; tangkapan dokumen; analisis visual; pengesanan jadual"], [{"string": "Efficient query subscription processing for prospective search engines No contact information provided yet.", "keywords": ["query processing", "inverted index", "prospective search"], "combined": "Efficient query subscription processing for prospective search engines No contact information provided yet. [[EENNDD]] query processing; inverted index; prospective search"}, "Pemprosesan langganan pertanyaan yang cekap untuk calon enjin carian Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pemprosesan pertanyaan; indeks terbalik; carian prospektif"], [{"string": "Towards the theoretical foundation of choreography With the growth of interest on the web services, people pay increasinglyattention to the choreography, that is, to describe collaborations ofparticipants in accomplishing a common business goal from a globalviewpoint. In this paper, based on a simple choreography language and arole-oriented process language, we study some fundamental issues relatedto choreography, especially those related to implementation, includingsemantics, projection and natural projection, dominant role in choices anditerations, etc. We propose the concept of dominant role and somenovel languages structures related to it. The study reveals some cluesabout the language, the semantics, the specification and theimplementation of choreography.", "keywords": ["dominated loop", "implementation", "projection", "dominated choice", "dominant role", "choreography"], "combined": "Towards the theoretical foundation of choreography With the growth of interest on the web services, people pay increasinglyattention to the choreography, that is, to describe collaborations ofparticipants in accomplishing a common business goal from a globalviewpoint. In this paper, based on a simple choreography language and arole-oriented process language, we study some fundamental issues relatedto choreography, especially those related to implementation, includingsemantics, projection and natural projection, dominant role in choices anditerations, etc. We propose the concept of dominant role and somenovel languages structures related to it. The study reveals some cluesabout the language, the semantics, the specification and theimplementation of choreography. [[EENNDD]] dominated loop; implementation; projection; dominated choice; dominant role; choreography"}, "Menuju landasan teoritis koreografi Dengan meningkatnya minat pada perkhidmatan web, orang semakin memperhatikan koreografi, yaitu, untuk menggambarkan kolaborasi para peserta dalam mencapai tujuan perniagaan bersama dari sudut pandang global. Dalam makalah ini, berdasarkan bahasa koreografi sederhana dan bahasa proses yang berorientasi arole, kami mengkaji beberapa masalah mendasar yang berkaitan dengan koreografi, terutama yang berkaitan dengan pelaksanaan, termasuk ilmiah, unjuran dan unjuran semula jadi, peranan dominan dalam pilihan dan pengulangan, dll. Kami mencadangkan konsep peranan dominan dan struktur bahasa somenovel yang berkaitan dengannya. Kajian ini mengungkapkan beberapa petunjuk mengenai bahasa, semantik, spesifikasi dan pelaksanaan koreografi. [[EENNDD]] gelung yang dikuasai; pelaksanaan; unjuran; pilihan yang dikuasai; peranan dominan; koreografi"], [{"string": "Fully automatic wrapper generation for search engines No contact information provided yet.", "keywords": ["search engine", "information extraction", "wrapper generation"], "combined": "Fully automatic wrapper generation for search engines No contact information provided yet. [[EENNDD]] search engine; information extraction; wrapper generation"}, "Penjanaan pembungkus automatik sepenuhnya untuk enjin carian Belum ada maklumat hubungan yang diberikan. [[EENNDD]] enjin carian; pengekstrakan maklumat; penghasilan pembalut"], [{"string": "Need for non-visual feedback with long response times in mobile HCI No contact information provided yet.", "keywords": ["multimodal feedback", "mobile web", "mobility", "usability", "attention"], "combined": "Need for non-visual feedback with long response times in mobile HCI No contact information provided yet. [[EENNDD]] multimodal feedback; mobile web; mobility; usability; attention"}, "Perlu maklum balas bukan visual dengan masa tindak balas yang panjang dalam HCI mudah alih Belum ada maklumat hubungan yang diberikan. [[EENNDD]] maklum balas multimodal; web mudah alih; mobiliti; kebolehgunaan; perhatian"], [{"string": "TJFast: effective processing of XML twig pattern matching No contact information provided yet.", "keywords": ["holistic twig join", "labeling scheme"], "combined": "TJFast: effective processing of XML twig pattern matching No contact information provided yet. [[EENNDD]] holistic twig join; labeling scheme"}, "TJFast: pemprosesan berkesan pemadanan corak ranting XML Belum ada maklumat hubungan yang diberikan. [[EENNDD]] ranting holistik bergabung; skema pelabelan"], [{"string": "A unified constraint model for XML An abstract is not available.", "keywords": ["constraint reasoning", "keys", "xml schema", "integrity constraints", "subtyping", "object identity", "xml"], "combined": "A unified constraint model for XML An abstract is not available. [[EENNDD]] constraint reasoning; keys; xml schema; integrity constraints; subtyping; object identity; xml"}, "Model kekangan bersatu untuk XML Abstrak tidak tersedia. [[EENNDD]] penaakulan kekangan; kunci; skema xml; kekangan integriti; subjenis; identiti objek; xml"], [{"string": "Fast dynamic reranking in large graphs In this paper we consider the problem of re-ranking search results by incorporating user feedback. We present a graph theoretic measure for discriminating irrelevant results from relevant results using a few labeled examples provided by the user. The key intuition is that nodes relatively closer (in graph topology) to the relevant nodes than the irrelevant nodes are more likely to be relevant. We present a simple sampling algorithm to evaluate this measure at specific nodes of interest, and an efficient branch and bound algorithm to compute the top k nodes from the entire graph under this measure. On quantifiable prediction tasks the introduced measure outperforms other diffusion-based proximity measures which take only the positive relevance feedback into account. On the Entity-Relation graph built from the authors and papers of the entire DBLP citation corpus (1.4 million nodes and 2.2 million edges) our branch and bound algorithm takes about 1.5 seconds to retrieve the top 10 nodes w.r.t. this measure with 10 labeled nodes.", "keywords": ["semi-supervised learning", "harmonic function", "search", "random walk"], "combined": "Fast dynamic reranking in large graphs In this paper we consider the problem of re-ranking search results by incorporating user feedback. We present a graph theoretic measure for discriminating irrelevant results from relevant results using a few labeled examples provided by the user. The key intuition is that nodes relatively closer (in graph topology) to the relevant nodes than the irrelevant nodes are more likely to be relevant. We present a simple sampling algorithm to evaluate this measure at specific nodes of interest, and an efficient branch and bound algorithm to compute the top k nodes from the entire graph under this measure. On quantifiable prediction tasks the introduced measure outperforms other diffusion-based proximity measures which take only the positive relevance feedback into account. On the Entity-Relation graph built from the authors and papers of the entire DBLP citation corpus (1.4 million nodes and 2.2 million edges) our branch and bound algorithm takes about 1.5 seconds to retrieve the top 10 nodes w.r.t. this measure with 10 labeled nodes. [[EENNDD]] semi-supervised learning; harmonic function; search; random walk"}, "Penarafan dinamik yang cepat dalam grafik besar Dalam makalah ini kami mempertimbangkan masalah penentuan semula keputusan carian dengan memasukkan maklum balas pengguna. Kami membentangkan ukuran teori grafik untuk membezakan hasil yang tidak relevan dari hasil yang relevan menggunakan beberapa contoh berlabel yang diberikan oleh pengguna. Intuisi utama adalah bahawa node relatif lebih dekat (dalam topologi grafik) ke nod yang relevan daripada nod yang tidak berkaitan lebih cenderung relevan. Kami membentangkan algoritma pensampelan sederhana untuk menilai ukuran ini pada node minat tertentu, dan algoritma cabang dan terikat yang cekap untuk mengira node k teratas dari keseluruhan graf di bawah ukuran ini. Pada tugas-tugas ramalan yang dapat diukur, ukuran yang diperkenalkan mengatasi tindakan jarak berdasarkan penyebaran lain yang hanya mengambil kira maklum balas relevan positif. Pada grafik Entity-Relation yang dibina daripada pengarang dan makalah keseluruhan korpus petikan DBLP (1.4 juta nod dan 2.2 juta tepi) algoritma cawangan dan terikat kami mengambil masa kira-kira 1.5 saat untuk mendapatkan 10 node teratas w.r.t. ukuran ini dengan 10 nod berlabel. [[EENNDD]] pembelajaran separa penyeliaan; fungsi harmonik; cari; jalan rawak"], [{"string": "Finding visual concepts by web image mining No contact information provided yet.", "keywords": ["web image mining", "probabilistic method", "image recognition", "image processing and computer vision"], "combined": "Finding visual concepts by web image mining No contact information provided yet. [[EENNDD]] web image mining; probabilistic method; image recognition; image processing and computer vision"}, "Mencari konsep visual dengan perlombongan gambar web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlombongan imej web; kaedah probabilistik; pengecaman gambar; pemprosesan gambar dan penglihatan komputer"], [{"string": "A non-syntactic approach for text sentiment classification with stopwords The present approach uses stopwords and the gaps that occur between successive stopwords -formed by contentwords- as features for sentiment classification.", "keywords": ["latent dirichlet allocation", "stop-words", "text and language applications", "topic models", "sentiment mining"], "combined": "A non-syntactic approach for text sentiment classification with stopwords The present approach uses stopwords and the gaps that occur between successive stopwords -formed by contentwords- as features for sentiment classification. [[EENNDD]] latent dirichlet allocation; stop-words; text and language applications; topic models; sentiment mining"}, "Pendekatan non-sintaksis untuk klasifikasi sentimen teks dengan kata kunci. Pendekatan ini menggunakan kata hubung dan jurang yang berlaku antara kata putus berturut-turut yang dibentuk oleh kata-kata kandungan- sebagai ciri untuk klasifikasi sentimen. [[EENNDD]] peruntukan laten laten; kata berhenti; aplikasi teks dan bahasa; model topik; perlombongan sentimen"], [{"string": "Enhancing web search with entity intent Web entities, such as documents and hyperlinks, are created for different purposes, or intents. Existing intent-based retrieval methods largely focus on information seekers' intent expressed by queries, ignoring the other side of the problem: web content creators' intent. We argue that understanding why the content was created is also important. In this work, we propose to classify such intents into two broad categories: \"navigational\" and \"informational\". Then we incorporate such intents into traditional retrieval models, and show their effect on ranking performance.", "keywords": ["web entity intent", "information search and retrieval", "query intent", "retrieval model"], "combined": "Enhancing web search with entity intent Web entities, such as documents and hyperlinks, are created for different purposes, or intents. Existing intent-based retrieval methods largely focus on information seekers' intent expressed by queries, ignoring the other side of the problem: web content creators' intent. We argue that understanding why the content was created is also important. In this work, we propose to classify such intents into two broad categories: \"navigational\" and \"informational\". Then we incorporate such intents into traditional retrieval models, and show their effect on ranking performance. [[EENNDD]] web entity intent; information search and retrieval; query intent; retrieval model"}, "Meningkatkan carian web dengan maksud entiti Entiti web, seperti dokumen dan hyperlink, dibuat untuk tujuan atau maksud yang berbeza. Kaedah pengambilan berdasarkan niat yang ada sebahagian besarnya menumpukan pada maksud pencari maklumat yang dinyatakan oleh pertanyaan, mengabaikan sisi lain dari masalah: maksud pencipta kandungan web. Kami berpendapat bahawa memahami mengapa kandungan dibuat juga penting. Dalam karya ini, kami mencadangkan untuk mengklasifikasikan maksud tersebut kepada dua kategori luas: \"navigasi\" dan \"maklumat\". Kemudian kami memasukkan niat sedemikian ke dalam model pengambilan tradisional, dan menunjukkan kesannya terhadap prestasi peringkat. [[EENNDD]] maksud entiti web; pencarian dan pengambilan maklumat; maksud pertanyaan; model pengambilan"], [{"string": "Mining for personal name aliases on the web We propose a novel approach to find aliases of a given name from the web. We exploit a set of known names and their aliases as training data and extract lexical patterns that convey information related to aliases of names from text snippets returned by a web search engine. The patterns are then used to find candidate aliases of a given name. We use anchor texts and hyperlinks to design a word co-occurrence model and define numerous ranking scores to evaluate the association between a name and its candidate aliases. The proposed method outperforms numerous baselines and previous work on alias extraction on a dataset of personal names, achieving a statistically significant mean reciprocal rank of 0.6718. Moreover, the aliases extracted using the proposed method improve recall by 20% in a relation-detection task.", "keywords": ["name alias extraction", "web mining", "semantic web"], "combined": "Mining for personal name aliases on the web We propose a novel approach to find aliases of a given name from the web. We exploit a set of known names and their aliases as training data and extract lexical patterns that convey information related to aliases of names from text snippets returned by a web search engine. The patterns are then used to find candidate aliases of a given name. We use anchor texts and hyperlinks to design a word co-occurrence model and define numerous ranking scores to evaluate the association between a name and its candidate aliases. The proposed method outperforms numerous baselines and previous work on alias extraction on a dataset of personal names, achieving a statistically significant mean reciprocal rank of 0.6718. Moreover, the aliases extracted using the proposed method improve recall by 20% in a relation-detection task. [[EENNDD]] name alias extraction; web mining; semantic web"}, "Perlombongan nama samaran nama peribadi di web Kami mencadangkan pendekatan baru untuk mencari alias nama tertentu dari web. Kami memanfaatkan sekumpulan nama yang dikenali dan aliasnya sebagai data latihan dan mengekstrak corak leksikal yang menyampaikan maklumat yang berkaitan dengan alias nama dari potongan teks yang dikembalikan oleh mesin carian web. Corak kemudian digunakan untuk mencari alias calon dengan nama tertentu. Kami menggunakan teks jangkar dan pautan hiper untuk merancang model kejadian bersama dan menentukan banyak skor peringkat untuk menilai perkaitan antara nama dan alias calonnya. Kaedah yang dicadangkan mengatasi banyak garis dasar dan karya sebelumnya mengenai pengekstrakan alias pada set data nama peribadi, mencapai kedudukan timbal balik min yang signifikan secara statistik dari 0.6718. Lebih-lebih lagi, alias yang diekstrak menggunakan kaedah yang dicadangkan meningkatkan penarikan sebanyak 20% dalam tugas pengesanan hubungan. [[EENNDD]] pengekstrakan alias nama; perlombongan web; web semantik"], [{"string": "Game theoretic models for social network analysis The existing methods and techniques for social network analysis are inadequate to capture both the behavior (such as rationality and intelligence) of individuals and the strategic interactions that occur among these individuals. Game theory is a natural tool to overcome this inadequacy since it provides rigorous mathematical models of strategic interaction among autonomous, intelligent, and rational agents. Motivated by the above observation, this tutorial provides the conceptual underpinnings of the use of game theoretic models in social network analysis. In the first part of the tutorial, we provide rigorous foundations of relevant concepts in game theory and social network analysis. In the second part of the tutorial, we present a comprehensive study of four contemporary and pertinent problems in social networks: social network formation, determining in influential individuals for viral marketing, query incentive networks, and community detection.", "keywords": ["game theory", "viral marketing", "query incentive networks", "social networks", "network formation", "community detection"], "combined": "Game theoretic models for social network analysis The existing methods and techniques for social network analysis are inadequate to capture both the behavior (such as rationality and intelligence) of individuals and the strategic interactions that occur among these individuals. Game theory is a natural tool to overcome this inadequacy since it provides rigorous mathematical models of strategic interaction among autonomous, intelligent, and rational agents. Motivated by the above observation, this tutorial provides the conceptual underpinnings of the use of game theoretic models in social network analysis. In the first part of the tutorial, we provide rigorous foundations of relevant concepts in game theory and social network analysis. In the second part of the tutorial, we present a comprehensive study of four contemporary and pertinent problems in social networks: social network formation, determining in influential individuals for viral marketing, query incentive networks, and community detection. [[EENNDD]] game theory; viral marketing; query incentive networks; social networks; network formation; community detection"}, "Model teori permainan untuk analisis rangkaian sosial Kaedah dan teknik yang ada untuk analisis rangkaian sosial tidak mencukupi untuk menangkap kedua-dua tingkah laku (seperti rasionaliti dan kecerdasan) individu dan interaksi strategik yang berlaku di antara individu-individu ini. Teori permainan adalah alat semula jadi untuk mengatasi kekurangan ini kerana ia menyediakan model matematik interaksi strategik antara agen autonomi, pintar, dan rasional. Didorong oleh pemerhatian di atas, tutorial ini memberikan asas konsep penggunaan model teori permainan dalam analisis rangkaian sosial. Pada bahagian pertama tutorial, kami menyediakan asas konsep yang relevan dalam teori permainan dan analisis rangkaian sosial. Pada bahagian kedua tutorial, kami menyajikan kajian komprehensif mengenai empat masalah kontemporari dan berkaitan dalam rangkaian sosial: pembentukan rangkaian sosial, menentukan individu berpengaruh untuk pemasaran virus, rangkaian insentif pertanyaan, dan pengesanan masyarakat. [[EENNDD]] teori permainan; pemasaran viral; rangkaian insentif pertanyaan; rangkaian sosial; pembentukan rangkaian; pengesanan masyarakat"], [{"string": "SafeVchat: detecting obscene content and misbehaving users in online video chat services Online video chat services such as Chatroulette, Omegle, and vChatter that randomly match pairs of users in video chat sessions are fast becoming very popular, with over a million users per month in the case of Chatroulette. A key problem encountered in such systems is the presence of flashers and obscene content. This problem is especially acute given the presence of underage minors in such systems. This paper presents SafeVchat, a novel solution to the problem of flasher detection that employs an array of image detection algorithms. A key contribution of the paper concerns how the results of the individual detectors are fused together into an overall decision classifying the user as misbehaving or not, based on Dempster-Shafer Theory. The paper introduces a novel, motion-based skin detection method that achieves significantly higher recall and better precision. The proposed methods have been evaluated over real-world data and image traces obtained from Chatroulette.com.", "keywords": ["chatroulette", "obscene content detection", "online video chat"], "combined": "SafeVchat: detecting obscene content and misbehaving users in online video chat services Online video chat services such as Chatroulette, Omegle, and vChatter that randomly match pairs of users in video chat sessions are fast becoming very popular, with over a million users per month in the case of Chatroulette. A key problem encountered in such systems is the presence of flashers and obscene content. This problem is especially acute given the presence of underage minors in such systems. This paper presents SafeVchat, a novel solution to the problem of flasher detection that employs an array of image detection algorithms. A key contribution of the paper concerns how the results of the individual detectors are fused together into an overall decision classifying the user as misbehaving or not, based on Dempster-Shafer Theory. The paper introduces a novel, motion-based skin detection method that achieves significantly higher recall and better precision. The proposed methods have been evaluated over real-world data and image traces obtained from Chatroulette.com. [[EENNDD]] chatroulette; obscene content detection; online video chat"}, "SafeVchat: mengesan kandungan lucah dan tidak berperilaku pengguna dalam perkhidmatan sembang video dalam talian Perkhidmatan sembang video dalam talian seperti Chatroulette, Omegle, dan vChatter yang memadankan pasangan pengguna secara rawak dalam sesi sembang video menjadi sangat popular, dengan lebih dari satu juta pengguna setiap bulan di kes Chatroulette. Masalah utama yang dihadapi dalam sistem tersebut adalah adanya flashers dan kandungan lucah. Masalah ini amat teruk kerana kehadiran kanak-kanak di bawah umur dalam sistem sedemikian. Makalah ini menyajikan SafeVchat, penyelesaian baru untuk masalah pengesanan flasher yang menggunakan pelbagai algoritma pengesanan gambar. Sumbangan utama makalah berkenaan bagaimana hasil pengesan individu digabungkan menjadi keputusan keseluruhan yang mengklasifikasikan pengguna sebagai tidak berperilaku atau tidak, berdasarkan Teori Dempster-Shafer. Makalah ini memperkenalkan kaedah pengesanan kulit berdasarkan gerakan yang mencapai penarikan yang lebih tinggi dan ketepatan yang lebih baik. Kaedah yang dicadangkan telah dinilai berdasarkan jejak data dan gambar dunia nyata yang diperoleh dari Chatroulette.com. [[EENNDD]] sembang; pengesanan kandungan lucah; sembang video dalam talian"], [{"string": "CONQUER: a system for efficient context-aware query suggestions Many of today's search engines provide autocompletion while the user is typing a query string. This type of dynamic query suggestion can help users to formulate queries that better represent their search intent during Web search interactions. In this paper, we demonstrate our query suggestion system called CONQUER, which allows to efficiently suggest queries for a given partial query and a number of available query context observations. The context-awareness allows for suggesting queries tailored to a given context, e.g., the user location or the time of day. CONQUER uses a suggestion model that is based on the combined probabilities of sequential query patterns and context observations. For this, the weight of a context in a query suggestion can be adjusted online, for example, based on the learned user behavior or user profiles. We demonstrate the functionality of CONQUER based on 6 million queries from an AOL query log using the time of day and the country domain of the clicked URLs in the search result as context observations.", "keywords": ["dynamic query suggestion", "query context"], "combined": "CONQUER: a system for efficient context-aware query suggestions Many of today's search engines provide autocompletion while the user is typing a query string. This type of dynamic query suggestion can help users to formulate queries that better represent their search intent during Web search interactions. In this paper, we demonstrate our query suggestion system called CONQUER, which allows to efficiently suggest queries for a given partial query and a number of available query context observations. The context-awareness allows for suggesting queries tailored to a given context, e.g., the user location or the time of day. CONQUER uses a suggestion model that is based on the combined probabilities of sequential query patterns and context observations. For this, the weight of a context in a query suggestion can be adjusted online, for example, based on the learned user behavior or user profiles. We demonstrate the functionality of CONQUER based on 6 million queries from an AOL query log using the time of day and the country domain of the clicked URLs in the search result as context observations. [[EENNDD]] dynamic query suggestion; query context"}, "CONQUER: sistem untuk cadangan pertanyaan yang peka dengan konteks yang cekap. Sebilangan besar enjin carian hari ini menyediakan pelengkap automatik semasa pengguna menaip rentetan pertanyaan. Jenis cadangan pertanyaan dinamik ini dapat membantu pengguna merumuskan pertanyaan yang lebih mewakili maksud carian mereka semasa interaksi carian Web. Dalam makalah ini, kami menunjukkan sistem cadangan pertanyaan kami yang disebut CONQUER, yang memungkinkan untuk mencadangkan pertanyaan dengan berkesan untuk pertanyaan separa tertentu dan sejumlah pemerhatian konteks pertanyaan yang tersedia. Kesedaran konteks memungkinkan untuk mencadangkan pertanyaan yang disesuaikan dengan konteks tertentu, mis., Lokasi pengguna atau waktu dalam sehari. CONQUER menggunakan model cadangan yang berdasarkan gabungan kebarangkalian corak pertanyaan berurutan dan pemerhatian konteks. Untuk ini, berat konteks dalam cadangan pertanyaan dapat disesuaikan secara dalam talian, misalnya, berdasarkan tingkah laku pengguna atau profil pengguna yang dipelajari. Kami menunjukkan fungsi CONQUER berdasarkan 6 juta pertanyaan dari log pertanyaan AOL menggunakan waktu dan domain negara dari URL yang diklik dalam hasil carian sebagai pemerhatian konteks. [[EENNDD]] cadangan pertanyaan dinamik; konteks pertanyaan"], [{"string": "Using proxy cache relocation to accelerate Web browsing in wireless/mobile communications An abstract is not available.", "keywords": ["learning automaton", "cache relocation", "proxy cache", "mobile computing", "w4", "path prediction"], "combined": "Using proxy cache relocation to accelerate Web browsing in wireless/mobile communications An abstract is not available. [[EENNDD]] learning automaton; cache relocation; proxy cache; mobile computing; w4; path prediction"}, "Menggunakan penempatan semula cache proksi untuk mempercepat penyemakan imbas Web dalam komunikasi tanpa wayar / mudah alih Abstrak tidak tersedia. [[EENNDD]] automaton pembelajaran; penempatan semula cache; cache proksi; pengkomputeran mudah alih; w4; ramalan laluan"], [{"string": "Preference-based selection of highly configurable web services A key challenge for dynamic Web service selection is that Web services are typically highly configurable and service requesters often have dynamic preferences on service configurations. Current approaches, such as WS-Agreement, describe Web services by enumerating the various possible service configurations, an inefficient approach when dealing with numerous service attributes with large value spaces. We model Web service configurations and associated prices and preferences more compactly using utility function policies, which also allows us to draw from multi-attribute decision theory methods to develop an algorithm for optimal service selection. In this paper, we present an OWL ontology for the specification of configurable Web service offers and requests, and a flexible and extensible framework for optimal service selection that combines declarative logic-based matching rules with optimization methods, such as linear programming. Assuming additive price/preference functions, experimental results indicate that our algorithm introduces an overhead of only around 2 sec.~compared to random service selection, while giving optimal results. The overhead, as percentage of total time, decreases as the number of offers and configurations increase.", "keywords": ["web services", "customisation", "preference-based service selection"], "combined": "Preference-based selection of highly configurable web services A key challenge for dynamic Web service selection is that Web services are typically highly configurable and service requesters often have dynamic preferences on service configurations. Current approaches, such as WS-Agreement, describe Web services by enumerating the various possible service configurations, an inefficient approach when dealing with numerous service attributes with large value spaces. We model Web service configurations and associated prices and preferences more compactly using utility function policies, which also allows us to draw from multi-attribute decision theory methods to develop an algorithm for optimal service selection. In this paper, we present an OWL ontology for the specification of configurable Web service offers and requests, and a flexible and extensible framework for optimal service selection that combines declarative logic-based matching rules with optimization methods, such as linear programming. Assuming additive price/preference functions, experimental results indicate that our algorithm introduces an overhead of only around 2 sec.~compared to random service selection, while giving optimal results. The overhead, as percentage of total time, decreases as the number of offers and configurations increase. [[EENNDD]] web services; customisation; preference-based service selection"}, "Pemilihan berasaskan perkhidmatan pilihan web yang sangat boleh dikonfigurasi Cabaran utama untuk pemilihan perkhidmatan Web dinamik adalah bahawa perkhidmatan Web biasanya sangat dapat dikonfigurasi dan pemohon perkhidmatan sering mempunyai pilihan dinamik pada konfigurasi perkhidmatan. Pendekatan semasa, seperti Perjanjian WS, menggambarkan perkhidmatan Web dengan menghitung berbagai kemungkinan konfigurasi perkhidmatan, pendekatan yang tidak efisien ketika berurusan dengan banyak atribut perkhidmatan dengan ruang nilai yang besar. Kami memodelkan konfigurasi perkhidmatan Web dan harga serta pilihan yang berkaitan dengan lebih padat menggunakan dasar fungsi utiliti, yang juga membolehkan kami mengambil kaedah teori keputusan pelbagai atribut untuk mengembangkan algoritma untuk pemilihan perkhidmatan yang optimum. Dalam makalah ini, kami menyajikan ontologi OWL untuk spesifikasi tawaran dan permintaan perkhidmatan Web yang dapat dikonfigurasi, dan kerangka yang fleksibel dan dapat diperluas untuk pemilihan perkhidmatan yang optimum yang menggabungkan peraturan pencocokan berdasarkan logik deklaratif dengan kaedah pengoptimuman, seperti pengaturcaraan linear. Dengan mengandaikan fungsi tambahan / fungsi pilihan, hasil eksperimen menunjukkan bahawa algoritma kami memperkenalkan overhead hanya sekitar 2 saat ~ dibandingkan dengan pemilihan perkhidmatan secara rawak, sambil memberikan hasil yang optimum. Overhead, sebagai peratusan jumlah masa, berkurang apabila jumlah tawaran dan konfigurasi meningkat. [[EENNDD]] perkhidmatan web; penyesuaian; pemilihan perkhidmatan berdasarkan pilihan"], [{"string": "Information integration over time in unreliable and uncertain environments Often an interesting true value such as a stock price, sports score, or current temperature is only available via the observations of noisy and potentially conflicting sources. Several techniques have been proposed to reconcile these conflicts by computing a weighted consensus based on source reliabilities, but these techniques focus on static values. When the real-world entity evolves over time, the noisy sources can delay, or even miss, reporting some of the real-world updates. This temporal aspect introduces two key challenges for consensus-based approaches: (i) due to delays, the mapping between a source's noisy observation and the real-world update it observes is unknown, and (ii) missed updates may translate to missing values for the consensus problem, even if the mapping is known. To overcome these challenges, we propose a formal approach that models the history of updates of the real-world entity as a hidden semi-Markovian process (HSMM). The noisy sources are modeled as observations of the hidden state, but the mapping between a hidden state (i.e. real-world update) and the observation (i.e. source value) is unknown. We propose algorithms based on Gibbs Sampling and EM to jointly infer both the history of real-world updates as well as the unknown mapping between them and the source values. We demonstrate using experiments on real-world datasets how our history-based techniques improve upon history-agnostic consensus-based approaches.", "keywords": ["probabilistic model", "heterogeneous databases", "information integration", "semi-markov"], "combined": "Information integration over time in unreliable and uncertain environments Often an interesting true value such as a stock price, sports score, or current temperature is only available via the observations of noisy and potentially conflicting sources. Several techniques have been proposed to reconcile these conflicts by computing a weighted consensus based on source reliabilities, but these techniques focus on static values. When the real-world entity evolves over time, the noisy sources can delay, or even miss, reporting some of the real-world updates. This temporal aspect introduces two key challenges for consensus-based approaches: (i) due to delays, the mapping between a source's noisy observation and the real-world update it observes is unknown, and (ii) missed updates may translate to missing values for the consensus problem, even if the mapping is known. To overcome these challenges, we propose a formal approach that models the history of updates of the real-world entity as a hidden semi-Markovian process (HSMM). The noisy sources are modeled as observations of the hidden state, but the mapping between a hidden state (i.e. real-world update) and the observation (i.e. source value) is unknown. We propose algorithms based on Gibbs Sampling and EM to jointly infer both the history of real-world updates as well as the unknown mapping between them and the source values. We demonstrate using experiments on real-world datasets how our history-based techniques improve upon history-agnostic consensus-based approaches. [[EENNDD]] probabilistic model; heterogeneous databases; information integration; semi-markov"}, "Penyatuan maklumat dari masa ke masa dalam persekitaran yang tidak boleh dipercayai dan tidak menentu Seringkali nilai benar yang menarik seperti harga saham, skor sukan, atau suhu semasa hanya tersedia melalui pemerhatian sumber yang bising dan berpotensi bertentangan. Beberapa teknik telah diusulkan untuk mendamaikan konflik ini dengan menghitung konsensus tertimbang berdasarkan keandalan sumber, tetapi teknik ini berfokus pada nilai statis. Apabila entiti dunia nyata berkembang dari masa ke masa, sumber yang bising boleh melambatkan, atau bahkan ketinggalan, melaporkan beberapa kemas kini dunia nyata. Aspek temporal ini memperkenalkan dua cabaran utama untuk pendekatan berdasarkan konsensus: (i) kerana kelewatan, pemetaan antara pemerhatian bising sumber dan kemas kini dunia nyata yang diperhatikannya tidak diketahui, dan (ii) kemas kini yang tidak dijawab boleh diterjemahkan ke nilai yang hilang untuk masalah permuafakatan, walaupun pemetaan diketahui. Untuk mengatasi cabaran ini, kami mencadangkan pendekatan formal yang memodelkan sejarah kemas kini entiti dunia nyata sebagai proses semi-Markovian tersembunyi (HSMM). Sumber yang bising dimodelkan sebagai pemerhatian keadaan tersembunyi, tetapi pemetaan antara keadaan tersembunyi (iaitu kemas kini dunia nyata) dan pemerhatian (iaitu nilai sumber) tidak diketahui. Kami mencadangkan algoritma berdasarkan Gibbs Sampling dan EM untuk bersama-sama menyimpulkan sejarah kemas kini dunia nyata serta pemetaan yang tidak diketahui antara mereka dan nilai sumber. Kami menunjukkan penggunaan eksperimen pada kumpulan data dunia nyata bagaimana teknik berdasarkan sejarah kami bertambah baik berdasarkan pendekatan berdasarkan konsensus sejarah-agnostik. [[EENNDD]] model kebarangkalian; pangkalan data heterogen; penyatuan maklumat; separa markov"], [{"string": "StatSnowball: a statistical approach to extracting entity relationships Traditional relation extraction methods require pre-specified relations and relation-specific human-tagged examples. Bootstrapping systems significantly reduce the number of training examples, but they usually apply heuristic-based methods to combine a set of strict hard rules, which limit the ability to generalize and thus generate a low recall. Furthermore, existing bootstrapping methods do not perform open information extraction (Open IE), which can identify various types of relations without requiring pre-specifications. In this paper, we propose a statistical extraction framework called Statistical Snowball (StatSnowball), which is a bootstrapping system and can perform both traditional relation extraction and Open IE.", "keywords": ["relationship extraction", "markov logic networks", "statistical models"], "combined": "StatSnowball: a statistical approach to extracting entity relationships Traditional relation extraction methods require pre-specified relations and relation-specific human-tagged examples. Bootstrapping systems significantly reduce the number of training examples, but they usually apply heuristic-based methods to combine a set of strict hard rules, which limit the ability to generalize and thus generate a low recall. Furthermore, existing bootstrapping methods do not perform open information extraction (Open IE), which can identify various types of relations without requiring pre-specifications. In this paper, we propose a statistical extraction framework called Statistical Snowball (StatSnowball), which is a bootstrapping system and can perform both traditional relation extraction and Open IE. [[EENNDD]] relationship extraction; markov logic networks; statistical models"}, "StatSnowball: pendekatan statistik untuk mengekstrak hubungan entiti Kaedah pengekstrakan hubungan tradisional memerlukan hubungan yang ditentukan sebelumnya dan contoh yang ditandai dengan hubungan manusia. Sistem bootstrap secara signifikan mengurangkan jumlah contoh latihan, tetapi mereka biasanya menggunakan kaedah berdasarkan heuristik untuk menggabungkan sekumpulan peraturan keras yang ketat, yang membatasi kemampuan untuk membuat generalisasi dan dengan demikian menghasilkan penarikan balik yang rendah. Selanjutnya, kaedah bootstrap yang ada tidak melakukan pengekstrakan maklumat terbuka (Open IE), yang dapat mengenal pasti pelbagai jenis hubungan tanpa memerlukan pra-spesifikasi. Dalam makalah ini, kami mengusulkan kerangka pengekstrakan statistik yang disebut Statistik Snowball (StatSnowball), yang merupakan sistem bootstrap dan dapat melakukan pengekstrakan hubungan tradisional dan Open IE. [[EENNDD]] pengekstrakan hubungan; rangkaian logik markov; model statistik"], [{"string": "Monitoring the dynamic web to respond to continuous queries No contact information provided yet.", "keywords": ["information systems applications", "continuous queries", "allocation policies"], "combined": "Monitoring the dynamic web to respond to continuous queries No contact information provided yet. [[EENNDD]] information systems applications; continuous queries; allocation policies"}, "Memantau web dinamik untuk menjawab pertanyaan berterusan Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] aplikasi sistem maklumat; pertanyaan berterusan; dasar peruntukan"], [{"string": "YAGO2: exploring and querying world knowledge in time, space, context, and many languages We present YAGO2, an extension of the YAGO knowledge base with focus on temporal and spatial knowledge. It is automatically built from Wikipedia, GeoNames, and WordNet, and contains nearly 10 million entities and events, as well as 80 million facts representing general world knowledge. An enhanced data representation introduces time and location as first-class citizens. The wealth of spatio-temporal information in YAGO can be explored either graphically or through a special time- and space-aware query language.", "keywords": ["general", "geo-spatial", "textual", "ontology", "knowledge base", "temporal", "multilingual"], "combined": "YAGO2: exploring and querying world knowledge in time, space, context, and many languages We present YAGO2, an extension of the YAGO knowledge base with focus on temporal and spatial knowledge. It is automatically built from Wikipedia, GeoNames, and WordNet, and contains nearly 10 million entities and events, as well as 80 million facts representing general world knowledge. An enhanced data representation introduces time and location as first-class citizens. The wealth of spatio-temporal information in YAGO can be explored either graphically or through a special time- and space-aware query language. [[EENNDD]] general; geo-spatial; textual; ontology; knowledge base; temporal; multilingual"}, "YAGO2: meneroka dan menanyakan pengetahuan dunia dalam masa, ruang, konteks, dan banyak bahasa Kami menyajikan YAGO2, lanjutan dari pangkalan pengetahuan YAGO dengan fokus pada pengetahuan temporal dan spasial. Ia dibina secara automatik dari Wikipedia, GeoNames, dan WordNet, dan mengandungi hampir 10 juta entiti dan acara, serta 80 juta fakta yang mewakili pengetahuan umum dunia. Perwakilan data yang dipertingkatkan memperkenalkan masa dan lokasi sebagai warga kelas pertama. Kekayaan maklumat spatio-temporal di YAGO dapat diterokai sama ada secara grafik atau melalui bahasa pertanyaan khusus masa dan ruang. [[EENNDD]] umum; geo-spatial; tekstual; ontologi; asas pengetahuan; temporal; berbilang bahasa"], [{"string": "Mining newsgroups using networks arising from social behavior No contact information provided yet.", "keywords": ["text mining", "newsgroup", "link analysis", "web mining", "social network"], "combined": "Mining newsgroups using networks arising from social behavior No contact information provided yet. [[EENNDD]] text mining; newsgroup; link analysis; web mining; social network"}, "Melombong kumpulan berita menggunakan rangkaian yang timbul dari tingkah laku sosial Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlombongan teks; kumpulan berita; analisis pautan; perlombongan web; rangkaian sosial"], [{"string": "Search engine retrieval of changing information In this paper we analyze the Web coverage of three search engines, Google, Yahoo and MSN. We conducted a 15 month study collecting 15,770 Web content or information pages linked from 260 Australian federal and local government Web pages. The key feature of this domain is that new information pages are constantly added but the 260 web pages tend to provide links only to the more recently added information pages. Search engines list only some of the information pages and their coverage varies from month to month. Meta-search engines do little to improve coverage of information pages, because the problem is not the size of web coverage, but the frequency with which information is updated. We conclude that organizations such as governments which post important information on the Web cannot rely on all relevant pages being found with conventional search engines, and need to consider other strategies to ensure important information can be found.", "keywords": ["search engine", "overlap of web search results", "web coverage"], "combined": "Search engine retrieval of changing information In this paper we analyze the Web coverage of three search engines, Google, Yahoo and MSN. We conducted a 15 month study collecting 15,770 Web content or information pages linked from 260 Australian federal and local government Web pages. The key feature of this domain is that new information pages are constantly added but the 260 web pages tend to provide links only to the more recently added information pages. Search engines list only some of the information pages and their coverage varies from month to month. Meta-search engines do little to improve coverage of information pages, because the problem is not the size of web coverage, but the frequency with which information is updated. We conclude that organizations such as governments which post important information on the Web cannot rely on all relevant pages being found with conventional search engines, and need to consider other strategies to ensure important information can be found. [[EENNDD]] search engine; overlap of web search results; web coverage"}, "Pencarian mesin mencari maklumat yang berubah Dalam makalah ini kami menganalisis liputan Web tiga mesin pencari, Google, Yahoo dan MSN. Kami menjalankan kajian selama 15 bulan yang mengumpulkan 15,770 laman web atau halaman maklumat yang dihubungkan dari 260 laman web kerajaan persekutuan dan tempatan Australia. Ciri utama domain ini ialah halaman maklumat baru sentiasa ditambah tetapi 260 halaman web cenderung hanya menyediakan pautan ke halaman maklumat yang baru ditambahkan. Enjin carian hanya menyenaraikan beberapa halaman maklumat dan liputannya berbeza dari bulan ke bulan. Enjin carian meta tidak banyak meningkatkan liputan halaman maklumat, kerana masalahnya bukanlah ukuran liputan web, tetapi frekuensi maklumat dikemas kini. Kami menyimpulkan bahawa organisasi seperti pemerintah yang menyiarkan maklumat penting di Web tidak boleh bergantung pada semua halaman yang relevan yang dijumpai dengan mesin carian konvensional, dan perlu mempertimbangkan strategi lain untuk memastikan maklumat penting dapat dijumpai. [[EENNDD]] enjin carian; pertindihan hasil carian web; liputan web"], [{"string": "A comparison of case-based reasoning approaches No contact information provided yet.", "keywords": ["general", "web hypermedia metrics", "web effort prediction", "web hypermedia", "case-based reasoning", "prediction models", "metrics"], "combined": "A comparison of case-based reasoning approaches No contact information provided yet. [[EENNDD]] general; web hypermedia metrics; web effort prediction; web hypermedia; case-based reasoning; prediction models; metrics"}, "Perbandingan pendekatan penaakulan berdasarkan kes Belum ada maklumat hubungan yang diberikan. [[EENNDD]] umum; metrik hipermedia web; ramalan usaha web; hypermedia laman web; penaakulan berdasarkan kes; model ramalan; sukatan"], [{"string": "Web service derivatives Web service development and usage has shifted from simple information processing services to high-value business services that are crucial to productivity and success. In order to deal with an increasing risk of unavailability or failure of mission-critical Web services we argue the need for advanced reservation of services in the form of derivatives.", "keywords": ["wavelets", "incomplete markets", "services mashups", "web services", "derivatives"], "combined": "Web service derivatives Web service development and usage has shifted from simple information processing services to high-value business services that are crucial to productivity and success. In order to deal with an increasing risk of unavailability or failure of mission-critical Web services we argue the need for advanced reservation of services in the form of derivatives. [[EENNDD]] wavelets; incomplete markets; services mashups; web services; derivatives"}, "Derivatif perkhidmatan web Pembangunan dan penggunaan perkhidmatan web telah beralih dari perkhidmatan pemprosesan maklumat yang sederhana kepada perkhidmatan perniagaan bernilai tinggi yang penting untuk produktiviti dan kejayaan. Untuk menangani peningkatan risiko ketiadaan atau kegagalan perkhidmatan Web yang kritikal misi, kami berpendapat perlunya tempahan perkhidmatan yang lebih maju dalam bentuk derivatif. [[EENNDD]] gelombang gelombang; pasaran yang tidak lengkap; mashup perkhidmatan; perkhidmatan web; terbitan"], [{"string": "Algorithms and programming models for efficient representation of XML for Internet applications An abstract is not available.", "keywords": ["sax", "dom", "internet", "wbxml", "compression", "xml"], "combined": "Algorithms and programming models for efficient representation of XML for Internet applications An abstract is not available. [[EENNDD]] sax; dom; internet; wbxml; compression; xml"}, "Algoritma dan model pengaturcaraan untuk perwakilan XML yang cekap untuk aplikasi Internet Abstrak tidak tersedia. [[EENNDD]] saks; dom; internet; wbxml; pemampatan; xml"], [{"string": "Session level techniques for improving web browsing performance on wireless links No contact information provided yet.", "keywords": ["wireless", "web", "optimizations", "miscellaneous"], "combined": "Session level techniques for improving web browsing performance on wireless links No contact information provided yet. [[EENNDD]] wireless; web; optimizations; miscellaneous"}, "Teknik tahap sesi untuk meningkatkan prestasi melayari web pada pautan tanpa wayar Belum ada maklumat hubungan yang diberikan. [[EENNDD]] tanpa wayar; laman web; pengoptimuman; pelbagai"], [{"string": "Semantic wiki aided business process specification This paper formulates a collaborative system for modeling business application. The system uses a Semantic Wiki to enable collaboration between the various stakeholders involved in the design of the system and translates the captured intelligence into business models which are used for designing a business system.", "keywords": ["business process modeling language", "business modeling", "semantic web", "rdf", "software"], "combined": "Semantic wiki aided business process specification This paper formulates a collaborative system for modeling business application. The system uses a Semantic Wiki to enable collaboration between the various stakeholders involved in the design of the system and translates the captured intelligence into business models which are used for designing a business system. [[EENNDD]] business process modeling language; business modeling; semantic web; rdf; software"}, "Spesifikasi proses perniagaan berbantuan wiki semantik Makalah ini merumuskan sistem kolaborasi untuk pemodelan aplikasi perniagaan. Sistem ini menggunakan Semantik Wiki untuk membolehkan kolaborasi antara pelbagai pihak berkepentingan yang terlibat dalam reka bentuk sistem dan menerjemahkan kecerdasan yang ditangkap menjadi model perniagaan yang digunakan untuk merancang sistem perniagaan. [[EENNDD]] bahasa pemodelan proses perniagaan; pemodelan perniagaan; web semantik; rdf; perisian"], [{"string": "A dynamic bayesian network click model for web search ranking As with any application of machine learning, web search ranking requires labeled data. The labels usually come in the form of relevance assessments made by editors. Click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. The main difficulty however comes from the so called position bias - urls appearing in lower positions are less likely to be clicked even if they are relevant. In this paper, we propose a Dynamic Bayesian Network which aims at providing us with unbiased estimation of the relevance from the click logs. Experiments show that the proposed click model outperforms other existing click models in predicting both click-through rate and relevance.", "keywords": ["ranking", "click-through rate", "on-line information services", "web search", "information search and retrieval", "learning", "click modeling", "dynamic bayesian network"], "combined": "A dynamic bayesian network click model for web search ranking As with any application of machine learning, web search ranking requires labeled data. The labels usually come in the form of relevance assessments made by editors. Click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. The main difficulty however comes from the so called position bias - urls appearing in lower positions are less likely to be clicked even if they are relevant. In this paper, we propose a Dynamic Bayesian Network which aims at providing us with unbiased estimation of the relevance from the click logs. Experiments show that the proposed click model outperforms other existing click models in predicting both click-through rate and relevance. [[EENNDD]] ranking; click-through rate; on-line information services; web search; information search and retrieval; learning; click modeling; dynamic bayesian network"}, "Model klik rangkaian bayesian yang dinamik untuk peringkat carian web Seperti mana-mana aplikasi pembelajaran mesin, peringkat carian web memerlukan data berlabel. Label biasanya terdapat dalam bentuk penilaian kesesuaian yang dibuat oleh penyunting. Klik log juga dapat memberikan sumber penting maklum balas tersirat dan dapat digunakan sebagai proksi murah untuk label editorial. Walau bagaimanapun, kesukaran utama datang dari apa yang disebut bias kedudukan - url yang muncul di kedudukan yang lebih rendah cenderung untuk diklik walaupun ia berkaitan. Dalam makalah ini, kami mengusulkan Dynamic Bayesian Network yang bertujuan memberi kami taksiran yang tidak berat sebelah mengenai relevansi dari log klik. Eksperimen menunjukkan bahawa model klik yang dicadangkan melebihi model klik lain yang ada dalam meramalkan kadar klik dan kaitannya. [[EENNDD]] kedudukan; kadar klik lalu; perkhidmatan maklumat dalam talian; carian sesawang; pencarian dan pengambilan maklumat; belajar; pemodelan klik; rangkaian bayesian dinamik"], [{"string": "Second international workshop on RESTful design (WS-REST 2011) Over the past few years, the discussion between the two major architectural styles for designing and implementing Web services, the RPC-oriented approach and the resource-oriented approach, has been mainly held outside of traditional research communities. Mailing lists, forums and developer communities have seen long and fascinating debates around the assumptions, strengths, and weaknesses of these two approaches. The Second International Workshop on RESTful Design (WS-REST 2011) has the goal of getting more researchers involved in the debate by providing a forum where discussions around the resource-oriented style of Web services design take place. Representational State Transfer (REST) is an architectural style and as such can be applied in different ways, can be extended by additional constraints, or can be specialized with more specific interaction patterns. WS-REST is the premier forum for discussing research ideas, novel applications and results centered around REST at the World Wide Web conference, which provides a great setting to host this second edition of the workshop dedicated to research on the architectural style underlying the Web.", "keywords": ["general", "web services", "rest", "http", "web architecture"], "combined": "Second international workshop on RESTful design (WS-REST 2011) Over the past few years, the discussion between the two major architectural styles for designing and implementing Web services, the RPC-oriented approach and the resource-oriented approach, has been mainly held outside of traditional research communities. Mailing lists, forums and developer communities have seen long and fascinating debates around the assumptions, strengths, and weaknesses of these two approaches. The Second International Workshop on RESTful Design (WS-REST 2011) has the goal of getting more researchers involved in the debate by providing a forum where discussions around the resource-oriented style of Web services design take place. Representational State Transfer (REST) is an architectural style and as such can be applied in different ways, can be extended by additional constraints, or can be specialized with more specific interaction patterns. WS-REST is the premier forum for discussing research ideas, novel applications and results centered around REST at the World Wide Web conference, which provides a great setting to host this second edition of the workshop dedicated to research on the architectural style underlying the Web. [[EENNDD]] general; web services; rest; http; web architecture"}, "Bengkel antarabangsa kedua mengenai reka bentuk RESTful (WS-REST 2011) Sejak beberapa tahun kebelakangan ini, perbincangan antara dua gaya seni bina utama untuk merancang dan melaksanakan perkhidmatan Web, pendekatan berorientasi RPC dan pendekatan berorientasikan sumber, telah diadakan di luar komuniti penyelidikan tradisional. Senarai surat, forum dan komuniti pemaju telah melihat perdebatan panjang dan menarik mengenai andaian, kekuatan, dan kelemahan kedua-dua pendekatan ini. Bengkel Antarabangsa Kedua mengenai RESTful Design (WS-REST 2011) mempunyai tujuan untuk melibatkan lebih banyak penyelidik dalam perbahasan dengan menyediakan forum di mana perbincangan mengenai gaya reka bentuk perkhidmatan Web yang berorientasikan sumber berlangsung. Perwakilan Negara Perwakilan (REST) adalah gaya seni bina dan dengan demikian dapat diterapkan dengan cara yang berbeda, dapat diperluas dengan batasan tambahan, atau dapat dikhususkan dengan pola interaksi yang lebih spesifik. WS-REST adalah forum utama untuk membincangkan idea penyelidikan, aplikasi novel dan hasil yang berpusat di sekitar REST pada persidangan World Wide Web, yang memberikan suasana yang baik untuk menjadi tuan rumah edisi kedua bengkel ini yang dikhaskan untuk penyelidikan mengenai gaya seni bina yang mendasari Web. [[EENNDD]] umum; perkhidmatan web; berehat; http; seni bina web"], [{"string": "News article extraction with template-independent wrapper We consider the problem of template-independent news extraction. The state-of-the-art news extraction method is based on template-level wrapper induction, which has two serious limitations. 1) It cannot correctly extract pages belonging to an unseen template until the wrapper for that template has been generated. 2) It is costly to maintain up-to-date wrappers for hundreds of websites, because any change of a template may lead to the invalidation of the corresponding wrapper. In this paper we formalize news extraction as a machine learning problem and learn a template-independent wrapper using a very small number of labeled news pages from a single site. Novel features dedicated to news titles and bodies are developed respectively. Correlations between the news title and the news body are exploited. Our template-independent wrapper can extract news pages from different sites regardless of templates. In experiments, a wrapper is learned from 40 pages from a single news site. It achieved 98.1% accuracy over 3,973 news pages from 12 news sites.", "keywords": ["miscellaneous", "data extraction"], "combined": "News article extraction with template-independent wrapper We consider the problem of template-independent news extraction. The state-of-the-art news extraction method is based on template-level wrapper induction, which has two serious limitations. 1) It cannot correctly extract pages belonging to an unseen template until the wrapper for that template has been generated. 2) It is costly to maintain up-to-date wrappers for hundreds of websites, because any change of a template may lead to the invalidation of the corresponding wrapper. In this paper we formalize news extraction as a machine learning problem and learn a template-independent wrapper using a very small number of labeled news pages from a single site. Novel features dedicated to news titles and bodies are developed respectively. Correlations between the news title and the news body are exploited. Our template-independent wrapper can extract news pages from different sites regardless of templates. In experiments, a wrapper is learned from 40 pages from a single news site. It achieved 98.1% accuracy over 3,973 news pages from 12 news sites. [[EENNDD]] miscellaneous; data extraction"}, "Pengekstrakan artikel berita dengan pembungkus bebas templat Kami mempertimbangkan masalah pengekstrakan berita tanpa templat. Kaedah pengekstrakan berita terkini berdasarkan induksi pembungkus peringkat templat, yang mempunyai dua batasan serius. 1) Tidak dapat mengekstrak halaman kepunyaan templat yang tidak dapat dilihat dengan betul sehingga pembungkus templat itu dihasilkan. 2) Sangat mahal untuk mengekalkan pembungkus terkini untuk beratus-ratus laman web, kerana sebarang perubahan templat boleh menyebabkan pembatalan pembungkus yang sesuai. Dalam makalah ini kami memformalkan pengambilan berita sebagai masalah pembelajaran mesin dan mempelajari pembungkus bebas templat menggunakan sebilangan kecil halaman berita berlabel dari satu laman web. Ciri-ciri novel yang dikhaskan untuk tajuk berita dan badan dikembangkan masing-masing. Hubungan antara tajuk berita dan badan berita dieksploitasi. Pembungkus bebas templat kami dapat mengekstrak halaman berita dari laman web yang berbeza tanpa mengira templat. Dalam eksperimen, pembungkus dipelajari dari 40 halaman dari satu laman berita. Ia mencapai ketepatan 98.1% daripada 3,973 halaman berita dari 12 laman web berita. [[EENNDD]] pelbagai; pengekstrakan data"], [{"string": "EPCI: extracting potentially copyright infringement texts from the web In this paper, we propose a new system extracting potentially copyright infringement texts from the Web, called EPCI. EPCI extracts them in the following way: (1) generating a set of queries based on a given copyright reserved seed-text, (2) putting every query to search engine API, (3) gathering the search result Web pages from high ranking until the similarity between the given seed-text and the search result pages becomes less than a given threshold value, and (4) merging all the gathered pages, then re-ranking them in the order of their similarity. Our experimental result using 40 seed-texts shows that EPCI is able to extract 132 potentially copyright infringement Web pages per a given copyright reserved seed-text with 94% precision in average.", "keywords": ["information search and retrieval", "copy detection", "information retrieval"], "combined": "EPCI: extracting potentially copyright infringement texts from the web In this paper, we propose a new system extracting potentially copyright infringement texts from the Web, called EPCI. EPCI extracts them in the following way: (1) generating a set of queries based on a given copyright reserved seed-text, (2) putting every query to search engine API, (3) gathering the search result Web pages from high ranking until the similarity between the given seed-text and the search result pages becomes less than a given threshold value, and (4) merging all the gathered pages, then re-ranking them in the order of their similarity. Our experimental result using 40 seed-texts shows that EPCI is able to extract 132 potentially copyright infringement Web pages per a given copyright reserved seed-text with 94% precision in average. [[EENNDD]] information search and retrieval; copy detection; information retrieval"}, "EPCI: mengekstrak teks pelanggaran hak cipta yang berpotensi dari web Dalam makalah ini, kami mengusulkan sistem baru yang mengekstrak teks pelanggaran hak cipta yang berpotensi dari Web, yang disebut EPCI. EPCI mengekstraknya dengan cara berikut: (1) menghasilkan satu set pertanyaan berdasarkan teks benih hak cipta yang ditentukan, (2) menempatkan setiap pertanyaan ke API mesin pencari, (3) mengumpulkan halaman web hasil carian dari peringkat tinggi hingga persamaan antara teks benih yang diberikan dan halaman hasil carian menjadi kurang dari nilai ambang yang ditentukan, dan (4) menggabungkan semua halaman yang dikumpulkan, kemudian menarafkannya semula mengikut urutan kesamaannya. Hasil percubaan kami menggunakan 40 teks-teks menunjukkan bahawa EPCI dapat mengekstrak 132 halaman Web yang berpotensi melanggar hak cipta untuk setiap teks benih yang dilindungi hak cipta dengan rata-rata ketepatan 94%. [[EENNDD]] carian dan pengambilan maklumat; pengesanan salinan; pengambilan maklumat"], [{"string": "Modeling anchor text and classifying queries to enhance web document retrieval Several types of queries are widely used on the World Wide Web and the expected retrieval method can vary depending on the query type. We propose a method for classifying queries into informational and navigational types. Because terms in navigational queries often appear in anchor text for links to other pages, we analyze the distribution of query terms in anchor texts on the Web for query classification purposes. While content-based retrieval is effective for informational queries, anchor-based retrieval is effective for navigational queries. Our retrieval system combines the results obtained with the content-based and anchor-based retrieval methods, in which the weight for each retrieval result is determined automatically depending on the result of the query classification. We also propose a method for improving anchor-based retrieval. Our retrieval method, which computes the probability that a document is retrieved in response to the given query, identifies synonyms of query terms in the anchor texts on the Web and uses these synonyms for smoothing purposes in the probability estimation. We use the NTCIR test collections and show the effectiveness of individual methods and the entire Web retrieval system experimentally.", "keywords": ["query classification", "anchor text", "web retrieval"], "combined": "Modeling anchor text and classifying queries to enhance web document retrieval Several types of queries are widely used on the World Wide Web and the expected retrieval method can vary depending on the query type. We propose a method for classifying queries into informational and navigational types. Because terms in navigational queries often appear in anchor text for links to other pages, we analyze the distribution of query terms in anchor texts on the Web for query classification purposes. While content-based retrieval is effective for informational queries, anchor-based retrieval is effective for navigational queries. Our retrieval system combines the results obtained with the content-based and anchor-based retrieval methods, in which the weight for each retrieval result is determined automatically depending on the result of the query classification. We also propose a method for improving anchor-based retrieval. Our retrieval method, which computes the probability that a document is retrieved in response to the given query, identifies synonyms of query terms in the anchor texts on the Web and uses these synonyms for smoothing purposes in the probability estimation. We use the NTCIR test collections and show the effectiveness of individual methods and the entire Web retrieval system experimentally. [[EENNDD]] query classification; anchor text; web retrieval"}, "Memodelkan jangkar teks dan mengklasifikasikan pertanyaan untuk meningkatkan pengambilan dokumen web Beberapa jenis pertanyaan banyak digunakan di World Wide Web dan kaedah pengambilan yang diharapkan dapat berbeza-beza bergantung pada jenis pertanyaan. Kami mencadangkan kaedah untuk mengklasifikasikan pertanyaan kepada jenis maklumat dan navigasi. Kerana istilah dalam pertanyaan navigasi sering muncul dalam teks jangkar untuk pautan ke halaman lain, kami menganalisis sebaran istilah pertanyaan dalam teks jangkar di Web untuk tujuan klasifikasi pertanyaan. Walaupun pengambilan berdasarkan kandungan berkesan untuk pertanyaan maklumat, pengambilan berdasarkan sauh berkesan untuk pertanyaan navigasi. Sistem pengambilan kami menggabungkan hasil yang diperoleh dengan kaedah pengambilan berdasarkan kandungan dan jangkar, di mana berat untuk setiap hasil pengambilan ditentukan secara automatik bergantung pada hasil klasifikasi pertanyaan. Kami juga mencadangkan kaedah untuk meningkatkan pengambilan berdasarkan sauh. Kaedah pengambilan kami, yang menghitung kebarangkalian bahawa dokumen diambil sebagai tindak balas kepada pertanyaan yang diberikan, mengenal pasti sinonim istilah pertanyaan dalam teks jangkar di Web dan menggunakan sinonim ini untuk melicinkan tujuan dalam anggaran kebarangkalian. Kami menggunakan koleksi ujian NTCIR dan menunjukkan keberkesanan kaedah individu dan keseluruhan sistem pengambilan Web secara eksperimen. [[EENNDD]] klasifikasi pertanyaan; teks sauh; pengambilan web"], [{"string": "Query-driven indexing for peer-to-peer text retrieval We describe a query-driven indexing framework for scalable text retrieval over structured P2P networks. To cope with the bandwidth consumption problem that has been identified as the major obstacle for full-text retrieval in P2P networks, we truncate posting lists associated with indexing features to a constant size storing only top-k ranked document references. To compensate for the loss of information caused by the truncation, we extend the set of indexing features with carefully chosen term sets. Indexing term sets are selected based on the query statistics extracted from query logs, thus we index only such combinations that are a) frequently present in user queries and b) non-redundant w.r.t the rest of the index. The distributed index is compact and efficient as it constantly evolves adapting to the current query popularity distribution. Moreover, it is possible to control the tradeoff between the storage/bandwidth requirements and the quality of query answering by tuning the indexing parameters. Our theoretical analysis and experimental results indicate that we can indeed achieve scalable P2P text retrieval for very large document collections and deliver good retrieval performance.", "keywords": ["text retrieval", "ir", "p2p", "query-driven indexing", "dht"], "combined": "Query-driven indexing for peer-to-peer text retrieval We describe a query-driven indexing framework for scalable text retrieval over structured P2P networks. To cope with the bandwidth consumption problem that has been identified as the major obstacle for full-text retrieval in P2P networks, we truncate posting lists associated with indexing features to a constant size storing only top-k ranked document references. To compensate for the loss of information caused by the truncation, we extend the set of indexing features with carefully chosen term sets. Indexing term sets are selected based on the query statistics extracted from query logs, thus we index only such combinations that are a) frequently present in user queries and b) non-redundant w.r.t the rest of the index. The distributed index is compact and efficient as it constantly evolves adapting to the current query popularity distribution. Moreover, it is possible to control the tradeoff between the storage/bandwidth requirements and the quality of query answering by tuning the indexing parameters. Our theoretical analysis and experimental results indicate that we can indeed achieve scalable P2P text retrieval for very large document collections and deliver good retrieval performance. [[EENNDD]] text retrieval; ir; p2p; query-driven indexing; dht"}, "Pengindeksan berdasarkan permintaan untuk pengambilan teks peer-to-peer Kami menerangkan kerangka pengindeksan yang didorong oleh pertanyaan untuk pengambilan teks berskala melalui rangkaian P2P berstruktur. Untuk mengatasi masalah penggunaan jalur lebar yang telah dikenal pasti sebagai halangan utama untuk pengambilan teks penuh dalam rangkaian P2P, kami memotong senarai pengeposan yang berkaitan dengan fitur pengindeksan ke ukuran tetap yang hanya menyimpan rujukan dokumen peringkat teratas. Untuk mengimbangi kehilangan maklumat yang disebabkan oleh pemotongan, kami memperluas set ciri pengindeksan dengan set istilah yang dipilih dengan teliti. Set istilah pengindeksan dipilih berdasarkan statistik pertanyaan yang diekstrak dari log pertanyaan, oleh itu kami hanya mengindeks kombinasi seperti yang a) sering terdapat dalam pertanyaan pengguna dan b) tidak berlebihan dengan seluruh indeks. Indeks yang diedarkan adalah padat dan cekap kerana sentiasa berubah menyesuaikan diri dengan sebaran populariti pertanyaan semasa. Lebih-lebih lagi, adalah mungkin untuk mengawal pertukaran antara keperluan penyimpanan / lebar jalur dan kualiti menjawab pertanyaan dengan menyesuaikan parameter pengindeksan. Analisis teoritis dan hasil eksperimen kami menunjukkan bahawa kami memang dapat mencapai pengambilan teks P2P yang dapat diskalakan untuk koleksi dokumen yang sangat besar dan memberikan prestasi pengambilan yang baik. [[EENNDD]] pengambilan teks; ir; p2p; pengindeksan berdasarkan pertanyaan; dht"], [{"string": "Aliasing on the world wide web: prevalence and performance implications No contact information provided yet.", "keywords": ["dtd", "duplicate suppression", "applications", "hypertext transfer protocol", "zipf's law", "resource modification", "duplicate transfer detection", "caching", "performance analysis", "redundant transfers", "www", "cache hierarchies", "http", "aliasing", "world wide web"], "combined": "Aliasing on the world wide web: prevalence and performance implications No contact information provided yet. [[EENNDD]] dtd; duplicate suppression; applications; hypertext transfer protocol; zipf's law; resource modification; duplicate transfer detection; caching; performance analysis; redundant transfers; www; cache hierarchies; http; aliasing; world wide web"}, "Mengasingkan diri di laman web seluruh dunia: kelaziman dan implikasi prestasi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] dtd; penindasan pendua; permohonan; Protokol Pemindahan Hiperteks; undang-undang zipf; pengubahsuaian sumber; pengesanan pemindahan pendua; caching; analisis prestasi; pemindahan berlebihan; www; hierarki cache; http; mengasingkan; web seluruh dunia"], [{"string": "Finding the right facts in the crowd: factoid question answering over social media Community Question Answering has emerged as a popular and effective paradigm for a wide range of information needs. For example, to find out an obscure piece of trivia, it is now possible and even very effective to post a question on a popular community QA site such as Yahoo! Answers, and to rely on other users to provide answers, often within minutes. The importance of such community QA sites is magnified as they create archives of millions of questions and hundreds of millions of answers, many of which are invaluable for the information needs of other searchers. However, to make this immense body of knowledge accessible, effective answer retrieval is required. In particular, as any user can contribute an answer to a question, the majority of the content reflects personal, often unsubstantiated opinions. A ranking that combines both relevance and quality is required to make such archives usable for factual information retrieval. This task is challenging, as the structure and the contents of community QA archives differ significantly from the web setting. To address this problem we present a general ranking framework for factual information retrieval from social media. Results of a large scale evaluation demonstrate that our method is highly effective at retrieving well-formed, factual answers to questions, as evaluated on a standard factoid QA benchmark. We also show that our learning framework can be tuned with the minimum of manual labeling. Finally, we provide result analysis to gain deeper understanding of which features are significant for social media search and retrieval. Our system can be used as a crucial building block for combining results from a variety of social media content with general web search results, and to better integrate social media content for effective information access.", "keywords": ["ranking", "community", "question answering"], "combined": "Finding the right facts in the crowd: factoid question answering over social media Community Question Answering has emerged as a popular and effective paradigm for a wide range of information needs. For example, to find out an obscure piece of trivia, it is now possible and even very effective to post a question on a popular community QA site such as Yahoo! Answers, and to rely on other users to provide answers, often within minutes. The importance of such community QA sites is magnified as they create archives of millions of questions and hundreds of millions of answers, many of which are invaluable for the information needs of other searchers. However, to make this immense body of knowledge accessible, effective answer retrieval is required. In particular, as any user can contribute an answer to a question, the majority of the content reflects personal, often unsubstantiated opinions. A ranking that combines both relevance and quality is required to make such archives usable for factual information retrieval. This task is challenging, as the structure and the contents of community QA archives differ significantly from the web setting. To address this problem we present a general ranking framework for factual information retrieval from social media. Results of a large scale evaluation demonstrate that our method is highly effective at retrieving well-formed, factual answers to questions, as evaluated on a standard factoid QA benchmark. We also show that our learning framework can be tuned with the minimum of manual labeling. Finally, we provide result analysis to gain deeper understanding of which features are significant for social media search and retrieval. Our system can be used as a crucial building block for combining results from a variety of social media content with general web search results, and to better integrate social media content for effective information access. [[EENNDD]] ranking; community; question answering"}, "Mencari fakta yang tepat di khalayak ramai: jawapan soalan factoid di media sosial Komuniti Menjawab Soalan telah muncul sebagai paradigma yang popular dan berkesan untuk pelbagai keperluan maklumat. Sebagai contoh, untuk mengetahui perkara-perkara sepele yang tidak jelas, kini mungkin dan bahkan sangat berkesan untuk menghantar soalan di laman QA komuniti yang popular seperti Yahoo! Jawapan, dan bergantung pada pengguna lain untuk memberikan jawapan, selalunya dalam beberapa minit. Kepentingan laman QA komuniti seperti ini diperbesar kerana mereka membuat arkib berjuta-juta soalan dan beratus-ratus juta jawapan, yang kebanyakannya sangat berharga untuk keperluan maklumat pencari lain. Walau bagaimanapun, untuk menjadikan pengetahuan yang banyak ini dapat diakses, pengambilan jawapan yang berkesan diperlukan. Khususnya, kerana mana-mana pengguna dapat memberikan sumbangan untuk menjawab soalan, sebahagian besar kandungan mencerminkan pendapat peribadi, sering tidak berasas. Peringkat yang menggabungkan kedua-dua relevan dan kualiti diperlukan untuk menjadikan arkib tersebut dapat digunakan untuk mendapatkan maklumat fakta. Tugas ini mencabar, kerana struktur dan kandungan arkib QA komuniti berbeza jauh dari tetapan web. Untuk mengatasi masalah ini, kami menyajikan kerangka peringkat umum untuk pengambilan maklumat fakta dari media sosial. Hasil penilaian berskala besar menunjukkan bahawa kaedah kami sangat berkesan untuk mendapatkan jawapan yang tepat dan tepat untuk soalan, seperti yang dinilai berdasarkan tanda aras QA factoid standard. Kami juga menunjukkan bahawa kerangka pembelajaran kami dapat disesuaikan dengan pelabelan manual minimum. Akhirnya, kami memberikan analisis hasil untuk mendapatkan pemahaman yang lebih mendalam mengenai ciri mana yang penting untuk carian dan pencarian media sosial. Sistem kami dapat digunakan sebagai blok bangunan penting untuk menggabungkan hasil dari pelbagai kandungan media sosial dengan hasil carian web umum, dan untuk menggabungkan kandungan media sosial dengan lebih baik untuk akses maklumat yang berkesan. [[EENNDD]] kedudukan; komuniti; menjawab soalan"], [{"string": "Affinity rank: a new scheme for efficient web search No contact information provided yet.", "keywords": ["affinity rank", "information richness", "diversity", "link analysis"], "combined": "Affinity rank: a new scheme for efficient web search No contact information provided yet. [[EENNDD]] affinity rank; information richness; diversity; link analysis"}, "Peringkat perkaitan: skema baru untuk carian web yang cekap Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kedudukan pertalian; kekayaan maklumat; kepelbagaian; analisis pautan"], [{"string": "Analyzing online discussion for marketing intelligence No contact information provided yet.", "keywords": ["computational linguistics", "text mining", "information retrieval", "information search and retrieval", "content systems", "machine learning"], "combined": "Analyzing online discussion for marketing intelligence No contact information provided yet. [[EENNDD]] computational linguistics; text mining; information retrieval; information search and retrieval; content systems; machine learning"}, "Menganalisis perbincangan dalam talian untuk kecerdasan pemasaran Belum ada maklumat hubungan yang diberikan. [[EENNDD]] linguistik komputasi; perlombongan teks; pengambilan maklumat; pencarian dan pengambilan maklumat; sistem kandungan; pembelajaran mesin"], [{"string": "Newsjunkie: providing personalized newsfeeds via analysis of information novelty No contact information provided yet.", "keywords": ["news", "novelty detection", "personalization"], "combined": "Newsjunkie: providing personalized newsfeeds via analysis of information novelty No contact information provided yet. [[EENNDD]] news; novelty detection; personalization"}, "Newsjunkie: menyediakan feed berita yang diperibadikan melalui analisis maklumat kebaruan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] berita; pengesanan kebaruan; pemperibadian"], [{"string": "A new suffix tree similarity measure for document clustering In this paper, we propose a new similarity measure to compute the pairwise similarity of text-based documents based on suffix tree document model. By applying the new suffix tree similarity measure in Group-average Agglomerative Hierarchical Clustering (GAHC) algorithm, we developed a new suffix tree document clustering algorithm (NSTC). Experimental results on two standard document clustering benchmark corpus OHSUMED and RCV1 indicate that the new clustering algorithm is a very effective document clustering algorithm. Comparing with the results of traditional word term weight tf-idf similarity measure in the same GAHC algorithm, NSTC achieved an improvement of 51% on the average of F-measure score. Furthermore, we apply the new clustering algorithm in analyzing the Web documents in online forum communities. A topic oriented clustering algorithm is developed to help people in assessing, classifying and searching the the Web documents in a large forum community.", "keywords": ["document model", "suffix tree", "similarity measure"], "combined": "A new suffix tree similarity measure for document clustering In this paper, we propose a new similarity measure to compute the pairwise similarity of text-based documents based on suffix tree document model. By applying the new suffix tree similarity measure in Group-average Agglomerative Hierarchical Clustering (GAHC) algorithm, we developed a new suffix tree document clustering algorithm (NSTC). Experimental results on two standard document clustering benchmark corpus OHSUMED and RCV1 indicate that the new clustering algorithm is a very effective document clustering algorithm. Comparing with the results of traditional word term weight tf-idf similarity measure in the same GAHC algorithm, NSTC achieved an improvement of 51% on the average of F-measure score. Furthermore, we apply the new clustering algorithm in analyzing the Web documents in online forum communities. A topic oriented clustering algorithm is developed to help people in assessing, classifying and searching the the Web documents in a large forum community. [[EENNDD]] document model; suffix tree; similarity measure"}, "Ukuran kesamaan pohon akhiran baru untuk pengelompokan dokumen Dalam makalah ini, kami mencadangkan ukuran kesamaan baru untuk menghitung kesamaan berpasangan dokumen berdasarkan teks berdasarkan model dokumen akhiran pokok. Dengan menerapkan ukuran kesamaan pokok akhiran baru dalam algoritma Kumpulan Agregasi Hierarki Rata-Rata (GAHC), kami mengembangkan algoritma pengelompokan dokumen pohon akhiran (NSTC). Hasil eksperimen pada dua standard standard clustering benchmark corpus OHSUMED dan RCV1 menunjukkan bahawa algoritma pengelompokan baru adalah algoritma pengelompokan dokumen yang sangat berkesan. Membandingkan dengan hasil pengukuran kesamaan berat kata tradisional tf-idf dalam algoritma GAHC yang sama, NSTC mencapai peningkatan 51% pada purata skor F-mengukur. Selanjutnya, kami menerapkan algoritma pengelompokan baru dalam menganalisis dokumen Web dalam komuniti forum dalam talian. Algoritma pengelompokan berorientasikan topik dikembangkan untuk membantu orang dalam menilai, mengklasifikasikan dan mencari dokumen Web di komuniti forum besar. [[EENNDD]] model dokumen; pokok akhiran; ukuran kesamaan"], [{"string": "Application specific data replication for edge services No contact information provided yet.", "keywords": ["distributed objects", "availability", "data replication", "wide area networks", "edge services"], "combined": "Application specific data replication for edge services No contact information provided yet. [[EENNDD]] distributed objects; availability; data replication; wide area networks; edge services"}, "Replikasi data khusus aplikasi untuk perkhidmatan canggih Belum ada maklumat hubungan yang diberikan. [[EENNDD]] objek yang diedarkan; ketersediaan; replikasi data; rangkaian kawasan luas; perkhidmatan tepi"], [{"string": "Detecting the origin of text segments efficiently In the origin detection problem an algorithm is given a set S of documents, ordered by creation time, and a query document D. It needs to output for every consecutive sequence of k alphanumeric terms in D the earliest document in $S$ in which the sequence appeared (if such a document exists). Algorithms for the origin detection problem can, for example, be used to detect the \"origin\" of text segments in D and thus to detect novel content in D. They can also find the document from which the author of D has copied the most (or show that D is mostly original.) We concentrate on solutions that use only a fixed amount of memory. We propose novel algorithms for this problem and evaluate them together with a large number of previously published algorithms. Our results show that (1) detecting the origin of text segments efficiently can be done with very high accuracy even when the space used is less than 1% of the size of the documents in $S$, (2) the precision degrades smoothly with the amount of available space, (3) various estimation techniques can be used to increase the performance of the algorithms.", "keywords": ["shingling", "document overlap", "miscellaneous"], "combined": "Detecting the origin of text segments efficiently In the origin detection problem an algorithm is given a set S of documents, ordered by creation time, and a query document D. It needs to output for every consecutive sequence of k alphanumeric terms in D the earliest document in $S$ in which the sequence appeared (if such a document exists). Algorithms for the origin detection problem can, for example, be used to detect the \"origin\" of text segments in D and thus to detect novel content in D. They can also find the document from which the author of D has copied the most (or show that D is mostly original.) We concentrate on solutions that use only a fixed amount of memory. We propose novel algorithms for this problem and evaluate them together with a large number of previously published algorithms. Our results show that (1) detecting the origin of text segments efficiently can be done with very high accuracy even when the space used is less than 1% of the size of the documents in $S$, (2) the precision degrades smoothly with the amount of available space, (3) various estimation techniques can be used to increase the performance of the algorithms. [[EENNDD]] shingling; document overlap; miscellaneous"}, "Mengesan asal segmen teks dengan cekap Dalam masalah pengesanan asal, algoritma diberikan satu set dokumen S, disusun mengikut waktu penciptaan, dan dokumen pertanyaan D. Ia perlu dikeluarkan untuk setiap urutan istilah alfanumerik k dalam D dokumen paling awal dalam $ S $ di mana urutan muncul (jika ada dokumen seperti itu). Algoritma untuk masalah pengesanan asal boleh, misalnya, digunakan untuk mengesan \"asal\" segmen teks di D dan dengan demikian untuk mengesan kandungan novel dalam D. Mereka juga dapat mencari dokumen yang paling banyak disalin oleh pengarang D ( atau menunjukkan bahawa D kebanyakannya asli.) Kami menumpukan perhatian pada penyelesaian yang hanya menggunakan jumlah memori yang tetap. Kami mencadangkan algoritma baru untuk masalah ini dan menilai mereka bersama dengan sebilangan besar algoritma yang diterbitkan sebelumnya. Hasil kajian kami menunjukkan bahawa (1) mengesan asal segmen teks dengan cekap dapat dilakukan dengan ketepatan yang sangat tinggi walaupun ruang yang digunakan kurang dari 1% ukuran dokumen dalam $ S $, (2) ketepatan merosot dengan lancar dengan jumlah ruang yang ada, (3) pelbagai teknik perkiraan dapat digunakan untuk meningkatkan prestasi algoritma. [[EENNDD]] kayap; pertindihan dokumen; pelbagai"], [{"string": "Authoring and annotation of web pages in CREAM No contact information provided yet.", "keywords": ["annotation", "semanticweb", "document and text editing", "rdf", "metadata"], "combined": "Authoring and annotation of web pages in CREAM No contact information provided yet. [[EENNDD]] annotation; semanticweb; document and text editing; rdf; metadata"}, "Pengarang dan anotasi halaman web di CREAM Belum ada maklumat hubungan yang diberikan. [[EENNDD]] anotasi; semanticweb; penyuntingan dokumen dan teks; rdf; metadata"], [{"string": "Living the TV revolution: unite MHP to the web or face IDTV irrelevance! No contact information provided yet.", "keywords": ["computers in other systems", "web contents transcoding", "idtv", "dvb", "mhp"], "combined": "Living the TV revolution: unite MHP to the web or face IDTV irrelevance! No contact information provided yet. [[EENNDD]] computers in other systems; web contents transcoding; idtv; dvb; mhp"}, "Menjalani revolusi TV: satukan MHP ke web atau hadapi IDTV yang tidak relevan! Belum ada maklumat hubungan yang diberikan. [[EENNDD]] komputer dalam sistem lain; transkoding kandungan web; idtv; dvb; mhp"], [{"string": "Focused crawling: experiences in a real world project An abstract is not available.", "keywords": ["information retrieval", "thesaurus", "miscellaneous", "topic", "crawling"], "combined": "Focused crawling: experiences in a real world project An abstract is not available. [[EENNDD]] information retrieval; thesaurus; miscellaneous; topic; crawling"}, "Perayapan fokus: pengalaman dalam projek dunia nyata Abstrak tidak tersedia. [[EENNDD]] pengambilan maklumat; tesaurus; pelbagai; topik; merangkak"], [{"string": "Detecting semantic cloaking on the web No contact information provided yet.", "keywords": ["web search engine", "information search and retrieval", "spam"], "combined": "Detecting semantic cloaking on the web No contact information provided yet. [[EENNDD]] web search engine; information search and retrieval; spam"}, "Mengesan semantik semantik di web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] enjin carian web; pencarian dan pengambilan maklumat; spam"], [{"string": "Web4CE: accessing web-based applications on consumer devices In a world where all devices will be interconnected, the boundaries between the different devices will start to disappear. Devices will be able to access each other's applications; sessions can be suspended on one device and resumed on another device; devices can serve as each other's input and output device, and all devices will be able to connect to the Internet. This will give true mobility to the user as he/she will not be restricted to the time and location where he/she accesses an application. Of course, we need a variety of different mechanisms and technologies to enable this, such as: Remote rendering of UIs on other devices in the network. Infrastructure for discovering client and servers in a network. Mechanisms to exchange capability information between devices, and to adapt the UI based on these capabilities. Mechanisms to deal with session migration.Support for a wide range of consumer devices, ranging from mobile phones to high-end TVs.", "keywords": ["mobile and tv-based services", "pervasive web applications", "embedded browsers", "on-line information services", "dlna", "ce-html", "user interfaces", "cea", "consumer electronics devices", "web 2.0", "upnp", "web4ce", "ajax", "w3c"], "combined": "Web4CE: accessing web-based applications on consumer devices In a world where all devices will be interconnected, the boundaries between the different devices will start to disappear. Devices will be able to access each other's applications; sessions can be suspended on one device and resumed on another device; devices can serve as each other's input and output device, and all devices will be able to connect to the Internet. This will give true mobility to the user as he/she will not be restricted to the time and location where he/she accesses an application. Of course, we need a variety of different mechanisms and technologies to enable this, such as: Remote rendering of UIs on other devices in the network. Infrastructure for discovering client and servers in a network. Mechanisms to exchange capability information between devices, and to adapt the UI based on these capabilities. Mechanisms to deal with session migration.Support for a wide range of consumer devices, ranging from mobile phones to high-end TVs. [[EENNDD]] mobile and tv-based services; pervasive web applications; embedded browsers; on-line information services; dlna; ce-html; user interfaces; cea; consumer electronics devices; web 2.0; upnp; web4ce; ajax; w3c"}, "Web4CE: mengakses aplikasi berasaskan web pada peranti pengguna Di dunia di mana semua peranti akan saling berkaitan, batas antara peranti yang berlainan akan mulai hilang. Peranti akan dapat mengakses aplikasi masing-masing; sesi boleh ditangguhkan pada satu peranti dan disambung semula pada peranti lain; peranti boleh berfungsi sebagai peranti input dan output masing-masing, dan semua peranti dapat menyambung ke Internet. Ini akan memberikan mobiliti sebenarnya kepada pengguna kerana dia tidak akan terhad pada waktu dan lokasi di mana dia mengakses aplikasi. Sudah tentu, kami memerlukan pelbagai mekanisme dan teknologi yang berbeza untuk memungkinkannya, seperti: Pemaparan UI jarak jauh pada peranti lain dalam rangkaian. Infrastruktur untuk menemui pelanggan dan pelayan dalam rangkaian. Mekanisme untuk menukar maklumat keupayaan antara peranti, dan untuk menyesuaikan UI berdasarkan kemampuan ini. Mekanisme untuk menangani migrasi sesi. Sokongan untuk pelbagai peranti pengguna, mulai dari telefon bimbit hingga TV kelas atas. [[EENNDD]] perkhidmatan berasaskan mudah alih dan tv; aplikasi web yang meluas; penyemak imbas terbenam; perkhidmatan maklumat dalam talian; dlna; ce-html; antara muka pengguna; cea; peranti elektronik pengguna; laman web 2.0; upnp; web4ce; ajax; w3c"], [{"string": "Handling forecast errors while bidding for display advertising Most of the online advertising today is sold via an auction, which requires the advertiser to respond with a valid bid within a fraction of a second. As such, most advertisers employ bidding agents to submit bids on their behalf. The architecture of such agents typically has (1) an offline optimization phase which incorporates the bidder's knowledge about the market and (2) an online bidding strategy which simply executes the offline strategy. The online strategy is typically highly dependent on both supply and expected price distributions, both of which are forecast using traditional machine learning methods. In this work we investigate the optimum strategy of the bidding agent when faced with incorrect forecasts. At a high level, the agent can invest resources in improving the forecasts, or can tighten the loop between successive offline optimization cycles in order to detect errors more quickly. We show analytically that the latter strategy, while simple, is extremely effective in dealing with forecast errors, and confirm this finding with experimental evaluations.", "keywords": ["adaptive bidding", "general", "bidding agents", "applications", "ad exchanges"], "combined": "Handling forecast errors while bidding for display advertising Most of the online advertising today is sold via an auction, which requires the advertiser to respond with a valid bid within a fraction of a second. As such, most advertisers employ bidding agents to submit bids on their behalf. The architecture of such agents typically has (1) an offline optimization phase which incorporates the bidder's knowledge about the market and (2) an online bidding strategy which simply executes the offline strategy. The online strategy is typically highly dependent on both supply and expected price distributions, both of which are forecast using traditional machine learning methods. In this work we investigate the optimum strategy of the bidding agent when faced with incorrect forecasts. At a high level, the agent can invest resources in improving the forecasts, or can tighten the loop between successive offline optimization cycles in order to detect errors more quickly. We show analytically that the latter strategy, while simple, is extremely effective in dealing with forecast errors, and confirm this finding with experimental evaluations. [[EENNDD]] adaptive bidding; general; bidding agents; applications; ad exchanges"}, "Mengendalikan kesilapan ramalan semasa membida iklan paparan Sebilangan besar iklan dalam talian hari ini dijual melalui lelong, yang mengharuskan pengiklan bertindak balas dengan tawaran yang sah dalam masa beberapa saat. Oleh itu, kebanyakan pengiklan menggunakan ejen pembida untuk mengemukakan tawaran bagi pihak mereka. Seni bina ejen tersebut biasanya mempunyai (1) fasa pengoptimuman luar talian yang menggabungkan pengetahuan penawar mengenai pasaran dan (2) strategi penawaran dalam talian yang hanya melaksanakan strategi luar talian. Strategi dalam talian biasanya sangat bergantung pada penawaran dan pengagihan harga yang diharapkan, kedua-duanya diramalkan menggunakan kaedah pembelajaran mesin tradisional. Dalam karya ini, kami menyiasat strategi optimum ejen pembida apabila menghadapi ramalan yang salah. Pada tahap yang tinggi, ejen boleh melaburkan sumber daya untuk meningkatkan ramalan, atau dapat mengetatkan lingkaran antara kitaran pengoptimuman luar talian berturut-turut untuk mengesan ralat dengan lebih cepat. Kami menunjukkan secara analitik bahawa strategi yang terakhir, walaupun sederhana, sangat berkesan untuk menangani kesalahan ramalan, dan mengesahkan penemuan ini dengan penilaian eksperimental. [[EENNDD]] pembidaan adaptif; umum; ejen pembida; permohonan; pertukaran iklan"], [{"string": "Discovering geographical topics in the twitter stream Micro-blogging services have become indispensable communication tools for online users for disseminating breaking news, eyewitness accounts, individual expression, and protest groups. Recently, Twitter, along with other online social networking services such as Foursquare, Gowalla, Facebook and Yelp, have started supporting location services in their messages, either explicitly, by letting users choose their places, or implicitly, by enabling geo-tagging, which is to associate messages with latitudes and longitudes. This functionality allows researchers to address an exciting set of questions: 1) How is information created and shared across geographical locations, 2) How do spatial and linguistic characteristics of people vary across regions, and 3) How to model human mobility. Although many attempts have been made for tackling these problems, previous methods are either complicated to be implemented or oversimplified that cannot yield reasonable performance. It is a challenge task to discover topics and identify users' interests from these geo-tagged messages due to the sheer amount of data and diversity of language variations used on these location sharing services. In this paper we focus on Twitter and present an algorithm by modeling diversity in tweets based on topical diversity, geographical diversity, and an interest distribution of the user. Furthermore, we take the Markovian nature of a user's location into account. Our model exploits sparse factorial coding of the attributes, thus allowing us to deal with a large and diverse set of covariates efficiently. Our approach is vital for applications such as user profiling, content recommendation and topic tracking. We show high accuracy in location estimation based on our model. Moreover, the algorithm identifies interesting topics based on location and language.", "keywords": ["general", "latent variable inference", "language model", "content analysis and indexing", "user profiling", "graphical model", "twitter", "topic models", "geolocation"], "combined": "Discovering geographical topics in the twitter stream Micro-blogging services have become indispensable communication tools for online users for disseminating breaking news, eyewitness accounts, individual expression, and protest groups. Recently, Twitter, along with other online social networking services such as Foursquare, Gowalla, Facebook and Yelp, have started supporting location services in their messages, either explicitly, by letting users choose their places, or implicitly, by enabling geo-tagging, which is to associate messages with latitudes and longitudes. This functionality allows researchers to address an exciting set of questions: 1) How is information created and shared across geographical locations, 2) How do spatial and linguistic characteristics of people vary across regions, and 3) How to model human mobility. Although many attempts have been made for tackling these problems, previous methods are either complicated to be implemented or oversimplified that cannot yield reasonable performance. It is a challenge task to discover topics and identify users' interests from these geo-tagged messages due to the sheer amount of data and diversity of language variations used on these location sharing services. In this paper we focus on Twitter and present an algorithm by modeling diversity in tweets based on topical diversity, geographical diversity, and an interest distribution of the user. Furthermore, we take the Markovian nature of a user's location into account. Our model exploits sparse factorial coding of the attributes, thus allowing us to deal with a large and diverse set of covariates efficiently. Our approach is vital for applications such as user profiling, content recommendation and topic tracking. We show high accuracy in location estimation based on our model. Moreover, the algorithm identifies interesting topics based on location and language. [[EENNDD]] general; latent variable inference; language model; content analysis and indexing; user profiling; graphical model; twitter; topic models; geolocation"}, "Mencari topik geografi dalam aliran twitter Perkhidmatan blog mikro telah menjadi alat komunikasi yang sangat diperlukan bagi pengguna dalam talian untuk menyebarkan berita terkini, akaun saksi mata, ekspresi individu, dan kumpulan protes. Baru-baru ini, Twitter, bersama dengan perkhidmatan rangkaian sosial dalam talian lain seperti Foursquare, Gowalla, Facebook dan Yelp, telah mula menyokong perkhidmatan lokasi dalam mesej mereka, baik secara eksplisit, dengan membiarkan pengguna memilih tempat mereka, atau secara tersirat, dengan mengaktifkan penandaan geo, yang adalah untuk mengaitkan mesej dengan garis lintang dan garis bujur. Fungsi ini membolehkan para penyelidik untuk menjawab satu set soalan yang menarik: 1) Bagaimana maklumat dibuat dan dikongsi di seluruh lokasi geografi, 2) Bagaimana ciri-ciri spasial dan linguistik orang berbeza-beza di seluruh wilayah, dan 3) Bagaimana model mobiliti manusia. Walaupun banyak usaha telah dilakukan untuk mengatasi masalah ini, kaedah sebelumnya rumit untuk dilaksanakan atau disederhanakan secara berlebihan yang tidak dapat menghasilkan prestasi yang wajar. Ini adalah tugas yang sukar untuk mencari topik dan mengenal pasti minat pengguna dari mesej bertanda geo ini kerana jumlah data dan kepelbagaian variasi bahasa yang digunakan pada perkhidmatan perkongsian lokasi ini. Dalam makalah ini kami memfokuskan pada Twitter dan menyajikan algoritma dengan memodelkan kepelbagaian dalam tweet berdasarkan kepelbagaian topikal, kepelbagaian geografi, dan pengagihan minat pengguna. Selain itu, kami mengambil kira lokasi Markovian dari lokasi pengguna. Model kami mengeksploitasi pengkodan faktor-faktor yang jarang, dan ini membolehkan kami menangani sekumpulan kovariat yang besar dan pelbagai dengan cekap. Pendekatan kami sangat penting untuk aplikasi seperti profil pengguna, cadangan kandungan dan penjejakan topik. Kami menunjukkan ketepatan tinggi dalam anggaran lokasi berdasarkan model kami. Lebih-lebih lagi, algoritma mengenal pasti topik menarik berdasarkan lokasi dan bahasa. [[EENNDD]] umum; inferens pemboleh ubah pendam; model bahasa; analisis kandungan dan pengindeksan; profil pengguna; model grafik; twitter; model topik; geolokasi"], [{"string": "Money, glory and cheap talk: analyzing strategic behavior of contestants in simultaneous crowdsourcing contests on TopCoder.com Crowdsourcing is a new Web phenomenon, in which a firm takes a function once performed in-house and outsources it to a crowd, usually in the form of an open contest.", "keywords": ["crowdsourcing", "all-pay auction", "cheap talk", "miscellaneous", "reputation", "electronic markets", "entry deterrence", "contest"], "combined": "Money, glory and cheap talk: analyzing strategic behavior of contestants in simultaneous crowdsourcing contests on TopCoder.com Crowdsourcing is a new Web phenomenon, in which a firm takes a function once performed in-house and outsources it to a crowd, usually in the form of an open contest. [[EENNDD]] crowdsourcing; all-pay auction; cheap talk; miscellaneous; reputation; electronic markets; entry deterrence; contest"}, "Wang, kemuliaan dan ceramah murah: menganalisis tingkah laku strategik para peserta dalam peraduan crowdsourcing secara serentak di TopCoder.com Crowdsourcing adalah fenomena Web baru, di mana firma mengambil fungsi sebaik sahaja dilakukan secara dalaman dan memberikan sumber kepada orang ramai, biasanya dalam bentuk pertandingan terbuka. [[EENNDD]] khalayak ramai; lelong semua-bayar; ceramah murah; pelbagai; reputasi; pasaran elektronik; pencegahan kemasukan; bertanding"], [{"string": "Querying for meta knowledge The Semantic Web is based on accessing and reusing RDF data from many different sources, which one may assign different levels of authority and credibility. Existing Semantic Web query languages, like SPARQL, have targeted the retrieval, combination and reuse of facts, but have so far ignored all aspects of meta knowledge, such as origins, authorship, recency or certainty of data, to name but a few.", "keywords": ["rdf", "sparql", "semantic web"], "combined": "Querying for meta knowledge The Semantic Web is based on accessing and reusing RDF data from many different sources, which one may assign different levels of authority and credibility. Existing Semantic Web query languages, like SPARQL, have targeted the retrieval, combination and reuse of facts, but have so far ignored all aspects of meta knowledge, such as origins, authorship, recency or certainty of data, to name but a few. [[EENNDD]] rdf; sparql; semantic web"}, "Meminta pengetahuan meta Web Semantik didasarkan pada mengakses dan menggunakan kembali data RDF dari banyak sumber yang berbeza, yang mana satu dapat memberikan tahap kewibawaan dan kredibiliti yang berbeza. Bahasa pertanyaan Semantik Web yang ada, seperti SPARQL, telah menargetkan pengambilan, kombinasi dan penggunaan semula fakta, tetapi sejauh ini telah mengabaikan semua aspek pengetahuan meta, seperti asal usul, kepengarangan, kebaruan atau kepastian data, untuk beberapa nama. [[EENNDD]] rdf; sparql; web semantik"], [{"string": "An experimental study on large-scale web categorization No contact information provided yet.", "keywords": ["text categorization", "algorithm complexity", "parameter tuning strategies", "very large web taxonomies"], "combined": "An experimental study on large-scale web categorization No contact information provided yet. [[EENNDD]] text categorization; algorithm complexity; parameter tuning strategies; very large web taxonomies"}, "Satu kajian eksperimental mengenai pengkategorian web berskala besar Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengkategorian teks; kerumitan algoritma; strategi penalaan parameter; taksonomi web yang sangat besar"], [{"string": "Towards extracting flickr tag semantics We address the problem of extracting semantics of tags -- short, unstructured text-labels assigned to resources on the Web -- based on each tag's metadata patterns. In particular, we describe an approach for extracting place and event semantics for tags that are assigned to photos on Flickr, a popular photo sharing website supporting time and location (latitude/longitude) metadata. The approach can be generalized to other domains where text terms can be extracted and associated with metadata patterns, such as geo-annotated web pages.", "keywords": ["place identification", "word semantics", "event identification", "miscellaneous", "tagging systems"], "combined": "Towards extracting flickr tag semantics We address the problem of extracting semantics of tags -- short, unstructured text-labels assigned to resources on the Web -- based on each tag's metadata patterns. In particular, we describe an approach for extracting place and event semantics for tags that are assigned to photos on Flickr, a popular photo sharing website supporting time and location (latitude/longitude) metadata. The approach can be generalized to other domains where text terms can be extracted and associated with metadata patterns, such as geo-annotated web pages. [[EENNDD]] place identification; word semantics; event identification; miscellaneous; tagging systems"}, "Ke arah mengekstrak semantik tag flickr Kami menangani masalah mengekstrak semantik tag - label teks pendek dan tidak berstruktur yang ditugaskan untuk sumber di Web - berdasarkan corak metadata setiap tag. Khususnya, kami menerangkan pendekatan untuk mengekstrak semantik tempat dan acara untuk tag yang diberikan pada foto di Flickr, sebuah laman web perkongsian foto yang popular yang menyokong metadata masa dan lokasi (garis lintang / garis bujur). Pendekatan ini dapat digeneralisasikan ke domain lain di mana istilah teks dapat diekstrak dan dikaitkan dengan corak metadata, seperti halaman web beranotasi geo. [[EENNDD]] pengenalan tempat; semantik perkataan; pengenalan acara; pelbagai; sistem penandaan"], [{"string": "Analyzing and accelerating web access in a school in peri-urban India While computers and Internet access have growing penetration amongst schools in the developing world, intermittent connectivity and limited bandwidth often prevent them from being fully utilized by students and teachers. In this paper, we make two contributions to help address this problem. First, we characterize six weeks of HTTP traffic from a primary school outside of Bangalore, India, illuminating opportunities and constraints for improving performance in such settings. Second, we deploy an aggressive caching and prefetching engine and show that it accelerates a user's overall browsing experience (apart from video content) by 2.8x. Our accelerator leverages innovative techniques that have been proposed, but not evaluated in detail, including the effectiveness of serving stale pages, cached page highlighting, and client-side prefetching. Unlike proxy-based techniques, our system is bundled as an open-source Firefox plugin and runs directly on client machines. This allows easy installation and configuration by end users, which is especially important in developing regions where a lack of permissions or technical expertise often prevents modification of internal network settings.", "keywords": ["computer uses in education", "hypertext/hypermedia", "connectivity", "web acceleration", "browser extension"], "combined": "Analyzing and accelerating web access in a school in peri-urban India While computers and Internet access have growing penetration amongst schools in the developing world, intermittent connectivity and limited bandwidth often prevent them from being fully utilized by students and teachers. In this paper, we make two contributions to help address this problem. First, we characterize six weeks of HTTP traffic from a primary school outside of Bangalore, India, illuminating opportunities and constraints for improving performance in such settings. Second, we deploy an aggressive caching and prefetching engine and show that it accelerates a user's overall browsing experience (apart from video content) by 2.8x. Our accelerator leverages innovative techniques that have been proposed, but not evaluated in detail, including the effectiveness of serving stale pages, cached page highlighting, and client-side prefetching. Unlike proxy-based techniques, our system is bundled as an open-source Firefox plugin and runs directly on client machines. This allows easy installation and configuration by end users, which is especially important in developing regions where a lack of permissions or technical expertise often prevents modification of internal network settings. [[EENNDD]] computer uses in education; hypertext/hypermedia; connectivity; web acceleration; browser extension"}, "Menganalisis dan mempercepat akses web di sebuah sekolah di bandar pinggir bandar India Walaupun akses komputer dan Internet semakin meningkat di antara sekolah-sekolah di negara membangun, penyambungan berselang dan lebar jalur yang terhad sering menghalangnya daripada digunakan sepenuhnya oleh pelajar dan guru. Dalam makalah ini, kami memberikan dua sumbangan untuk membantu mengatasi masalah ini. Pertama, kami mencirikan trafik HTTP selama enam minggu dari sekolah rendah di luar Bangalore, India, menerangi peluang dan kekangan untuk meningkatkan prestasi dalam tetapan tersebut. Kedua, kami menggunakan mesin cache dan prefetching yang agresif dan menunjukkan bahawa ia mempercepat keseluruhan pengalaman melayari pengguna (selain daripada kandungan video) sebanyak 2.8x. Pemecut kami memanfaatkan teknik inovatif yang telah diusulkan, tetapi tidak dinilai secara terperinci, termasuk keberkesanan penyampaian halaman basi, penonjolan halaman cache, dan prapetap sisi pelanggan. Tidak seperti teknik berasaskan proksi, sistem kami digabungkan sebagai plugin Firefox sumber terbuka dan berjalan terus pada mesin pelanggan. Ini memungkinkan pemasangan dan konfigurasi yang mudah oleh pengguna akhir, yang sangat penting di kawasan membangun di mana kekurangan izin atau kepakaran teknikal sering menghalang pengubahsuaian tetapan rangkaian dalaman. [[EENNDD]] penggunaan komputer dalam pendidikan; hiperteks / hipermedia; penyambungan; pecutan web; pelanjutan penyemak imbas"], [{"string": "CiteSeerx: an architecture and web service design for an academic document search engine No contact information provided yet.", "keywords": ["system architecture", "scalability", "data model"], "combined": "CiteSeerx: an architecture and web service design for an academic document search engine No contact information provided yet. [[EENNDD]] system architecture; scalability; data model"}, "CiteSeerx: seni bina dan reka bentuk perkhidmatan web untuk mesin carian dokumen akademik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] seni bina sistem; skalabiliti; model data"], [{"string": "AwareDAV: a generic WebDAV notification framework and implementation No contact information provided yet.", "keywords": ["applications", "event notification", "webdav", "awaredav", "cscw"], "combined": "AwareDAV: a generic WebDAV notification framework and implementation No contact information provided yet. [[EENNDD]] applications; event notification; webdav; awaredav; cscw"}, "AwareDAV: rangka dan pelaksanaan pemberitahuan WebDAV generik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] aplikasi; pemberitahuan acara; webdav; awaredav; cscw"], [{"string": "WebViews: accessing personalized web content and services An abstract is not available.", "keywords": ["smart bookmarks", "personalization", "web clipping", "information delivery", "portable devices", "dynamic content", "voice interfaces", "electronic commerce", "content transcoding", "wrappers"], "combined": "WebViews: accessing personalized web content and services An abstract is not available. [[EENNDD]] smart bookmarks; personalization; web clipping; information delivery; portable devices; dynamic content; voice interfaces; electronic commerce; content transcoding; wrappers"}, "Paparan Web: mengakses kandungan dan perkhidmatan web yang diperibadikan Abstrak tidak tersedia. [[EENNDD]] penanda buku pintar; pemperibadian; keratan web; penyampaian maklumat; peranti mudah alih; kandungan dinamik; antara muka suara; perdagangan elektronik; transkoding kandungan; pembungkus"], [{"string": "Conceptual linking: ontology-based open hypermedia An abstract is not available.", "keywords": ["open hypermedia", "hypertext/hypermedia", "ontology", "navigation", "link service", "metadata"], "combined": "Conceptual linking: ontology-based open hypermedia An abstract is not available. [[EENNDD]] open hypermedia; hypertext/hypermedia; ontology; navigation; link service; metadata"}, "Menghubung kait konseptual: hipermedia terbuka berasaskan ontologi Abstrak tidak tersedia. [[EENNDD]] hipermedia terbuka; hiperteks / hipermedia; ontologi; pelayaran; perkhidmatan pautan; metadata"], [{"string": "Web services security configuration in a service-oriented architecture No contact information provided yet.", "keywords": ["model-driven architecture", "best practice pattern", "service-oriented architecture", "methodologies", "web services security", "security configuration"], "combined": "Web services security configuration in a service-oriented architecture No contact information provided yet. [[EENNDD]] model-driven architecture; best practice pattern; service-oriented architecture; methodologies; web services security; security configuration"}, "Konfigurasi keselamatan perkhidmatan web dalam seni bina berorientasikan perkhidmatan Belum ada maklumat hubungan yang diberikan. [[EENNDD]] seni bina berdasarkan model; corak amalan terbaik; seni bina berorientasikan perkhidmatan; metodologi; keselamatan perkhidmatan web; konfigurasi keselamatan"], [{"string": "Using static analysis for Ajax intrusion detection We present a static control-flow analysis for JavaScript programs running in a web browser. Our analysis tackles numerous challenges posed by modern web applications including asynchronous communication, frameworks, and dynamic code generation. We use our analysis to extract a model of expected client behavior as seen from the server, and build an intrusion-prevention proxy for the server: the proxy intercepts client requests and disables those that do not meet the expected behavior. We insert random asynchronous requests to foil mimicry attacks. Finally, we evaluate our technique against several real applications and show that it protects against an attack in a widely-used web application.", "keywords": ["ajax", "intrusion detection", "javascript", "control-flow analysis"], "combined": "Using static analysis for Ajax intrusion detection We present a static control-flow analysis for JavaScript programs running in a web browser. Our analysis tackles numerous challenges posed by modern web applications including asynchronous communication, frameworks, and dynamic code generation. We use our analysis to extract a model of expected client behavior as seen from the server, and build an intrusion-prevention proxy for the server: the proxy intercepts client requests and disables those that do not meet the expected behavior. We insert random asynchronous requests to foil mimicry attacks. Finally, we evaluate our technique against several real applications and show that it protects against an attack in a widely-used web application. [[EENNDD]] ajax; intrusion detection; javascript; control-flow analysis"}, "Menggunakan analisis statik untuk pengesanan pencerobohan Ajax Kami menyajikan analisis aliran-aliran statik untuk program JavaScript yang berjalan di penyemak imbas web. Analisis kami menangani banyak cabaran yang ditimbulkan oleh aplikasi web moden termasuk komunikasi, kerangka kerja tak segerak, dan penjanaan kod dinamik. Kami menggunakan analisis kami untuk mengekstrak model tingkah laku klien yang diharapkan seperti yang dilihat dari pelayan, dan membina proksi pencegahan pencerobohan untuk pelayan: proksi memintas permintaan pelanggan dan melumpuhkan mereka yang tidak memenuhi tingkah laku yang diharapkan. Kami memasukkan permintaan tak segerak secara rawak untuk menggagalkan serangan peniruan. Akhirnya, kami menilai teknik kami terhadap beberapa aplikasi nyata dan menunjukkan bahawa ia melindungi dari serangan dalam aplikasi web yang banyak digunakan. [[EENNDD]] ajax; pengesanan pencerobohan; javascript; analisis aliran kawalan"], [{"string": "Using web structure for classifying and describing web pages No contact information provided yet.", "keywords": ["web structure", "classification", "anchortext", "svm", "evaluation", "cluster naming", "entropy based feature extraction", "web directory"], "combined": "Using web structure for classifying and describing web pages No contact information provided yet. [[EENNDD]] web structure; classification; anchortext; svm; evaluation; cluster naming; entropy based feature extraction; web directory"}, "Menggunakan struktur web untuk mengelaskan dan menerangkan laman web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] struktur web; pengelasan; sauh; svm; penilaian; penamaan kluster; pengekstrakan ciri berdasarkan entropi; direktori web"], [{"string": "Using web browser interactions to predict task No contact information provided yet.", "keywords": ["field study", "task prediction", "decision tree", "web", "information filtering", "task"], "combined": "Using web browser interactions to predict task No contact information provided yet. [[EENNDD]] field study; task prediction; decision tree; web; information filtering; task"}, "Menggunakan interaksi penyemak imbas web untuk meramalkan tugas Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kajian lapangan; ramalan tugas; pokok keputusan; laman web; tapisan maklumat; tugas"], [{"string": "Semantic link based top-K join queries in P2P networks No contact information provided yet.", "keywords": ["peer-to-peer", "semantic link", "top-k", "join query"], "combined": "Semantic link based top-K join queries in P2P networks No contact information provided yet. [[EENNDD]] peer-to-peer; semantic link; top-k; join query"}, "Pertanyaan gabungan K-top berasaskan pautan semantik dalam rangkaian P2P Belum ada maklumat hubungan yang diberikan. [[EENNDD]] rakan sebaya; pautan semantik; bahagian atas-k; sertai pertanyaan"], [{"string": "Compressing and searching XML data via two zips No contact information provided yet.", "keywords": ["labeled trees", "xml compression and indexing"], "combined": "Compressing and searching XML data via two zips No contact information provided yet. [[EENNDD]] labeled trees; xml compression and indexing"}, "Memampatkan dan mencari data XML melalui dua zip Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] pokok berlabel; pemampatan dan pengindeksan xml"], [{"string": "Constructing virtual documents for ontology matching No contact information provided yet.", "keywords": ["neighboring operation", "linguistic matching", "vector space model", "ontology matching", "formulation", "description"], "combined": "Constructing virtual documents for ontology matching No contact information provided yet. [[EENNDD]] neighboring operation; linguistic matching; vector space model; ontology matching; formulation; description"}, "Membuat dokumen maya untuk pemadanan ontologi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] operasi jiran; padanan linguistik; model ruang vektor; pemadanan ontologi; rumusan; penerangan"], [{"string": "LSH forest: self-tuning indexes for similarity search No contact information provided yet.", "keywords": ["similarity indexes", "peer-to-peer", "performance evaluation"], "combined": "LSH forest: self-tuning indexes for similarity search No contact information provided yet. [[EENNDD]] similarity indexes; peer-to-peer; performance evaluation"}, "Hutan LSH: indeks penyesuaian diri untuk carian kesamaan Belum ada maklumat hubungan. [[EENNDD]] indeks kesamaan; rakan sebaya; penilaian prestasi"], [{"string": "HPG: a tool for presentation generation in WIS No contact information provided yet.", "keywords": ["xslt", "presentation generation", "wis", "rdf", "hypermedia"], "combined": "HPG: a tool for presentation generation in WIS No contact information provided yet. [[EENNDD]] xslt; presentation generation; wis; rdf; hypermedia"}, "HPG: alat untuk menghasilkan persembahan dalam WIS Belum ada maklumat hubungan yang diberikan. [[EENNDD]] xslt; penjanaan persembahan; bijaksana; rdf; hipermedia"], [{"string": "XQuery containment in presence of variable binding dependencies No contact information provided yet.", "keywords": ["variable binding dependency", "xquery containment", "database applications", "expressions and their representation"], "combined": "XQuery containment in presence of variable binding dependencies No contact information provided yet. [[EENNDD]] variable binding dependency; xquery containment; database applications; expressions and their representation"}, "Pembendungan XQuery dengan adanya pergantungan pengikatan berubah-ubah Belum ada maklumat hubungan yang diberikan. [[EENNDD]] kebergantungan pengikat pemboleh ubah; pembendungan xquery; aplikasi pangkalan data; ungkapan dan perwakilannya"], [{"string": "Ranking definitions with supervised learning methods No contact information provided yet.", "keywords": ["text mining", "search of definitions", "classification", "web search", "miscellaneous", "ordinal regression", "web mining"], "combined": "Ranking definitions with supervised learning methods No contact information provided yet. [[EENNDD]] text mining; search of definitions; classification; web search; miscellaneous; ordinal regression; web mining"}, "Definisi peringkat dengan kaedah pembelajaran yang diawasi Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlombongan teks; mencari definisi; pengelasan; carian sesawang; pelbagai; regresi ordinal; perlombongan web"], [{"string": "Context-sensitive query auto-completion Query auto completion is known to provide poor predictions of the user's query when her input prefix is very short (e.g., one or two characters). In this paper we show that context, such as the user's recent queries, can be used to improve the prediction quality considerably even for such short prefixes. We propose a context-sensitive query auto completion algorithm, NearestCompletion, which outputs the completions of the user's input that are most similar to the context queries. To measure similarity, we represent queries and contexts as high-dimensional term-weighted vectors and resort to cosine similarity. The mapping from queries to vectors is done through a new query expansion technique that we introduce, which expands a query by traversing the query recommendation tree rooted at the query.", "keywords": ["context-awareness", "information search and retrieval", "query auto-completion", "query expansion"], "combined": "Context-sensitive query auto-completion Query auto completion is known to provide poor predictions of the user's query when her input prefix is very short (e.g., one or two characters). In this paper we show that context, such as the user's recent queries, can be used to improve the prediction quality considerably even for such short prefixes. We propose a context-sensitive query auto completion algorithm, NearestCompletion, which outputs the completions of the user's input that are most similar to the context queries. To measure similarity, we represent queries and contexts as high-dimensional term-weighted vectors and resort to cosine similarity. The mapping from queries to vectors is done through a new query expansion technique that we introduce, which expands a query by traversing the query recommendation tree rooted at the query. [[EENNDD]] context-awareness; information search and retrieval; query auto-completion; query expansion"}, "Penyelesaian automatik pertanyaan sensitif konteks Penyelesaian automatik pertanyaan diketahui memberikan ramalan buruk mengenai pertanyaan pengguna apabila awalan inputnya sangat pendek (mis., Satu atau dua watak). Dalam makalah ini kami menunjukkan bahawa konteks, seperti pertanyaan pengguna baru-baru ini, dapat digunakan untuk meningkatkan kualiti ramalan dengan lebih banyak walaupun untuk awalan pendek tersebut. Kami mencadangkan algoritma penyelesaian automatik pertanyaan sensitif konteks, NearestCompletion, yang menghasilkan penyelesaian input pengguna yang paling serupa dengan pertanyaan konteks. Untuk mengukur kesamaan, kami mewakili pertanyaan dan konteks sebagai vektor berwajaran term dimensi tinggi dan menggunakan persamaan kosinus. Pemetaan dari pertanyaan ke vektor dilakukan melalui teknik pengembangan pertanyaan baru yang kami perkenalkan, yang memperluas permintaan dengan melintasi pohon saran pertanyaan yang berakar pada pertanyaan. [[EENNDD]] kesedaran konteks; pencarian dan pengambilan maklumat; penyelesaian automatik pertanyaan; pengembangan pertanyaan"], [{"string": "Hierarchical organization of unstructured consumer reviews In this paper, we propose to organize the aspects of a specific product into a hierarchy by simultaneously taking advantages of domain structure knowledge as well as consumer reviews. Based on the derived hierarchy, we generate a hierarchical organization of the consumer reviews based on various aspects of the product, and aggregate consumer opinions on the aspects. With such hierarchical organization, people can easily grasp the overview of consumer reviews and opinions on various aspects, as well as seek consumer reviews and opinions on any specific aspect by navigating through the hierarchy. We conduct evaluation on two product review data sets: Liu et al.'s data set containing 314 reviews for five products [2], and our review corpus which is collected from forum Web sites containing 60,786 reviews for five popular products. The experimental results demonstrate the effectiveness of our approach.", "keywords": ["consumer review organization", "product aspect hierarchy"], "combined": "Hierarchical organization of unstructured consumer reviews In this paper, we propose to organize the aspects of a specific product into a hierarchy by simultaneously taking advantages of domain structure knowledge as well as consumer reviews. Based on the derived hierarchy, we generate a hierarchical organization of the consumer reviews based on various aspects of the product, and aggregate consumer opinions on the aspects. With such hierarchical organization, people can easily grasp the overview of consumer reviews and opinions on various aspects, as well as seek consumer reviews and opinions on any specific aspect by navigating through the hierarchy. We conduct evaluation on two product review data sets: Liu et al.'s data set containing 314 reviews for five products [2], and our review corpus which is collected from forum Web sites containing 60,786 reviews for five popular products. The experimental results demonstrate the effectiveness of our approach. [[EENNDD]] consumer review organization; product aspect hierarchy"}, "Organisasi hierarki ulasan pengguna tidak berstruktur Dalam makalah ini, kami mencadangkan untuk mengatur aspek produk tertentu menjadi hierarki dengan mengambil kesempatan secara serentak dari pengetahuan struktur domain serta ulasan pengguna. Berdasarkan hierarki yang diturunkan, kami menghasilkan organisasi pengguna hierarki ulasan berdasarkan pelbagai aspek produk, dan mengumpulkan pendapat pengguna mengenai aspek tersebut. Dengan organisasi hirarki seperti itu, orang dapat dengan mudah memahami gambaran keseluruhan ulasan dan pendapat pengguna mengenai pelbagai aspek, serta mendapatkan ulasan dan pendapat pengguna mengenai aspek tertentu dengan menavigasi hierarki. Kami melakukan penilaian terhadap dua set data tinjauan produk: kumpulan data Liu et al. Yang mengandungi 314 ulasan untuk lima produk [2], dan kumpulan ulasan kami yang dikumpulkan dari laman web forum yang mengandungi 60,786 ulasan untuk lima produk popular. Hasil eksperimen menunjukkan keberkesanan pendekatan kami. [[EENNDD]] organisasi tinjauan pengguna; hierarki aspek produk"], [{"string": "Super-peer-based routing and clustering strategies for RDF-based peer-to-peer networks No contact information provided yet.", "keywords": ["distributed rdf repositories", "schema-based routing", "semantic web", "peer-to-peer"], "combined": "Super-peer-based routing and clustering strategies for RDF-based peer-to-peer networks No contact information provided yet. [[EENNDD]] distributed rdf repositories; schema-based routing; semantic web; peer-to-peer"}, "Strategi perutean dan pengelompokan berasaskan super-peer untuk rangkaian peer-to-peer berasaskan RDF Tidak ada maklumat hubungan yang diberikan. [[EENNDD]] diedarkan repositori rdf; penghalaan berdasarkan skema; web semantik; rakan sebaya"], [{"string": "Towards a theory model for product search With the growing pervasiveness of the Internet, online search for products and services is constantly increasing. Most product search engines are based on adaptations of theoretical models devised for information retrieval. However, the decision mechanism that underlies the process of buying a product is different than the process of locating relevant documents or objects.", "keywords": ["consumer surplus", "ranking", "text mining", "utility theory", "information search and retrieval", "user-generated content", "product search"], "combined": "Towards a theory model for product search With the growing pervasiveness of the Internet, online search for products and services is constantly increasing. Most product search engines are based on adaptations of theoretical models devised for information retrieval. However, the decision mechanism that underlies the process of buying a product is different than the process of locating relevant documents or objects. [[EENNDD]] consumer surplus; ranking; text mining; utility theory; information search and retrieval; user-generated content; product search"}, "Menuju model teori untuk pencarian produk Dengan semakin meluasnya Internet, carian produk dan perkhidmatan dalam talian sentiasa meningkat. Sebilangan besar enjin carian produk dibuat berdasarkan penyesuaian model teori yang dirancang untuk mendapatkan maklumat. Walau bagaimanapun, mekanisme keputusan yang mendasari proses membeli produk adalah berbeza daripada proses mencari dokumen atau objek yang berkaitan. [[EENNDD]] lebihan pengguna; peringkat; perlombongan teks; teori utiliti; pencarian dan pengambilan maklumat; kandungan yang dihasilkan pengguna; carian produk"], [{"string": "Standing on the shoulders of ants: stigmergy in the web Stigmergy is a biological term used when discussing insect or swarm behaviour, and describes a model supporting environmental communication separately from artefacts or agents. This phenomenon is demonstrated in the behavior of ants and their food gathering process when following pheromone trails, or similarly termites and their termite mound building process. What is interesting with this mechanism is that highly organized societies are achieved without an apparent management structure.", "keywords": ["virtual pheromones", "systems and software", "web collaboration", "stigmergy"], "combined": "Standing on the shoulders of ants: stigmergy in the web Stigmergy is a biological term used when discussing insect or swarm behaviour, and describes a model supporting environmental communication separately from artefacts or agents. This phenomenon is demonstrated in the behavior of ants and their food gathering process when following pheromone trails, or similarly termites and their termite mound building process. What is interesting with this mechanism is that highly organized societies are achieved without an apparent management structure. [[EENNDD]] virtual pheromones; systems and software; web collaboration; stigmergy"}, "Berdiri di atas semut: stigmergy di web Stigmergy adalah istilah biologi yang digunakan ketika membincangkan tingkah laku serangga atau kawanan, dan menggambarkan model yang menyokong komunikasi persekitaran secara berasingan dari artefak atau agen. Fenomena ini ditunjukkan dalam tingkah laku semut dan proses pengumpulan makanan mereka ketika mengikuti jejak feromon, atau serupa juga anai-anai dan proses pembuatan gundukan anai-anai mereka. Apa yang menarik dengan mekanisme ini ialah masyarakat yang teratur dapat dicapai tanpa struktur pengurusan yang jelas. [[EENNDD]] feromon maya; sistem dan perisian; kerjasama web; stigmergi"], [{"string": "Leveraging user comments for aesthetic aware image search reranking The increasing number of images available online has created a growing need for efficient ways to search for relevant content. Text-based query search is the most common approach to retrieve images from the Web. In this approach, the similarity between the input query and the metadata of images is used to find relevant information. However, as the amount of available images grows, the number of relevant images also increases, all of them sharing very similar metadata but differing in other visual characteristics. This paper studies the influence of visual aesthetic quality in search results as a complementary attribute to relevance. By considering aesthetics, a new ranking parameter is introduced aimed at improving the quality at the top ranks when large amounts of relevant results exist. Two strategies for aesthetic rating inference are proposed: one based on visual content, another based on the analysis of user comments to detect opinions about the quality of images. The results of a user study with $58$ participants show that the comment-based aesthetic predictor outperforms the visual content-based strategy, and reveals that aesthetic-aware rankings are preferred by users searching for photographs on the Web.", "keywords": ["sentiment analysis", "visual aesthetics modeling", "multimedia information systems", "information search and retrieval", "image search reranking", "opinion mining", "user comments"], "combined": "Leveraging user comments for aesthetic aware image search reranking The increasing number of images available online has created a growing need for efficient ways to search for relevant content. Text-based query search is the most common approach to retrieve images from the Web. In this approach, the similarity between the input query and the metadata of images is used to find relevant information. However, as the amount of available images grows, the number of relevant images also increases, all of them sharing very similar metadata but differing in other visual characteristics. This paper studies the influence of visual aesthetic quality in search results as a complementary attribute to relevance. By considering aesthetics, a new ranking parameter is introduced aimed at improving the quality at the top ranks when large amounts of relevant results exist. Two strategies for aesthetic rating inference are proposed: one based on visual content, another based on the analysis of user comments to detect opinions about the quality of images. The results of a user study with $58$ participants show that the comment-based aesthetic predictor outperforms the visual content-based strategy, and reveals that aesthetic-aware rankings are preferred by users searching for photographs on the Web. [[EENNDD]] sentiment analysis; visual aesthetics modeling; multimedia information systems; information search and retrieval; image search reranking; opinion mining; user comments"}, "Memanfaatkan komen pengguna untuk penarafan carian imej yang peka terhadap estetika Peningkatan jumlah gambar yang tersedia dalam talian telah menimbulkan keperluan untuk kaedah yang efisien untuk mencari kandungan yang relevan. Pencarian pertanyaan berdasarkan teks adalah pendekatan yang paling biasa untuk mengambil gambar dari Web. Dalam pendekatan ini, persamaan antara pertanyaan input dan metadata gambar digunakan untuk mencari maklumat yang relevan. Namun, apabila jumlah gambar yang ada bertambah, jumlah gambar yang relevan juga meningkat, semuanya berkongsi metadata yang sangat serupa tetapi berbeza dengan ciri visual yang lain. Makalah ini mengkaji pengaruh kualiti estetika visual dalam hasil carian sebagai atribut pelengkap kepada relevansi. Dengan mempertimbangkan estetika, parameter peringkat baru diperkenalkan bertujuan untuk meningkatkan kualiti di peringkat teratas ketika sejumlah besar hasil yang relevan ada. Dua strategi untuk inferensi penilaian estetik dicadangkan: satu berdasarkan kandungan visual, yang lain berdasarkan analisis komen pengguna untuk mengesan pendapat mengenai kualiti gambar. Hasil kajian pengguna dengan peserta $ 58 $ menunjukkan bahawa peramal estetik berdasarkan komen mengungguli strategi berasaskan kandungan visual, dan mendedahkan bahawa kedudukan yang menyedari estetik lebih disukai oleh pengguna yang mencari gambar di Web. [[EENNDD]] analisis sentimen; pemodelan estetika visual; sistem maklumat multimedia; carian dan pengambilan maklumat; kedudukan carian imej; perlombongan pendapat; komen pengguna"], [{"string": "Volunteer computing: a model of the factors determining contribution to community-based scientific research Volunteer computing is a powerful way to harness distributed resources to perform large-scale tasks, similarly to other types of community-based initiatives. Volunteer computing is based on two pillars: the first is computational - allocating and managing large computing tasks; the second is participative - making large numbers of individuals volunteer their computer resources to a project. While the computational aspects of volunteer computing received much research attention, the participative aspect remains largely unexplored. In this study we aim to address this gap: by drawing on social psychology and online communities research, we develop and test a three-dimensional model of the factors determining volunteer computing users' contribution. We investigate one of the largest volunteer computing projects - SETI@home - by linking survey data about contributors' motivations to their activity logs. Our findings highlight the differences between volunteer computing and other forms of community-based projects, and reveal the intricate relationship between individual motivations, social affiliation, tenure in the project, and resource contribution. Implications for research and practice are discussed.", "keywords": ["seti@home", "crowdsourcing", "motivations", "citizen science", "volunteer computing", "boinc", "online communities"], "combined": "Volunteer computing: a model of the factors determining contribution to community-based scientific research Volunteer computing is a powerful way to harness distributed resources to perform large-scale tasks, similarly to other types of community-based initiatives. Volunteer computing is based on two pillars: the first is computational - allocating and managing large computing tasks; the second is participative - making large numbers of individuals volunteer their computer resources to a project. While the computational aspects of volunteer computing received much research attention, the participative aspect remains largely unexplored. In this study we aim to address this gap: by drawing on social psychology and online communities research, we develop and test a three-dimensional model of the factors determining volunteer computing users' contribution. We investigate one of the largest volunteer computing projects - SETI@home - by linking survey data about contributors' motivations to their activity logs. Our findings highlight the differences between volunteer computing and other forms of community-based projects, and reveal the intricate relationship between individual motivations, social affiliation, tenure in the project, and resource contribution. Implications for research and practice are discussed. [[EENNDD]] seti@home; crowdsourcing; motivations; citizen science; volunteer computing; boinc; online communities"}, "Pengkomputeran sukarelawan: model faktor yang menentukan sumbangan dalam penyelidikan saintifik berasaskan komuniti Pengkomputeran sukarelawan adalah cara yang ampuh untuk memanfaatkan sumber yang diedarkan untuk melakukan tugas berskala besar, sama dengan jenis inisiatif berasaskan komuniti yang lain. Pengkomputeran sukarela berdasarkan dua tonggak: yang pertama adalah pengkomputeran - memperuntukkan dan menguruskan tugas pengkomputeran yang besar; yang kedua adalah partisipatif - menjadikan sebilangan besar individu menjadi sukarelawan sumber komputer mereka untuk projek. Walaupun aspek komputasi sukarelawan pengkomputeran mendapat banyak perhatian penyelidikan, aspek partisipatif masih belum dapat diterokai. Dalam kajian ini kami bertujuan untuk mengatasi jurang ini: dengan menggunakan psikologi sosial dan penyelidikan komuniti dalam talian, kami mengembangkan dan menguji model tiga dimensi faktor-faktor yang menentukan sumbangan pengguna pengkomputeran sukarelawan. Kami menyiasat salah satu projek pengkomputeran sukarelawan terbesar - SETI @ home - dengan menghubungkan data tinjauan mengenai motivasi penyumbang ke log aktiviti mereka. Penemuan kami menyoroti perbezaan antara pengkomputeran sukarelawan dan bentuk lain dari projek berasaskan komuniti, dan mendedahkan hubungan yang rumit antara motivasi individu, hubungan sosial, tempoh dalam projek, dan sumbangan sumber. Implikasi untuk penyelidikan dan praktik dibincangkan. [[EENNDD]] seti @ rumah; sumber orang ramai; motivasi; sains warganegara; pengkomputeran sukarelawan; boinc; komuniti dalam talian"], [{"string": "Learning to model relatedness for news recommendation With the explosive growth of online news readership, recommending interesting news articles to users has become extremely important. While existing Web services such as Yahoo! and Digg attract users' initial clicks by leveraging various kinds of signals, how to engage such users algorithmically after their initial visit is largely under-explored. In this paper, we study the problem of post-click news recommendation. Given that a user has perused a current news article, our idea is to automatically identify \"related\" news articles which the user would like to read afterwards. Specifically, we propose to characterize relatedness between news articles across four aspects: relevance, novelty, connection clarity, and transition smoothness. Motivated by this understanding, we define a set of features to capture each of these aspects and put forward a learning approach to model relatedness. In order to quantitatively evaluate our proposed measures and learn a unified relatedness function, we construct a large test collection based on a four-month commercial news corpus with editorial judgments. The experimental results show that the proposed heuristics can indeed capture relatedness, and that the learned unified relatedness function works quite effectively.", "keywords": ["connection clarity", "novelty", "learning", "relatedness", "transition smoothness", "post-click news recommendation", "relevance"], "combined": "Learning to model relatedness for news recommendation With the explosive growth of online news readership, recommending interesting news articles to users has become extremely important. While existing Web services such as Yahoo! and Digg attract users' initial clicks by leveraging various kinds of signals, how to engage such users algorithmically after their initial visit is largely under-explored. In this paper, we study the problem of post-click news recommendation. Given that a user has perused a current news article, our idea is to automatically identify \"related\" news articles which the user would like to read afterwards. Specifically, we propose to characterize relatedness between news articles across four aspects: relevance, novelty, connection clarity, and transition smoothness. Motivated by this understanding, we define a set of features to capture each of these aspects and put forward a learning approach to model relatedness. In order to quantitatively evaluate our proposed measures and learn a unified relatedness function, we construct a large test collection based on a four-month commercial news corpus with editorial judgments. The experimental results show that the proposed heuristics can indeed capture relatedness, and that the learned unified relatedness function works quite effectively. [[EENNDD]] connection clarity; novelty; learning; relatedness; transition smoothness; post-click news recommendation; relevance"}, "Belajar untuk memperagakan model untuk mendapatkan saranan berita Dengan pertumbuhan pembaca berita dalam talian yang sangat meluas, mengesyorkan artikel berita yang menarik kepada pengguna menjadi sangat penting. Walaupun perkhidmatan Web yang ada seperti Yahoo! dan Digg menarik klik awal pengguna dengan memanfaatkan pelbagai jenis isyarat, bagaimana melibatkan pengguna sedemikian secara algoritma setelah lawatan awal mereka sebahagian besarnya tidak diterokai. Dalam makalah ini, kami mengkaji masalah cadangan berita pasca klik. Memandangkan pengguna telah membaca artikel berita terkini, idea kami adalah untuk secara automatik mengenal pasti artikel berita \"berkaitan\" yang ingin dibaca pengguna selepas itu. Secara khusus, kami mencadangkan untuk mencirikan hubungan antara artikel berita dalam empat aspek: relevansi, kebaruan, kejelasan hubungan, dan kelancaran peralihan. Didorong oleh pemahaman ini, kami menentukan satu set fitur untuk menangkap setiap aspek ini dan mengemukakan pendekatan pembelajaran untuk kaitan model. Untuk menilai secara kuantitatif langkah-langkah yang dicadangkan dan mempelajari fungsi kesatuan yang disatukan, kami membuat koleksi ujian besar berdasarkan korporat berita komersial empat bulan dengan penilaian editorial. Hasil eksperimen menunjukkan bahawa heuristik yang dicadangkan memang dapat menangkap hubungan, dan fungsi hubungan yang dipelajari berfungsi dengan berkesan. [[EENNDD]] kejelasan sambungan; kebaharuan; belajar; pertalian; kelancaran peralihan; cadangan berita pasca klik; kesesuaian"], [{"string": "Comparing apples and oranges: normalized pagerank for evolving graphs PageRank is the best known technique for link-based importance ranking. The computed importance scores, however, are not directly comparable across different snapshots of an evolving graph. We present an efficiently computable normalization for PageRank scores that makes them comparable across graphs. Furthermore, we show that the normalized PageRank scores are robust to non-local changes in the graph, unlike the standard PageRank measure.", "keywords": ["web dynamics", "pagerank", "miscellaneous", "web graph"], "combined": "Comparing apples and oranges: normalized pagerank for evolving graphs PageRank is the best known technique for link-based importance ranking. The computed importance scores, however, are not directly comparable across different snapshots of an evolving graph. We present an efficiently computable normalization for PageRank scores that makes them comparable across graphs. Furthermore, we show that the normalized PageRank scores are robust to non-local changes in the graph, unlike the standard PageRank measure. [[EENNDD]] web dynamics; pagerank; miscellaneous; web graph"}, "Membandingkan epal dan oren: pagerank dinormalisasi untuk grafik berkembang PageRank adalah teknik yang paling terkenal untuk peringkat kepentingan berdasarkan pautan. Walau bagaimanapun, skor kepentingan yang dikira tidak dapat dibandingkan secara langsung di antara gambar-gambar grafik yang berbeza. Kami menyajikan normalisasi yang dapat dikira dengan berkesan untuk skor PageRank yang menjadikannya sebanding di seluruh grafik. Selanjutnya, kami menunjukkan bahawa skor PageRank yang dinormalisasi adalah perubahan yang kuat terhadap perubahan bukan tempatan dalam grafik, tidak seperti ukuran PageRank standard. [[EENNDD]] dinamika web; pagerank; pelbagai; grafik web"], [{"string": "A data-driven sketch of Wikipedia editors Who edits Wikipedia? We attempt to shed light on this question by using aggregated log data from Yahoo!'s browser toolbar in order to analyze Wikipedians' editing behavior in the context of their online lives beyond Wikipedia. We broadly characterize editors by investigating how their online behavior differs from that of other users; e.g., we find that Wikipedia editors search more, read more news, play more games, and, perhaps surprisingly, are more immersed in pop culture. Then we inspect how editors' general interests relate to the articles to which they contribute; e.g., we confirm the intuition that editors show more expertise in their active domains than average users. Our results are relevant as they illuminate novel aspects of what has become many Web users' prevalent source of information and can help in recruiting new editors.", "keywords": ["wikipedia", "web usage", "expertise", "editors"], "combined": "A data-driven sketch of Wikipedia editors Who edits Wikipedia? We attempt to shed light on this question by using aggregated log data from Yahoo!'s browser toolbar in order to analyze Wikipedians' editing behavior in the context of their online lives beyond Wikipedia. We broadly characterize editors by investigating how their online behavior differs from that of other users; e.g., we find that Wikipedia editors search more, read more news, play more games, and, perhaps surprisingly, are more immersed in pop culture. Then we inspect how editors' general interests relate to the articles to which they contribute; e.g., we confirm the intuition that editors show more expertise in their active domains than average users. Our results are relevant as they illuminate novel aspects of what has become many Web users' prevalent source of information and can help in recruiting new editors. [[EENNDD]] wikipedia; web usage; expertise; editors"}, "Sketsa berdasarkan data editor Wikipedia Siapa yang menyunting Wikipedia? Kami cuba menjelaskan persoalan ini dengan menggunakan data log gabungan dari bar alat penyemak imbas Yahoo! untuk menganalisis tingkah laku penyuntingan Wikipedians dalam konteks kehidupan dalam talian mereka di luar Wikipedia. Kami secara amnya mencirikan editor dengan menyiasat bagaimana tingkah laku dalam talian mereka berbeza dengan tingkah laku pengguna lain; mis., kami mendapati bahawa editor Wikipedia mencari lebih banyak, membaca lebih banyak berita, bermain lebih banyak permainan, dan, mungkin mengejutkan, lebih banyak terlibat dalam budaya pop. Kemudian kami memeriksa bagaimana kepentingan umum editor berkaitan dengan artikel yang mereka sumbangkan; mis., kami mengesahkan intuisi bahawa editor menunjukkan lebih banyak kepakaran dalam domain aktif mereka daripada pengguna biasa. Hasil kami relevan kerana mereka menerangkan aspek baru dari apa yang telah menjadi sumber maklumat pengguna Web yang banyak dan dapat membantu dalam merekrut editor baru. [[EENNDD]] wikipedia; penggunaan laman web; kepakaran; penyunting"], [{"string": "Learning information diffusion process on the web Many text documents on the Web are not originally created but forwarded or copied from other source documents. The phenomenon of document forwarding or transmission between various web sites is denoted as Web information diffusion. This paper focuses on mining information diffusion processes for specific topics on the Web. A novel system called LIDPW is proposed to address this problem using matching learning techniques. The source site and source document of each document are identified and the diffusion process composed of a sequence of diffusion relationships is visually presented to users. The effectiveness of LIDPW is validated on a real data set. A preliminary user study is performed and the results show that LIDPW does benefit users to monitor the information diffusion process of a specific topic, and aid them to discover the diffusion start and diffusion center of the topic.", "keywords": ["miscellaneous", "information flow", "web mining", "information diffusion"], "combined": "Learning information diffusion process on the web Many text documents on the Web are not originally created but forwarded or copied from other source documents. The phenomenon of document forwarding or transmission between various web sites is denoted as Web information diffusion. This paper focuses on mining information diffusion processes for specific topics on the Web. A novel system called LIDPW is proposed to address this problem using matching learning techniques. The source site and source document of each document are identified and the diffusion process composed of a sequence of diffusion relationships is visually presented to users. The effectiveness of LIDPW is validated on a real data set. A preliminary user study is performed and the results show that LIDPW does benefit users to monitor the information diffusion process of a specific topic, and aid them to discover the diffusion start and diffusion center of the topic. [[EENNDD]] miscellaneous; information flow; web mining; information diffusion"}, "Proses penyebaran maklumat pembelajaran di web Banyak dokumen teks di Web pada asalnya tidak dibuat tetapi diteruskan atau disalin dari dokumen sumber lain. Fenomena penghantaran dokumen atau penghantaran antara pelbagai laman web dilambangkan sebagai penyebaran maklumat Web. Makalah ini memberi tumpuan kepada proses penyebaran maklumat perlombongan untuk topik tertentu di Web. Sistem novel bernama LIDPW dicadangkan untuk mengatasi masalah ini dengan menggunakan teknik pembelajaran yang sepadan. Laman sumber dan dokumen sumber setiap dokumen dikenal pasti dan proses penyebaran yang terdiri daripada urutan hubungan penyebaran dipersembahkan secara visual kepada pengguna. Keberkesanan LIDPW disahkan pada set data sebenar. Satu kajian awal pengguna dilakukan dan hasilnya menunjukkan bahawa LIDPW memberi manfaat kepada pengguna untuk memantau proses penyebaran maklumat topik tertentu, dan membantu mereka untuk mengetahui pusat penyebaran dan pusat penyebaran topik. [[EENNDD]] pelbagai; aliran maklumat; perlombongan web; penyebaran maklumat"], [{"string": "A fault model and mutation testing of access control policies To increase confidence in the correctness of specified policies, policy developers can conduct policy testing by supplying typical test inputs (requests) and subsequently checking test outputs (responses) against expected ones. Unfortunately, manual testing is tedious and few tools exist for automated testing of access control policies. We present a fault model for access control policies and a framework to explore it. The framework includes mutation operators used to implement the fault model, mutant generation, equivalent-mutant detection, and mutant-killing determination. This framework allows us to investigate our fault model, evaluate coverage criteria for test generation and selection, and determine a relationship between structural coverage and fault-detection effectiveness. We have implemented the framework and applied it to various policies written in XACML. Our experimental results offer valuable insights into choosing mutation operators in mutation testing and choosing coverage criteria in test generation and selection.", "keywords": ["fault model", "test generation", "testing tools", "access control policies", "mutation testing"], "combined": "A fault model and mutation testing of access control policies To increase confidence in the correctness of specified policies, policy developers can conduct policy testing by supplying typical test inputs (requests) and subsequently checking test outputs (responses) against expected ones. Unfortunately, manual testing is tedious and few tools exist for automated testing of access control policies. We present a fault model for access control policies and a framework to explore it. The framework includes mutation operators used to implement the fault model, mutant generation, equivalent-mutant detection, and mutant-killing determination. This framework allows us to investigate our fault model, evaluate coverage criteria for test generation and selection, and determine a relationship between structural coverage and fault-detection effectiveness. We have implemented the framework and applied it to various policies written in XACML. Our experimental results offer valuable insights into choosing mutation operators in mutation testing and choosing coverage criteria in test generation and selection. [[EENNDD]] fault model; test generation; testing tools; access control policies; mutation testing"}, "Model kesalahan dan pengujian mutasi terhadap dasar kawalan akses Untuk meningkatkan keyakinan terhadap kebenaran dasar yang ditentukan, pembangun dasar dapat melakukan pengujian polisi dengan menyediakan input ujian (permintaan) khas dan kemudian memeriksa output ujian (respons) terhadap yang diharapkan. Malangnya, ujian manual membosankan dan ada sedikit alat yang ada untuk pengujian automatik dasar kawalan akses. Kami menyajikan model kesalahan untuk dasar kawalan akses dan kerangka untuk menerokainya. Kerangka ini merangkumi pengendali mutasi yang digunakan untuk menerapkan model kesalahan, generasi mutan, pengesanan mutan setara, dan penentuan pembunuhan mutan. Rangka kerja ini membolehkan kita menyiasat model kesalahan kita, menilai kriteria liputan untuk penjanaan dan pemilihan ujian, dan menentukan hubungan antara liputan struktur dan keberkesanan pengesanan kesalahan. Kami telah menerapkan kerangka tersebut dan menerapkannya pada berbagai kebijakan yang ditulis dalam XACML. Hasil eksperimen kami menawarkan pandangan berharga untuk memilih pengendali mutasi dalam pengujian mutasi dan memilih kriteria liputan dalam penjanaan dan pemilihan ujian. [[EENNDD]] model kesalahan; penjanaan ujian; alat ujian; dasar kawalan akses; ujian mutasi"], [{"string": "An xpath-based discourse analysis module for spoken dialogue systems No contact information provided yet.", "keywords": ["xpath", "discourse analysis", "spoken dialogue systems"], "combined": "An xpath-based discourse analysis module for spoken dialogue systems No contact information provided yet. [[EENNDD]] xpath; discourse analysis; spoken dialogue systems"}, "Modul analisis wacana berasaskan xpath untuk sistem dialog lisan Tidak ada maklumat hubungan yang disediakan. [[EENNDD]] xpath; analisis wacana; sistem dialog lisan"], [{"string": "Understanding user goals in web search No contact information provided yet.", "keywords": ["information retrieval", "query classification", "web search", "user goals", "user behavior"], "combined": "Understanding user goals in web search No contact information provided yet. [[EENNDD]] information retrieval; query classification; web search; user goals; user behavior"}, "Memahami tujuan pengguna dalam carian web Belum ada maklumat hubungan yang diberikan. [[EENNDD]] pengambilan maklumat; klasifikasi pertanyaan; carian sesawang; tujuan pengguna; tingkah laku pengguna"], [{"string": "Design and development of learning management system at universiti Putra Malaysia: a case study of e-SPRINT No contact information provided yet.", "keywords": ["internet", "general", "learning management system", "information systems applications"], "combined": "Design and development of learning management system at universiti Putra Malaysia: a case study of e-SPRINT No contact information provided yet. [[EENNDD]] internet; general; learning management system; information systems applications"}, "Reka bentuk dan pengembangan sistem pengurusan pembelajaran di universiti Putra Malaysia: kajian kes e-SPRINT Belum ada maklumat hubungan yang disediakan. [[EENNDD]] internet; umum; sistem Pengurusan Pembelajaran; aplikasi sistem maklumat"], [{"string": "Can link analysis tell us about web traffic? No contact information provided yet.", "keywords": ["rbs", "pagerank", "miscellaneous", "link analysis", "web traffic analysis"], "combined": "Can link analysis tell us about web traffic? No contact information provided yet. [[EENNDD]] rbs; pagerank; miscellaneous; link analysis; web traffic analysis"}, "Bolehkah analisis pautan memberitahu kami tentang lalu lintas web? Belum ada maklumat hubungan yang diberikan. [[EENNDD]] rbs; pagerank; pelbagai; analisis pautan; analisis trafik web"], [{"string": "Rank aggregation methods for the Web An abstract is not available.", "keywords": ["metasearch", "multi-word queries", "web", "rank aggregation", "spam", "ranking functions"], "combined": "Rank aggregation methods for the Web An abstract is not available. [[EENNDD]] metasearch; multi-word queries; web; rank aggregation; spam; ranking functions"}, "Kaedah pengagregatan peringkat untuk Web Abstrak tidak tersedia. [[EENNDD]] carian metas; pertanyaan berbilang perkataan; laman web; pengagregatan peringkat; spam; fungsi peringkat"], [{"string": "Buy-it-now or take-a-chance: a simple sequential screening mechanism We present a simple auction mechanism which extends the second-price auction with reserve and is truthful in expectation. This mechanism is particularly effective in private value environments where the distribution of valuations are irregular. Bidders can \"buy-it-now\", or alternatively \"take-a-chance\" where the top d bidders are equally likely to win. The randomized take-a-chance allocation incentivizes high valuation bidders to buy-it-now. We show that for a large class of valuations, this mechanism achieves similar allocations and revenues as Myerson's optimal mechanism, and outperforms the second-price auction with reserve.", "keywords": ["sequential screening", "ad auctions", "mechanism design", "adecn", "electronic commerce", "online advertising"], "combined": "Buy-it-now or take-a-chance: a simple sequential screening mechanism We present a simple auction mechanism which extends the second-price auction with reserve and is truthful in expectation. This mechanism is particularly effective in private value environments where the distribution of valuations are irregular. Bidders can \"buy-it-now\", or alternatively \"take-a-chance\" where the top d bidders are equally likely to win. The randomized take-a-chance allocation incentivizes high valuation bidders to buy-it-now. We show that for a large class of valuations, this mechanism achieves similar allocations and revenues as Myerson's optimal mechanism, and outperforms the second-price auction with reserve. [[EENNDD]] sequential screening; ad auctions; mechanism design; adecn; electronic commerce; online advertising"}, "Beli-sekarang-atau ambil kesempatan: mekanisme penyaringan berurutan yang sederhana Kami menyajikan mekanisme lelongan sederhana yang memperluas lelongan harga kedua dengan cadangan dan jujur dalam jangkaan. Mekanisme ini sangat berkesan dalam persekitaran nilai swasta di mana pengedaran penilaian tidak teratur. Pembida boleh \"membeli-sekarang-sekarang\", atau sebagai alternatif \"mengambil kesempatan\" di mana penawar-penawar teratas kemungkinan besar akan menang. Peruntukan pengambilan peluang secara rawak memberi insentif kepada pembida bernilai tinggi untuk membeli sekarang. Kami menunjukkan bahawa untuk kelas penilaian yang besar, mekanisme ini mencapai peruntukan dan pendapatan yang serupa dengan mekanisme optimum Myerson, dan mengungguli lelongan harga kedua dengan cadangan. [[EENNDD]] pemeriksaan berurutan; lelongan iklan; reka bentuk mekanisme; adecn; perdagangan elektronik; iklan dalam talian"], [{"string": "The web of nations In this paper, we report on a large-scale study of structural differences among the national webs. The study is based on a web-scale crawl conducted in the summer 2008. More specifically, we study two graphs derived from this crawl, the nation graph, with nodes corresponding to nations and edges - to links among nations, and the host graph, with nodes corresponding to hosts and edges - to hyperlinks among pages on the hosts. Contrary to some of the previous work [2], our results show that webs of different nations are often very different from each other, both in terms of their internal structure, and in terms of their connectivity with other nations.", "keywords": ["host graph", "web structure", "nation graph", "web graph"], "combined": "The web of nations In this paper, we report on a large-scale study of structural differences among the national webs. The study is based on a web-scale crawl conducted in the summer 2008. More specifically, we study two graphs derived from this crawl, the nation graph, with nodes corresponding to nations and edges - to links among nations, and the host graph, with nodes corresponding to hosts and edges - to hyperlinks among pages on the hosts. Contrary to some of the previous work [2], our results show that webs of different nations are often very different from each other, both in terms of their internal structure, and in terms of their connectivity with other nations. [[EENNDD]] host graph; web structure; nation graph; web graph"}, "Web of nation Dalam makalah ini, kami melaporkan kajian skala besar perbezaan struktur di antara web nasional. Kajian ini dibuat berdasarkan perayapan berskala web yang dilakukan pada musim panas 2008. Lebih khusus lagi, kami mengkaji dua graf yang berasal dari perayapan ini, iaitu grafik bangsa, dengan node yang sesuai dengan negara dan pinggir - untuk hubungan antara negara, dan grafik tuan rumah, dengan nod yang sepadan dengan hos dan tepi - ke pautan hiperpautan di antara halaman di hos. Berbeza dengan beberapa karya sebelumnya [2], hasil kami menunjukkan bahawa jaring dari berbagai bangsa sering sangat berbeza antara satu sama lain, baik dari segi struktur dalaman mereka, dan dari segi hubungan mereka dengan bangsa lain. [[EENNDD]] grafik hos; struktur web; grafik bangsa; grafik web"], [{"string": "Optimized query planning of continuous aggregation queries in dynamic data dissemination networks Continuous queries are used to monitor changes to time varying data and to provide results useful for online decision making. Typically a user desires to obtain the value of some aggregation function over distributed data items, for example, to know (a) the average of temperatures sensed by a set of sensors (b) the value of index of mid-cap stocks. In these queries a client specifies a coherency requirement as part of the query. In this paper we present a low-cost, scalable technique to answer continuous aggregation queries using a content distribution network of dynamic data items. In such a network of data aggregators, each data aggregator serves a set of data items at specific coherencies. Just as various fragments of a dynamic web-page are served by one or more nodes of a content distribution network, our technique involves decomposing a client query into sub-queries and executing sub-queries on judiciously chosen data aggregators with their individual sub-query incoherency bounds. We provide a technique of getting the optimal query plan (i.e., set of sub-queries and their chosen data aggregators) which satisfies client query.s coherency requirement with least cost, measured in terms of the number of refresh messages sent from aggregators to the client. For estimating query execution cost, we build a continuous query cost model which can be used to estimate the number of messages required to satisfy the client specified incoherency bound. Performance results using real-world traces show that our cost based query planning leads to queries being executed using less than one third the number of messages required by existing schemes.", "keywords": ["data coherency", "dynamic data", "continuous aggregation queries", "content distribution networks", "query dissemination cost"], "combined": "Optimized query planning of continuous aggregation queries in dynamic data dissemination networks Continuous queries are used to monitor changes to time varying data and to provide results useful for online decision making. Typically a user desires to obtain the value of some aggregation function over distributed data items, for example, to know (a) the average of temperatures sensed by a set of sensors (b) the value of index of mid-cap stocks. In these queries a client specifies a coherency requirement as part of the query. In this paper we present a low-cost, scalable technique to answer continuous aggregation queries using a content distribution network of dynamic data items. In such a network of data aggregators, each data aggregator serves a set of data items at specific coherencies. Just as various fragments of a dynamic web-page are served by one or more nodes of a content distribution network, our technique involves decomposing a client query into sub-queries and executing sub-queries on judiciously chosen data aggregators with their individual sub-query incoherency bounds. We provide a technique of getting the optimal query plan (i.e., set of sub-queries and their chosen data aggregators) which satisfies client query.s coherency requirement with least cost, measured in terms of the number of refresh messages sent from aggregators to the client. For estimating query execution cost, we build a continuous query cost model which can be used to estimate the number of messages required to satisfy the client specified incoherency bound. Performance results using real-world traces show that our cost based query planning leads to queries being executed using less than one third the number of messages required by existing schemes. [[EENNDD]] data coherency; dynamic data; continuous aggregation queries; content distribution networks; query dissemination cost"}, "Perancangan permintaan yang dioptimumkan untuk pertanyaan agregasi berterusan dalam rangkaian penyebaran data dinamik Pertanyaan berterusan digunakan untuk memantau perubahan terhadap data yang berbeza-beza waktu dan untuk memberikan hasil yang berguna untuk membuat keputusan dalam talian. Biasanya pengguna ingin memperoleh nilai beberapa fungsi pengagregatan daripada item data yang diedarkan, misalnya, untuk mengetahui (a) rata-rata suhu yang dirasakan oleh sekumpulan sensor (b) nilai indeks stok pertengahan. Dalam pertanyaan ini, pelanggan menentukan keperluan koherensi sebagai sebahagian daripada pertanyaan. Dalam makalah ini kami menyajikan teknik murah dan berskala untuk menjawab pertanyaan agregasi berterusan menggunakan rangkaian pengedaran kandungan item data dinamik. Dalam rangkaian agregator data seperti itu, setiap agregator data melayani sekumpulan item data pada koherensi tertentu. Sama seperti pelbagai fragmen halaman web dinamik dilayan oleh satu atau lebih node rangkaian pengedaran kandungan, teknik kami melibatkan menguraikan pertanyaan pelanggan menjadi sub-pertanyaan dan melaksanakan sub-pertanyaan pada agregator data yang dipilih dengan bijak dengan sub-pertanyaan individu mereka batasan ketidaktepatan. Kami menyediakan teknik untuk mendapatkan rancangan pertanyaan yang optimum (iaitu, set sub-pertanyaan dan agregator data pilihan mereka) yang memenuhi kehendak koherensi permintaan pelanggan dengan kos paling sedikit, diukur dari segi jumlah mesej penyegaran yang dihantar dari agregator ke pelanggan. Untuk menganggarkan kos pelaksanaan pertanyaan, kami membina model kos pertanyaan berterusan yang dapat digunakan untuk menganggarkan jumlah mesej yang diperlukan untuk memuaskan pelanggan yang ditentukan ketidakteraturan. Hasil prestasi menggunakan jejak dunia nyata menunjukkan bahawa perancangan pertanyaan berdasarkan kos kami menyebabkan pertanyaan dilaksanakan dengan menggunakan kurang dari satu pertiga jumlah mesej yang diperlukan oleh skema yang ada. [[EENNDD]] koheren data; data dinamik; pertanyaan agregasi berterusan; rangkaian pengedaran kandungan; kos penyebaran pertanyaan"], [{"string": "Feature weighting in content based recommendation system using social network analysis We propose a hybridization of collaborative filtering and content based recommendation system. Attributes used for content based recommendations are assigned weights depending on their importance to users. The weight values are estimated from a set of linear regression equations obtained from a social network graph which captures human judgment about similarity of items.", "keywords": ["feature similarity", "social network", "recommender system"], "combined": "Feature weighting in content based recommendation system using social network analysis We propose a hybridization of collaborative filtering and content based recommendation system. Attributes used for content based recommendations are assigned weights depending on their importance to users. The weight values are estimated from a set of linear regression equations obtained from a social network graph which captures human judgment about similarity of items. [[EENNDD]] feature similarity; social network; recommender system"}, "Pembobotan fitur dalam sistem cadangan berdasarkan kandungan menggunakan analisis rangkaian sosial Kami mencadangkan hibridisasi sistem penapisan kolaboratif dan sistem cadangan berdasarkan kandungan. Atribut yang digunakan untuk cadangan berdasarkan kandungan diberikan bobot bergantung kepada kepentingannya kepada pengguna. Nilai berat dianggarkan dari satu set persamaan regresi linear yang diperoleh dari grafik rangkaian sosial yang menangkap pertimbangan manusia mengenai kesamaan item. [[EENNDD]] persamaan ciri; rangkaian sosial; sistem cadangan"], [{"string": "Statistical analysis of the social network and discussion threads in slashdot We analyze the social network emerging from the user comment activity on the website Slashdot. The network presents common features of traditional social networks such as a giant component, small average path length and high clustering, but differs from them showing moderate reciprocity and neutral assortativity by degree. Using Kolmogorov-Smirnov statistical tests, we show that the degree distributions are better explained by log-normal instead of power-law distributions. We also study the structure of discussion threads using an intuitive radial tree representation. Threads show strong heterogeneity and self-similarity throughout the different nesting levels of a conversation. We use these results to propose a simple measure to evaluate the degree of controversy provoked by a post.", "keywords": ["weblogs", "h-index", "radial tree", "bulletin board", "social networks", "log-normal", "power-law", "thread", "online communities"], "combined": "Statistical analysis of the social network and discussion threads in slashdot We analyze the social network emerging from the user comment activity on the website Slashdot. The network presents common features of traditional social networks such as a giant component, small average path length and high clustering, but differs from them showing moderate reciprocity and neutral assortativity by degree. Using Kolmogorov-Smirnov statistical tests, we show that the degree distributions are better explained by log-normal instead of power-law distributions. We also study the structure of discussion threads using an intuitive radial tree representation. Threads show strong heterogeneity and self-similarity throughout the different nesting levels of a conversation. We use these results to propose a simple measure to evaluate the degree of controversy provoked by a post. [[EENNDD]] weblogs; h-index; radial tree; bulletin board; social networks; log-normal; power-law; thread; online communities"}, "Analisis statistik rangkaian sosial dan utas perbincangan di slashdot Kami menganalisis rangkaian sosial yang muncul dari aktiviti komen pengguna di laman web Slashdot. Rangkaian ini menyajikan ciri umum rangkaian sosial tradisional seperti komponen raksasa, panjang jalan rata-rata kecil dan pengelompokan tinggi, tetapi berbeza dari mereka yang menunjukkan timbal balik yang sederhana dan kepelbagaian neutral mengikut tahap. Dengan menggunakan ujian statistik Kolmogorov-Smirnov, kami menunjukkan bahawa pengagihan darjah lebih baik dijelaskan oleh log-normal dan bukannya pengedaran undang-undang kuasa. Kami juga mengkaji struktur utas perbincangan menggunakan perwakilan pokok radial intuitif. Thread menunjukkan heterogenitas dan persamaan diri yang kuat sepanjang tahap perbualan yang berbeza. Kami menggunakan hasil ini untuk mencadangkan langkah mudah untuk menilai tahap kontroversi yang diprovokasi oleh sebuah jawatan. [[EENNDD]] blog web; h-indeks; pokok jejari; papan buletin; rangkaian sosial; log-normal; kuasa undang-undang; benang; komuniti dalam talian"], [{"string": "WPBench: a benchmark for evaluating the client-side performance of web 2.0 applications In this paper, a benchmark called WPBench is reported to evaluate the responsiveness of Web browsers for modern Web 2.0 applications. In WPBench, variations of servers and networks are removed and the benchmark result is the closest to what Web users would perceive. To achieve these, WPBench records users' interactions with typical Web 2.0 applications, and then replays Web navigations when benchmarking browsers. The replay mechanism can emulate the actual user interactions and the characteristics of the servers and the networks in a consistent way independent of browsers so that any browser compliant to the standards can be benchmarked fairly. In addition to describing the design and generation of WPBench, we also report the WPBench comparison results on the responsiveness performance for three popular Web browsers: Internet Explorer, Firefox and Chrome.", "keywords": ["web", "javascript", "browser", "replay", "benchmark"], "combined": "WPBench: a benchmark for evaluating the client-side performance of web 2.0 applications In this paper, a benchmark called WPBench is reported to evaluate the responsiveness of Web browsers for modern Web 2.0 applications. In WPBench, variations of servers and networks are removed and the benchmark result is the closest to what Web users would perceive. To achieve these, WPBench records users' interactions with typical Web 2.0 applications, and then replays Web navigations when benchmarking browsers. The replay mechanism can emulate the actual user interactions and the characteristics of the servers and the networks in a consistent way independent of browsers so that any browser compliant to the standards can be benchmarked fairly. In addition to describing the design and generation of WPBench, we also report the WPBench comparison results on the responsiveness performance for three popular Web browsers: Internet Explorer, Firefox and Chrome. [[EENNDD]] web; javascript; browser; replay; benchmark"}, "WPBench: penanda aras untuk menilai prestasi pelanggan dari aplikasi web 2.0 Dalam makalah ini, tanda aras yang disebut WPBench dilaporkan untuk menilai daya tindak balas penyemak imbas Web untuk aplikasi Web 2.0 moden. Di WPBench, variasi pelayan dan rangkaian dikeluarkan dan hasil penanda aras adalah yang paling dekat dengan apa yang dilihat oleh pengguna Web. Untuk mencapainya, WPBench merakam interaksi pengguna dengan aplikasi Web 2.0 khas, dan kemudian memainkan semula navigasi Web ketika menanda aras penyemak imbas. Mekanisme replay dapat meniru interaksi pengguna yang sebenarnya dan ciri-ciri pelayan dan rangkaian dengan cara yang konsisten bebas daripada penyemak imbas sehingga mana-mana penyemak imbas yang mematuhi piawaian dapat ditanda aras dengan adil. Selain menerangkan reka bentuk dan generasi WPBench, kami juga melaporkan hasil perbandingan WPBench mengenai prestasi responsif untuk tiga penyemak imbas Web yang popular: Internet Explorer, Firefox dan Chrome. [[EENNDD]] web; javascript; penyemak imbas; main semula; penanda aras"], [{"string": "Image classification for mobile web browsing No contact information provided yet.", "keywords": ["web browsing", "web images", "mobile computing", "miscellaneous"], "combined": "Image classification for mobile web browsing No contact information provided yet. [[EENNDD]] web browsing; web images; mobile computing; miscellaneous"}, "Klasifikasi gambar untuk melayari web mudah alih Belum ada maklumat hubungan yang diberikan. [[EENNDD]] melayari laman web; gambar web; pengkomputeran mudah alih; pelbagai"], [{"string": "Representing personal web information using a topic-oriented interface No contact information provided yet.", "keywords": ["user information mining", "topic classfication", "user interface", "personal web information", "clustering"], "combined": "Representing personal web information using a topic-oriented interface No contact information provided yet. [[EENNDD]] user information mining; topic classfication; user interface; personal web information; clustering"}, "Mewakili maklumat web peribadi menggunakan antara muka berorientasikan topik Belum ada maklumat hubungan yang diberikan. [[EENNDD]] perlombongan maklumat pengguna; pengelasan topik; antaramuka pengguna; maklumat web peribadi; pengelompokan"], [{"string": "A semantic-link-based infrastructure for web service discovery in P2P networks No contact information provided yet.", "keywords": ["semantic link", "web service", "peer-to-peer"], "combined": "A semantic-link-based infrastructure for web service discovery in P2P networks No contact information provided yet. [[EENNDD]] semantic link; web service; peer-to-peer"}, "Infrastruktur berasaskan pautan semantik untuk penemuan perkhidmatan web di rangkaian P2P Belum ada maklumat hubungan yang disediakan. [[EENNDD]] pautan semantik; perkhidmatan web; rakan sebaya"], [{"string": "Semantic search of schema repositories An abstract is not available.", "keywords": ["information search and retrieval"], "combined": "Semantic search of schema repositories An abstract is not available. [[EENNDD]] information search and retrieval"}, "Pencarian semantik repositori skema Abstrak tidak tersedia. [[EENNDD]] carian dan pengambilan maklumat"]]