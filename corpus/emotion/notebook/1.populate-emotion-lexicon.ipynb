{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "laughing = {\n",
    "    'huhu',\n",
    "    'haha',\n",
    "    'gagaga',\n",
    "    'hihi',\n",
    "    'wkawka',\n",
    "    'wkwk',\n",
    "    'kiki',\n",
    "    'keke',\n",
    "    'huehue',\n",
    "    'hshs',\n",
    "    'hoho',\n",
    "    'hewhew',\n",
    "    'uwu',\n",
    "    'sksk',\n",
    "    'ksks',\n",
    "    'gituu',\n",
    "    'gitu',\n",
    "    'mmeeooww',\n",
    "    'meow',\n",
    "    'alhamdulillah',\n",
    "    'muah',\n",
    "    'mmuahh',\n",
    "    'hehe',\n",
    "    'salamramadhan',\n",
    "    'happywomensday',\n",
    "    'jahagaha',\n",
    "    'ahakss',\n",
    "    'ahksk'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_lexicon = {}\n",
    "\n",
    "emotion_lexicon['anger'] = \\\n",
    "['sial bodoh', 'apa lancau', 'butoh pak hang', 'sial af', 'cepat marah',\n",
    "'babi bodoh', 'babi bodo', 'wtf jantan sial', 'butoh', 'mcm bodoh', 'mcm bodo',\n",
    "'bodohnya', 'bodoh nya', 'lancau la', 'tengah marah', 'benci bodoh', 'benci bodo',\n",
    "'bnci bodoh', 'sakit jiwanya', 'nak je maki', 'kerajaan mengambil tindakan',\n",
    "'jangan terkejut', 'sakit hati', 'sakit tau tak', 'awak sape sampai',\n",
    "'yang suka marah', 'bodoh jika', 'busuk hati', 'sial madey', 'sila meninggal',\n",
    "'fuck you', 'fck u', 'fuck u', 'pukul kau kang', 'sial lah', 'selalu marah',\n",
    "'yang bodoh', 'nak benci', 'bodoh betul', 'babi anwar', 'bodoh nak',\n",
    " 'bapak acah', 'bebal', 'orang kecam', 'apa bodoh', 'bodoh benak',\n",
    "'jangan bodoh', 'kepala babi', 'diam lah', 'malas nak', 'aku benci',\n",
    "'cakap babi', 'kapitalis', 'mahal babi', 'kena marah', 'cam bodoh',\n",
    "'ke babi', 'buat sakit hati', 'nampak bodoh', 'langsung tak baca', 'bapak acah',\n",
    "'bodoh benak', 'la bangang', 'bukan typo', 'sakit hati tengok', 'sakit hati tgk',\n",
    "'bodoh membodohi', 'alah bodoh', 'paling benci', 'bodoh nian', 'annoying nya',\n",
    "'apa anjing', 'kok haram', 'macam bodoh', 'sama bodoh', 'berdenyut bodoh',\n",
    "'muka marah', 'takyah paksa', 'just tak suka', 'anjirr', 'benci tido',\n",
    "'sebab anjing', 'paling benci', 'minyak babi', 'parti penghasut', 'perasaan benci',\n",
    "'aku cringe', 'aku stress', 'cannot brain', 'tak bagi signal', 'yg pertikai',\n",
    "'tibai dap', 'semua trash', 'siall babi', 'ada puki', 'ur trash', 'mcm bodo',\n",
    "'warga emas kot', 'dapat balasan', 'menyirap ja', 'cina dap', 'takde otak',\n",
    "'video berentem', 'terus potong', 'mcm sial', 'sering ngebacot', 'dapat dosa', 'macam toxic',\n",
    "'padan muka', 'unfollowing', 'what the heck', 'what the fuck', 'what the hell', 'wtf ahh', 'kaki fitnah',\n",
    "'selalu marah', 'jahanam', 'buat pasal', 'tukang carut', 'kali rapuh', 'mengamuk',\n",
    "'sendawa kuat', 'sebab dia malas', 'kena rogol', 'rapist', 'sexist', 'racist',\n",
    "'duper tired', 'super tired', 'bikin sakit', 'komunis', 'busuk hati', 'liberalis', 'liberal',\n",
    "'kepoh bising', 'kotor busuk', 'kesan dedak', 'takde otak', 'ingat semua soalan',\n",
    "'pig guy', 'setan betul', 'punya ambisi', 'kote la', 'kena rotan', 'pukimak', 'i fobia',\n",
    "'fobia fuck', 'fobia fck', 'phobia fuck', 'phobia fck', 'sampai mati', 'sampe mati',\n",
    "'mat dadah', 'mat rempit', 'kondem', 'sosialis', 'bila malas', 'zionis', 'denggi',\n",
    "'kempret', 'sbb sombong', 'jalan tutup', 'bkn pergi', 'bkn pegih', 'so stress', 'soo stress',\n",
    "'pakai akun palsu', 'pakai akaun palsu', 'watafak sial', 'nak terminate']\n",
    "\n",
    "\n",
    "emotion_lexicon['fear'] = \\\n",
    "['getting scared', 'ketakutan', 'im scared', 'sexual harassment',\n",
    "'fear for me', 'takut dibuatnya', 'cuak laa', 'ngeri',\n",
    "'cuak gila', 'cuak lah', 'cuak sial', 'pastu muka hantu',\n",
    "'dah cuak dah', 'masukkan rasa takut', 'seram sejuk', 'nonton horror', 'merasa takut',\n",
    "'masih ragu', 'takut aa', 'takut diketawain', 'takut nak', 'semakin ngeri', 'takut dipanggil',\n",
    "'takut hacker', 'takut kehilangan', 'hih ngeri', 'part hantu', 'sumpah seram', 'seram bodoh',\n",
    "'seram babi', 'takut tertipu', 'takut nak', 'ini ketakutan', 'risau klau', 'depa takut', 'jadi takut',\n",
    "'dihantui kesalahan', 'menimbulkan ketakutan', 'takut miskin', 'didalam ketakutan', 'seram babi',\n",
    "'sis takut', 'pun takut', 'takut pocong', 'takut puntianak', 'takut jebon', 'takut pencuri', 'aku takut',\n",
    "'seram sial', 'seram bodoh', 'seram bodo', 'azab allah', 'treler kanan kiri',\n",
    "'takut neraka', 'panik bodoh', 'panik sial', 'panik bodo', 'panik babi', 'panic bodoh',\n",
    "'panic sial', 'panic babi', 'panic bodo', 'frightening', 'takut salib', 'distress', 'nampak hantu',\n",
    "'sekarang cuak', 'kena ragut', 'kena rompak', 'kena rogol', 'nmpak hntu', 'nmpk hntu', 'tkut salib',\n",
    "'takut tuhan', 'tkut tuhan', 'takut kiamat', 'tkut kiamat', 'kna rogol', 'takut nk', 'tkut nk']\n",
    "\n",
    "\n",
    "emotion_lexicon['happy'] = ['sebak terharu', 'happynya', 'syoknya kalau', 'gembira tengok',\n",
    "'lega hati', 'lega bila', 'lega sikit hati', 'happy gila']\n",
    "\n",
    "emotion_lexicon['love'] = \\\n",
    "['cinta semoga', 'aku rindu', 'dah sayang', 'dah syg', 'dh syg',\n",
    "'cinta bangat', 'cinta sangat', 'syg tau', 'cair saya',\n",
    "'cair aku', 'cair hati', 'chenta hati', 'dijalan sayang',\n",
    "'real love', 'kami rindu', 'ily', 'cia cia', 'love u',\n",
    "'so sweet', 'love it', 'sumpah rindu', 'hati tetap',\n",
    "'rindu dirimu', 'rindu diri', 'jatuh cinta', 'pun syg',\n",
    "'pun sayang', 'baby sayang', 'baby syg', 'bby syg',\n",
    "'jangan sakit', 'lagi ya sayang', 'big baby', 'jantung kota',\n",
    "'marah tandanya sayang', 'marah tandanya syg', 'menahan rindu',\n",
    "'ikhlas mencintainya', 'terlalu lawa', 'rindu hg', 'rndu hg',\n",
    "'rindu hang', 'rndu hang', 'rindu kau', 'rndu kau', 'so sweet',\n",
    "'hati tetap bersatu', 'handsome nak mampus', 'rindu ma', 'syg sgt', 'syg syg',\n",
    "'baby baru bangun', 'bby dah bangun', 'baby dah bangun', 'suami orang handsome']\n",
    "\n",
    "emotion_lexicon['sadness'] = \\\n",
    "['suka-duka', 'terasa sedih', 'menikam terus', 'sakit balik',\n",
    "'sakit blk', 'tidak sakit hati', 'tak sakit hati', 'bahagia seadanya',\n",
    "'hanya partner', 'sedih jugak', 'pulak sakit', 'rasanya sakit',\n",
    "'sedih ni', 'confirm sendiri sakit', 'ditinggalkan', 'masih sakit',\n",
    "'dia punya pedih', 'berita hibah', 'kesal marah', 'kecewa is',\n",
    "'tapi sayang duit', 'sedih gila', 'sedih bodoh', 'sedih dengan', 'sedih dgn', 'duka cita', 'dukacita',\n",
    "'kecewa', 'sad af', 'im sad', 'sdih bisa', 'sedih bisa', 'yg sedih', 'kekasih bayangan',\n",
    "'menjerit kesakitan', 'nak happiness bkn', 'nak happiness bukan']\n",
    "\n",
    "emotion_lexicon['surprise'] = \\\n",
    "['jantung mau', 'terkejut sial', 'terkejut babi', 'sakit jantung',\n",
    "'terperanjat aku', 'astaga terkejut', 'terkejut aku', 'sbb terkejut',\n",
    "'sbb tekejut', 'i terkejut', 'jantung automatically', 'surprise katanya',\n",
    "'cuma terkejut', 'almost tak kenal', 'terperanjat babi', 'terperanjat bodoh',\n",
    "'awas anak', 'aku tekejut', 'aku terkejut', 'terkejut saya', 'pastu terkejut',\n",
    "'hilang nyawaku', 'tejut boboi', 'tejut babi', 'tejut bodo', 'tejut bodoh',\n",
    "'tejut gila', 'tekejut babi', 'tekejut bodoh', 'tekejut bodo', 'tekejut siall',\n",
    "'terkejut ssiiaall', 'surprise', 'surprised', 'tajut', 'nak tahu tak', 'nak tahu x',\n",
    "'nk thu x', 'nk thu tak', 'tekejut', 'tkjut', 'trkejut', 'terkejut', 'terkjut', 'tejut']\n",
    "\n",
    "emotion_not = {}\n",
    "emotion_not['anger'] = {'sendu', 'glowing'}\n",
    "emotion_not['fear'] = {'without fear', 'jatuh cinta', 'kau sedap', 'kebencian', 'semoga', 'takut hatiku',\n",
    "                      'terlalu lawa', 'in future', 'terlupa'}\n",
    "emotion_not['love'] = {'lukakan', 'luka', 'kecewa', 'duka', 'but my', 'luah lah',\n",
    "                      'terasa', 'janji palsu', 'janji palsuu', 'tawar hati', 'terkunci',\n",
    "                      'broken', 'moveon', 'move on', 'kesian', 'no proper', 'tak sanggup',\n",
    "                      'minta cerai', 'seram', 'nak bunuh'}\n",
    "emotion_not['surprise'] = {'jangan terkejut', 'jgn terkejut', 'x surprise', 'tak surprise',\n",
    "                          'not surprise', 'elak sakit', 'jangan haraplah', 'merawat', 'menghidap',\n",
    "                          'meninggal', 'yg sakit', 'yang sakit', 'takziah', 'badmood', 'tukang',\n",
    "                          'tak terkejut', 'x terkejut', 'xterkejut', 'xtejut', 'tk terkejut',\n",
    "                          'tidak sihat'}\n",
    "emotion_not['sadness'] = {'haramkan', 'jom'}\n",
    "\n",
    "longest = 0\n",
    "\n",
    "for k, v in emotion_lexicon.items():\n",
    "    v = {w.lower() for w in v}\n",
    "    emotion_lexicon[k] = v\n",
    "    for w in v:\n",
    "        l = len(w.split())\n",
    "        if l > longest:\n",
    "            longest = l\n",
    "            \n",
    "longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6597867"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../language-detection/dumping-twitter-6-july-2019.json') as fopen:\n",
    "    twitter = json.load(fopen)\n",
    "    \n",
    "len(twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../language-detection/2020-02-22-twitter-dump-in.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1617795/1617795 [00:01<00:00, 1392236.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../language-detection/2020-03-08-twitter-dump-in.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1711555/1711555 [00:01<00:00, 1276500.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../language-detection/2020-03-28-twitter-dump-in.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250595/1250595 [00:00<00:00, 1328124.42it/s]\n"
     ]
    }
   ],
   "source": [
    "files = ['2020-02-22-twitter-dump-in.json', '2020-03-08-twitter-dump-in.json', '2020-03-28-twitter-dump-in.json']\n",
    "\n",
    "for file in files:\n",
    "    file = f'../language-detection/{file}'\n",
    "    print(file)\n",
    "\n",
    "    with open(file) as fopen:\n",
    "        temp = json.load(fopen)\n",
    "\n",
    "    for i in tqdm(range(len(temp))):\n",
    "        retweet = temp[i]['retweet_text_full']\n",
    "        t = temp[i]['data_text']\n",
    "        if retweet != 'NULL' and len(retweet) > len(t):\n",
    "            t = retweet\n",
    "        twitter.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11177812"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "def preprocessing(string):\n",
    "    string = re.sub(\n",
    "        'http\\S+|www.\\S+',\n",
    "        '',\n",
    "        ' '.join(\n",
    "            [i for i in string.split() if i.find('#') < 0 and i.find('@') < 0]\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    chars = ',.()!:\\'\"/;=-'\n",
    "    for c in chars:\n",
    "        string = string.replace(c, f' {c} ')\n",
    "        \n",
    "    string = re.sub(\n",
    "        u'[0-9!@#$%^&*()_\\-+{}|\\~`\\'\";:?/.>,<]',\n",
    "        ' ',\n",
    "        string,\n",
    "        flags = re.UNICODE,\n",
    "    )\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    string = (\n",
    "        ''.join(''.join(s)[:2] for _, s in itertools.groupby(string))\n",
    "    )\n",
    "    \n",
    "    return string.lower()\n",
    "\n",
    "def loop(strings):\n",
    "    results = []\n",
    "    for i in tqdm(range(len(strings))):\n",
    "        results.append((strings[i], preprocessing(strings[i])))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'akkuu la'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing('akkuuuu la')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698613/698613 [00:38<00:00, 18049.53it/s]\n",
      "100%|██████████| 698613/698613 [00:39<00:00, 17849.95it/s]\n",
      "100%|██████████| 698613/698613 [00:40<00:00, 17271.34it/s]\n",
      "100%|██████████| 698613/698613 [00:40<00:00, 17358.60it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3112.08it/s]155.26it/s]\n",
      "100%|██████████| 698613/698613 [00:41<00:00, 16673.72it/s]\n",
      "100%|██████████| 698613/698613 [00:39<00:00, 17499.99it/s]\n",
      "100%|██████████| 698613/698613 [00:40<00:00, 17181.57it/s]\n",
      "100%|██████████| 698613/698613 [00:42<00:00, 16621.17it/s]\n",
      "100%|██████████| 698613/698613 [00:42<00:00, 16600.68it/s]\n",
      "100%|██████████| 698613/698613 [00:46<00:00, 15072.89it/s]\n",
      "100%|██████████| 698613/698613 [00:49<00:00, 14239.30it/s]\n",
      "100%|██████████| 698613/698613 [00:50<00:00, 13966.90it/s]\n",
      "100%|██████████| 698613/698613 [00:50<00:00, 13878.93it/s]\n",
      "100%|██████████| 698613/698613 [00:50<00:00, 13728.08it/s]\n",
      "100%|██████████| 698613/698613 [00:50<00:00, 13739.05it/s]\n",
      "100%|██████████| 698613/698613 [00:51<00:00, 13623.73it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned = cleaning.multiprocessing(twitter, loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "negate_words = {\n",
    "    'tak',\n",
    "    'jangan',\n",
    "    'tidak',\n",
    "    'enggak',\n",
    "    'tiada',\n",
    "    'bukan',\n",
    "    'usah',\n",
    "    'tidaklah',\n",
    "    'jgn',\n",
    "    'tk',\n",
    "    'bkn',\n",
    "    \"shouldnt\",\n",
    "    \"dont\",\n",
    "    \"doesnt\",\n",
    "}\n",
    "\n",
    "import itertools\n",
    "\n",
    "def _pad_sequence(\n",
    "    sequence,\n",
    "    n,\n",
    "    pad_left = False,\n",
    "    pad_right = False,\n",
    "    left_pad_symbol = None,\n",
    "    right_pad_symbol = None,\n",
    "):\n",
    "    sequence = iter(sequence)\n",
    "    if pad_left:\n",
    "        sequence = itertools.chain((left_pad_symbol,) * (n - 1), sequence)\n",
    "    if pad_right:\n",
    "        sequence = itertools.chain(sequence, (right_pad_symbol,) * (n - 1))\n",
    "    return sequence\n",
    "\n",
    "def ngrams(\n",
    "    sequence,\n",
    "    n: int,\n",
    "    pad_left = False,\n",
    "    pad_right = False,\n",
    "    left_pad_symbol = None,\n",
    "    right_pad_symbol = None,\n",
    "):\n",
    "    sequence = _pad_sequence(\n",
    "        sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol\n",
    "    )\n",
    "\n",
    "    history = []\n",
    "    while n > 1:\n",
    "        try:\n",
    "            next_item = next(sequence)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        history.append(next_item)\n",
    "        n -= 1\n",
    "    for item in sequence:\n",
    "        history.append(item)\n",
    "        yield tuple(history)\n",
    "        del history[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(cleaned):\n",
    "    results = {e: [] for e in emotion_lexicon.keys()}\n",
    "    results['not_in'] = []\n",
    "    for s in tqdm(cleaned):\n",
    "        splitted = s[1].split()\n",
    "        ngs = splitted[:]\n",
    "        for n in range(2, longest + 1):\n",
    "            ngs.extend([' '.join(n) for n in ngrams(splitted, n)])\n",
    "            \n",
    "        ngs = set(ngs)\n",
    "        string = [\n",
    "            word\n",
    "            for word in splitted\n",
    "            if any([laugh in word for laugh in laughing])\n",
    "        ]\n",
    "        love = ngs & emotion_lexicon['love']\n",
    "        if len(string) and not len(love):\n",
    "            results['happy'].append(s[0])\n",
    "            continue\n",
    "        \n",
    "        found = False\n",
    "        for k, v in emotion_lexicon.items():\n",
    "            r = ngs & v\n",
    "            sn = ngs & emotion_not.get(k, set())\n",
    "            if len(r) and not len(sn):\n",
    "                results[k].append(s[0])\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            results['not_in'].append(s[0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698613/698613 [00:39<00:00, 17830.30it/s]\n",
      "100%|██████████| 698613/698613 [00:38<00:00, 18030.55it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 6912.74it/s]55.62it/s]]\n",
      "100%|██████████| 698613/698613 [00:38<00:00, 18199.75it/s]\n",
      "100%|██████████| 698613/698613 [00:39<00:00, 17666.44it/s]\n",
      "100%|██████████| 698613/698613 [00:39<00:00, 17899.94it/s]\n",
      "100%|██████████| 698613/698613 [00:39<00:00, 17541.70it/s]\n",
      "100%|██████████| 698613/698613 [00:38<00:00, 18054.83it/s]\n",
      "100%|██████████| 698613/698613 [00:39<00:00, 17485.10it/s]\n",
      "100%|██████████| 698613/698613 [00:40<00:00, 17084.21it/s]\n",
      "100%|██████████| 698613/698613 [00:46<00:00, 14927.89it/s]\n",
      "100%|██████████| 698613/698613 [00:51<00:00, 13694.14it/s]\n",
      "100%|██████████| 698613/698613 [00:50<00:00, 13916.47it/s]\n",
      "100%|██████████| 698613/698613 [00:49<00:00, 14214.77it/s]\n",
      "100%|██████████| 698613/698613 [00:48<00:00, 14262.53it/s]\n",
      "100%|██████████| 698613/698613 [00:49<00:00, 14032.81it/s]\n",
      "100%|██████████| 698613/698613 [00:52<00:00, 13418.80it/s]\n"
     ]
    }
   ],
   "source": [
    "results = cleaning.multiprocessing(cleaned, loop, list_mode = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = {e: [] for e in emotion_lexicon.keys()}\n",
    "combined['not_in'] = []\n",
    "\n",
    "for r in results:\n",
    "    for k, v in r.items():\n",
    "        combined[k].extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger 100808\n",
      "fear 18656\n",
      "happy 998541\n",
      "love 17786\n",
      "sadness 22133\n",
      "surprise 11923\n",
      "not_in 10007965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for k, v in combined.items():\n",
    "    print(k, len(v))\n",
    "    total += len(v)\n",
    "    \n",
    "total == len(twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = combined.pop('not_in', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger 100808\n",
      "fear 18656\n",
      "happy 998541\n",
      "love 17786\n",
      "sadness 22133\n",
      "surprise 11923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for k, v in combined.items():\n",
    "    print(k, len(v))\n",
    "    total += len(v)\n",
    "    \n",
    "total == len(twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emotion-twitter-lexicon.json', 'w') as fopen:\n",
    "    json.dump(combined, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "bucketName = 'malaya-dataset'\n",
    "Key = 'emotion-twitter-lexicon.json'\n",
    "outPutname = 'emotion/emotion-twitter-lexicon.json'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file(Key,bucketName,outPutname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
