{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp=spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clearstring(string):\n",
    "    string = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', string, flags=re.MULTILINE)\n",
    "    string = re.sub('[^@#A-Za-z0-9 ]+', '', string)\n",
    "    string = string.split(' ')\n",
    "    string = filter(None, string)\n",
    "    string = [y.strip() for y in string]\n",
    "    string = ' '.join(string)\n",
    "    return string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender:confidence</th>\n",
       "      <th>profile_yn</th>\n",
       "      <th>profile_yn:confidence</th>\n",
       "      <th>created</th>\n",
       "      <th>...</th>\n",
       "      <th>profileimage</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sidebar_color</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>815719226</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:24</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12/5/13 1:48</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/414342229...</td>\n",
       "      <td>0</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>Robbie E Responds To Critics After Win Against...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110964</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>main; @Kan1shk3</td>\n",
       "      <td>Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>815719227</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:30</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/1/12 13:51</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/539604221...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>ÛÏIt felt like they were my friends and I was...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7471</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>815719228</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:33</td>\n",
       "      <td>male</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11/28/14 11:30</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/657330418...</td>\n",
       "      <td>1</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>i absolutely adore when louis starts the songs...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5617</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>clcncl</td>\n",
       "      <td>Belgrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>815719229</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:10</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6/11/09 22:39</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/259703936...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>Hi @JordanSpieth - Looking at the url - do you...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1693</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>815719230</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/27/15 1:15</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4/16/14 13:23</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/564094871...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Watching Neighbours on Sky+ catching up with t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31462</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  815719226    False   finalized                   3    10/26/15 23:24   \n",
       "1  815719227    False   finalized                   3    10/26/15 23:30   \n",
       "2  815719228    False   finalized                   3    10/26/15 23:33   \n",
       "3  815719229    False   finalized                   3    10/26/15 23:10   \n",
       "4  815719230    False   finalized                   3     10/27/15 1:15   \n",
       "\n",
       "   gender  gender:confidence profile_yn  profile_yn:confidence  \\\n",
       "0    male             1.0000        yes                    1.0   \n",
       "1    male             1.0000        yes                    1.0   \n",
       "2    male             0.6625        yes                    1.0   \n",
       "3    male             1.0000        yes                    1.0   \n",
       "4  female             1.0000        yes                    1.0   \n",
       "\n",
       "          created             ...              \\\n",
       "0    12/5/13 1:48             ...               \n",
       "1   10/1/12 13:51             ...               \n",
       "2  11/28/14 11:30             ...               \n",
       "3   6/11/09 22:39             ...               \n",
       "4   4/16/14 13:23             ...               \n",
       "\n",
       "                                        profileimage  retweet_count  \\\n",
       "0  https://pbs.twimg.com/profile_images/414342229...              0   \n",
       "1  https://pbs.twimg.com/profile_images/539604221...              0   \n",
       "2  https://pbs.twimg.com/profile_images/657330418...              1   \n",
       "3  https://pbs.twimg.com/profile_images/259703936...              0   \n",
       "4  https://pbs.twimg.com/profile_images/564094871...              0   \n",
       "\n",
       "  sidebar_color                                               text  \\\n",
       "0        FFFFFF  Robbie E Responds To Critics After Win Against...   \n",
       "1        C0DEED  ÛÏIt felt like they were my friends and I was...   \n",
       "2        C0DEED  i absolutely adore when louis starts the songs...   \n",
       "3        C0DEED  Hi @JordanSpieth - Looking at the url - do you...   \n",
       "4             0  Watching Neighbours on Sky+ catching up with t...   \n",
       "\n",
       "  tweet_coord tweet_count   tweet_created      tweet_id   tweet_location  \\\n",
       "0         NaN      110964  10/26/15 12:40  6.587300e+17  main; @Kan1shk3   \n",
       "1         NaN        7471  10/26/15 12:40  6.587300e+17              NaN   \n",
       "2         NaN        5617  10/26/15 12:40  6.587300e+17           clcncl   \n",
       "3         NaN        1693  10/26/15 12:40  6.587300e+17    Palo Alto, CA   \n",
       "4         NaN       31462  10/26/15 12:40  6.587300e+17              NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0                     Chennai  \n",
       "1  Eastern Time (US & Canada)  \n",
       "2                    Belgrade  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4                         NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('gender-classifier-DFE-791531.csv', encoding = \"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix_description = df[['text', 'description', 'gender']].values\n",
    "for i in range(matrix_description.shape[0]):\n",
    "    try:\n",
    "        matrix_description[i, 0] = clearstring(matrix_description[i, 0])\n",
    "    except:\n",
    "        matrix_description[i, 0] = ''\n",
    "    try:\n",
    "        matrix_description[i, 1] = clearstring(matrix_description[i, 1])\n",
    "    except:\n",
    "        matrix_description[i, 1] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "vocab = (' '.join(matrix_description[:, 0].tolist())).split()\n",
    "vocab += (' '.join(matrix_description[:, 1].tolist())).split()\n",
    "unique_vocab = list(set(vocab))\n",
    "with open('vocab.p', 'wb') as fopen:\n",
    "    pickle.dump(unique_vocab, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dictionary(words, vocabulary_size):\n",
    "    import collections\n",
    "    count = []\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        data.append(index)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dictionary, reverse_dictionary = build_dictionary(vocab, len(unique_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary.p', 'wb') as fopen:\n",
    "    pickle.dump(dictionary, fopen)\n",
    "with open('dict_reverse.p', 'wb') as fopen:\n",
    "    pickle.dump(reverse_dictionary, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brand', 'female', 'male', 'nan', 'unknown']\n"
     ]
    }
   ],
   "source": [
    "list_unique = np.unique(matrix_description[:, 2].astype('str')).tolist()\n",
    "del list_unique[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_unique:\n",
    "    lists = matrix_description[np.where(matrix_description[:, 2] == i)[0], :]\n",
    "    with open(i, 'w') as fopen:\n",
    "        for k in range(lists.shape[0] - 1):\n",
    "            fopen.write(lists[k, 0] + '\\n' + lists[k, 1] + '\\n')\n",
    "        fopen.write(lists[-1, 0] + '\\n' + lists[-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_unique:\n",
    "    with open(i, 'r') as fopen:\n",
    "        lists = fopen.read().split('\\n')\n",
    "    lists = list(filter(None, lists))\n",
    "    with open(i, 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = []\n",
    "    for k in range(len(lists)):\n",
    "        tokens = nlp(lists[k])\n",
    "        lists[k] = ' '.join([str(token) for token in tokens if token.pos_ == 'VERB' or token.pos_ == 'NOUN'])\n",
    "        lists_name_entity += [str(token) for token in tokens if token.ent_type_ == 'ORG' or token.ent_type_ == 'PERSON' or token.ent_type_ == 'GPE']\n",
    "    with open(i + '_pos', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists = list(set((' '.join(lists)).split()))\n",
    "    with open(i + '_token', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = list(set(lists_name_entity))\n",
    "    with open(i + '_entity', 'w') as fopen:\n",
    "        for k in range(len(lists_name_entity) - 1):\n",
    "            fopen.write(lists_name_entity[k] + '\\n')\n",
    "        fopen.write(lists_name_entity[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness', 'anger', 'fear', 'joy', 'surprise', 'love']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "list_emotion = os.listdir('/home/huseinzol05/Desktop/tree/emotion/data')\n",
    "print(list_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_emotion:\n",
    "    direct = '/home/huseinzol05/Desktop/tree/emotion/data/' + i + '/'\n",
    "    with open(direct + os.listdir(direct)[0] , 'r') as fopen:\n",
    "        lists = fopen.read().split('\\n')\n",
    "    lists = list(filter(None, lists))\n",
    "    with open(i, 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = []\n",
    "    for k in range(len(lists)):\n",
    "        tokens = nlp(lists[k])\n",
    "        lists[k] = ' '.join([str(token) for token in tokens if token.pos_ == 'VERB' or token.pos_ == 'NOUN'])\n",
    "        lists_name_entity += [str(token) for token in tokens if token.ent_type_ == 'ORG' or token.ent_type_ == 'PERSON' or token.ent_type_ == 'GPE']\n",
    "    with open(i + '_pos', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists = list(set((' '.join(lists)).split()))\n",
    "    with open(i + '_token', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = list(set(lists_name_entity))\n",
    "    with open(i + '_entity', 'w') as fopen:\n",
    "        for k in range(len(lists_name_entity) - 1):\n",
    "            fopen.write(lists_name_entity[k] + '\\n')\n",
    "        fopen.write(lists_name_entity[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in list_emotion:\n",
    "    direct = '/home/huseinzol05/Desktop/tree/emotion/data/' + i + '/'\n",
    "    with open(direct + os.listdir(direct)[0] , 'r') as fopen:\n",
    "        words += fopen.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set((' '.join(words)).split()))\n",
    "unique_vocab = list(set(vocab))\n",
    "with open('vocab_emotion.p', 'wb') as fopen:\n",
    "    pickle.dump(unique_vocab, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dictionary, reverse_dictionary = build_dictionary(vocab, len(unique_vocab))\n",
    "with open('dictionary_emotion.p', 'wb') as fopen:\n",
    "    pickle.dump(dictionary, fopen)\n",
    "with open('dict_reverse_emotion.p', 'wb') as fopen:\n",
    "    pickle.dump(reverse_dictionary, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "list_irony = os.listdir('/home/huseinzol05/Desktop/tree/irony/data')\n",
    "print(list_irony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in list_irony:\n",
    "    direct = '/home/huseinzol05/Desktop/tree/irony/data/' + i + '/'\n",
    "    with open(direct + os.listdir(direct)[0] , 'r') as fopen:\n",
    "        lists = fopen.read().split('\\n')\n",
    "    lists = list(filter(None, lists))\n",
    "    with open(i, 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            lists[k] = clearstring(lists[k])\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        lists[-1] = clearstring(lists[-1])\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = []\n",
    "    for k in range(len(lists)):\n",
    "        tokens = nlp(lists[k])\n",
    "        lists[k] = ' '.join([str(token) for token in tokens if token.pos_ == 'VERB' or token.pos_ == 'NOUN'])\n",
    "        lists_name_entity += [str(token) for token in tokens if token.ent_type_ == 'ORG' or token.ent_type_ == 'PERSON' or token.ent_type_ == 'GPE']\n",
    "    with open(i + '_pos', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists = list(set((' '.join(lists)).split()))\n",
    "    with open(i + '_token', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = list(set(lists_name_entity))\n",
    "    with open(i + '_entity', 'w') as fopen:\n",
    "        for k in range(len(lists_name_entity) - 1):\n",
    "            fopen.write(lists_name_entity[k] + '\\n')\n",
    "        fopen.write(lists_name_entity[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in list_irony:\n",
    "    direct = '/home/huseinzol05/Desktop/tree/irony/data/' + i + '/'\n",
    "    with open(direct + os.listdir(direct)[0] , 'r') as fopen:\n",
    "        words += fopen.read().split('\\n')\n",
    "        \n",
    "vocab = list(set((' '.join(words)).split()))\n",
    "unique_vocab = list(set(vocab))\n",
    "with open('vocab_irony.p', 'wb') as fopen:\n",
    "    pickle.dump(unique_vocab, fopen)\n",
    "_, dictionary, reverse_dictionary = build_dictionary(vocab, len(unique_vocab))\n",
    "with open('dictionary_irony.p', 'wb') as fopen:\n",
    "    pickle.dump(dictionary, fopen)\n",
    "with open('dict_reverse_irony.p', 'wb') as fopen:\n",
    "    pickle.dump(reverse_dictionary, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "list_polarity = os.listdir('/home/huseinzol05/Desktop/tree/polarity/data')\n",
    "print(list_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in list_polarity:\n",
    "    direct = '/home/huseinzol05/Desktop/tree/polarity/data/' + i + '/'\n",
    "    with open(direct + os.listdir(direct)[0] , 'r') as fopen:\n",
    "        lists = fopen.read().split('\\n')\n",
    "    lists = list(filter(None, lists))\n",
    "    with open(i, 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            lists[k] = clearstring(lists[k])\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        lists[-1] = clearstring(lists[-1])\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = []\n",
    "    for k in range(len(lists)):\n",
    "        tokens = nlp(lists[k])\n",
    "        lists[k] = ' '.join([str(token) for token in tokens if token.pos_ == 'VERB' or token.pos_ == 'NOUN'])\n",
    "        lists_name_entity += [str(token) for token in tokens if token.ent_type_ == 'ORG' or token.ent_type_ == 'PERSON' or token.ent_type_ == 'GPE']\n",
    "    with open(i + '_pos', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists = list(set((' '.join(lists)).split()))\n",
    "    with open(i + '_token', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = list(set(lists_name_entity))\n",
    "    with open(i + '_entity', 'w') as fopen:\n",
    "        for k in range(len(lists_name_entity) - 1):\n",
    "            fopen.write(lists_name_entity[k] + '\\n')\n",
    "        fopen.write(lists_name_entity[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in list_polarity:\n",
    "    direct = '/home/huseinzol05/Desktop/tree/polarity/data/' + i + '/'\n",
    "    with open(direct + os.listdir(direct)[0] , 'r') as fopen:\n",
    "        words += fopen.read().split('\\n')\n",
    "        \n",
    "vocab = list(set((' '.join(words)).split()))\n",
    "unique_vocab = list(set(vocab))\n",
    "with open('vocab_polarity.p', 'wb') as fopen:\n",
    "    pickle.dump(unique_vocab, fopen)\n",
    "_, dictionary, reverse_dictionary = build_dictionary(vocab, len(unique_vocab))\n",
    "with open('dictionary_polarity.p', 'wb') as fopen:\n",
    "    pickle.dump(dictionary, fopen)\n",
    "with open('dict_reverse_polarity.p', 'wb') as fopen:\n",
    "    pickle.dump(reverse_dictionary, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>audience</th>\n",
       "      <th>audience:confidence</th>\n",
       "      <th>bias</th>\n",
       "      <th>bias:confidence</th>\n",
       "      <th>message</th>\n",
       "      <th>...</th>\n",
       "      <th>orig__golden</th>\n",
       "      <th>audience_gold</th>\n",
       "      <th>bias_gold</th>\n",
       "      <th>bioid</th>\n",
       "      <th>embed</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>message_gold</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>766192484</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>1</td>\n",
       "      <td>8/4/15 21:17</td>\n",
       "      <td>national</td>\n",
       "      <td>1.0</td>\n",
       "      <td>partisan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>policy</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R000596</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>3.83249E+17</td>\n",
       "      <td>From: Trey Radel (Representative from Florida)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>766192485</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>1</td>\n",
       "      <td>8/4/15 21:20</td>\n",
       "      <td>national</td>\n",
       "      <td>1.0</td>\n",
       "      <td>partisan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>attack</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M000355</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>3.11208E+17</td>\n",
       "      <td>From: Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter</td>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>766192486</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>1</td>\n",
       "      <td>8/4/15 21:14</td>\n",
       "      <td>national</td>\n",
       "      <td>1.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>support</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S001180</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>3.39069E+17</td>\n",
       "      <td>From: Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter</td>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>766192487</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>1</td>\n",
       "      <td>8/4/15 21:08</td>\n",
       "      <td>national</td>\n",
       "      <td>1.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>policy</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C000880</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>2.98528E+17</td>\n",
       "      <td>From: Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>766192488</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>1</td>\n",
       "      <td>8/4/15 21:26</td>\n",
       "      <td>national</td>\n",
       "      <td>1.0</td>\n",
       "      <td>partisan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>policy</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U000038</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>4.07643E+17</td>\n",
       "      <td>From: Mark Udall (Senator from Colorado)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter</td>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  766192484    False   finalized                   1      8/4/15 21:17   \n",
       "1  766192485    False   finalized                   1      8/4/15 21:20   \n",
       "2  766192486    False   finalized                   1      8/4/15 21:14   \n",
       "3  766192487    False   finalized                   1      8/4/15 21:08   \n",
       "4  766192488    False   finalized                   1      8/4/15 21:26   \n",
       "\n",
       "   audience  audience:confidence      bias  bias:confidence  message  \\\n",
       "0  national                  1.0  partisan              1.0   policy   \n",
       "1  national                  1.0  partisan              1.0   attack   \n",
       "2  national                  1.0   neutral              1.0  support   \n",
       "3  national                  1.0   neutral              1.0   policy   \n",
       "4  national                  1.0  partisan              1.0   policy   \n",
       "\n",
       "                         ...                          orig__golden  \\\n",
       "0                        ...                                   NaN   \n",
       "1                        ...                                   NaN   \n",
       "2                        ...                                   NaN   \n",
       "3                        ...                                   NaN   \n",
       "4                        ...                                   NaN   \n",
       "\n",
       "   audience_gold  bias_gold    bioid  \\\n",
       "0            NaN        NaN  R000596   \n",
       "1            NaN        NaN  M000355   \n",
       "2            NaN        NaN  S001180   \n",
       "3            NaN        NaN  C000880   \n",
       "4            NaN        NaN  U000038   \n",
       "\n",
       "                                               embed           id  \\\n",
       "0  <blockquote class=\"twitter-tweet\" width=\"450\">...  3.83249E+17   \n",
       "1  <blockquote class=\"twitter-tweet\" width=\"450\">...  3.11208E+17   \n",
       "2  <blockquote class=\"twitter-tweet\" width=\"450\">...  3.39069E+17   \n",
       "3  <blockquote class=\"twitter-tweet\" width=\"450\">...  2.98528E+17   \n",
       "4  <blockquote class=\"twitter-tweet\" width=\"450\">...  4.07643E+17   \n",
       "\n",
       "                                              label message_gold   source  \\\n",
       "0    From: Trey Radel (Representative from Florida)          NaN  twitter   \n",
       "1     From: Mitch McConnell (Senator from Kentucky)          NaN  twitter   \n",
       "2  From: Kurt Schrader (Representative from Oregon)          NaN  twitter   \n",
       "3          From: Michael Crapo (Senator from Idaho)          NaN  twitter   \n",
       "4          From: Mark Udall (Senator from Colorado)          NaN  twitter   \n",
       "\n",
       "                                                text  \n",
       "0  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...  \n",
       "1  VIDEO - #Obamacare:  Full of Higher Costs and ...  \n",
       "2  Please join me today in remembering our fallen...  \n",
       "3  RT @SenatorLeahy: 1st step toward Senate debat...  \n",
       "4  .@amazon delivery #drones show need to update ...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/huseinzol05/Desktop/tree/politics/Political-media-DFE.csv', encoding = \"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix_description = df[['text', 'audience', 'bias', 'message']].values\n",
    "for i in range(matrix_description.shape[0]):\n",
    "    try:\n",
    "        matrix_description[i, 0] = clearstring(matrix_description[i, 0])\n",
    "    except:\n",
    "        matrix_description[i, 0] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['constituency', 'national']\n",
      "['neutral', 'partisan']\n",
      "['attack', 'constituency', 'information', 'media', 'mobilization', 'other', 'personal', 'policy', 'support']\n"
     ]
    }
   ],
   "source": [
    "unique_audience = np.unique(matrix_description[:, 1].astype('str')).tolist()\n",
    "unique_bias = np.unique(matrix_description[:, 2].astype('str')).tolist()\n",
    "unique_message = np.unique(matrix_description[:, 3].astype('str')).tolist()\n",
    "print(unique_audience)\n",
    "print(unique_bias)\n",
    "print(unique_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in unique_audience:\n",
    "    lists = matrix_description[np.where(matrix_description[:, 1] == i)[0], 0].tolist()\n",
    "    lists = list(filter(None, lists))\n",
    "    with open(i, 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = []\n",
    "    for k in range(len(lists)):\n",
    "        tokens = nlp(lists[k])\n",
    "        lists[k] = ' '.join([str(token) for token in tokens if token.pos_ == 'VERB' or token.pos_ == 'NOUN'])\n",
    "        lists_name_entity += [str(token) for token in tokens if token.ent_type_ == 'ORG' or token.ent_type_ == 'PERSON' or token.ent_type_ == 'GPE']\n",
    "    with open(i + '_pos', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists = list(set((' '.join(lists)).split()))\n",
    "    with open(i + '_token', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = list(set(lists_name_entity))\n",
    "    with open(i + '_entity', 'w') as fopen:\n",
    "        for k in range(len(lists_name_entity) - 1):\n",
    "            fopen.write(lists_name_entity[k] + '\\n')\n",
    "        fopen.write(lists_name_entity[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in unique_audience:\n",
    "    direct = '/home/huseinzol05/Desktop/tree/audience/data/' + i + '/'\n",
    "    with open(direct + os.listdir(direct)[0] , 'r') as fopen:\n",
    "        words += fopen.read().split('\\n')\n",
    "        \n",
    "vocab = list(set((' '.join(words)).split()))\n",
    "unique_vocab = list(set(vocab))\n",
    "with open('vocab_audience.p', 'wb') as fopen:\n",
    "    pickle.dump(unique_vocab, fopen)\n",
    "_, dictionary, reverse_dictionary = build_dictionary(vocab, len(unique_vocab))\n",
    "with open('dictionary_audience.p', 'wb') as fopen:\n",
    "    pickle.dump(dictionary, fopen)\n",
    "with open('dict_reverse_audience.p', 'wb') as fopen:\n",
    "    pickle.dump(reverse_dictionary, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in unique_bias:\n",
    "    lists = matrix_description[np.where(matrix_description[:, 2] == i)[0], 0].tolist()\n",
    "    lists = list(filter(None, lists))\n",
    "    with open(i, 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = []\n",
    "    for k in range(len(lists)):\n",
    "        tokens = nlp(lists[k])\n",
    "        lists[k] = ' '.join([str(token) for token in tokens if token.pos_ == 'VERB' or token.pos_ == 'NOUN'])\n",
    "        lists_name_entity += [str(token) for token in tokens if token.ent_type_ == 'ORG' or token.ent_type_ == 'PERSON' or token.ent_type_ == 'GPE']\n",
    "    with open(i + '_pos', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists = list(set((' '.join(lists)).split()))\n",
    "    with open(i + '_token', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = list(set(lists_name_entity))\n",
    "    with open(i + '_entity', 'w') as fopen:\n",
    "        for k in range(len(lists_name_entity) - 1):\n",
    "            fopen.write(lists_name_entity[k] + '\\n')\n",
    "        fopen.write(lists_name_entity[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in unique_bias:\n",
    "    direct = '/home/huseinzol05/Desktop/tree/bias/data/' + i + '/'\n",
    "    with open(direct + os.listdir(direct)[0] , 'r') as fopen:\n",
    "        words += fopen.read().split('\\n')\n",
    "        \n",
    "vocab = list(set((' '.join(words)).split()))\n",
    "unique_vocab = list(set(vocab))\n",
    "with open('vocab_bias.p', 'wb') as fopen:\n",
    "    pickle.dump(unique_vocab, fopen)\n",
    "_, dictionary, reverse_dictionary = build_dictionary(vocab, len(unique_vocab))\n",
    "with open('dictionary_bias.p', 'wb') as fopen:\n",
    "    pickle.dump(dictionary, fopen)\n",
    "with open('dict_reverse_bias.p', 'wb') as fopen:\n",
    "    pickle.dump(reverse_dictionary, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in unique_message:\n",
    "    lists = matrix_description[np.where(matrix_description[:, 3] == i)[0], 0].tolist()\n",
    "    lists = list(filter(None, lists))\n",
    "    with open(i, 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = []\n",
    "    for k in range(len(lists)):\n",
    "        tokens = nlp(lists[k])\n",
    "        lists[k] = ' '.join([str(token) for token in tokens if token.pos_ == 'VERB' or token.pos_ == 'NOUN'])\n",
    "        lists_name_entity += [str(token) for token in tokens if token.ent_type_ == 'ORG' or token.ent_type_ == 'PERSON' or token.ent_type_ == 'GPE']\n",
    "    with open(i + '_pos', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists = list(set((' '.join(lists)).split()))\n",
    "    with open(i + '_token', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = list(set(lists_name_entity))\n",
    "    with open(i + '_entity', 'w') as fopen:\n",
    "        for k in range(len(lists_name_entity) - 1):\n",
    "            fopen.write(lists_name_entity[k] + '\\n')\n",
    "        fopen.write(lists_name_entity[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in unique_message:\n",
    "    direct = '/home/huseinzol05/Desktop/tree/message/data/' + i + '/'\n",
    "    with open(direct + os.listdir(direct)[0] , 'r') as fopen:\n",
    "        words += fopen.read().split('\\n')\n",
    "        \n",
    "vocab = list(set((' '.join(words)).split()))\n",
    "unique_vocab = list(set(vocab))\n",
    "with open('vocab_message.p', 'wb') as fopen:\n",
    "    pickle.dump(unique_vocab, fopen)\n",
    "_, dictionary, reverse_dictionary = build_dictionary(vocab, len(unique_vocab))\n",
    "with open('dictionary_message.p', 'wb') as fopen:\n",
    "    pickle.dump(dictionary, fopen)\n",
    "with open('dict_reverse_message.p', 'wb') as fopen:\n",
    "    pickle.dump(reverse_dictionary, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "list_sentiment = os.listdir('/home/huseinzol05/Desktop/tree/sentiment/data')\n",
    "print(list_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in list_sentiment:\n",
    "    direct = '/home/huseinzol05/Desktop/tree/sentiment/data/' + i + '/'\n",
    "    with open(direct + os.listdir(direct)[0] , 'r') as fopen:\n",
    "        lists = fopen.read().split('\\n')\n",
    "    lists = list(filter(None, lists))\n",
    "    with open(i, 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            lists[k] = clearstring(lists[k])\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        lists[-1] = clearstring(lists[-1])\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = []\n",
    "    for k in range(len(lists)):\n",
    "        tokens = nlp(lists[k])\n",
    "        lists[k] = ' '.join([str(token) for token in tokens if token.pos_ == 'VERB' or token.pos_ == 'NOUN'])\n",
    "        lists_name_entity += [str(token) for token in tokens if token.ent_type_ == 'ORG' or token.ent_type_ == 'PERSON' or token.ent_type_ == 'GPE']\n",
    "    with open(i + '_pos', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists = list(set((' '.join(lists)).split()))\n",
    "    with open(i + '_token', 'w') as fopen:\n",
    "        for k in range(len(lists) - 1):\n",
    "            fopen.write(lists[k] + '\\n')\n",
    "        fopen.write(lists[-1])\n",
    "    lists_name_entity = list(set(lists_name_entity))\n",
    "    with open(i + '_entity', 'w') as fopen:\n",
    "        for k in range(len(lists_name_entity) - 1):\n",
    "            fopen.write(lists_name_entity[k] + '\\n')\n",
    "        fopen.write(lists_name_entity[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in list_sentiment:\n",
    "    direct = '/home/huseinzol05/Desktop/tree/sentiment/data/' + i + '/'\n",
    "    with open(direct + os.listdir(direct)[0] , 'r') as fopen:\n",
    "        words += fopen.read().split('\\n')\n",
    "        \n",
    "vocab = list(set((' '.join(words)).split()))\n",
    "unique_vocab = list(set(vocab))\n",
    "with open('vocab_sentiment.p', 'wb') as fopen:\n",
    "    pickle.dump(unique_vocab, fopen)\n",
    "_, dictionary, reverse_dictionary = build_dictionary(vocab, len(unique_vocab))\n",
    "with open('dictionary_sentiment.p', 'wb') as fopen:\n",
    "    pickle.dump(dictionary, fopen)\n",
    "with open('dict_reverse_sentiment.p', 'wb') as fopen:\n",
    "    pickle.dump(reverse_dictionary, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
